---
title: "Automatically cropping the background of a horse body photo with Pytorch's Pre-trained model"
author: admin
date: 2020-08-12T00:00:00Z
categories: ["horse racing"]
tags: ["Python","machine_learning","preprocessing"]
draft: true
featured: false
slug: ["Pytorch"]
image:
  caption: ''
  focal_point: ""
  placement: 2
  preview_only: false
lastmod: ""
projects: []
summary: I cropped it because the background of the stables was in the way of the horse model I made in the previous post.
output: 
  blogdown::html_page:
    number_sections: false
    toc: true
---

<script src="index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#downloading-the-pre-trained-model">1. Downloading the pre-trained model</a></li>
<li><a href="#executing-the-background-deletion-process">2. Executing the Background Deletion Process</a></li>
<li><a href="#analysis-using-cnn">3. Analysis using CNN</a></li>
<li><a href="#interpretation-of-results-using-shap-values">4. Interpretation of results using Shap values</a></li>
<li><a href="#summary">5.Summary</a></li>
</ul>
</div>

<p>Hi. In the last post, I built a model to predict the order of a racehorse using CNN based on its body image. The results were not so good, and we needed to refine our analysis, especially when we analyzed the factors using <code>shap</code> values, we found that the horses responded more to the stables in the background than to their bodies. This time, I would like to re-analyze the background from the horse’s body photo using <code>Pytorch</code> pre-trained model, and analyze the photo with only the horse’s body.</p>
<div id="downloading-the-pre-trained-model" class="section level2">
<h2>1. Downloading the pre-trained model</h2>
<p>The code is adapted from <a href="https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/">here</a>. First, install the packages.</p>
<pre class="python"><code>import numpy as np
import cv2
import matplotlib.pyplot as plt
import torch
import torchvision
from torchvision import transforms
import glob
from PIL import Image
import PIL
import os</code></pre>
<p>Next, install the pre-trained model.</p>
<pre class="python"><code>#Install a pre-trained model
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)
model = model.to(device)
model.eval()</code></pre>
<p>Apparently, all pre-trained models assume a mini-batch of 3-channel RGB images of shape <span class="math inline">\((N, 3, H, W)\)</span> normalized in the same way. Here, <span class="math inline">\(N\)</span> is assumed to be the number of images and <span class="math inline">\(H\)</span> and <span class="math inline">\(W\)</span> are assumed to be at least 224 pixels. The images need to be scaled to a range of [0, 1] and then normalized using the mean value = [0.485, 0.456, 0.406] and the standard value = [0.229, 0.224, 0.225]. So, we define a function that does the preprocessing.</p>
<pre class="python"><code>#preprocessing
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])</code></pre>
</div>
<div id="executing-the-background-deletion-process" class="section level2">
<h2>2. Executing the Background Deletion Process</h2>
<p>Now, let’s load the images collected by the <code>selenium</code> code in the previous post and remove the background one by one.</p>
<pre class="python"><code>#Specify a folder
folders = os.listdir(r&quot;C:\Users\aashi\umanalytics\photo\image&quot;)

#Reads an image from each folder and converts it to a numpy array of RGB values using the Image function
for i, folder in enumerate(folders):
  files = glob.glob(&quot;C:/Users/aashi/umanalytics/photo/image/&quot; + folder + &quot;/*.jpg&quot;)
  index = i
  for k, file in enumerate(files):
    img_array = np.fromfile(file, dtype=np.uint8)
    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
    h,w,_ = img.shape
    input_tensor = preprocess(img)
    input_batch = input_tensor.unsqueeze(0).to(device)

    with torch.no_grad():
      output = model(input_batch)[&#39;out&#39;][0]
    output_predictions = output.argmax(0)
    mask_array = output_predictions.byte().cpu().numpy()
    Image.fromarray(mask_array*255).save(r&#39;C:\Users\aashi\umanalytics\photo\image\mask.jpg&#39;)
    mask = cv2.imread(r&#39;C:\Users\aashi\umanalytics\photo\image\mask.jpg&#39;)
    bg = np.full_like(img,255)
    img = cv2.multiply(img.astype(float), mask.astype(float)/255)
    bg = cv2.multiply(bg.astype(float), 1.0 - mask.astype(float)/255)
    outImage = cv2.add(img, bg)
    Image.fromarray(outImage.astype(np.uint8)).convert(&#39;L&#39;).save(file)</code></pre>
<p>The following <code>mask</code> image is output using the pre-trained model, and the <code>numpy</code> array and the <code>mask</code> image are merged to create a background delete image. The output looks like the following.</p>
<pre class="python"><code>plt.gray()
plt.figure(figsize=(20,20))
plt.subplot(1,3,1)
plt.imshow(img)
plt.subplot(1,3,2)
plt.imshow(mask)
plt.subplot(1,3,3)
plt.imshow(outImage)
plt.show()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-8-1.png" width="1920" /></p>
<pre class="python"><code>plt.close()</code></pre>
<p>Here’s what the folders look like. Some of them are handled well and some of them show the trainer. I know there is a way to identify the objects and <code>mask</code> only the horses, but this model doesn’t allow for object labeling, so we’ll continue with that.</p>
<div class="figure">
<img src="maskhorse.PNG" alt="" />
<p class="caption">folder</p>
</div>
</div>
<div id="analysis-using-cnn" class="section level2">
<h2>3. Analysis using CNN</h2>
<p>Here is the same content as in the previous article. I will only post the results.</p>
<pre><code>## Test accuracy: 0.6779661016949152</code></pre>
<pre><code>## &lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x0000000030D91888&gt;</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>The results have been disastrous.
I have not been able to identify it at all. Aren’t there any characteristics in the horse photos that would predict the rankings? Or is it that there is no variation in the photos of the horses in G1 races and it is impossible to identify them? Either way, it seems a bit harsh.</p>
</div>
<div id="interpretation-of-results-using-shap-values" class="section level2">
<h2>4. Interpretation of results using Shap values</h2>
<p>As before, let’s see how it fails using the <code>shap</code> value. We will use this image as an example.</p>
<pre class="python"><code>plt.imshow(X_test[4])
plt.show()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<pre class="python"><code>plt.close()</code></pre>
<pre class="python"><code>import shap
background = X_resampled[np.random.choice(X_resampled.shape[0],100,replace=False)]

e = shap.GradientExplainer(model,background)

shap_values = e.shap_values(X_test[[4]])
shap.image_plot(shap_values[1],X_test[[4]])</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-12-1.png" width="576" /></p>
<p>They seem to be evaluating from the paws to the face. Surprisingly, they do not seem to be evaluating the buttocks.
I would like to try to visualize which aspect of the image is captured in each layer.</p>
<pre class="python"><code>from keras import models

layer_outputs = [layer.output for layer in model.layers[:8]]
layer_names = []
for layer in model.layers[:8]:
    layer_names.append(layer.name)
images_per_row = 16

activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
activations = activation_model.predict(X_train[[0]])

for layer_name, layer_activation in zip(layer_names, activations):
    n_features = layer_activation.shape[-1]

    size = layer_activation.shape[1]

    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))

    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0,
                                             :, :,
                                             col * images_per_row + row]
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype(&#39;uint8&#39;)
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, cmap=&#39;viridis&#39;)
    plt.show()
    plt.close()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-13-1.png" width="1536" /><img src="index_files/figure-html/unnamed-chunk-13-2.png" width="1536" /><img src="index_files/figure-html/unnamed-chunk-13-3.png" width="1536" /><img src="index_files/figure-html/unnamed-chunk-13-4.png" width="1536" /><img src="index_files/figure-html/unnamed-chunk-13-5.png" width="1536" /><img src="index_files/figure-html/unnamed-chunk-13-6.png" width="1536" /><img src="index_files/figure-html/unnamed-chunk-13-7.png" width="1536" /><img src="index_files/figure-html/unnamed-chunk-13-8.png" width="1536" /><img src="index_files/figure-html/unnamed-chunk-13-9.png" width="576" /></p>
<p>I still can’t understand this one.</p>
</div>
<div id="summary" class="section level2">
<h2>5.Summary</h2>
<p>I removed the stable background and rerun it, but the results were the same - it was a good experience using PyTorch and removing the background, but not with any results, so I’ll stop with the horse photos for now.</p>
</div>
