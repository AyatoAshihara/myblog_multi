---
title: "日次GDP推計に使用する経済統計を統計ダッシュボードから集めてみた"
author: admin
date: 2018-07-14T00:00:00Z
categories: ["マクロ経済学","Webスクレイピング"]
tags: ["R","API"]
draft: false
featured: false
image:
  caption: ''
  focal_point: ""
  placement: 2
  preview_only: false
lastmod: ""
projects: []
summary: Rでデータ集めをします。データ分析はデータ集めと前処理が7割を占めるといわれる中、データ集めを自動化すべくウェブスクレイピングを行いました。
output: 
  blogdown::html_page:
    number_sections: false
    toc: true
    df_print: "paged"
---

<script src="index_files/header-attrs/header-attrs.js"></script>
<link href="index_files/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="index_files/pagedtable/js/pagedtable.js"></script>

<div id="TOC">
<ul>
<li><a href="#先行研究と具体的にやりたいこと">1. 先行研究と具体的にやりたいこと</a></li>
<li><a href="#統計ダッシュボードからのデータの収集">2. 統計ダッシュボードからのデータの収集</a></li>
<li><a href="#得られたデータを主成分分析にかけてみる">3. 得られたデータを主成分分析にかけてみる</a></li>
</ul>
</div>

<p>おはこんばんちわ。</p>
<p>最近、競馬ばっかりやってましたが、そろそろ本業のマクロの方もやらないとなということで今回は日次GDP推計に使用するデータを総務省が公開している統計ダッシュボードから取ってきました。
そもそも、前の記事では四半期GDP速報の精度が低いことをモチベーションに高頻度データを用いてより精度の高い予測値をはじき出すモデルを作れないかというテーマで研究を進めていました。しかし、先行研究を進めていくうちに、どうやら大規模な経済指標を利用することで日次で四半期GDPの予測値を計算することが可能であることが判明しました。しかも、精度も良い(米国ですが)ということで、なんとかこの方向で研究を進めていけないかということになりました。</p>
<div id="先行研究と具体的にやりたいこと" class="section level2">
<h2>1. 先行研究と具体的にやりたいこと</h2>
<p><a href="http://dept.ku.edu/~empirics/Courses/Econ844/papers/Nowcasting%20GDP.pdf">先行研究</a></p>
<p>Giannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標をいくつかのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しました。なお、ファクターの計算にはカルマンスムージングを用いています(詳しい推計方法は論文&amp;Technical Appendixを参照)。理論的な定式化は無いのですが、なかなか当たります。そもそも私がこの研究に興味を持ったのは、以下の本を立ち読みした際に参考文献として出てきたからで、いよいよ運用機関などでも使用され始めるのかと思い、やっておこうと思った次第です。</p>
<p><a href="http://www.amazon.co.jp/exec/obidos/ASIN/4532134811/hatena-blog-22/">実践 金融データサイエンス 隠れた構造をあぶり出す6つのアプローチ</a>
<img src="https://images-na.ssl-images-amazon.com/images/I/51KIE5GeV+L._SX350_BO1,204,203,200_.jpg" /></p>
<p>とりあえずはGiannoneの日本版をやろうかなと思っています。実はこの後に、ファクターモデルとDSGEを組み合わせたモデルがありましてそこまで発展させたいなーなんて思っておりますが。とにかく、ファクターを計算するための経済統計が必要ですので、今回はそれを集めてきたというのがこの記事の趣旨です。</p>
</div>
<div id="統計ダッシュボードからのデータの収集" class="section level2">
<h2>2. 統計ダッシュボードからのデータの収集</h2>
<p>政府や日銀が公表しているデータの一部は統計ダッシュボードから落とすことができます。これは総務省統計局が提供しているもので、これまで利用しにくかった経済統計をより身近に使用してもらおうというのが一応のコンセプトとなっています。似たものに総務省統計局が提供しているestatがありますが、日銀の公表データがなかったり、メールアドレスの登録が必要だったりと非常に使い勝手が悪いです(個人的感想)。ただ、estatにはestatapiというRパッケージがあり、データを整形するのは比較的容易であると言えます。今回、統計ダッシュボードを選択した理由はそうは言っても日銀のデータがないのはダメだろうという理由で、データの整形に関しては関数を組みました。
そもそも統計ダッシュボードは経済統計をグラフなどで見て楽しむ？ものですが、私のような研究をしたい者を対象にAPIを提供してくれています。取得できるデータは大きく分けて6つあります。</p>
<p><img src="20180714160029.png" /></p>
<p>やり方は簡単で、ベースのurlと欲しい統計のIDをGET関数で渡すことによって、データを取得することができます。公式にも以下のように書かれています。</p>
<blockquote>
<p>基本的な使い方としては、まず①「統計メタ情報（系列）取得」で取得したいデータの[系列コード]を検索し、 その後⑥「統計データ取得」で[系列コード]を検索条件に指定し、その系列の情報を取得します。
（②③④⑤は補助的な情報として独立して取得できるようにしています。データのみ必要な場合は当該機能は不要です。）
具体的な使い方は、以下の「WebAPIの詳細仕様」に記載する[ベースURL]に検索条件となる[パラメータ]を“&amp;”で連結し、HTTPリクエスト（GET）を送信することで目的のデータを取得できます。
各パラメータは「パラメータ名=値」のように名称と値を’=‘で結合し、複数のパラメータを指定する場合は「パラメータ名=値&amp;パラメータ名=値&amp;…」のようにそれぞれのパラメータ指定を’&amp;’で結合してください。
また、パラメータ値は必ずURLエンコード(文字コードUTF-8)してから結合してください。</p>
</blockquote>
<p>今回も以下の文献を参考にデータを取ってきたいと思います。</p>
<p><a href="https://www.amazon.co.jp/dp/486354216X">Rによるスクレイピング入門</a>
<img src="https://images-na.ssl-images-amazon.com/images/I/51ZBnu8oSvL._SX350_BO1,204,203,200_.jpg" /></p>
<p>まず、最初にこのAPIからデータを取得し、得られた結果を分析しやすいように整形する関数を定義したいと思います。</p>
<pre class="r"><code>library(httr)
library(estatapi)
library(dplyr)
library(XML)
library(stringr)
library(xts)
library(GGally)
library(ggplot2)
library(seasonal)
library(dlm)
library(vars)
library(MASS)

# 関数を定義
get_dashboard &lt;- function(ID){
  base_url &lt;- &quot;https://dashboard.e-stat.go.jp/api/1.0/JsonStat/getData?&quot;
  res &lt;- GET(
    url = base_url,
    query = list(
      IndicatorCode=ID
    )
  )
  result &lt;- content(res)
  x &lt;- result$link$item[[1]]$value
  x &lt;- t(do.call(&quot;data.frame&quot;,x))
  date_x &lt;- result$link$item[[1]]$dimension$Time$category$label
  date_x &lt;- t(do.call(&quot;data.frame&quot;,date_x))
  date_x &lt;- str_replace_all(date_x, pattern=&quot;年&quot;, replacement=&quot;/&quot;)
  date_x &lt;- str_replace_all(date_x, pattern=&quot;月&quot;, replacement=&quot;&quot;)
  date_x &lt;- as.Date(gsub(&quot;([0-9]+)/([0-9]+)&quot;, &quot;\\1/\\2/1&quot;, date_x))
  date_x &lt;- as.Date(date_x, format = &quot;%m/%d/%Y&quot;)
  date_x &lt;- as.numeric(date_x)
  date_x &lt;- as.Date(date_x, origin=&quot;1970-01-01&quot;)
  #x &lt;- cbind(x,date_x)
  x &lt;- data.frame(x)
  x[,1] &lt;- as.character(x[,1])%&gt;%as.numeric(x[,1])
  colnames(x) &lt;- c(result$link$item[[1]]$label)
  x &lt;- x %&gt;% mutate(&quot;publication&quot; = date_x)
  return(x)
}</code></pre>
<p>まずベースのurlを定義しています。今回はデータが欲しいので⑥統計データのベースurlを使用します（
<a href="https://dashboard.e-stat.go.jp/static/api">参考</a>）。次にベースurlと統計ID（IndicatorCode）を<code>GET</code>関数で渡し、結果を取得しています。統計IDについてはエクセルファイルで公開されています。得られた結果の中身（リスト形式）をresultに格納し、リストの深層にある原数値データ（value）をxに格納します。原数値データもリスト形式なので、それを<code>do.call</code>でデータフレームに変換しています。次に、データ日付を取得します。resultの中を深くたどるとTime→category→labelというデータがあり、そこに日付データが保存されているので、それをdate_xに格納し、同じようにデータフレームへ変換します。データの仕様上、日付は「yyyy年mm月」になっていますが、これだと<code>R</code>は日付データとして読み取ってくれないので、<code>str_replace_all</code>等で変換したのち、<code>Date</code>型に変換しています。列名にデータ名（result→link→item[[1]]→label）をつけ、データ日付をxに追加したら完成です。
そのほか、<code>data_connect</code>という関数も定義しています。これはデータ系列によれば、たとえば推計方法の変更などで1980年～2005年の系列と2003年～2018年までの系列の2系列があるようなデータも存在し、この2系列を接続するための関数です。これは単純に接続しているだけなので、説明は省略します。</p>
<pre class="r"><code>data_connect &lt;- function(x){
  a &lt;- min(which(x[,ncol(x)] != &quot;NA&quot;))
  b &lt;- x[a,ncol(x)]/x[a,1]
  c &lt;- x[1:a-1,1]*b
  return(c)
}</code></pre>
<p>では、実際にデータを取得していきます。今回取得するデータは月次データとなっています。これは統計dashboardが月次以下のデータがとれないからです。なので、例えば日経平均などは月末の終値を引っ張っています。ただし、GDPは四半期データとなっています。さきほど定義したget_dashboardの使用方法は簡単で、引数に統計ダッシュボードで公開されている統計IDを入力するだけでデータが取れます。今回使用するデータを以下の表にまとめました。</p>
<p><img src="20180714212330.png" /></p>
<pre class="r"><code># データを取得
Nikkei &lt;- get_dashboard(&quot;0702020501000010010&quot;)
callrate &lt;- get_dashboard(&quot;0702020300000010010&quot;)
TOPIX &lt;- get_dashboard(&quot;0702020590000090010&quot;)
kikai &lt;- get_dashboard(&quot;0701030000000010010&quot;)
kigyo.bukka &lt;- get_dashboard(&quot;0703040300000090010&quot;)
money.stock1 &lt;- get_dashboard(&quot;0702010201000010030&quot;)
money.stock2 &lt;- get_dashboard(&quot;0702010202000010030&quot;)
money.stock &lt;- dplyr::full_join(money.stock1,money.stock2,by=&quot;publication&quot;)
c &lt;- data_connect(money.stock)
a &lt;- min(which(money.stock[,ncol(money.stock)] != &quot;NA&quot;))
money.stock[1:a-1,ncol(money.stock)] &lt;- c
money.stock &lt;- money.stock[,c(2,3)]
cpi &lt;- get_dashboard(&quot;0703010401010090010&quot;)
export.price &lt;- get_dashboard(&quot;0703050301000090010&quot;)
import.price &lt;- get_dashboard(&quot;0703060301000090010&quot;)
import.price$`輸出物価指数（総平均）（円ベース）2015年基準` &lt;- NULL
public.expenditure1 &lt;- get_dashboard(&quot;0802020200000010010&quot;)
public.expenditure2 &lt;- get_dashboard(&quot;0802020201000010010&quot;)
public.expenditure &lt;- dplyr::full_join(public.expenditure1,public.expenditure2,by=&quot;publication&quot;)
c &lt;- data_connect(public.expenditure)
a &lt;- min(which(public.expenditure[,ncol(public.expenditure)] != &quot;NA&quot;))
public.expenditure[1:a-1,ncol(public.expenditure)] &lt;- c
public.expenditure &lt;- public.expenditure[,c(2,3)]
export.service &lt;- get_dashboard(&quot;1601010101000010010&quot;)
working.population &lt;- get_dashboard(&quot;0201010010000010020&quot;)
yukoukyuujinn &lt;- get_dashboard(&quot;0301020001000010010&quot;)
hours_worked &lt;- get_dashboard(&quot;0302010000000010000&quot;)
nominal.wage &lt;- get_dashboard(&quot;0302020000000010000&quot;) 
iip &lt;- get_dashboard(&quot;0502070101000090010&quot;)
shukka.shisu &lt;- get_dashboard(&quot;0502070102000090010&quot;)
zaiko.shisu &lt;- get_dashboard(&quot;0502070103000090010&quot;)
sanji.sangyo &lt;- get_dashboard(&quot;0603100100000090010&quot;)
retail.sells &lt;- get_dashboard(&quot;0601010201010010000&quot;)
GDP1 &lt;- get_dashboard(&quot;0705020101000010000&quot;)
GDP2 &lt;- get_dashboard(&quot;0705020301000010000&quot;)
GDP &lt;- dplyr::full_join(GDP1,GDP2,by=&quot;publication&quot;)
c &lt;- data_connect(GDP)
a &lt;- min(which(GDP[,ncol(GDP)] != &quot;NA&quot;))
GDP[1:a-1,ncol(GDP)] &lt;- c
GDP &lt;- GDP[,c(2,3)]
yen &lt;- get_dashboard(&quot;0702020401000010010&quot;)
household.consumption &lt;- get_dashboard(&quot;0704010101000010001&quot;)
JGB10y &lt;- get_dashboard(&quot;0702020300000010020&quot;)</code></pre>
<p>今取得したデータは原数値系列のデータが多いので、それらは季節調整をかけます。なぜ季節調整済みのデータを取得しないのかというとそれらのデータは何故か極端にサンプル期間が短くなってしまうからです。ここらへんは使い勝手が悪いです。</p>
<pre class="r"><code># 季節調整をかける
Sys.setenv(X13_PATH = &quot;C:\\Program Files\\WinX13\\x13as&quot;)
checkX13()
seasoning &lt;- function(data,i,start.y,start.m){
  timeseries &lt;- ts(data[,i],frequency = 12,start=c(start.y,start.m))
  m &lt;- seas(timeseries)
  summary(m$data)
  return(m$series$s11)
}
k &lt;- seasoning(kikai,1,2005,4)
kikai$`機械受注額（船舶・電力を除く民需）` &lt;- as.numeric(k)
k &lt;- seasoning(kigyo.bukka,1,1960,1)
kigyo.bukka$`国内企業物価指数（総平均）2015年基準` &lt;- as.numeric(k)
k &lt;- seasoning(cpi,1,1970,1)
cpi$`消費者物価指数（生鮮食品を除く総合）2015年基準` &lt;- as.numeric(k)
k &lt;- seasoning(export.price,1,1960,1)
export.price$`輸出物価指数（総平均）（円ベース）2015年基準` &lt;- as.numeric(k)
k &lt;- seasoning(import.price,1,1960,1)
import.price$`輸入物価指数（総平均）（円ベース）2015年基準` &lt;- as.numeric(k)
k &lt;- seasoning(public.expenditure,2,2004,4)
public.expenditure$公共工事受注額 &lt;- as.numeric(k)
k &lt;- seasoning(export.service,1,1996,1)
export.service$`貿易・サービス収支` &lt;- as.numeric(k)
k &lt;- seasoning(yukoukyuujinn,1,1963,1)
yukoukyuujinn$有効求人倍率 &lt;- as.numeric(k)
k &lt;- seasoning(hours_worked,1,1990,1)
hours_worked$総実労働時間 &lt;- as.numeric(k)
k &lt;- seasoning(nominal.wage,1,1990,1)
nominal.wage$現金給与総額 &lt;- as.numeric(k)
k &lt;- seasoning(iip,1,1978,1)
iip$`鉱工業生産指数　2010年基準` &lt;- as.numeric(k)
k &lt;- seasoning(shukka.shisu,1,1990,1)
shukka.shisu$`鉱工業出荷指数　2010年基準` &lt;- as.numeric(k)
k &lt;- seasoning(zaiko.shisu,1,1990,1)
zaiko.shisu$`鉱工業在庫指数　2010年基準` &lt;- as.numeric(k)
k &lt;- seasoning(sanji.sangyo,1,1988,1)
sanji.sangyo$`第３次産業活動指数　2010年基準` &lt;- as.numeric(k)
k &lt;- seasoning(retail.sells,1,1980,1)
retail.sells$`小売業販売額（名目）` &lt;- as.numeric(k)
k &lt;- seasoning(household.consumption,1,2010,1)
household.consumption$`二人以上の世帯　消費支出（除く住居等）` &lt;- as.numeric(k)
GDP.ts &lt;- ts(GDP[,2],frequency = 4,start=c(1980,1))
m &lt;- seas(GDP.ts)
GDP$`国内総生産（支出側）（実質）2011年基準` &lt;- m$series$s11</code></pre>
<p>ここでは詳しく季節調整のかけ方は説明しません。x13arimaを使用しています。上述のコードを回す際はx13arimaがインストールされている必要があります。以下の記事を参考にしてください。</p>
<p>[<a href="http://sinhrks.hatenablog.com/entry/2014/11/09/003632" class="uri">http://sinhrks.hatenablog.com/entry/2014/11/09/003632</a>]</p>
<p>では、データ日付を基準に落としてきたデータを結合し、データセットを作成します。</p>
<pre class="r"><code># データセットに結合
dataset &lt;- dplyr::full_join(kigyo.bukka,callrate,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,kikai,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,Nikkei,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,money.stock,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,cpi,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,export.price,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,import.price,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,public.expenditure,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,export.service,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,working.population,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,yukoukyuujinn,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,hours_worked,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,nominal.wage,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,iip,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,shukka.shisu,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,zaiko.shisu,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,sanji.sangyo,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,retail.sells,by=&quot;publication&quot;)
dataset &lt;- dplyr::full_join(dataset,yen,by=&quot;publication&quot;)
colnames(dataset) &lt;- c(&quot;DCGPI&quot;,&quot;publication&quot;,&quot;callrate&quot;,&quot;Machinery_Orders&quot;,
                       &quot;Nikkei225&quot;,&quot;money_stock&quot;,&quot;CPI&quot;,&quot;export_price&quot;,
                       &quot;import_price&quot;,&quot;public_works_order&quot;,
                       &quot;trade_service&quot;,&quot;working_population&quot;,
                       &quot;active_opening_ratio&quot;,&quot;hours_worked&quot;,
                       &quot;wage&quot;,&quot;iip_production&quot;,&quot;iip_shipment&quot;,&quot;iip_inventory&quot;,
                       &quot;ITIA&quot;,&quot;retail_sales&quot;,&quot;yen&quot;)</code></pre>
<p>最後に列名をつけています。datasetはそれぞれのデータの公表開始時期が異なるために大量の<code>NA</code>を含むデータフレームとなっているので、<code>NA</code>を削除するために最もデータの開始時期が遅い機械受注統計に合わせてデータセットを再構築します。</p>
<pre class="r"><code>a &lt;- min(which(dataset$Machinery_Orders != &quot;NA&quot;))
dataset1 &lt;- dataset[a:nrow(dataset),]
dataset1 &lt;- na.omit(dataset1)
rownames(dataset1) &lt;- dataset1$publication
dataset1 &lt;- dataset1[,-2]
dataset1.xts &lt;- xts(dataset1,order.by = as.Date(rownames(dataset1)))</code></pre>
<p>これでとりあえずデータの収集は終わりました。</p>
</div>
<div id="得られたデータを主成分分析にかけてみる" class="section level2">
<h2>3. 得られたデータを主成分分析にかけてみる</h2>
<p>本格的な分析はまた今後にしたいのですが、データを集めるだけでは面白くないので、Gianonneらのように主成分分析を行いたいと思います。主成分分析をこれまでに学んだことのない方は以下を参考にしてください。個人的にはわかりやすいと思っています。</p>
<p><a href="https://datachemeng.com/principalcomponentanalysis/">主成分分析(Principal Component Analysis, PCA)～データセットの見える化・可視化といったらまずはこれ！～</a></p>
<p>では主成分分析を実行してみます。<code>R</code>では<code>princomp</code>関数を使用することで非常に簡単に主成分分析を行うことができます。</p>
<pre class="r"><code># 主成分分析を実行
factor.pca &lt;- princomp(~.,cor = TRUE,data = dataset1) # cor = TRUEでデータの基準化を自動で行ってくれる。
summary(factor.pca)</code></pre>
<pre><code>## Importance of components:
##                           Comp.1    Comp.2    Comp.3     Comp.4     Comp.5
## Standard deviation     3.2538097 1.8606047 1.5142917 1.05061250 0.88155352
## Proportion of Variance 0.5293639 0.1730925 0.1146540 0.05518933 0.03885683
## Cumulative Proportion  0.5293639 0.7024563 0.8171103 0.87229965 0.91115648
##                           Comp.6     Comp.7     Comp.8      Comp.9     Comp.10
## Standard deviation     0.7504599 0.63476742 0.48794520 0.413374289 0.358909911
## Proportion of Variance 0.0281595 0.02014648 0.01190453 0.008543915 0.006440816
## Cumulative Proportion  0.9393160 0.95946246 0.97136699 0.979910904 0.986351721
##                            Comp.11     Comp.12     Comp.13      Comp.14
## Standard deviation     0.296125594 0.254294037 0.233119328 0.1394055697
## Proportion of Variance 0.004384518 0.003233273 0.002717231 0.0009716956
## Cumulative Proportion  0.990736239 0.993969512 0.996686743 0.9976584386
##                             Comp.15      Comp.16      Comp.17     Comp.18
## Standard deviation     0.1356026290 0.1104356585 0.0861468897 0.070612034
## Proportion of Variance 0.0009194036 0.0006098017 0.0003710643 0.000249303
## Cumulative Proportion  0.9985778423 0.9991876440 0.9995587083 0.999808011
##                            Comp.19      Comp.20
## Standard deviation     0.053565104 3.115371e-02
## Proportion of Variance 0.000143461 4.852767e-05
## Cumulative Proportion  0.999951472 1.000000e+00</code></pre>
<pre class="r"><code>screeplot(factor.pca)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code>pc &lt;- predict(factor.pca,dataset1)[,1:3] # 主成分を計算
pc.xts &lt;- xts(pc,order.by = as.Date(rownames(dataset1)))
plot.zoo(pc.xts,col=c(&quot;red&quot;,&quot;blue&quot;,&quot;green&quot;,&quot;purple&quot;,&quot;yellow&quot;),plot.type = &quot;single&quot;) # 主成分を時系列プロット</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-2.png" width="672" /></p>
<p>第3主成分まででデータの約80％が説明できる結果を得たので、第3主成分までのプロットをお見せします。第1主成分（赤）はリーマンショックや東日本大震災、消費税増税のあたりで急上昇しています。ゆえに経済全体のリスクセンチメントを表しているのではないかと思っています。第2主成分（青）と第3主成分（緑）はリーマンショックのあたりで大きく落ち込んでいることは共通していますが2015年～現在の動きが大きく異なっています。また、第2主成分（青）はサンプル期間を通して過去トレンドを持つことから日本経済の潜在能力のようなものを表しているのではないでしょうか（そうするとリーマンショックまで上昇傾向にあることが疑問なのですが）。第3主成分（緑）はいまだ解読不能です（物価＆為替動向を表しているのではないかと思っています）。とりあえず今日はこれまで。次回はGianonne et. al.(2008)の日本版の再現を行いたいと思います。</p>
</div>
