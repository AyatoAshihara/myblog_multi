---
title: "GPLVMでマルチファクターモデルを構築してみた"
author: admin
date: 2019-05-26T00:00:00Z
categories: ["マクロ経済学"]
tags: ["R","ガウス過程","GPLVM"]
draft: false
featured: false
slug: ["GPLVM"]
image:
  caption: ''
  focal_point: ""
  placement: 2
  preview_only: false
lastmod: ""
projects: []
summary: 最近私が注目しているのがガウス過程。今回はそのガウス過程の中でも、主成分分析のようにデータセットの次元削減のために使用されるGaussian Process Latent Variable Model（GPLVM）の紹介をします。
output: 
  blogdown::html_page:
    number_sections: false
    toc: true
    df_print: "paged"
---

<script src="index_files/header-attrs/header-attrs.js"></script>
<link href="index_files/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="index_files/pagedtable/js/pagedtable.js"></script>

<div id="TOC">
<ul>
<li><a href="#gplvmとは">1. GPLVMとは</a></li>
<li><a href="#最もprimitiveなgp-lvm">2. <span>最もPrimitiveなGP-LVM</span></a></li>
<li><a href="#rでの実装">3. Rでの実装</a></li>
</ul>
</div>

<p>おはこんばんにちは。
ずいぶん前にGianonne et al (2008)のマルチファクターモデルで四半期GDPの予想を行いました。
結果としては、ある程度は予測精度が出ていたものの彼らの論文ほどは満足のいくものではありませんでした。原因としてはクロスセクショナルなデータ不足が大きいと思われ、現在収集方法についてもEXCELを用いて改修中です。しかし一方で、マルチファクターモデルの改善も考えたいと思っています。前回は月次経済統計を主成分分析（実際にはカルマンフィルタ）を用いて次元削減を行い、主成分得点を説明変数としてGDPに回帰しました。今回はこの主成分分析のド発展版である<code>Gaussian Process Latent Variable Model</code>(GPLVM)を用いてファクターを計算し、それをGDPに回帰したいと思います。</p>
<div id="gplvmとは" class="section level2">
<h2>1. GPLVMとは</h2>
<p>GPLVMとは、<code>Gaussian Process</code> Modelの一種です。以前、<code>Gaussian Process Regression</code>の記事を書きました。</p>
<p>最も基本的な<code>Gaussian Process</code> Modelは上の記事のようなモデルで、非説明変数<span class="math inline">\(Y=(y_{1},y_{2},...,y_{n})\)</span>と説明変数<span class="math inline">\(X=(\textbf{x}_{1},\textbf{x}_{2},...,\textbf{x}_{n})\)</span>があり、以下のような関係式で表される際にそのモデルを直接推定することなしに新たな説明変数<span class="math inline">\(X\)</span>の入力に対し、非説明変数<span class="math inline">\(Y\)</span>の予測値をはじき出すというものでした。</p>
<p><span class="math display">\[
\displaystyle y_{i}  = \textbf{w}^{T}\phi(\textbf{x}_{i})
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{x}_{i}\)</span>は<span class="math inline">\(i\)</span>番目の説明変数ベクトル、<span class="math inline">\(\phi(・)\)</span>は非線形関数、 <span class="math inline">\(\textbf{w}^{T}\)</span>は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、<span class="math inline">\(\phi(\textbf{x}_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\)</span>を想定しています（<span class="math inline">\(x_{1,i}\)</span>は<span class="math inline">\(i\)</span>番目の入力データ<span class="math inline">\(\textbf{x}_{i}\)</span>の１番目の変数）。詳しくは過去記事を参照してください。</p>
<p>今回やるGPLVMは説明変数ベクトルが観測できない潜在変数（Latent Variable）であるところが特徴です。以下のスライドが非常にわかりやすいですが、GP-LVMは確率的主成分分析（PPCA）の非線形版という位置付けになっています。</p>
<p><a href="https://www.slideshare.net/antiplastics/pcagplvm">資料</a></p>
<p>では具体的な説明に移ります。GPLVMは主成分分析の発展版ですので、主に次元削減のために行われることを想定しています。つまり、データセットがあったとして、サンプルサイズ<span class="math inline">\(n\)</span>よりも変数の次元<span class="math inline">\(p\)</span>が大きいような場合を想定しています。</p>
</div>
<div id="最もprimitiveなgp-lvm" class="section level2">
<h2>2. <a href="http://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf">最もPrimitiveなGP-LVM</a></h2>
<p>先述したようにGPLVMはPPCAの非線形版です。なので、GPLVMを説明するスタートはPPCAになります。観測可能な<span class="math inline">\(D\)</span>次元データセットを<span class="math inline">\(\{\textbf{y}_{n}\}_{n=1}^{N}\)</span>とします。そして、潜在変数を<span class="math inline">\(\textbf{x}_{n}\)</span>とおきます。今、データセットと潜在変数の間には以下のような関係があるとします。</p>
<p><span class="math display">\[
\textbf{y}_{n} = \textbf{W}\textbf{x}_{n} + \epsilon_{n}
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{W}\)</span>はウェイト行列、<span class="math inline">\(\epsilon_{n}\)</span>はかく乱項で<span class="math inline">\(N(0,\beta^{-1}\textbf{I})\)</span>に従います（被説明変数が多次元になることに注意）。また、<span class="math inline">\(\textbf{x}_{n}\)</span>は<span class="math inline">\(N(0,\textbf{I})\)</span>に従います。このとき、<span class="math inline">\(\textbf{y}_{n}\)</span>の尤度を<span class="math inline">\(\textbf{x}_{n}\)</span>を周辺化することで表現すると、</p>
<p><span class="math display">\[
\begin{eqnarray*}
\displaystyle p(\textbf{y}_{n}|\textbf{W},\beta) &amp;=&amp; \int p(\textbf{y}_{n}|\textbf{x}_{n},\textbf{W},\beta)N(0,\textbf{I})d\textbf{x}_{n} \\
\displaystyle &amp;=&amp; \int N(\textbf{W}\textbf{x}_{n},\beta^{-1}\textbf{I})N(0,\textbf{I})d\textbf{x}_{n} \\
&amp;=&amp; N(0,\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}) \\
\displaystyle &amp;=&amp; \frac{1}{(2\pi)^{DN/2}|\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}|^{N/2}}\exp(\frac{1}{2}\textbf{tr}( (\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I})^{-1}\textbf{YY}^{T}))
\end{eqnarray*}
\]</span></p>
<p>となります。ここで、<span class="math inline">\(p(\textbf{y}_{n}|\textbf{x}_{n},\textbf{W},\beta)=N(\textbf{W}\textbf{x}_{n},\beta^{-1}\textbf{I})\)</span>です。平均と分散は以下から求めました。</p>
<p><span class="math display">\[
\begin{eqnarray*}
E(\textbf{y}_{n}|\textbf{W},\beta) &amp;=&amp; E(\textbf{W}\textbf{x}_{n} + \epsilon_{n}) \\
&amp;=&amp; E(\textbf{W}\textbf{x}_{n}) + E(\epsilon_{n}) \\
&amp;=&amp; \textbf{W}E(\textbf{x}_{n}) + E(\epsilon_{n}) = 0
\end{eqnarray*}
\]</span></p>
<p><span class="math display">\[
\begin{eqnarray*}
E[(\textbf{y}_{n}|\textbf{W},\beta)(\textbf{y}_{n}|\textbf{W},\beta)^{T}] &amp;=&amp; E[ (\textbf{W}\textbf{x}_{n} + \epsilon_{n} - 0)(\textbf{W}\textbf{x}_{n} + \epsilon_{n} - 0)^{T} ] \\
&amp;=&amp; E[ (\textbf{W}\textbf{x}_{n} + \epsilon_{n})(\textbf{W}\textbf{x}_{n} + \epsilon_{n})^{T} ] \\
&amp;=&amp; E[ \textbf{W}\textbf{x}_{n}(\textbf{W}\textbf{x}_{n})^{T} + \textbf{W}\textbf{x}_{n}\epsilon_{n}^{T} + \epsilon_{n}\textbf{W}\textbf{x}_{n}^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;=&amp; E[ \textbf{W}\textbf{x}_{n}(\textbf{W}\textbf{x}_{n})^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;=&amp; E[ \textbf{W}\textbf{x}_{n}\textbf{x}_{n}^{T}\textbf{W}^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;=&amp; E[ \textbf{W}\textbf{x}_{n}\textbf{x}_{n}^{T}\textbf{W}^{T}] + E[\epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;=&amp; \textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}
\end{eqnarray*}
\]</span></p>
<p><span class="math inline">\(\textbf{W}\)</span>を求めるためには<span class="math inline">\(\textbf{y}_{n}\)</span>がi.i.d.と仮定し、以下のようなデータセット全体の尤度を最大化すれば良いことになります。</p>
<p><span class="math display">\[
\displaystyle p(\textbf{Y}|\textbf{W},\beta) = \prod_{n=1}^{N}p(\textbf{y}_{n}|\textbf{W},\beta)
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{Y}\)</span>は<span class="math inline">\(N×D\)</span>の計画行列です。このように、PPCAでは<span class="math inline">\(\textbf{x}_{n}\)</span>を周辺化し、<span class="math inline">\(\textbf{W}\)</span>を最適化します。逆に、Lawrence(2004)では<span class="math inline">\(\textbf{W}\)</span>を周辺化し、<span class="math inline">\(\textbf{x}_{n}\)</span>します（理由は後述）。<span class="math inline">\(\textbf{W}\)</span>を周辺化するために、<span class="math inline">\(\textbf{W}\)</span>に事前分布を与えましょう。</p>
<p><span class="math display">\[
\displaystyle p(\textbf{W}) = \prod_{i=1}^{D}N(\textbf{w}_{i}|0,\alpha^{-1}\textbf{I})
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{w}_{i}\)</span>はウェイト行列<span class="math inline">\(\textbf{W}\)</span>の<span class="math inline">\(i\)</span>番目の列です。では、<span class="math inline">\(\textbf{W}\)</span>を周辺化して<span class="math inline">\(\textbf{Y}\)</span>の尤度関数を導出してみます。やり方はさっきとほぼ同じなので省略します。</p>
<p><span class="math display">\[
\displaystyle p(\textbf{Y}|\textbf{X},\beta) = \frac{1}{(2\pi)^{DN/2}|K|^{D/2}}\exp(\frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T}))
\]</span></p>
<p>ここで、<span class="math inline">\(\textbf{K}=\alpha^2\textbf{X}\textbf{X}^{T} + \beta^{-1}\textbf{I}\)</span>は<span class="math inline">\(p(\textbf{Y}|\textbf{X},\beta)\)</span>の分散共分散行列で、<span class="math inline">\(\textbf{X}=(\textbf{x}_{1},\textbf{x}_{2},...,\textbf{x}_{N})^{T}\)</span>は入力ベクトルです。対数尤度は</p>
<p><span class="math display">\[
\displaystyle L = - \frac{DN}{2}\ln{2\pi} - \frac{1}{2}\ln{|\textbf{K}|} - \frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T})
\]</span></p>
<p>周辺化のおかげでウェイト<span class="math inline">\(\textbf{W}\)</span>が消えたのでこれを<span class="math inline">\(X\)</span>で微分してみましょう。</p>
<p><span class="math display">\[
\displaystyle\frac{\partial L}{ \partial \textbf{X}} = \alpha^2 \textbf{K}^{-1}\textbf{Y}\textbf{Y}^{T}\textbf{K}^{-1}\textbf{X} - \alpha^2 D\textbf{K}^{-1}\textbf{X}
\]</span></p>
<p>ここから、</p>
<p><span class="math display">\[
\displaystyle \frac{1}{D}\textbf{Y}\textbf{Y}^{T}\textbf{K}^{-1}\textbf{X} = \textbf{X}
\]</span></p>
<p>ここで、特異値分解を用いると</p>
<p><span class="math display">\[
\textbf{X} = \textbf{ULV}^{T}
\]</span></p>
<p>となります。<span class="math inline">\(\textbf{U} = (\textbf{u}_{1},\textbf{u}_{2},...,\textbf{u}_{q})\)</span>は<span class="math inline">\(N×q\)</span>直交行列、<span class="math inline">\(\textbf{L} = diag(l_{1},l_{2},..., l_{q})\)</span>は<span class="math inline">\(q×q\)</span>の特異値を対角成分に並べた行列、<span class="math inline">\(\textbf{V}\)</span>は<span class="math inline">\(q×q\)</span>直交行列です。これを先ほどの式に代入すると、</p>
<p><span class="math display">\[
\begin{eqnarray*}
\textbf{K}^{-1}\textbf{X} &amp;=&amp; (\alpha^2\textbf{X}\textbf{X}^{T} + \beta^{-1}\textbf{I})^{-1}\textbf{X} \\
&amp;=&amp; \textbf{X}(\alpha^2\textbf{X}^{T}\textbf{X} + \beta^{-1}\textbf{I})^{-1} \\
&amp;=&amp; \textbf{ULV}^{T}(\alpha^2\textbf{VLU}^{T}\textbf{ULV}^{T} + \beta^{-1}\textbf{I})^{-1} \\
&amp;=&amp; \textbf{ULV}^{T}\textbf{V}(\alpha^2\textbf{LU}^{T}\textbf{UL} + \beta^{-1}\textbf{I}^{-1})\textbf{V}^{T} \\
&amp;=&amp; \textbf{UL}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1}\textbf{V}^{T}
\end{eqnarray*}
\]</span></p>
<p>なので、</p>
<p><span class="math display">\[
\begin{eqnarray*}
\displaystyle \frac{1}{D}\textbf{Y}\textbf{Y}^{T}\textbf{UL}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1})\textbf{V}^{T} &amp;=&amp; \textbf{ULV}^{T}\\
\displaystyle \textbf{Y}\textbf{Y}^{T}\textbf{UL} &amp;=&amp; D\textbf{U}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1}\textbf{L} \\
\end{eqnarray*}
\]</span></p>
<p>となります。<span class="math inline">\(l_{j}\)</span>が0でなければ、<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\textbf{u}_{j} = D(\alpha^2 l_{j}^{2} + \beta^{-1})\textbf{u}_{j}\)</span>となり、<span class="math inline">\(\textbf{U}\)</span>のそれぞれの列は<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\)</span>の固有ベクトルであり、対応する固有値<span class="math inline">\(\lambda_{j}\)</span>は<span class="math inline">\(D(\alpha^2 l_{j}^{2} + \beta^{-1})\)</span>となります。つまり、未知であった<span class="math inline">\(X=ULV\)</span>が実は<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\)</span>の固有値問題から求めることが出来るというわけです。<span class="math inline">\(l_{j}\)</span>は上式を利用して、</p>
<p><span class="math display">\[
\displaystyle l_{j} = (\frac{\lambda_{j}}{D\alpha^2} - \frac{1}{\beta\alpha^2})^{1/2}
\]</span></p>
<p>と<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\)</span>の固有値<span class="math inline">\(\lambda_{j}\)</span>とパラメータから求められることがわかります。よって、<span class="math inline">\(X=ULV\)</span>は</p>
<p><span class="math display">\[
\textbf{X} = \textbf{U}_{q}\textbf{L}\textbf{V}^{T}
\]</span></p>
<p>となります。ここで、<span class="math inline">\(\textbf{U}_{q}\)</span>は<span class="math inline">\(\textbf{Y}\textbf{Y}^{T}\)</span>の固有ベクトルを<span class="math inline">\(q\)</span>個取り出したものです。<span class="math inline">\(\beta\)</span>が限りなく大きければ（=観測誤差が限りなく小さければ）通常のPCAと一致します。</p>
<p>以上がPPCAです。GPLVMはPPCAで確率モデルとして想定していた以下のモデルを拡張します。</p>
<p><span class="math display">\[
\textbf{y}_{n} = \textbf{W}^{T}\textbf{x}_{n} + \epsilon_{n}
\]</span></p>
<p>具体的には、通常のガウス過程と同様、</p>
<p><span class="math display">\[
\displaystyle \textbf{y}_{n}  = \textbf{W}^{T}\phi(\textbf{x}_{n})+ \epsilon_{n}
\]</span></p>
<p>という風に基底関数<span class="math inline">\(\phi(\textbf{x}_{n})\)</span>をかませて拡張します。<span class="math inline">\(\phi(・)\)</span>は平均<span class="math inline">\(\textbf{0}\)</span>、分散共分散行列<span class="math inline">\(\textbf{K}_{\textbf{x}}\)</span>のガウス過程と仮定します。分散共分散行列<span class="math inline">\(\textbf{K}_{\textbf{x}}\)</span>は</p>
<p><span class="math display">\[
\textbf{K}_{\textbf{x}} = \alpha^2\phi(\textbf{x})\phi(\textbf{x})^T
\]</span></p>
<p>であり、入力ベクトル<span class="math inline">\(\textbf{X}\)</span>を<span class="math inline">\(\phi(\textbf{・})\)</span>で非線形変換した特徴量<span class="math inline">\(\phi(\textbf{x})\)</span>が近いほど、出力値<span class="math inline">\(\textbf{Y}\)</span>も近くなりやすいという性質があることになります。GPLVMではこの性質を逆に利用しています。つまり、出力値<span class="math inline">\(Y_i\)</span>と<span class="math inline">\(Y_j\)</span>が近い→<span class="math inline">\(\phi(\textbf{x}_i)\)</span>と<span class="math inline">\(\phi(\textbf{x}_j)\)</span>が近い（内積が大きい）→<span class="math inline">\(\textbf{K}_{x,ij}\)</span>が大きい→観測不可能なデータ<span class="math inline">\(X_{i}\)</span>と<span class="math inline">\(X_{j}\)</span>は近い値（or同じようなパターン）をとる。
この議論からもわかるように、<span class="math inline">\(\textbf{K}_{\textbf{x}}\)</span>は入力ベクトル<span class="math inline">\(\textbf{X}\)</span>それぞれの距離を表したものになります。分散共分散行列の計算には入力ベクトル<span class="math inline">\(\textbf{X}\)</span>を基底関数<span class="math inline">\(\phi(\textbf{・})\)</span>で非線形変換した後、内積を求めるといったことをする必要はなく、カーネル関数を計算するのみでOKです。今回は王道中の王道RBFカーネルを使用していますので、これを例説明します。</p>
<p>RBFカーネル（スカラーに対する）
<span class="math display">\[
\theta_{1}\exp(-\frac{1}{\theta_{2}}(x-x^T)^2)
\]</span></p>
<p>このRBFカーネルは以下の基底関数と対応しています。</p>
<p><span class="math display">\[
\phi(x)_h = \tau\exp(-\frac{1}{r}(x-h)^2)
\]</span></p>
<p>例えば、この基底関数で入力<span class="math inline">\(x\)</span>を変換したものを<span class="math inline">\(2H^2+1\)</span>個並べた関数を</p>
<p><span class="math display">\[
\phi(x) = (\phi(x)_{-H^2}, ..., \phi(x)_{0},...,\phi(x)_{H^2})
\]</span></p>
<p>入力<span class="math inline">\(x\)</span>の特徴量だとすると<span class="math inline">\(x&#39;\)</span>との共分散<span class="math inline">\(K_{x}(x,x&#39;)\)</span>は内積の和なので</p>
<p><span class="math display">\[
K_{x}(x,x&#39;) = \sum_{h=-H^2}^{H^2}\phi_{h}(x)\phi_{h}(x&#39;)
\]</span></p>
<p>となります。ここで、<span class="math inline">\(H \to \infty\)</span>とし、グリッドを極限まで細かくしてみます。</p>
<p><span class="math display">\[
\begin{eqnarray*}
K_{x}(x,x&#39;) &amp;=&amp; \lim_{H \to \infty}\sum_{h=-H^2}^{H^2}\phi_{h}(x)\phi_{h}(x&#39;) \\
&amp;\to&amp;\int_{-\infty}^{\infty}\tau\exp(-\frac{1}{r}(x-h)^2)\tau\exp(-\frac{1}{r}(x&#39;-h)^2)dh \\
&amp;=&amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{1}{r}\{(x-h)^2+(x&#39;-h)^2\})dh \\
&amp;=&amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{1}{r}\{2(h-\frac{x+x&#39;}{2})^2+\frac{1}{2}(x-x&#39;)^2\})dh \\
\end{eqnarray*}
\]</span></p>
<p>となります。<span class="math inline">\(h\)</span>に関係のない部分を積分の外に出します。</p>
<p><span class="math display">\[
\begin{eqnarray*}
&amp;=&amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{2}{r}(h-\frac{x+x&#39;}{2})^2)dh\exp(-\frac{1}{2r}(x-x&#39;)^2) \\
\end{eqnarray*}
\]</span></p>
<p>残った積分を見ると、正規分布の正規化定数と等しいことがわかります。</p>
<p><span class="math display">\[
\begin{eqnarray*}
\int_{-\infty}^{\infty}\exp(-\frac{1}{2\sigma}(h-\frac{x+x&#39;}{2})^2)dh &amp;=&amp;  \int_{-\infty}^{\infty}\exp(-\frac{2}{r}(h-\frac{x+x&#39;}{2})^2)dh\\
\sigma &amp;=&amp; \frac{r}{4}
\end{eqnarray*}
\]</span></p>
<p>となるので、ガウス積分の公式を用いて</p>
<p><span class="math display">\[
\begin{eqnarray*}
&amp;=&amp; \tau^2 \sqrt{\frac{\pi r}{2}}\exp(-\frac{1}{2r}(x-x&#39;)^2)　\\
&amp;=&amp; \theta_{1}\exp(-\frac{1}{\theta_{2}}(x-x’)^2)
\end{eqnarray*}
\]</span></p>
<p>となり、RBFカーネルと等しくなることがわかります。よって、RBFカーネルで計算した共分散は上述した基底関数で入力<span class="math inline">\(x\)</span>を無限次元へ拡張した特徴量ベクトルの内積から計算した共分散と同値になることがわかります。つまり、入力<span class="math inline">\(x\)</span>と<span class="math inline">\(x&#39;\)</span>のスカラーの計算のみで<span class="math inline">\(K_{x}(x,x&#39;)\)</span>ができてしまうという夢のような計算効率化が可能になるわけです。無限次元特徴量ベクトルの回帰問題なんて普通計算できませんからね。。。カーネル関数は偉大です。
前の記事にも載せましたが、RBFカーネルで分散共分散行列を計算したガウス過程のサンプルパスは以下通りです（<span class="math inline">\(\theta_1=1,\theta_2=0.5\)</span>）。</p>
<pre class="r"><code># Define Kernel function
Kernel_Mat &lt;- function(X,sigma,beta){
  N &lt;- NROW(X)
  K &lt;- matrix(0,N,N)
  for (i in 1:N) {
    for (k in 1:N) {
      if(i==k) kdelta = 1 else kdelta = 0
      K[i,k] &lt;- K[k,i] &lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta
    }
  }
  return(K)
}

N &lt;- 10 # max value of X
M &lt;- 1000 # sample size
X &lt;- matrix(seq(1,N,length=M),M,1) # create X
testK &lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix

library(MASS)

P &lt;- 6 # num of sample path
Y &lt;- matrix(0,M,P) # define Y

for(i in 1:P){
  Y[,i] &lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y
}

# Plot
matplot(x=X,y=Y,type = &quot;l&quot;,lwd = 2)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>非常に滑らかな関数となっていることがわかります。RBFのほかにもカーネル関数は存在します。カーネル関数を変えると基底関数が変わりますから、サンプルパスは大きく変わることになります。</p>
<p>GPLVMの推定方法に話を進めましょう。PPCAの時と同じく、以下の尤度関数を最大化する観測不能な入力<span class="math inline">\(x\)</span>を推定値とします。</p>
<p><span class="math display">\[
\displaystyle L = - \frac{DN}{2}\ln{2\pi} - \frac{1}{2}\ln{|\textbf{K}|} - \frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T})
\]</span></p>
<p>ただ、PPCAとは異なり、その値は解析的に求めることができません。尤度関数の導関数は今や複雑な関数であり、展開することができないからです。よって、共役勾配法を用いて数値的に計算するのが主流なようです（自分は準ニュートン法で実装）。解析的、数値的のどちらにせよ導関数を求めておくことは必要なので、導関数を求めてみます。</p>
<p><span class="math display">\[
\frac{\partial L}{\partial \textbf{K}_x} = \frac{1}{2}(\textbf{K}_x^{-1}\textbf{Y}\textbf{Y}^T\textbf{K}_x^{-1}-D\textbf{K}_x^{-1})
\]</span></p>
<p>なので、チェーンルールから</p>
<p><span class="math display">\[
\frac{\partial L}{\partial \textbf{x}} = \frac{\partial L}{\partial \textbf{K}_x}\frac{\partial \textbf{K}_x}{\partial \textbf{x}}
\]</span></p>
<p>なので、カーネル関数を決め、<span class="math inline">\(\textbf{x}\)</span>に初期値を与えてやれば勾配法によって尤度<span class="math inline">\(L\)</span>が最大となる点を探索することができます。今回使用するRBFカーネルで<span class="math inline">\(\frac{\partial \textbf{K}_x}{\partial x_{nj}}\)</span>を計算してみます。</p>
<p><span class="math display">\[
\begin{eqnarray*}
\frac{\partial \textbf{K}_x(\textbf{x}_n,\textbf{x}_n&#39;)}{x_{nj}}&amp;=&amp;\frac{\partial\theta_{1}\exp(-\frac{|\textbf{x}_n-\textbf{x}_n&#39;|^2}{\theta_{2}})}{\partial x_{nk}} \\
&amp;=&amp; \frac{\partial\theta_{1}\exp(-\frac{(\textbf{x}_n-\textbf{x}_n&#39;)^T(\textbf{x}_n-\textbf{x}_n&#39;)}{\theta_{2}})}{\partial x_{nk}} \\
&amp;=&amp; \frac{\partial\theta_{1}\exp(-\frac{-(\textbf{x}_n^T\textbf{x}_n-2\textbf{x}_n&#39;^T\textbf{x}_n+\textbf{x}_n&#39;^T\textbf{x}_n&#39;)}{\theta_{2}})}{\partial x_{nk}} \\
&amp;=&amp; -2\textbf{K}_x(\textbf{x}_n\textbf{x}_n&#39;)\frac{(x_{nj}-x_{n&#39;j})}{\theta_2}
\end{eqnarray*}
\]</span></p>
<p><span class="math inline">\(j\)</span>番目の潜在変数の<span class="math inline">\(n\)</span>番目のサンプルそれぞれに導関数を計算し、それを分散共分散行列と同じ行列に整理したものと<span class="math inline">\(\frac{\partial L}{\partial \textbf{K}_x}\)</span>との要素ごとの積を足し合わせたものが勾配となります。</p>
</div>
<div id="rでの実装" class="section level2">
<h2>3. Rでの実装</h2>
<p>GPLVMを<code>R</code>で実装します。使用するデータは以前giannoneの記事で使用したものと同じものです。</p>
<pre class="r"><code>ESTIMATE_GPLVM &lt;- function(Y,P,sigma){
  # 1. Set initial value
  Y &lt;- as.matrix(Y)
  eigenvector &lt;- eigen(cov(Y))$vectors
  X &lt;- Y%*%eigenvector[,1:P] # initial value
  N &lt;- NROW(Y) # Sample Size
  D &lt;- NCOL(Y) # Dimention of dataset
  X0 &lt;- c(as.vector(X))
  sigma &lt;- var(matrix(Y,dim(Y)[1]*dim(Y)[2],1))
  
  # 2. Define log likelihood function
  loglik &lt;- function(X0,Y,N,P,D,beta,sigma){
    X &lt;- matrix(X0,N,P)
    K &lt;- matrix(0,N,N)
    scale &lt;- diag(sqrt(3/((apply(X, 2, max) -apply(X, 2, min))^2)))
    X &lt;- X%*%scale
    for (i in 1:N) {
      for (k in 1:N) {
         if(i==k) kdelta = 1 else kdelta = 0
         K[i,k] &lt;- K[k,i] &lt;- sigma*exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)
       }
     }
 
    L &lt;- - D*N/2*log(2*pi) - D/2*log(det(K)) - 1/2*sum(diag(ginv(K)%*%Y%*%t(Y))) #loglikelihood

    return(L)
  }
  
  # 3. Define derivatives of log likelihood function
  dloglik &lt;- function(X0,P,D,N,Y,beta,sigma){
    X &lt;- matrix(X0,N,P)
    K &lt;- matrix(0,N,N)
    for (i in 1:N) {
      for (k in 1:N) {
        if(i==k) kdelta = 1 else kdelta = 0
        K[i,k] &lt;- K[k,i] &lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)
      }
    }
    invK &lt;- ginv(K)
    dLdK &lt;- invK%*%Y%*%t(Y)%*%invK - D*invK
    dLdx &lt;- matrix(0,N,P)
    
    for (j in 1:P){
      for(i in 1:N){
        dKdx &lt;- matrix(0,N,N)
        for (k in 1:N){
          dKdx[i,k] &lt;- dKdx[k,i] &lt;- -exp(-(t(X[i,]-X[k,])%*%(X[i,]-X[k,]))*0.5)*((X[i,j]-X[k,j])*0.5)
        }
        dLdx[i,j] &lt;- sum(dLdK*dKdx)
      }
    }
    
    return(dLdx)
  }
  
  # 4. Optimization
  res &lt;- optim(X0, loglik, dloglik, Y = Y, N=N, P=P, D=D, beta = exp(2), sigma = sigma,
               method = &quot;BFGS&quot;, control = list(fnscale = -1,trace=1000,maxit=10000))
  output &lt;- matrix(res$par,N,P)
  result &lt;- list(output,res,P)
  names(result) &lt;- c(&quot;output&quot;,&quot;res&quot;,&quot;P&quot;)
  return(result)
}

GPLVM_SELECT &lt;- function(Y){
  D &lt;- NCOL(Y)
  library(stringr)
  for (i in 1:D){
    if (i == 1){
      result &lt;- ESTIMATE_GPLVM(Y,i)
      P &lt;- 2
      print(str_c(&quot;STEP&quot;, i, &quot; loglikelihood &quot;, as.numeric(result$res$value)))
    }else{
      temp &lt;- ESTIMATE_GPLVM(Y,i)
      print(str_c(&quot;STEP&quot;, i, &quot; loglikelihood &quot;, as.numeric(temp$res$value)))
      if (result$res$value &lt; temp$res$value){
        result &lt;- temp
        P &lt;- i
      }
    }
  }
  print(str_c(&quot;The optimal number of X is &quot;, P))
  print(str_c(&quot;loglikelihood &quot;, as.numeric(result$res$value)))
  return(result)
}

result &lt;- ESTIMATE_GPLVM(scale(Y),5)</code></pre>
<pre><code>## initial  value 613.864608 
## final  value 609.080329 
## converged</code></pre>
<pre class="r"><code>library(tidyverse)

ggplot(gather(as.data.frame(result$output),key = name,value = value),
       aes(x=rep(dataset1$publication,5),y=value,colour=name)) + 
  geom_line(size=1) +
  xlab(&quot;Date&quot;) +
  ggtitle(&quot;5 economic factors&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>library(xts)
X.xts &lt;- xts(result$output,order.by = dataset1$publication)
X.q.xts &lt;- apply.quarterly(X.xts,mean)
X.3m.xts &lt;- X.xts[endpoints(X.xts,on=&quot;quarters&quot;),]

if (months(index(X.q.xts)[NROW(X.q.xts)]) %in% c(&quot;3月&quot;,&quot;6月&quot;,&quot;9月&quot;,&quot;12月&quot;)){
} else X.q.xts &lt;- X.q.xts[-NROW(X.q.xts),]
if (months(index(X.3m.xts)[NROW(X.3m.xts)]) %in% c(&quot;3月&quot;,&quot;6月&quot;,&quot;9月&quot;,&quot;12月&quot;)){
} else X.3m.xts &lt;- X.3m.xts[-NROW(X.3m.xts),]

colnames(X.xts) &lt;- c(&quot;factor1&quot;,&quot;factor2&quot;,&quot;factor3&quot;,&quot;factor4&quot;,&quot;factor5&quot;) 

GDP$publication &lt;- GDP$publication + months(2)
GDP.q &lt;- GDP[GDP$publication&gt;=index(X.q.xts)[1] &amp; GDP$publication&lt;=index(X.q.xts)[NROW(X.q.xts)],]

rg &lt;- lm(scale(GDP.q$GDP)~X.q.xts[-54])
rg2 &lt;- lm(scale(GDP.q$GDP)~X.3m.xts[-54])

summary(rg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = scale(GDP.q$GDP) ~ X.q.xts[-54])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.31774 -0.06927 -0.03224  0.06690  0.31062 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      0.0445000  0.0317777   1.400    0.177    
## X.q.xts[-54]X.1 -0.3181108  0.0106335 -29.916  &lt; 2e-16 ***
## X.q.xts[-54]X.2  0.0004621  0.0175913   0.026    0.979    
## X.q.xts[-54]X.3  0.1737612  0.0249186   6.973 9.09e-07 ***
## X.q.xts[-54]X.4 -0.0060035  0.0376324  -0.160    0.875    
## X.q.xts[-54]X.5  0.0646009  0.0430678   1.500    0.149    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1617 on 20 degrees of freedom
## Multiple R-squared:  0.9791, Adjusted R-squared:  0.9739 
## F-statistic: 187.3 on 5 and 20 DF,  p-value: 4.402e-16</code></pre>
<pre class="r"><code>summary(rg2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = scale(GDP.q$GDP) ~ X.3m.xts[-54])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.35613 -0.11430  0.00258  0.15171  0.36506 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    -0.003798   0.041789  -0.091    0.928    
## X.3m.xts[-54]1 -0.312411   0.014008 -22.303 1.34e-15 ***
## X.3m.xts[-54]2  0.022873   0.025672   0.891    0.384    
## X.3m.xts[-54]3  0.152350   0.031080   4.902 8.62e-05 ***
## X.3m.xts[-54]4 -0.019237   0.047809  -0.402    0.692    
## X.3m.xts[-54]5 -0.032419   0.055662  -0.582    0.567    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.21 on 20 degrees of freedom
## Multiple R-squared:  0.9647, Adjusted R-squared:  0.9559 
## F-statistic: 109.3 on 5 and 20 DF,  p-value: 8.102e-14</code></pre>
<p>決定係数が大幅に改善しました。
3ヶ月の平均をとったファクターの方がパフォーマンスが良さそうなので、こちらで実際のGDPと予測値のプロットを行ってみます。</p>
<pre class="r"><code>ggplot(gather(data.frame(fit=rg$fitted.values,actual=scale(GDP.q$GDP),Date=GDP.q$publication),key,value,-Date),aes(y=value,x=Date,colour=key)) +
  geom_line(size=1) +
  ggtitle(&quot;fit v.s. actual GDP&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>いかがでしょうか。個人的にはかなりフィッティングできている印象があります（もはや経済理論など不要なのでしょうか）。ただ、最も新しい値を除いては未来の値が情報量として加味された上で推計されていることになりますから、フェアではありません。正しく予測能力を検証するためには四半期ごとに逐次的に回帰を行う必要があります。</p>
<p>というわけで、アウトサンプルの予測力がどれほどあるのかをテストしてみたいと思います。まず、2005年4月から2007年3月までの月次統計データでファクターを計算し、データ頻度を四半期に集約します。そして、2007年1Qのデータを除いて、GDPに回帰します。回帰したモデルの係数を用いて、2007年1Qのファクターデータで同時点のGDPの予測値を計算し、それを実績値と比較します。次は2005年4月から2007年6月までのデータを用いて･･･という感じでアウトサンプルの予測を行ってみます。</p>
<pre class="r"><code>library(lubridate)
test_df &lt;- data.frame()
for (i in as.list(seq(as.Date(&quot;2015-04-01&quot;),as.Date(&quot;2019-03-01&quot;),by=&quot;quarter&quot;))){
  day(i) &lt;- days_in_month(i)
  traindata &lt;- dataset1[dataset1$publication&lt;=i,]
  X_train &lt;- ESTIMATE_GPLVM(scale(traindata[,-2]),5)
  X_train.xts &lt;- xts(X_train$output,order.by = traindata$publication)
  X_train.q.xts &lt;- apply.quarterly(X_train.xts,mean)
  
  if (months(index(X_train.q.xts)[NROW(X_train.q.xts)]) %in% c(&quot;3月&quot;,&quot;6月&quot;,&quot;9月&quot;,&quot;12月&quot;)){
  } else X_train.q.xts &lt;- X_train.q.xts[-NROW(X_train.q.xts),]
  
  colnames(X_train.q.xts) &lt;- c(&quot;factor1&quot;,&quot;factor2&quot;,&quot;factor3&quot;,&quot;factor4&quot;,&quot;factor5&quot;) 

  GDP_train.q &lt;- scale(GDP[GDP$publication&gt;=index(X_train.q.xts)[1] &amp; GDP$publication&lt;=index(X_train.q.xts)[NROW(X_train.q.xts)],2])

  rg_train &lt;- lm(GDP_train.q[-NROW(GDP_train.q)]~.,data=X_train.q.xts[-NROW(X_train.q.xts)])
  summary(rg_train)
  test_df &lt;- rbind(test_df,data.frame(predict(rg_train,X_train.q.xts[NROW(X_train.q.xts)],interval = &quot;prediction&quot;,level=0.90),GDP=GDP_train.q[NROW(GDP_train.q)]))
}</code></pre>
<p>計算できました。グラフにしてみましょう。先ほどのグラフよりは精度が悪くなりました。特にリーマンの後は予測値の信頼区間（90%）が大きく拡大しており、不確実性が増大していることもわかります。2010年以降に関しては実績値は信頼区間にほど入っており、予測モデルとしての性能はまあまあなのかなと思います。ただ、リーマンのような金融危機もズバッと当てるところにロマンがあると思うので元データの改善を図りたいと思います。この記事はいったんここで終了です。</p>
<pre class="r"><code>ggplot(gather(data.frame(test_df,Date=as.Date(rownames(test_df))),,,-c(lwr,upr,Date)),aes(y=value,x=Date,colour=key)) +
  geom_ribbon(aes(ymax=upr,ymin=lwr,fill=&quot;band&quot;),alpha=0.1,linetype=&quot;blank&quot;) +
  geom_line(size=1) +
  ggtitle(&quot;out-sample test&quot;)</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
