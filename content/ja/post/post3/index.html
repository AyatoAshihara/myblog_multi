---
title: "カルマンフィルタの実装"
author: admin
date: 2019-02-10T00:00:00Z
categories: ["マクロ経済学"]
tags: ["カルマンフィルタ","R"]
draft: false
featured: false
slug: ["kalman_filter"]
image:
  caption: ''
  focal_point: ""
  placement: 2
  preview_only: false
lastmod: ""
projects: []
summary: 時系列解析には欠かせないカルマンフィルタ。Rでもパッケージが用意されていて非常に便利ですが、ここではいったん理解を優先し、カルマンフィルタの実装を行ってみたいと思います。
output:
  blogdown::html_page:
    number_sections: false
    toc: true
    df_print: "paged"
---

<script src="index_files/header-attrs/header-attrs.js"></script>
<link href="index_files/pagedtable/css/pagedtable.css" rel="stylesheet" />
<script src="index_files/pagedtable/js/pagedtable.js"></script>

<div id="TOC">
<ul>
<li><a href="#カルマンフィルタとは">1. カルマンフィルタとは？</a></li>
<li><a href="#カルマンフィルタアルゴリズムの導出">2. カルマンフィルタアルゴリズムの導出</a></li>
<li><a href="#rで実装する">3. <code>R</code>で実装する。</a></li>
<li><a href="#カルマンスムージング">4. カルマンスムージング</a></li>
</ul>
</div>

<p>おはこんばんにちは。かなり久しぶりの投稿となってしまいました。決して研究をさぼっていたのではなく、<code>BVAR</code>のコーディングに手こずっていました。あと少しで完成します。さて、今回は<code>BVAR</code>やこの前のGiannnone et a (2008)のような分析でも大活躍のカルマンフィルタを実装してしまいたいと思います。このブログではパッケージソフトに頼らず、基本的に自分で一から実装を行い、研究することをポリシーとしていますので、これから頻繁に使用するであろうカルマンフィルタを関数として実装してしまうことは非常に有益であると考えます。今回はRで実装をしましたので、そのご報告をします。</p>
<div id="カルマンフィルタとは" class="section level2">
<h2>1. カルマンフィルタとは？</h2>
<p>まず、カルマンフィルタに関する簡単な説明を行います。非常にわかりやすい記事があるので、<a href="https://qiita.com/MoriKen/items/0c80ef75749977767b43">こちら</a>を読んでいただいたほうがより分かりやすいかと思います。</p>
<p>カルマンフィルタとは、状態空間モデルを解くアルゴリズムの一種です。状態空間モデルとは、手元の観測可能な変数から観測できない変数を推定するモデルであり、以下のような形をしています。</p>
<p><span class="math display">\[
Y_{t} = Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} \\
\alpha_{t} = T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}
\]</span></p>
<p>ここで、<span class="math inline">\(Y_{t}\)</span>は<span class="math inline">\(g×1\)</span>ベクトルの観測可能な変数(観測変数)、<span class="math inline">\(Z_{t}\)</span>は<span class="math inline">\(g×k\)</span>係数行列、<span class="math inline">\(\alpha_{t}\)</span>は<span class="math inline">\(k×1\)</span>ベクトルの観測不可能な変数(状態変数)、<span class="math inline">\(T_{t}\)</span>は<span class="math inline">\(k×k\)</span>係数行列です。また、<span class="math inline">\(\epsilon_{t}\)</span>は観測変数の誤差項、<span class="math inline">\(\eta_{t}\)</span>は状態変数の誤差項です。これらの誤差項はそれぞれ<span class="math inline">\(N(0,H_{t})\)</span>, <span class="math inline">\(N(0,Q_{t})\)</span>に従います（<span class="math inline">\(H_{t},Q_{t}\)</span>は分散共分散行列）。<span class="math inline">\(d_{t}\)</span>, <span class="math inline">\(c_{t}\)</span>は定数項です。1本目の式は観測方程式、2本目の式は遷移方程式と呼ばれます。
状態空間モデルを使用する例として、しばしば池の魚の数を推定する問題が使用されます。今、池の中の魚の全数が知りたいとして、その推定を考えます。観測時点毎に池の中の魚をすべて捕まえてその数を調べるのは現実的に困難なので、一定期間釣りをして釣れた魚をサンプルに全数を推定することを考えます。ここで、釣れた魚は観測変数、池にいる魚の全数は状態変数と考えることができます。今、経験的に釣れた魚の数と全数の間に以下のような関係があるとします。</p>
<p><span class="math display">\[
Y_{t} = 0.01\alpha_{t} + 5 + \epsilon_{t}
\]</span>
これが観測方程式になります。また、魚の全数は過去の値からそれほど急速には変化しないと考えられるため、以下のようなランダムウォークに従うと仮定します。</p>
<p><span class="math display">\[
\alpha_{t} = \alpha_{t-1}  + 500 + \eta_{t}
\]</span>
これが遷移方程式になります。あとは、これをカルマンフィルタアルゴリズムを用いて計算すれば、観測できない魚の全数を推定することができます。
このように状態空間モデルは非常に便利なモデルであり、また応用範囲も広いです。例えば、販売額から潜在顧客数を推定したり、クレジットスプレッドやトービンのQ等経済モデル上の概念として存在する変数を推定する、<code>BVAR</code>のように<code>VAR</code>や回帰式の時変パラメータ推定などにも使用されます。</p>
</div>
<div id="カルマンフィルタアルゴリズムの導出" class="section level2">
<h2>2. カルマンフィルタアルゴリズムの導出</h2>
<p>さて、非常に便利な状態空間モデルの説明はこれくらいにして、カルマンフィルタの説明に移りたいと思います。カルマンフィルタは状態空間モデルを解くアルゴリズムの一種であると先述しました。つまり、他にも状態空間モデルを解くアルゴリズムは存在します。カルマンフィルタアルゴリズムは一般に誤差項の正規性の仮定を必要としないフィルタリングアルゴリズムであり、観測方程式と遷移方程式の線形性の仮定さえあれば、線形最小分散推定量となります。カルマンフィルタアルゴリズムの導出にはいくつかの方法がありますが、今回はこの線形最小分散推定量としての導出を行います。まず、以下の３つの仮定を置きます。</p>
<ol style="list-style-type: decimal">
<li>初期値<span class="math inline">\(\alpha_{0}\)</span>は正規分布<span class="math inline">\(N(a_{0},\Sigma_{0})\)</span>に従う確率ベクトルである(<span class="math inline">\(a_{t}\)</span>は<span class="math inline">\(\alpha_{t}\)</span>の推定値)。</li>
<li>誤差項<span class="math inline">\(\epsilon_{t}\)</span>,<span class="math inline">\(\eta_{s}\)</span>は全ての<span class="math inline">\(t\)</span>,<span class="math inline">\(s\)</span>で互いに独立で、初期値ベクトル<span class="math inline">\(\alpha_{0}\)</span>と無相関である（<span class="math inline">\(E(\epsilon_{t}\eta_{s})=0\)</span>, <span class="math inline">\(E(\epsilon_{t}\alpha_{0})=0\)</span>, <span class="math inline">\(E(\eta_{t}\alpha_{0})=0\)</span>）。</li>
<li>2より、<span class="math inline">\(E(\epsilon_{t}\alpha_{t}&#39;)=0\)</span>、<span class="math inline">\(E(\eta_{t}\alpha_{t-1}&#39;)=0\)</span>。</li>
</ol>
<p>まず、<span class="math inline">\(t-1\)</span>期の情報集合<span class="math inline">\(\Omega_{t-1}\)</span>が既知の状態での<span class="math inline">\(\alpha_{t}\)</span>と<span class="math inline">\(Y_{t}\)</span>の期待値（予測値）を求めてみましょう。上述した状態空間モデルと誤差項の期待値がどちらもゼロである事実を用いると、以下のように計算することができます。</p>
<p><span class="math display">\[
E(\alpha_{t}|\Omega_{t-1}) = a_{t|t-1} = T_{t}a_{t-1|t-1} + c_{t}
E(Y_{t}|\Omega_{t-1}) = Y_{t|t-1} = Z_{t}a_{t|t-1} + d_{t}
\]</span></p>
<p>ここで、次に、これらの分散を求めます。</p>
<p><span class="math display">\[
\begin{eqnarray}
E( (\alpha_{t}-a_{t|t-1})(\alpha_{t}-a_{t|t-1})&#39;|\Omega_{t-1}) &amp;=&amp; E( (T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}-a_{t|t-1})(T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}-a_{t|t-1})&#39;|\Omega_{t-1}) \\ 
&amp;=&amp; E(T_{t}\alpha_{t-1}\alpha_{t-1}&#39;T_{t}&#39; + R_{t}\eta_{t}\eta_{t}&#39;R_{t}&#39;|\Omega_{t-1}) \\
&amp;=&amp; E(T_{t}\alpha_{t-1}\alpha_{t-1}&#39;T_{t}&#39;|\Omega_{t-1}) + E(R_{t}\eta_{t}\eta_{t}&#39;R_{t}&#39;|\Omega_{t-1}) \\
&amp;=&amp; T_{t}E(\alpha_{t-1}\alpha_{t-1}&#39;|\Omega_{t-1})T_{t}&#39; + R_{t}E(\eta_{t}\eta_{t}&#39;|\Omega_{t-1})R_{t}&#39; \\
&amp;=&amp; T_{t}\Sigma_{t-1|t-1}T_{t}&#39; + R_{t}Q_{t}R_{t}&#39; \\
&amp;=&amp; \Sigma_{t|t-1}
\end{eqnarray}
\]</span></p>
<p><span class="math display">\[
\begin{eqnarray}
E( (Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})&#39;|\Omega_{t-1}) &amp;=&amp; E( (Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t}-Y_{t|t-1})(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t}-Y_{t|t-1})&#39;|\Omega_{t-1}) \\
&amp;=&amp; E(Z_{t}\alpha_{t}\alpha_{t}&#39;Z_{t}&#39; + S_{t}\epsilon_{t}\epsilon_{t}&#39;S_{t}&#39;|\Omega_{t-1}) \\
&amp;=&amp; E(Z_{t}\alpha_{t}\alpha_{t}&#39;Z_{t}&#39;|\Omega_{t-1}) + E(S_{t}\epsilon_{t}\epsilon_{t}&#39;S_{t}&#39;|\Omega_{t-1}) \\
&amp;=&amp; Z_{t}E(\alpha_{t}\alpha_{t}&#39;|\Omega_{t-1})Z_{t}&#39; + S_{t}E(\epsilon_{t}\epsilon_{t}&#39;|\Omega_{t-1})S_{t}&#39; \\
&amp;=&amp; Z_{t}\Sigma_{t|t-1}Z_{t}&#39; + S_{t}H_{t}S_{t}&#39; \\
&amp;=&amp; F_{t|t-1}
\end{eqnarray}
\]</span></p>
<p>ここで、<span class="math inline">\(t\)</span>期の情報集合<span class="math inline">\(\Omega_{t}\)</span>が得られたとします（つまり、観測値<span class="math inline">\(Y_{t}\)</span>を入手）。カルマンフィルタでは、<span class="math inline">\(t\)</span>期の情報である観測値<span class="math inline">\(Y_{t}\)</span>を用いて<span class="math inline">\(a_{t|t-1}\)</span>を以下の方程式で更新します。</p>
<p><span class="math display">\[
E(\alpha_{t}|\Omega_{t}) = a_{t|t} = a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1})
\]</span>
つまり、観測値と<span class="math inline">\(Y_{t}\)</span>の期待値（予測値）の差をあるウェイト<span class="math inline">\(k_{t}\)</span>（<span class="math inline">\(k×g\)</span>行列）でかけたもので補正をかけるわけです。よって、観測値と予測値が完全に一致していた場合は補正は行われないことになります。ここで重要なのは、ウエイト<span class="math inline">\(k_{t}\)</span>をどのように決めるのかです。<span class="math inline">\(k_{t}\)</span>は更新後の状態変数の分散<span class="math inline">\(E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&#39;)= \Sigma_{t|t}\)</span>を最小化するよう決定します。これが、カルマンフィルタが線形最小分散推定量である根拠です。最小化にあたっては以下のベクトル微分が必要になりますので、おさらいをしておきましょう。今回使用するのは以下の事実です。</p>
<p><span class="math display">\[
\displaystyle \frac{\partial a&#39;b}{\partial b} = \frac{\partial b&#39;a}{\partial b} = a \\
\displaystyle \frac{\partial b&#39;Ab}{\partial b} = 2Ab
\]</span></p>
<p>ここで、<span class="math inline">\(a,b\)</span>はベクトル（それぞれ<span class="math inline">\(n×1\)</span>ベクトル、<span class="math inline">\(1×n\)</span>ベクトル）、<span class="math inline">\(A\)</span>は<span class="math inline">\(n×n\)</span>の対称行列です。まず、１つ目から証明していきます。<span class="math inline">\(\displaystyle y = a&#39;b = b&#39;a = \sum_{i=1}^{n}a_{i}b_{i}\)</span>とします。
このとき、<span class="math inline">\(\frac{\partial y}{\partial b_{i}}=a_{i}\)</span>なので、</p>
<p><span class="math display">\[
\displaystyle \frac{\partial a&#39;b}{\partial b} = \frac{\partial b&#39;a}{\partial b} = a
\]</span></p>
<p>次に２つ目です。<span class="math inline">\(y = b&#39;Ab = \sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}b_{i}b_{j}\)</span>とします。このとき、</p>
<p><span class="math display">\[
\displaystyle \frac{\partial y}{\partial b_{i}} = \sum_{j=1}^{n}a_{ij}b_{j} + \sum_{j=1}^{n}a_{ji}b_{j} = 2\sum_{j=1}^{n}a_{ij}b_{j} = 2a_{i}&#39;b
\]</span>
よって、</p>
<p><span class="math display">\[
\displaystyle \frac{\partial y}{\partial b} =
\left(
    \begin{array}{cccc}
      \frac{\partial y}{\partial b_{1}} \\
      \vdots \\
      \frac{\partial y}{\partial b_{n}} \\
    \end{array}
  \right) = 2
\left(
    \begin{array}{cccc}
      \sum_{j=1}^{n}a_{1j}b_{j} \\
      \vdots \\
      \sum_{j=1}^{n}a_{nj}b_{j} \\
    \end{array}
  \right) = 2
\left(
    \begin{array}{cccc}
      a_{1}&#39;b \\
      \vdots \\
      a_{n}&#39;b \\
    \end{array}
  \right)
= 2Ab
\]</span>
さて、準備ができたので、更新後の状態変数の分散<span class="math inline">\(E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&#39;)\)</span>を求めてみましょう。</p>
<p><span class="math display">\[
\begin{eqnarray}
E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&#39;) &amp;=&amp; \Sigma_{t|t} \\
&amp;=&amp; E\{ (\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))(\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))&#39;\} \\
&amp;=&amp; E\{ ( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )\} \\
&amp;=&amp; E\{ ( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1}) )( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1}) )&#39;\} \\
&amp;=&amp; E\{ ( (I - k_{t}Z_{t})\alpha_{t} - k_{t}S_{t}\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})( (I - k_{t}Z_{t})\alpha_{t} - k_{t}S_{t}\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})&#39; \} \\
&amp;=&amp; E\{( (I - k_{t}Z_{t})(\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\epsilon_{t})( (I - k_{t}Z_{t})(\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\epsilon_{t})&#39;\} \\
&amp;=&amp; (I - k_{t}Z_{t})\Sigma_{t|t-1}(I - k_{t}Z_{t})&#39; + k_{t}S_{t}H_{t}S_{t}&#39;k_{t}&#39; \\
&amp;=&amp; (\Sigma_{t|t-1} - k_{t}Z_{t}\Sigma_{t|t-1})(I - k_{t}Z_{t})&#39; + k_{t}S_{t}H_{t}S_{t}&#39;k_{t}&#39; \\
&amp;=&amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}(k_{t}Z_{t})&#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}Z_{t}\Sigma_{t|t-1}Z_{t}&#39;k_{t}&#39; + k_{t}S_{t}H_{t}S_{t}&#39;k_{t}&#39; \\
&amp;=&amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&#39;k_{t}&#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}(Z_{t}\Sigma_{t|t-1}Z_{t}&#39; + S_{t}H_{t}S_{t}&#39;)k_{t}&#39; \\
\end{eqnarray}
\]</span></p>
<p>１回目の式変形で、<span class="math inline">\(a_{t|t}\)</span>に上述した更新式を代入し、２回目の式変形で観測方程式と上で計算した<span class="math inline">\(E(Y_{t}|\Omega_{t-1})\)</span>を代入しています。さて、更新後の状態変数の分散<span class="math inline">\(\Sigma_{t|t}\)</span>を<span class="math inline">\(k_{t}\)</span>の関数として書き表すことができたので、これを<span class="math inline">\(k_{t}\)</span>で微分し、0と置き、<span class="math inline">\(\Sigma_{t|t}\)</span>を最小化する<span class="math inline">\(k_{t}\)</span>を求めます。先述した公式で、<span class="math inline">\(a=\Sigma_{t|t-1}Z_{t}&#39;\)</span>、<span class="math inline">\(b=k_{t}&#39;\)</span>、<span class="math inline">\(A=(Z_{t}\Sigma_{t|t-1}Z_{t}&#39; + S_{t}H_{t}S_{t}&#39;)\)</span>とすると（<span class="math inline">\(A\)</span>は分散共分散行列の和なので対称行列）、</p>
<p><span class="math display">\[
\frac{\partial \Sigma_{t|t}}{\partial k_{t}&#39;} = -2(Z_{t}\Sigma_{t|t-1})&#39; + 2(Z_{t}\Sigma_{t|t-1}Z_{t}&#39; + S_{t}H_{t}S_{t}&#39;)k_{t} = 0
\]</span></p>
<p>ここから、<span class="math inline">\(k_{t}\)</span>を解きなおすと、</p>
<p><span class="math display">\[
\begin{eqnarray}
k_{t} &amp;=&amp; \Sigma_{t|t-1}Z_{t}&#39;(Z_{t}\Sigma_{t|t-1}Z_{t}&#39; + S_{t}H_{t}S_{t}&#39;)^{-1} \\
&amp;=&amp; \Sigma_{t|t-1}Z_{t}&#39;F_{t|t-1}^{-1}
\end{eqnarray}
\]</span></p>
<p>突然、<span class="math inline">\(F_{t|t-1}\)</span>が出てきました。これは観測変数の予測値の分散<span class="math inline">\(E((Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})&#39;|\Omega_{t-1})\)</span>でした。一方、<span class="math inline">\(\Sigma_{t|t-1}Z_{t}\)</span>は状態変数の予測値の分散を観測変数のスケールに調整したものです（観測空間に写像したもの）。つまり、カルマンゲイン<span class="math inline">\(k_{t}\)</span>は状態変数と観測変数の予測値の分散比となっているのです。観測変数にはノイズがあり、観測方程式はいつも誤差０で満たされるわけではありません。また、状態方程式にも誤差項が存在します。状態の遷移も100%モデル通りにはいかないということです。<span class="math inline">\(t\)</span>期の観測変数<span class="math inline">\(Y_{t}\)</span>が得られたとして、それをどれほど信頼して状態変数を更新するかは観測変数のノイズが状態変数のノイズに比べてどれほど小さいかによります。つまり、相対的に観測方程式が遷移方程式よりも信頼できる場合には状態変数を大きく更新するのです。このように、カルマンフィルタでは、観測方程式と遷移方程式の相対的な信頼度によって、更新の度合いを毎期調整しています。その度合いが分散比であり、カルマンゲインだというわけです。ちなみに欠損値が発生した場合には、観測変数の分散を無限大にし、状態変数の更新を全く行わないという対処を行います。観測変数に信頼がないので当たり前の処置です。この場合は遷移方程式を100%信頼します。これがカルマンフィルタのコアの考え方になります。
更新後の分散を計算しておきます。</p>
<p><span class="math display">\[
\begin{eqnarray}
\Sigma_{t|t} &amp;=&amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&#39;k_{t}&#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}F_{t|t-1}k_{t}&#39; \\
&amp;=&amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&#39;k_{t}&#39; - k_{t}Z_{t}\Sigma_{t|t-1} + (\Sigma_{t|t-1}Z_{t}&#39;F_{t|t-1}^{-1})F_{t|t-1}k_{t}&#39; \\
&amp;=&amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&#39;k_{t}&#39; - k_{t}Z_{t}\Sigma_{t|t-1} + \Sigma_{t|t-1}Z_{t}&#39;k_{t}&#39; \\
&amp;=&amp; \Sigma_{t|t-1} - k_{t}Z_{t}\Sigma_{t|t-1} \\
&amp;=&amp; \Sigma_{t|t-1} - k_{t}F_{t|t-1}F_{t|t-1}^{-1}Z_{t}\Sigma_{t|t-1} \\
&amp;=&amp; \Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}&#39;
\end{eqnarray}
\]</span></p>
<p>では、最終的に導出されたアルゴリズムをまとめたいと思います。</p>
<p><span class="math display">\[
\begin{eqnarray}
a_{t|t-1} &amp;=&amp; T_{t}a_{t-1|t-1} + c_{t} \\
\Sigma_{t|t-1} &amp;=&amp; T_{t}\Sigma_{t-1|t-1}T_{t}&#39; + R_{t}Q_{t}R_{t}&#39; \\
Y_{t|t-1} &amp;=&amp; Z_{t}a_{t|t-1} + d_{t} \\
F_{t|t-1} &amp;=&amp;  Z_{t}\Sigma_{t|t-1}Z_{t}&#39; + S_{t}H_{t}S_{t}&#39; \\
k_{t} &amp;=&amp; \Sigma_{t|t-1}Z_{t}&#39;F_{t|t-1}^{-1} \\
a_{t|t} &amp;=&amp; a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}) \\
\Sigma_{t|t} &amp;=&amp;  \Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}&#39;
\end{eqnarray}
\]</span>
初期値<span class="math inline">\(a_{0},\Sigma_{0}\)</span>が所与の元で、まず状態変数の予測値<span class="math inline">\(a_{1|0},\Sigma_{1|0}\)</span>を計算します。その結果を用いて、次は観測変数の予測値<span class="math inline">\(Y_{t|t-1},F_{t|t-1}\)</span>を計算し、カルマンゲイン<span class="math inline">\(k_{t}\)</span>を得ます。<span class="math inline">\(t\)</span>期の観測可能なデータを入手したら、更新方程式を用いて<span class="math inline">\(a_{t|t},\Sigma_{t|t}\)</span>を更新します。これをサンプル期間繰り返していくことになります。ちなみに、遷移方程式の誤差項<span class="math inline">\(\eta_{t}\)</span>と定数項<span class="math inline">\(c_{t}\)</span>がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致します。つまり、新しいサンプルを入手するたびに<code>OLS</code>をやり直す推計方法ということです（今回はその証明は勘弁してください）。</p>
</div>
<div id="rで実装する" class="section level2">
<h2>3. <code>R</code>で実装する。</h2>
<p>以下が<code>R</code>での実装コードです。</p>
<pre class="r"><code>kalmanfiter &lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){
  #-------------------------------------------------------------------
  # Implemention of Kalman filter
  #   y - observed variable
  #   I - the number of unobserved variable
  #   t - parameter of endogenous variable in state equation
  #   z - parameter of endogenous variable in observable equation
  #   c - constant in state equaion
  #   R - parameter of exogenous variable in state equation
  #   Q - var-cov matrix of exogenous variable in state equation
  #   d - constant in observable equaion
  #   S - parameter of exogenous variable in observable equation
  #   h - var-cov matrix of exogenous variable in observable equation
  #   a_int - initial value of endogenous variable
  #   sig_int - initial value of variance of endogenous variable
  #-------------------------------------------------------------------
  
  library(MASS)
  
  # 1.Define Variable
  if (class(y)!=&quot;matrix&quot;){
    y &lt;- as.matrix(y)
  }
  N &lt;- NROW(y) # sample size
  L &lt;- NCOL(y) # the number of observable variable 
  a_pre &lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable
  a_fil &lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable
  sig_pre &lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable
  sig_fil &lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable
  y_pre &lt;- array(0,dim = c(L,1,N)) # prediction of observed variable
  F_pre &lt;- array(0,dim = c(L,L,N)) # prediction of var-cov mat. of observable variable 
  F_inv &lt;- array(0,dim = c(L,L,N)) # inverse of F_pre
  k &lt;- array(0,dim = c(I,L,N)) # kalman gain
  
  if (any(is.na(a_int))==TRUE){
    a_int &lt;- matrix(0,nrow = I,ncol = 1)
  }
  if (any(is.na(sig_int))==TRUE){
    sig_int &lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(R))==TRUE){
    R &lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(Q))==TRUE){
    Q &lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(S))==TRUE){
    S &lt;- matrix(1,nrow = L,ncol = L)
  }
  if (any(is.na(h))==TRUE){
    H &lt;- array(0,dim = c(L,L,N))
    for(i in 1:N){
      diag(H[,,i]) = 1
    }
  }else if (class(h)!=&quot;array&quot;){
    H &lt;- array(h,dim = c(NROW(h),NCOL(h),N))
  }
  
  # fill infinite if observed data is NA
  for(i in 1:N){
    miss &lt;- is.na(y[i,])
    diag(H[,,i])[miss] &lt;- 1e+32
  }
  y[is.na(y)] &lt;- 0
  
  # 2.Set Initial Value
  a_fil[,,1] &lt;- a_int
  sig_fil[,,1] &lt;- sig_int
  
  # 3.Implement Kalman filter
  for (i in 1:N){
    if(class(z)==&quot;array&quot;){
      Z &lt;- z[,,i]
    }else{
      Z &lt;- z
    }
    a_pre[,,i] &lt;- t%*%a_fil[,,i] + c
    sig_pre[,,i] &lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)
    y_pre[,,i] &lt;- Z%*%a_pre[,,i] + d
    F_pre[,,i] &lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)
    k[,,i] &lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])
    a_fil[,,i+1] &lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])
    sig_fil[,,i+1] &lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])
  }
  
  # 4.Aggregate results
  result &lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)
  names(result) &lt;- c(&quot;state prediction&quot;, &quot;state filtered&quot;, &quot;state var prediction&quot;, 
                     &quot;state var filtered&quot;, &quot;observable prediction&quot;, &quot;kalman gain&quot;,
                     &quot;parameter of state eq&quot;, &quot;parameter of observable eq&quot;)
  return(result)
}</code></pre>
<p>案外簡単に書けるもんですね。これを使って、Giannone et al (2008)をやり直してみます。データセットは前回記事と変わりません。</p>
<p>以下、分析用の<code>R</code>コードです。</p>
<pre class="r"><code>#------------------------
# Giannone et. al. 2008 
#------------------------

library(MASS)
library(xts)

# ファクターを計算
f &lt;- 3
z &lt;- scale(dataset1)
for (i in 1:nrow(z)){
  eval(parse(text = paste(&quot;S_i &lt;- z[i,]%*%t(z[i,])&quot;,sep = &quot;&quot;)))
  if (i==1){
    S &lt;- S_i
  }else{
    S &lt;- S + S_i
  }
}
S &lt;- (1/nrow(z))*S
gamma &lt;- eigen(S)
D &lt;- diag(gamma$values[1:f])
V &lt;- gamma$vectors[,1:f]
F_t &lt;- matrix(0,nrow(z),f)
for (i in 1:nrow(z)){
  eval(parse(text = paste(&quot;F_t[&quot;,i,&quot;,]&lt;- z[&quot;,i,&quot;,]%*%V&quot;,sep = &quot;&quot;)))
}
lambda_hat &lt;- V
psi &lt;- diag(diag(S-V%*%D%*%t(V)))
R &lt;- diag(diag(cov(z-z%*%V%*%t(V))))

a &lt;- matrix(0,f,f)
b &lt;- matrix(0,f,f)
for(t in 2:nrow(z)){
  a &lt;- a + F_t[t,]%*%t(F_t[t-1,])
  b &lt;- b + F_t[t-1,]%*%t(F_t[t-1,])
}
b_inv &lt;- solve(b)
A_hat &lt;- a%*%b_inv

e &lt;- numeric(f)
for (t in 2:nrow(F_t)){
  e &lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat
}
H &lt;- t(e)%*%e
Q &lt;- diag(1,f,f)
Q[1:f,1:f] &lt;- H

p &lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))

result1 &lt;- kalmanfiter(z,f,A_hat,lambda_hat,c=0,R=NA,Q=Q,d=0,S=NA,h=R,a_int=F_t[1,],sig_int=matrix(p,f,f))</code></pre>
<p>プロットしてみます。</p>
<pre class="r"><code>library(ggplot2)
library(tidyverse)

ggplot(gather(data.frame(factor1=result1$`state filtered`[1,1,-dim(result1$`state filtered`)[3]],factor2=result1$`state filtered`[2,1,-dim(result1$`state filtered`)[3]],factor3=result1$`state filtered`[3,1,-dim(result1$`state filtered`)[3]],time=as.Date(rownames(dataset1))),key = factor,value = value,-time),aes(x=time,y=value,colour=factor)) + geom_line()</code></pre>
<p><img src="index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>giannoneの記事を書いた際は、元論文の<code>MATLAB</code>コードを参考にRで書いたのですが、通常のカルマンフィルタとは観測変数の分散共分散行列の逆数の計算方法が違うらしくグラフの形が異なっています。まあでも、概形はほとんど同じですが（なので、ちゃんと動いているはず）。</p>
</div>
<div id="カルマンスムージング" class="section level2">
<h2>4. カルマンスムージング</h2>
<p>カルマンフィルタの実装は以上で終了なのですが、誤差項の正規性を仮定すれば<span class="math inline">\(T\)</span>期までの情報集合<span class="math inline">\(\Omega_{T}\)</span>を用いて、<span class="math inline">\(a_{i|i}, \Sigma_{i|i}(i = 1:T)\)</span>を<span class="math inline">\(a_{i|T}, \Sigma_{i|T}(i = 1:T)\)</span>へ更新することができます。これをカルマンスムージングと呼びます。これを導出してみましょう。その準備として、以下のような<span class="math inline">\(\alpha_{t|t}\)</span>と<span class="math inline">\(\alpha_{t+1|t}\)</span>の混合分布を計算しておきます。</p>
<p><span class="math display">\[
\begin{eqnarray}
(\alpha_{t|t},\alpha_{t+1|t}) &amp;=&amp; N(
\left(
    \begin{array}{cccc}
      E(\alpha_{t|t}) \\
      E(\alpha_{t+1|t})
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      Var(\alpha_{t|t}), Cov(\alpha_{t|t},\alpha_{t+1|t}) \\
      Cov(\alpha_{t+1|t},\alpha_{t|t}), Var(\alpha_{t+1|t})
    \end{array}
  \right)
) \\
&amp;=&amp; N(
\left(
    \begin{array}{cccc}
      a_{t|t} \\
      a_{t+1|t}
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      \Sigma_{t|t}, \Sigma_{t|t}T_{t}&#39; \\
      T_{t}\Sigma_{t|t}, \Sigma_{t+1|t} 
    \end{array}
  \right)
)
\end{eqnarray}
\]</span>
ここで、<span class="math inline">\(Cov(\alpha_{t|t},\alpha_{t+1|t})\)</span>は</p>
<p><span class="math display">\[
\begin{eqnarray}
Cov(\alpha_{t+1|t},\alpha_{t|t}) &amp;=&amp; Cov(T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}, \alpha_{t|t}) \\
&amp;=&amp; T_{t}Cov(\alpha_{t|t},\alpha_{t|t}) + Cov(c_{t},\alpha_{t|t}) + Cov(R_{t}\eta_{t},\alpha_{t|t}) \\
&amp;=&amp; T_{t}Var(\alpha_{t|t}) = T_{t}\Sigma_{t|t}
\end{eqnarray}
\]</span></p>
<p>ここで、条件付き多変量正規分布は以下のような分布をしていることを思い出しましょう（
<a href="https://mathwords.net/gaussjoken">参考</a>
）。</p>
<p><span class="math display">\[
(X_{1},X_{2}) = N(
\left(
    \begin{array}{cccc}
      \mu_{1} \\
      \mu_{2}
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      \Sigma_{11}, \Sigma_{12} \\
      \Sigma_{21}, \Sigma_{22}
    \end{array}
  \right)
) \\
\]</span>
<span class="math display">\[
(X_{1}|X_{2}=x_{2}) = N(\mu_{1} + \Sigma_{12}\Sigma_{22}^{-1}(x_{2}-\mu_{2}),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
\]</span></p>
<p>これを用いて、<span class="math inline">\((\alpha_{t|t}|\alpha_{t+1|t}=a_{t+1})\)</span>を計算してみましょう。</p>
<p><span class="math display">\[
\begin{eqnarray}
(\alpha_{t|t}|\alpha_{t+1|t}=a_{t+1}) &amp;=&amp; N(a_{t|t} + \Sigma_{t|t}T_{t}&#39;\Sigma_{t+1|t}^{-1}(a_{t+1}-a_{t+1|t}), \Sigma_{t|t}-\Sigma_{t|t}T_{t}&#39;\Sigma_{t+1|t}^{-1}T_{t}\Sigma_{t|t}) \\
&amp;=&amp;N(a_{t|t} + L_{t}(a_{t+1}-a_{t+1|t}), \Sigma_{t|t}-L_{t}\Sigma_{t+1|t}L_{t}&#39;)
\end{eqnarray}
\]</span></p>
<p>ただし、<span class="math inline">\(a_{t+1}\)</span>の値は観測不可能なので、上式を用いて状態変数を更新することはできません。今、わかるのは<span class="math inline">\(T\)</span>期における<span class="math inline">\(a_{t+1|T}\)</span>の分布のみです。ということで、<span class="math inline">\(a_{t+1}\)</span>を<span class="math inline">\(a_{t+1|T}\)</span>で代用し、<span class="math inline">\(\alpha_{t|T}\)</span>の分布を求めてみます。では、計算していきます。<span class="math inline">\(\alpha_{t|T} = N(E(\alpha_{t|T}),Var(\alpha_{t|T}))\)</span>ですが、</p>
<p><span class="math display">\[
\begin{eqnarray}
E(\alpha_{t|T}) &amp;=&amp; E_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
Var(\alpha_{t|T}) &amp;=&amp; E_{\alpha_{t+1|T}}(Var(\alpha_{t|t}|\alpha_{t+1|t} = \alpha_{t+1|T})) + Var_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T}))
\end{eqnarray}
\]</span></p>
<p>というように、<span class="math inline">\(\alpha_{t+1|T}\)</span>も確率変数となるので、繰り返し期待値の法則と繰り返し分散の法則を使用します（<a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/video-lectures/lecture-12-iterated-expectations-sum-of-a-random-number-of-random-variables/MIT6_041F10_L12.pdf">こちら</a>を参照）。</p>
<p>*繰り返し期待値の法則
<span class="math inline">\(E(x) = E_{Z}(E(X|Y=Z))\)</span></p>
<p>*繰り返し分散の法則
<span class="math inline">\(Var(X) = E_{Z}(Var(X|Y=Z))+Var_{Z}(E(X|Y=Z))\)</span></p>
<p>よって、</p>
<p><span class="math display">\[
\begin{eqnarray}
E(\alpha_{t|T}) &amp;=&amp; E_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
&amp;=&amp; a_{t|t} + L_{t}(a_{t+1|T}-a_{t+1|t}) \\
Var(\alpha_{t|T}) &amp;=&amp; E_{\alpha_{t+1|T}}(Var(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) + Var_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
&amp;=&amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&#39; + E( (a_{t|t} + L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))(a_{t|t} + L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))&#39;) \\
&amp;=&amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&#39; + E( (L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))(L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))&#39;) \\
&amp;=&amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&#39; + E( (L_{t}\alpha_{t+1|T} - L_{t}a_{t+1|T})(L_{t}\alpha_{t+1|T} - L_{t}(a_{t+1|T})&#39;) \\
&amp;=&amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&#39; + E( L_{t}(\alpha_{t+1|T} - a_{t+1|T})(\alpha_{t+1|T} - a_{t+1|T})&#39;L_{t}&#39;) \\
&amp;=&amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&#39; + L_{t}E( (\alpha_{t+1|T} - a_{t+1|T})(\alpha_{t+1|T} - a_{t+1|T})&#39;)L_{t}&#39; \\
&amp;=&amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&#39; + L_{t}\Sigma_{t+1|T}L_{t}&#39;
\end{eqnarray}
\]</span></p>
<p>となります。カルマンスムージングのアルゴリズムをまとめておきます。</p>
<p><span class="math display">\[
\begin{eqnarray}
L_{t} &amp;=&amp; \Sigma_{t|t}T_{t}\Sigma_{t+1|t}^{-1} \\
a_{t|T} &amp;=&amp; a_{t|t} + L_{t}(a_{t+1|T} - a_{t+1|t}) \\
\Sigma_{t|T} &amp;=&amp; \Sigma_{t+1|t} + L_{t}(\Sigma_{t+1|T}-\Sigma_{t+1|t})L_{t}&#39;
\end{eqnarray}
\]</span>
カルマンスムージングの特徴的な点は後ろ向きに計算をしていく点です。つまり、<span class="math inline">\(T\)</span>期から1期に向けて計算を行っていきます。<span class="math inline">\(L_{t}\)</span>に関してはそもそもカルマンフィルタを回した時点で計算可能ですが、<span class="math inline">\(\alpha_{t|T}\)</span>は<span class="math inline">\(T\)</span>期までのデータが手元にないと計算できません。今、<span class="math inline">\(T\)</span>期まで観測可能なデータが入手できたとしましょう。すると、２番目の方程式を用いて、<span class="math inline">\(a_{T-1|T}\)</span>を計算します。ちなみに<span class="math inline">\(a_{T|T}\)</span>はカルマンフィルタを回した時点ですでに手に入っているので、計算する必要はありません。同時に、３番目の式を用いて<span class="math inline">\(\Sigma_{T-1|T}\)</span>を計算します。そして、<span class="math inline">\(a_{T-1|T},\Sigma_{T-1|T}\)</span>と<span class="math inline">\(L_{T-1}\)</span>を用いて<span class="math inline">\(a_{T-2|T},\Sigma_{T-2|T}\)</span>を計算、というように1期に向けて後ろ向きに計算をしていくのです。さきほど、遷移方程式の誤差項<span class="math inline">\(\eta_{t}\)</span>と定数項<span class="math inline">\(c_{t}\)</span>がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致すると書きましたが、カルマンスムージングの場合は<span class="math inline">\(T\)</span>期までのサンプルで<code>OLS</code>を行った結果と一致します。
<code>R</code>で実装してみます。</p>
<pre class="r"><code>kalmansmoothing &lt;- function(filter){
  #-------------------------------------------------------------------
  # Implemention of Kalman smoothing
  #   t - parameter of endogenous variable in state equation
  #   z - parameter of endogenous variable in observable equation
  #   a_pre - prediction of state
  #   a_fil - filtered value of state
  #   sig_pre - prediction of var of state
  #   sig_fil - filtered value of state
  #-------------------------------------------------------------------
  
  library(MASS)
  
  # 1.Define variable
  a_pre &lt;- filter$`state prediction`
  a_fil &lt;- filter$`state filtered`
  sig_pre &lt;- filter$`state var prediction`
  sig_fil &lt;- filter$`state var filtered`
  t &lt;- filter$`parameter of state eq`
  C &lt;- array(0,dim = dim(sig_pre))
  a_sm &lt;- array(0,dim = dim(a_pre))
  sig_sm &lt;- array(0,dim = dim(sig_pre))
  N &lt;- dim(C)[3]
  a_sm[,,N] &lt;- a_fil[,,N]
  sig_sm[,,N] &lt;- sig_fil[,,N]
  
  for (i in N:2){
    C[,,i-1] &lt;- sig_fil[,,i-1]%*%t(t)%*%ginv(sig_pre[,,i])
    a_sm[,,i-1] &lt;- a_fil[,,i-1] + C[,,i-1]%*%(a_sm[,,i]-a_pre[,,i])
    sig_sm[,,i-1] &lt;- sig_fil[,,i-1] + C[,,i-1]%*%(sig_sm[,,i]-sig_pre[,,i])%*%t(C[,,i-1])
  }
  
  
  result &lt;- list(a_sm,sig_sm,C)
  names(result) &lt;- c(&quot;state smoothed&quot;, &quot;state var smoothed&quot;, &quot;c&quot;)
  return(result)
}</code></pre>
<p>先ほどのコードの続きで<code>R</code>コードを書いてみます。</p>
<pre class="r"><code>result2 &lt;- kalmansmoothing(result1)</code></pre>
<p>かなりシンプルですね。ちなみにグラフにしましたが、１個目とほぼ変わりませんでした。とりあえず、今日はここまで。</p>
</div>
