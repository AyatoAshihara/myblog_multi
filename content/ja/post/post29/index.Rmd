---
title: "【徹底比較】センチメントスコア算出手法！！ - 第4回"
author: admin
date: 2022-04-02T00:00:00Z
categories: ["単発"]
tags: ["Python","前処理","機械学習","テキスト解析"]
draft: false
featured: false
slug: ["sentiment_score"]
image:
  caption: ''
  focal_point: ""
  placement: 2
  preview_only: false
lastmod: ""
projects: []
summary: 単語埋め込み(word embedding)を用いたセンチメントスコアの算出を実践します！
output: 
  blogdown::html_page:
    toc: true
codefolding_show: "show"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

```{python,include=FALSE}
import os
os.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = r'C:\Users\aashi\Anaconda3\Library\plugins\platforms'
```

## 1. 単語埋め込み(Word Embedding)手法について

前回までのpostで、教師なし学習である辞書ベース分類法と教師あり学習であるナイーブベイズ分類器を紹介しました。教師あり学習のほうが精度は出そうだが、教師データの準備にコストがかかり、教師なし学習は教師データは不要である一方、精度に課題があります。どちらも一長一短がある手法であり、この両者を埋め合わせる良い手法はないのかと調べていると準教師学習を用いた手法であるLatent Semantic Scalingを発見しました。

**Latent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages**[@watanabe2020]

この手法は単語埋め込み(word embedding)を用いているため、まずはそちらを紹介します。

### 単語埋め込み(Word Embedding)とは

この論文では単語埋め込み(Word Embedding)を使用しています。単語埋め込み(Word Embedding)とは、**自然言語処理における言語モデリング手法の一種であり、単語や語句を固定長の実数ベクトルとして表現する手法のこと**です。

Works Applicationsが提供しているモデルChiveを例とすると、以下のように各単語にその単語の特徴を表現する300次元の実数ベクトル(=Embeddingベクトル)が与えられています。

```{python, include=FALSE}
chive_path = r"C:\Users\aashi\Desktop\TextMining\chive-1.2-mc15_gensim\chive-1.2-mc15.kv"
```

```{python, eval=FALSE}
chive_path = r"C:\Users\hogehoge\Watcher\chive-1.2-mc15_gensim\chive-1.2-mc15.kv"
```

```{python}
import gensim
vectors = gensim.models.KeyedVectors.load(chive_path)
print("Embeddingベクトルの次元数：", vectors["王様"].shape[0])
print("Embeddingベクトルの値：", vectors["王様"][1:10])
```

このEmbeddingベクトルを用いて、単語同士の演算を行うことも可能です。例えば、「王様 - 男 + 女 = 女王」のようなもので、具体的には「王様」のEmbeddingベクトルから「男」のEmbeddingベクトルを減算、「女」のEmbeddingベクトルを加算し、計算結果と最も値が近い単語を返す処理をします。

```{python}
print("王様 - 男 - 女：", vectors.most_similar(positive=["王様","女"], negative=["男"], topn=3))
```

上記では「王女」が最も近い単語となっていますが、2番目が「女王」であり、意図した結果が返ったことが分かります。  
以下のように、各単語のEmbeddingベクトルのコサイン類似度を測ることによって、或る単語と意味が近い単語を調べることも可能です。

```{python}
print("関西学院大学と意味の近い単語上位3位：", vectors.most_similar(["関西学院大学"],topn=3))
```
やはり関関同立というだけあって想定通りの結果です。また、その順番も多くの人のイメージ通りかも知れません(同じキリスト教の大学と言うことであれば、同志社大学の近いという見方もあるかも知れません)。  
では、この実数ベクトルをどのように計算しているのでしょうか。Word Embeddingの手法としては、Word2Vec([Efficient Estimation of Word Representations in Vector Space (arxiv.org)](https://arxiv.org/abs/1301.3781))が有名です。

### Latent Semantic Scalingとは

政治学者である渡辺耕平さんによって提案されたWord Embeddingを利用してセンチメントスコアを単語へ付与する手法です。種語(Seed Words)と呼ばれる抽象的な意味を持つ幾つかの単語からの近さでセンチメントスコアの付与対象とする単語群の実数ベクトルを計算する準教師学習となっており、教師あり学習、教師なし学習のメリデメを補完しています。

-   教師あり学習

    メリット - 学習のスピードが速く、精度も高い

    デメリット - 教師データを整備するのが大変

-   教師なし学習

    メリット - 教師データを用意する必要がないので、分析を始めやすい

    デメリット - 学習のスピードが遅く、精度も低い

Word Embeddingの算出には特異値分解を用いています。各要素がその文書(行)におけるその単語(列)の出現頻度が入力されている行列である文書行列に対して特異値分解を行い、各単語のEmbeddingベクトルを計算します。ここで文書行列を$D$、文書行列$D$における単語(列)数を$n$とすると、

$$
D \approx U \Sigma V'
$$

と分解でき、$V$が$k×n$行列となるので各単語の特徴量ベクトルと見なすことができます($k$は分析者が決定)。$V$のうち$i$列目のベクトルと$V_i$と表すと、センチメントスコアは種語$s \in S$の特徴量ベクトル$V_s$と各単語の特徴量ベクトル$V_j$のコサイン類似度から算出します。単語$f$のセンチメントスコアを$g_f$とすると、算出式は

$$
g_f = \frac{1}{|S|}\sum_{s\in S} \cos(v_s,v_f)p_s
$$

となります。ここで、$p_s$は分析者が定める種語のセンチメントスコアです。種語が複数個存在する場合には平均値を使用します。。

次に、単語のセンチメントスコアを用いて文書のセンチメントスコアを算出します。文書$d$のセンチメントスコアを$y_d$とすると、その文書に含まれる単語$f\in F$を用いて

$$
y_d = \frac{1}{N}\sum_{f\in F}g_fh_f
$$

で計算します。ここで、$h_f$は各単語$f$のその文書内での出現回数、$N$はその文書に含まれる単語の総数です($V$に含まれない単語は除く)。

## 2. Latent Semantic Scalingの実践

Latent Semantic Scalingによるセンチメントスコアの算出を行います。

### 前処理

`R`で行っていきます。まず、サンプルデータを読み込みます。

```{r, eval=FALSE}
filepath <- r"(C:\Users\hogehoge\Watcher\RawData)"
```

```{r, include=FALSE}
filepath <- r"(C:\Users\aashi\Desktop\TextMining\景気ウオッチャー\生データ\)"
```

```{r message=FALSE, warning=FALSE}
library(magrittr)
files <- stringr::str_c(filepath, list.files(filepath, pattern = "*.csv"))
sample <- readr::read_csv(files,locale=readr::locale(encoding="Shift-JIS"),show_col_types = FALSE)
sample <- sample[sample$追加説明及び具体的状況の説明!="−"|sample$追加説明及び具体的状況の説明!="＊",]
sample <- sample[sample$景気の現状判断!="□",]
sample$景気の現状判断 <- factor(sample$景気の現状判断, levels=c("◎","○","▲","×"))
```

次に、`sudachi`を用いてテキストデータのToken化を行います。`R`には`sudachi`を使えるパッケージが存在しないので、Pythonパッケージの`sudachipy`を`reticulate`で呼び出して使用します。

```{r message=FALSE, warning=FALSE, cache=TRUE}
# sudachiによる形態素解析→Token化

# コーパス生成
corp <- quanteda::corpus(sample,text_field="追加説明及び具体的状況の説明")
# sudachipy呼び出し→インスタンス化
sudachipy <- reticulate::import("sudachipy")
# 正規化関数定義
sudachi_normalize <- function(tokens){
  res <- c()
  for(i in 0:(length(tokens)-1)){
    res <- append(res,tokens[i]$normalized_form())
  }
  return(res)
}
# tokenize関数定義
sudachi_tokenize <- function(sentence, mode = "A"){
  tokenizer_obj <- sudachipy$dictionary$Dictionary()$create()
  mode <- switch(mode, 
                 "A" = sudachipy$tokenizer$Tokenizer$SplitMode$A,
                 "B" = sudachipy$tokenizer$Tokenizer$SplitMode$B,
                 "C" = sudachipy$tokenizer$Tokenizer$SplitMode$C,
                 stop("Only Can Use A, B, C"))
  res <- purrr::map(sentence, ~sudachi_normalize(tokenizer_obj$tokenize(., mode)))
  return(res)
}

stopwords <- c("の","に","は","を","た","が","で","て","と","し","れ","さ","ある","いる",
                "も","する","から","な","こと","い","や","れる","など","なっ","ない","この",
                "ため","その","あっ","よう","また","もの","あり","まで","られ","なる","へ",
                "か","だ","これ","おり","より","ず","なり","られる","ば","なかっ","なく",
                "しかし","せ","だっ","できる","それ","う","なお","のみ","でき","き","つ",
                "および","いう","さらに","ら","たり","たち","ます","ん","なら","特に",
                "せる","及び","とき","にて","ほか","ながら","うち","そして","ただし",
                "かつて","それぞれ","お","ほど","ほとんど","です","とも","ところ","ここ",
               "居る","為る","有る","成る","来る","よる","おる")

# Token化実行
toks_sent <- corp %>%
              sudachi_tokenize() %>% 
              quanteda::as.tokens() %>% 
              quanteda::tokens_remove(quanteda::stopwords("ja", source = "marimo")) %>% 
              quanteda::tokens_remove(c("、", "。", "・")) %>% 
              quanteda::tokens_remove(stopwords)

# メタデータをTokenに再付与
quanteda::docvars(toks_sent) <- quanteda::docvars(corp)
```

この`toks_sent`から文書行列を生成します。文書行列とは、単語と文書の関係を表す行列で、各行が単語(token)、各列が文書を表し、各要素は文書中の単語の出現回数となっています。

```{r}
# 文書行列の生成
dfmt_sent <- toks_sent %>% 
              quanteda::dfm() %>% 
              quanteda::dfm_remove(pattern="") %>% 
              quanteda::dfm_trim(min_termfreq = 10)
```

出現回数Top20は以下のようになっています。

```{r}
print(quanteda::topfeatures(dfmt_sent, n = 20))
```

### センチメントスコアの算出

次に、種語として以下の単語を定義します。

```{r}
library(magrittr)
library(LSX)

ja_seedwords <- list(positive = c("安心","好調","秀逸","改善","良好","適切","安堵"),
                     negative=c("不安","不調","稚拙","悪化","不良","過剰","過少","懸念")) %>%
                as.seedwords()
print(ja_seedwords)
```

上記のように、種語のスコアはポジティブ/ネガティブの最大値となっています。  
文書行列に対して特異値分解を行い、センチメントスコアを算出します。Word Embeddingのサイズ$k$は300とします。

```{r}
lss <- textmodel_lss(dfmt_sent, seeds = ja_seedwords, k = 300)
```

センチメントスコア上位・下位20単語を抜き出します。`LSX::coef()`で抜き出すことができ、降順で並んでいます。

```{r}
print(head(coef(lss), 20)) # most positive words
print(tail(coef(lss), 20)) # most negative words
```

ポジティブな単語に「米飯」、「コンクリート」、「デザート」、「フード」が入っていたり、ネガティブな単語に「贈与」、「老後」が入っているなどツッコミどころはあるものの、7割はそれっぽい単語がリストアップされているのではないでしょうか。  
次に、このスコアを用いて、辞書ベース分類法と同様のやり方で文書のセンチメントスコアを算出します。

```{r}
# 文書のセンチメントスコア算出
coef_df <- data.frame(word=names(coef(lss)), values=as.numeric(coef(lss)))
dfmt_sent_arranged <- quanteda::dfm_match(dfmt_sent, coef_df$word)

n <- unname(quanteda::rowSums(dfmt_sent_arranged))
score_lss <- ifelse(n>0, quanteda::rowSums(dfmt_sent_arranged %*% coef_df$values)/n, NA)
sample_with_score_lss <- sample %>% cbind(score_lss)
```

センチメントスコア上位10位のサンプルを確認します。

```{r}
sample_with_score_lss %>% dplyr::slice_max(n=10, order_by = score_lss) %>% dplyr::select(追加説明及び具体的状況の説明, score_lss)
```

非常に良い結果となっているように思えます。
「米飯」、「デザート」、「フード」の謎が解けましたね。「好調」、「改善」、「増加」など肯定的な単語に共起しているため、これらの単語も肯定的と捉えられたようです。サンプルにこれらの単語を含む否定的(または中立)な文章が含まれれば、これらの単語のスコアもあるべき値に収斂すると思われます。  

センチメントスコア下位10位のサンプルも確認しましょう。

```{r}
sample_with_score_lss %>% dplyr::slice_min(n=10, order_by = score_lss) %>% dplyr::select(追加説明及び具体的状況の説明)
```

こちらも良さそうです。

### 文書分類結果

boxplotでデータ全体を見てみましょう。

```{r}
# 結果の可視化
library(ggplot2)
ggplot(sample_with_score_lss, aes(x=景気の現状判断,y=score_lss)) + geom_boxplot()
```

先ほどとは、様相が異なっています。
上図では、横軸に「景気の現状判断」をとっていますが、景気が良い/悪いでセンチメントスコアがあまり変わらない結果となっています。サンプル全体でみると、文書分類を行える特徴量といえます。
念のため、ロジスティック回帰を行ってみます。

```{r}
dataset <- sample_with_score_lss %>% 
            dplyr::mutate(condition=as.factor(dplyr::if_else(景気の現状判断=="○"|景気の現状判断=="◎",1,0))) %>% 
            dplyr::select(condition, score_lss)
sample_recipe <- recipes::recipe(condition~., dataset) %>% 
                  recipes::step_center(recipes::all_predictors()) %>% 
                  recipes::step_scale(recipes::all_predictors()) %>%
                  recipes::prep(dataset)

lr_fitted <- parsnip::logistic_reg() %>% 
              parsnip::set_engine("glm") %>% 
              parsnip::fit(condition~., data=sample_recipe %>% recipes::juice())

lr_fitted %>% 
  purrr::pluck("fit") %>% 
  gtsummary::tbl_regression() %>% 
  gtsummary::add_n()
```

推定したモデルの精度を確認するため、データセットの予測分類を計算し、正解ラベルと突合します。

```{r}
lr_pred <- lr_fitted %>% 
              predict(dataset) %>%
              dplyr::mutate(truth = dataset %>% dplyr::select(condition) %>% dplyr::pull())
```

正解率は約60%となりました。

```{r}
lr_pred %>% yardstick::metrics(truth = truth, estimate = .pred_class)
```

混合行列(Confusion Matrix)を描画します。

```{r}
lr_pred %>% yardstick::conf_mat(truth = truth, estimate = .pred_class) %>% 
              ggplot2::autoplot(type = "heatmap")
```

全く文書分類できていませんね。全てのサンプルの予測値が0(=ネガティブ)となってしまっています。データセットは不均衡ですが、それにしても1(=ポジティブ)の予測がまったくないのは、そもそもの特徴量がいけていないためだと思われます。  
上手くいかない仮説としては、センチメントスコアがゼロ周りに多く分布しているためという理由が考えられます(つまり、センチメントスコアが大きい単語はあまり文章に含まれていない or 様々な言葉で言い換えられている)。
可視化を行い、確認してみます。

```{r}
textplot_terms(lss, names(ja_seedwords))
```

上記の図は縦軸が頻度(対数)、横軸が極性(Polarity、センチメントスコアと解釈)となっています。極性が大きく、頻度が高い単語は種語が多く、頻度の高い単語はゼロ周りに分布していることが確認できました。
語調の強い単語以外は極性の絶対値が低いと出るようで、日本語のように強い表現があまり取られない言語では機能しずらいのかもしれません。

## 3. 終わりに

辞書ベース手法の時と同じく、教師あり学習以外の手法で文書分類を行うことは難しいという印象です。  
次は、深層学習ベースの手法を用いて、語順や文脈を加味した分析を行います！
