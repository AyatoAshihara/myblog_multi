[{"authors":null,"categories":null,"content":"奈良県出身の院卒。2016年に社会人となり、現在は社会人8年目です。専攻はマクロ経済（主にDynamic Stochastic General Equilibrium model）、時系列解析やテキストマイニングにも手を出していました。2018年に新卒で東京の資産運用会社に就職し、主に企画業務に従事。2022年3月より転職し、京都の電子部品メーカーでデータコンサルタントとしてのキャリアパスをスタートしました。広瀬すずより有村架純よりお姉ちゃんのほうがタイプ。\n","date":1522454400,"expirydate":-62135596800,"kind":"term","lang":"ja","lastmod":1522454400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/ayato-ashihara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ayato-ashihara/","section":"authors","summary":"奈良県出身の院卒。2016年に社会人となり、現在は社会人8年","tags":null,"title":"Ayato Ashihara","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"ja","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"\rClick on the Slides button above to view the built-in slides feature.\r\r\rSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["仕事関連"],"content":"1. 転職後の近況について おはこんばんにちは。またまた、かなり久しぶりの投稿になってしまいました。\n2022年3月に今の会社に転職し、早くも4年が経とうとしています。\n転職時の報告は「転職します」で記載しましたので、まずそちらをご覧ください。\n今回は、転職してからの近況について書いてみようと思います。\n2. 仕事について 現在はデータサイエンスやAI関連のプロジェクトを企画する仕事をしています 退職エントリでは、プロジェクトマネージャーとしての仕事にやりがいを感じていると書きました。\n現在でもプロジェクトマネージャーをやるときはありますが、より上流の企画段階から関わることが多くなりました。\nデータサイエンスやAIをどのように用いれば、ビジネス戦略を実現できるかを考える仕事です。\nそして、そのためにどういったデータをどのように蓄積すればよいのか、といったことも考えます。\n解くべき問題を定義し、その解き方を考える仕事です。やりがいを感じています。\n転職してすぐは可視化ダッシュボードを作成していました 転職してすぐは、主にPower BIを用いて可視化ダッシュボードを作る仕事をしていました。\nユーザー部門からデータと要件をもらい、Power BIで認知負荷の低いダッシュボードを作る仕事です。 Power BIは使用したことがありませんでしたが、すぐに慣れました。\n高度な分析を行うことはありませんでしたが、可視化を通じて特に営業部門のドメイン知識を深めることができました。\nまた、可視化に関する本を読み漁り、KTさんに心酔し、意思決定に貢献するためには綿密にダッシュボードを作りこむ必要があることを学びました。\nあるプロジェクトでは、取り組みの内容について社長に直接報告することもありました。\n広く市民開発のサポートへ Power BIでのダッシュボード作成をしばらく続けた後、広く市民開発のサポートをするようになりました。\nまず、UiPathの社内受託開発サービスを立ち上げました。また、主に営業アシスタント向けにPower BIやUiPathを用いた教育プログラムを企画・運営する仕事もしました。\nデジタルツールの活用を促進し、会社全体に対して業務効率化を広めていく仕事にスコープが広がっていきました。教育プログラムの取り組みについては、社長表彰を受賞することができました（びっくり）。\n高度データ分析チームへの配属 2023年に部署が異動になり、高度データ分析を行うチームに配属されました。\n事業損益管理の高度化、機械学習モデルを用いたレコメンドアルゴリズムの開発、損益シミュレーションツールの開発などを行っています。\n過去事例や社外知見を引っ提げて、部門を回り、プロジェクト企画を提案する仕事もしています。\n所属する部の中期方針の策定を任されたり、役員や管理職が参加する重要会議で生成AI活用を企画することもありました。 現在の関心事は、以下のようなことです。\n 売り上げや利益の拡大につながるプロジェクトを企画・達成すること 産業組織論や因果推論の知見を活用して、ビジネス戦略の材料となるナレッジを蓄積すること 非常に多くの社員の意見を可視化、集約し、経営の透明化や社員のエンゲージメント向上に貢献すること 若手メンバーへプロジェクト企画、推進のノウハウを共同化・表出化すること  3. 私生活について 関西で生活できる幸せ 東京から関西に帰ってきて、なにより食べ物がおいしい。行き慣れた店や場所も多く、刺激はないけど安心感がある生活を送っています。お互いの実家が近く、両親とも仲が良いので、家族で集まる機会も多いです。\n子供が生まれた 2025年2月14日に第一子（長男）が生まれました。妻も子供も健康で、毎日幸せな日々を過ごしています。子供が生まれてからは、育児休暇を約半年取得していました。育児休暇中は、子供の世話をしながら、家事をほぼすべて行っていました。\n子供が生まれたことでワークライフバランスに対する考え方が変わり、私生活の時間をより多く確保するようになりました。先日、保育所の入所が決まり、ほっとしているところです。まだ一歳になったばかりなので、関西弁はしゃべりませんが、バリバリの関西弁を仕込むつもりです。\n4. 仕事を変えた所感 市場が拡大していく企業は社内の雰囲気から違う 新卒で入社した会社はアセットマネジメント業界でした。アセットマネジメント業界は、おもに国内のアセットオーナーを顧客とする業界です。日本は人口減少や高齢化が進んでいるため、アセットマネジメント業界は市場が縮小していくと考えられています。そのため、縮小していくパイを数社で奪い合う構造で、コスト削減や効率化が求められ、新しいことに挑戦する雰囲気はありませんでした(あったとしても少数派で大多数は冷ややかな目で見ている)。また、ビジネス戦略と呼べるものもなく、金融専門職はいるが、ビジネスパーソンが不在という状況でした。\n一方、転職して入社した会社は、電子部品業界です。自動車の電装化やAIサーバなどの需要を受け、今後も市場が拡大していくと考えられています。市場が拡大していく企業は、社内の雰囲気から違うと感じました。拡大する需要をどのように取り込んでいくか、中華系企業の台頭にどのように対抗していくか、など競争が激しい領域なので積極的にリスクをとることが求められます。事業部門でビジネス戦略を立てられる人材がいて、その戦略を実現するために我々を頼ってくれるので、やりがいを感じます。\n勉強熱心な社風で自分も技術的に成長した これは社風によるのかもしれませんが、非常に勉強熱心な方が多いです。社内での勉強会は定期的に開催されます。最新のデータサイエンス技術やソフトウェアエンジニアリングのナレッジについて、社員が自発的に発信しています。私も転職してから応用情報技術者を取得し、最近はクラウドサービス（おもにAWS）を活用してアプリケーションを構築することも多いです。フロントエンドもバックエンドも（まだまだ勉強中ですが）自分で構築できるようになりました。\n前職自体のExcel, PowerPointで作業していたころからすると、技術的にかなり成長したと感じます。\n読む本が変わった 以前は経済学とアセットマネジメントに関する本ばかり読んでいました。CLS決済、金融工学、バックオフィス業務などです。\n現在は、色々な範囲の本を読むようになりました（マクロ経済学からは遠ざかった）。\n 統計学（産業組織論と因果推論多め） 機械学習 ソフトウェアエンジニアリング データマネジメント UXデザイン UIデザイン/データビジュアライゼーション フロントエンドエンジニアリング(React, HTML/CSS) 経営戦略 電気回路/電子回路  マクロ経済学からはかなり遠ざかった 大学院ではマクロ経済学を専攻していましたが、転職してからはかなり遠ざかりました。\n時系列分析や金融の知識を生かす機会もあまりありません。\nむしろ、運用企画をやっていたときの管理会計の知識や予算管理、企画立案、プロジェクトマネジメントの経験が生かされていると感じます。\nマクロ経済学は政策立案には役立つと思いますが、中央官庁、日銀、シンクタンクでしか役立たず、なかなかお金になりにくいと思います。ミクロ実証だと営利企業でも役に立つと思いますが。ここは、大学生の皆さんには注意してほしいポイントだと思います。\nビジネスパーソンとしても成長できた 前職では、金融専門職が多く、特にファンドマネジャーを目指す若手が多かったです。ファンドマネジャー以外のキャリアパスが不透明で、ビジネスパーソン（と呼べる人）が少ない環境でした。\n転職してからは、電子部品の専門性が高い人もいれば、BizDevを深める人、アカウント営業を極める人、マーケティングやインサイドセールスの戦略を立てる人など、様々なキャリアパスを歩む人がいます。\n私は、ビジネス戦略を実現するためのデータ戦略を立案する立場で、ビジネスパーソンとしての成長も感じています。今後も、ビジネス部門とデータサイエンス部門をつなぎ、ビジネス成果を出すことにコミットしていきたいと思います。\n今の会社は人がいい 今の会社はやさしい人が多いと感じます。前職も周りの人には恵まれていたと思います。一方で、ただ退職を待ちながら仕事している人やプロフェッショナルさを感じない人もそれなりにいました。また、組織間の事でストレスを感じる機会もそれなりにあったように思います。胃が痛い思いも何度かしました。\n時代が変わっているのかもしれませんが、今の会社は詰めてくるような人がほとんどいません。人間関係のストレスが少ない環境で仕事できていることには感謝です。\n人が多すぎるところはネック いわゆる伝統的な日本の大企業なので、社員数が多いことはネックに感じることもあります。社内の人とコミュニケーションをとるのが大変なこともありますし、社内の人と関わるための会議が多いこともあります。また、役割が重複している部署があったり、情報連携が不足しているなど、組織構造の複雑さも感じることがあります。ここは前職ではあまり感じなかったポイントです。\n電子部品のドメイン知識のキャッチアップが課題 経済学がバックグラウンドの私にとって、電子部品は全く未経験の領域です。いまだに商品に対する理解は不十分だと感じています。育休中には、電気回路・電子回路に関する入門本を読んでみましたが、まだまだ理解が足りないと感じています。また、業界の構造や競合企業の動向なども理解が不十分だと感じています。お客様がどのような課題を抱えているのか、どのようなニーズがあるのかを理解するためにも、ドメイン知識のキャッチアップは引き続き課題だと感じています。\n5. おわりに 転職してからの近況について書いてみました。 仕事も私生活も充実していて、幸せな日々を過ごしています。転職は正解でした。機会に恵まれていたと思います。 今後は、日本を飛び出して海外拠点で働いてみたいなぁと思い始めています。\n ","date":1771632000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1771718400,"objectID":"0aecf5b72bebea19542a7bea21684d23","permalink":"/post/tenshoku-kinjou/","publishdate":"2026-02-21T00:00:00Z","relpermalink":"/post/tenshoku-kinjou/","section":"post","summary":"転職してからの仕事や生活について書きました。","tags":["近況","転職"],"title":"転職後の近況について","type":"post"},{"authors":null,"categories":["単発"],"content":"1. 単語埋め込み(Word Embedding)手法について 前回までのpostで、教師なし学習である辞書ベース分類法と教師あり学習であるナイーブベイズ分類器を紹介しました。教師あり学習のほうが精度は出そうだが、教師データの準備にコストがかかり、教師なし学習は教師データは不要である一方、精度に課題があります。どちらも一長一短がある手法であり、この両者を埋め合わせる良い手法はないのかと調べていると準教師学習を用いた手法であるLatent Semantic Scalingを発見しました。\nLatent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages(watanabe2020?)\nこの手法は単語埋め込み(word embedding)を用いているため、まずはそちらを紹介します。\n単語埋め込み(Word Embedding)とは この論文では単語埋め込み(Word Embedding)を使用しています。単語埋め込み(Word Embedding)とは、自然言語処理における言語モデリング手法の一種であり、単語や語句を固定長の実数ベクトルとして表現する手法のことです。\nWorks Applicationsが提供しているモデルChiveを例とすると、以下のように各単語にその単語の特徴を表現する300次元の実数ベクトル(=Embeddingベクトル)が与えられています。\nchive_path = r\u0026#34;C:\\Users\\hogehoge\\Watcher\\chive-1.2-mc15_gensim\\chive-1.2-mc15.kv\u0026#34; import gensim vectors = gensim.models.KeyedVectors.load(chive_path) print(\u0026#34;Embeddingベクトルの次元数：\u0026#34;, vectors[\u0026#34;王様\u0026#34;].shape[0]) ## Embeddingベクトルの次元数： 300\r print(\u0026#34;Embeddingベクトルの値：\u0026#34;, vectors[\u0026#34;王様\u0026#34;][1:10]) ## Embeddingベクトルの値： [ 0.08844764 -0.02172066 0.11218461 0.00986984 -0.26948628 -0.14956778\r## 0.08493619 0.16736646 0.03643996]\r このEmbeddingベクトルを用いて、単語同士の演算を行うことも可能です。例えば、「王様 - 男 + 女 = 女王」のようなもので、具体的には「王様」のEmbeddingベクトルから「男」のEmbeddingベクトルを減算、「女」のEmbeddingベクトルを加算し、計算結果と最も値が近い単語を返す処理をします。\nprint(\u0026#34;王様 - 男 - 女：\u0026#34;, vectors.most_similar(positive=[\u0026#34;王様\u0026#34;,\u0026#34;女\u0026#34;], negative=[\u0026#34;男\u0026#34;], topn=3)) ## 王様 - 男 - 女： [('王女', 0.5806534886360168), ('女王', 0.5754266381263733), ('王妃', 0.5491030216217041)]\r 上記では「王女」が最も近い単語となっていますが、2番目が「女王」であり、意図した結果が返ったことが分かります。\n以下のように、各単語のEmbeddingベクトルのコサイン類似度を測ることによって、或る単語と意味が近い単語を調べることも可能です。\nprint(\u0026#34;関西学院大学と意味の近い単語上位3位：\u0026#34;, vectors.most_similar([\u0026#34;関西学院大学\u0026#34;],topn=3)) ## 関西学院大学と意味の近い単語上位3位： [('関西大学', 0.798370361328125), ('立命館大学', 0.7937482595443726), ('同志社大学', 0.7829927206039429)]\r やはり関関同立というだけあって想定通りの結果です。また、その順番も多くの人のイメージ通りかも知れません(同じキリスト教の大学と言うことであれば、同志社大学の近いという見方もあるかも知れません)。\nでは、この実数ベクトルをどのように計算しているのでしょうか。Word Embeddingの手法としては、Word2Vec(Efficient Estimation of Word Representations in Vector Space (arxiv.org))が有名です。\nLatent Semantic Scalingとは 政治学者である渡辺耕平さんによって提案されたWord Embeddingを利用してセンチメントスコアを単語へ付与する手法です。種語(Seed Words)と呼ばれる抽象的な意味を持つ幾つかの単語からの近さでセンチメントスコアの付与対象とする単語群の実数ベクトルを計算する準教師学習となっており、教師あり学習、教師なし学習のメリデメを補完しています。\n  教師あり学習\nメリット - 学習のスピードが速く、精度も高い\nデメリット - 教師データを整備するのが大変\n  教師なし学習\nメリット - 教師データを用意する必要がないので、分析を始めやすい\nデメリット - 学習のスピードが遅く、精度も低い\n  Word Embeddingの算出には特異値分解を用いています。各要素がその文書(行)におけるその単語(列)の出現頻度が入力されている行列である文書行列に対して特異値分解を行い、各単語のEmbeddingベクトルを計算します。ここで文書行列を$D$、文書行列$D$における単語(列)数を$n$とすると、\n$$ D \\approx U \\Sigma V' $$\nと分解でき、$V$が$k×n$行列となるので各単語の特徴量ベクトルと見なすことができます($k$は分析者が決定)。$V$のうち$i$列目のベクトルと$V_i$と表すと、センチメントスコアは種語$s \\in S$の特徴量ベクトル$V_s$と各単語の特徴量ベクトル$V_j$のコサイン類似度から算出します。単語$f$のセンチメントスコアを$g_f$とすると、算出式は\n$$ g_f = \\frac{1}{|S|}\\sum_{s\\in S} \\cos(v_s,v_f)p_s $$\nとなります。ここで、$p_s$は分析者が定める種語のセンチメントスコアです。種語が複数個存在する場合には平均値を使用します。。\n次に、単語のセンチメントスコアを用いて文書のセンチメントスコアを算出します。文書$d$のセンチメントスコアを$y_d$とすると、その文書に含まれる単語$f\\in F$を用いて\n$$ y_d = \\frac{1}{N}\\sum_{f\\in F}g_fh_f $$\nで計算します。ここで、$h_f$は各単語$f$のその文書内での出現回数、$N$はその文書に含まれる単語の総数です($V$に含まれない単語は除く)。\n2. Latent Semantic Scalingの実践 Latent Semantic Scalingによるセンチメントスコアの算出を行います。\n前処理 Rで行っていきます。まず、サンプルデータを読み込みます。\nfilepath \u0026lt;- r\u0026#34;(C:\\Users\\hogehoge\\Watcher\\RawData)\u0026#34; library(magrittr) files \u0026lt;- stringr::str_c(filepath, list.files(filepath, pattern = \u0026#34;*.csv\u0026#34;)) sample \u0026lt;- readr::read_csv(files,locale=readr::locale(encoding=\u0026#34;Shift-JIS\u0026#34;),show_col_types = FALSE) sample \u0026lt;- sample[sample$追加説明及び具体的状況の説明!=\u0026#34;−\u0026#34;|sample$追加説明及び具体的状況の説明!=\u0026#34;＊\u0026#34;,] sample \u0026lt;- sample[sample$景気の現状判断!=\u0026#34;□\u0026#34;,] sample$景気の現状判断 \u0026lt;- factor(sample$景気の現状判断, levels=c(\u0026#34;◎\u0026#34;,\u0026#34;○\u0026#34;,\u0026#34;▲\u0026#34;,\u0026#34;×\u0026#34;)) 次に、sudachiを用いてテキストデータのToken化を行います。Rにはsudachiを使えるパッケージが存在しないので、Pythonパッケージのsudachipyをreticulateで呼び出して使用します。\n# sudachiによる形態素解析→Token化 # コーパス生成 corp \u0026lt;- quanteda::corpus(sample,text_field=\u0026#34;追加説明及び具体的状況の説明\u0026#34;) # sudachipy呼び出し→インスタンス化 sudachipy \u0026lt;- reticulate::import(\u0026#34;sudachipy\u0026#34;) # 正規化関数定義 sudachi_normalize \u0026lt;- function(tokens){ res \u0026lt;- c() for(i in 0:(length(tokens)-1)){ res \u0026lt;- append(res,tokens[i]$normalized_form()) } return(res) } # tokenize関数定義 sudachi_tokenize \u0026lt;- function(sentence, mode = \u0026#34;A\u0026#34;){ tokenizer_obj \u0026lt;- sudachipy$dictionary$Dictionary()$create() mode \u0026lt;- switch(mode, \u0026#34;A\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$A, \u0026#34;B\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$B, \u0026#34;C\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$C, stop(\u0026#34;Only Can Use A, B, C\u0026#34;)) res \u0026lt;- purrr::map(sentence, ~sudachi_normalize(tokenizer_obj$tokenize(., mode))) return(res) } stopwords \u0026lt;- c(\u0026#34;の\u0026#34;,\u0026#34;に\u0026#34;,\u0026#34;は\u0026#34;,\u0026#34;を\u0026#34;,\u0026#34;た\u0026#34;,\u0026#34;が\u0026#34;,\u0026#34;で\u0026#34;,\u0026#34;て\u0026#34;,\u0026#34;と\u0026#34;,\u0026#34;し\u0026#34;,\u0026#34;れ\u0026#34;,\u0026#34;さ\u0026#34;,\u0026#34;ある\u0026#34;,\u0026#34;いる\u0026#34;, \u0026#34;も\u0026#34;,\u0026#34;する\u0026#34;,\u0026#34;から\u0026#34;,\u0026#34;な\u0026#34;,\u0026#34;こと\u0026#34;,\u0026#34;い\u0026#34;,\u0026#34;や\u0026#34;,\u0026#34;れる\u0026#34;,\u0026#34;など\u0026#34;,\u0026#34;なっ\u0026#34;,\u0026#34;ない\u0026#34;,\u0026#34;この\u0026#34;, \u0026#34;ため\u0026#34;,\u0026#34;その\u0026#34;,\u0026#34;あっ\u0026#34;,\u0026#34;よう\u0026#34;,\u0026#34;また\u0026#34;,\u0026#34;もの\u0026#34;,\u0026#34;あり\u0026#34;,\u0026#34;まで\u0026#34;,\u0026#34;られ\u0026#34;,\u0026#34;なる\u0026#34;,\u0026#34;へ\u0026#34;, \u0026#34;か\u0026#34;,\u0026#34;だ\u0026#34;,\u0026#34;これ\u0026#34;,\u0026#34;おり\u0026#34;,\u0026#34;より\u0026#34;,\u0026#34;ず\u0026#34;,\u0026#34;なり\u0026#34;,\u0026#34;られる\u0026#34;,\u0026#34;ば\u0026#34;,\u0026#34;なかっ\u0026#34;,\u0026#34;なく\u0026#34;, \u0026#34;しかし\u0026#34;,\u0026#34;せ\u0026#34;,\u0026#34;だっ\u0026#34;,\u0026#34;できる\u0026#34;,\u0026#34;それ\u0026#34;,\u0026#34;う\u0026#34;,\u0026#34;なお\u0026#34;,\u0026#34;のみ\u0026#34;,\u0026#34;でき\u0026#34;,\u0026#34;き\u0026#34;,\u0026#34;つ\u0026#34;, \u0026#34;および\u0026#34;,\u0026#34;いう\u0026#34;,\u0026#34;さらに\u0026#34;,\u0026#34;ら\u0026#34;,\u0026#34;たり\u0026#34;,\u0026#34;たち\u0026#34;,\u0026#34;ます\u0026#34;,\u0026#34;ん\u0026#34;,\u0026#34;なら\u0026#34;,\u0026#34;特に\u0026#34;, \u0026#34;せる\u0026#34;,\u0026#34;及び\u0026#34;,\u0026#34;とき\u0026#34;,\u0026#34;にて\u0026#34;,\u0026#34;ほか\u0026#34;,\u0026#34;ながら\u0026#34;,\u0026#34;うち\u0026#34;,\u0026#34;そして\u0026#34;,\u0026#34;ただし\u0026#34;, \u0026#34;かつて\u0026#34;,\u0026#34;それぞれ\u0026#34;,\u0026#34;お\u0026#34;,\u0026#34;ほど\u0026#34;,\u0026#34;ほとんど\u0026#34;,\u0026#34;です\u0026#34;,\u0026#34;とも\u0026#34;,\u0026#34;ところ\u0026#34;,\u0026#34;ここ\u0026#34;, \u0026#34;居る\u0026#34;,\u0026#34;為る\u0026#34;,\u0026#34;有る\u0026#34;,\u0026#34;成る\u0026#34;,\u0026#34;来る\u0026#34;,\u0026#34;よる\u0026#34;,\u0026#34;おる\u0026#34;) # Token化実行 toks_sent \u0026lt;- corp %\u0026gt;% sudachi_tokenize() %\u0026gt;% quanteda::as.tokens() %\u0026gt;% quanteda::tokens_remove(quanteda::stopwords(\u0026#34;ja\u0026#34;, source = \u0026#34;marimo\u0026#34;)) %\u0026gt;% quanteda::tokens_remove(c(\u0026#34;、\u0026#34;, \u0026#34;。\u0026#34;, \u0026#34;・\u0026#34;)) %\u0026gt;% quanteda::tokens_remove(stopwords) # メタデータをTokenに再付与 quanteda::docvars(toks_sent) \u0026lt;- quanteda::docvars(corp) このtoks_sentから文書行列を生成します。文書行列とは、単語と文書の関係を表す行列で、各行が単語(token)、各列が文書を表し、各要素は文書中の単語の出現回数となっています。\n# 文書行列の生成 dfmt_sent \u0026lt;- toks_sent %\u0026gt;% quanteda::dfm() %\u0026gt;% quanteda::dfm_remove(pattern=\u0026#34;\u0026#34;) %\u0026gt;% quanteda::dfm_trim(min_termfreq = 10) 出現回数Top20は以下のようになっています。\nprint(quanteda::topfeatures(dfmt_sent, n = 20)) ## 数 客 前年 売り上げ 影響 来客 減少 新型 ## 11912 11752 11238 9461 9247 7908 7656 6100 ## コロナ % ウイルス 販売 良い 為 増える 動き ## 5928 5893 5865 5114 5045 5037 4832 4602 ## 状況 3 増加 量 ## 4590 4587 4483 4402\r センチメントスコアの算出 次に、種語として以下の単語を定義します。\nlibrary(magrittr) library(LSX) ja_seedwords \u0026lt;- list(positive = c(\u0026#34;安心\u0026#34;,\u0026#34;好調\u0026#34;,\u0026#34;秀逸\u0026#34;,\u0026#34;改善\u0026#34;,\u0026#34;良好\u0026#34;,\u0026#34;適切\u0026#34;,\u0026#34;安堵\u0026#34;), negative=c(\u0026#34;不安\u0026#34;,\u0026#34;不調\u0026#34;,\u0026#34;稚拙\u0026#34;,\u0026#34;悪化\u0026#34;,\u0026#34;不良\u0026#34;,\u0026#34;過剰\u0026#34;,\u0026#34;過少\u0026#34;,\u0026#34;懸念\u0026#34;)) %\u0026gt;% as.seedwords() print(ja_seedwords) ## 安心 好調 秀逸 改善 良好 適切 安堵 不安 不調 稚拙 悪化 不良 過剰 過少 懸念 ## 1 1 1 1 1 1 1 -1 -1 -1 -1 -1 -1 -1 -1\r 上記のように、種語のスコアはポジティブ/ネガティブの最大値となっています。\n文書行列に対して特異値分解を行い、センチメントスコアを算出します。Word Embeddingのサイズ$k$は300とします。\nlss \u0026lt;- textmodel_lss(dfmt_sent, seeds = ja_seedwords, k = 300) センチメントスコア上位・下位20単語を抜き出します。LSX::coef()で抜き出すことができ、降順で並んでいます。\nprint(head(coef(lss), 20)) # most positive words ## 安心 改善 好調 重点 米飯 前期 ## 0.15075889 0.14272040 0.12926093 0.12218111 0.11418503 0.11297516 ## di コンクリート デザート 誕生 フード 調理 ## 0.10698720 0.10633358 0.09668659 0.09597283 0.09563199 0.09549041 ## 努める ファースト 政権 占める 良好 景況 ## 0.09523555 0.09523126 0.09446756 0.09153319 0.09054534 0.09019622 ## 今期 有り難い ## 0.09003611 0.08986984\r print(tail(coef(lss), 20)) # most negative words ## 贈与 老後 ローン 繰り 不良 減税 ## -0.09561125 -0.09683069 -0.09875152 -0.09982278 -0.10067741 -0.10109826 ## 韓国 益々 情勢 煽る 不調 日韓 ## -0.10140439 -0.10567625 -0.10567861 -0.10844798 -0.11014713 -0.11054370 ## 将来 要素 先行き 辿る 一途 不安 ## -0.11360784 -0.11364341 -0.11439845 -0.11930636 -0.11956281 -0.13344996 ## 悪化 懸念 ## -0.15213723 -0.15636390\r ポジティブな単語に「米飯」、「コンクリート」、「デザート」、「フード」が入っていたり、ネガティブな単語に「贈与」、「老後」が入っているなどツッコミどころはあるものの、7割はそれっぽい単語がリストアップされているのではないでしょうか。\n次に、このスコアを用いて、辞書ベース分類法と同様のやり方で文書のセンチメントスコアを算出します。\n# 文書のセンチメントスコア算出 coef_df \u0026lt;- data.frame(word=names(coef(lss)), values=as.numeric(coef(lss))) dfmt_sent_arranged \u0026lt;- quanteda::dfm_match(dfmt_sent, coef_df$word) n \u0026lt;- unname(quanteda::rowSums(dfmt_sent_arranged)) score_lss \u0026lt;- ifelse(n\u0026gt;0, quanteda::rowSums(dfmt_sent_arranged %*% coef_df$values)/n, NA) sample_with_score_lss \u0026lt;- sample %\u0026gt;% cbind(score_lss) センチメントスコア上位10位のサンプルを確認します。\nsample_with_score_lss %\u0026gt;% dplyr::slice_max(n=10, order_by = score_lss) %\u0026gt;% dplyr::select(追加説明及び具体的状況の説明, score_lss) ## 追加説明及び具体的状況の説明\r## 1 ・景況感が良好である。\r## 2 ・好調に推移している。\r## 3 ・受注が好調である。\r## 4 ・主要商品である米飯、調理パンやファストフードが好調に推移している。企画商品のデリカも好調である。\r## 5 ・県内企業の景況感は改善している。業況判断ＤＩが製造業で前期に比べて改善し、プラス水準を維持している。\r## 6 ・客先の決算が好調である。納税額も増えている。\r## 7 ・ファーストフード、デザートの陳列を増やしたので売上が増加した。\r## 8 ・ファーストフード、デザートの陳列を増やしたので売上が増加した。\r## 9 ・受注量が好調である。\r## 10 ・受注量は好調である。\r## score_lss\r## 1 0.08265024\r## 2 0.07564103\r## 3 0.07239461\r## 4 0.05859545\r## 5 0.05651628\r## 6 0.05048285\r## 7 0.04896476\r## 8 0.04896476\r## 9 0.04726825\r## 10 0.04726825\r 非常に良い結果となっているように思えます。 「米飯」、「デザート」、「フード」の謎が解けましたね。「好調」、「改善」、「増加」など肯定的な単語に共起しているため、これらの単語も肯定的と捉えられたようです。サンプルにこれらの単語を含む否定的(または中立)な文章が含まれれば、これらの単語のスコアもあるべき値に収斂すると思われます。\nセンチメントスコア下位10位のサンプルも確認しましょう。\nsample_with_score_lss %\u0026gt;% dplyr::slice_min(n=10, order_by = score_lss) %\u0026gt;% dplyr::select(追加説明及び具体的状況の説明) ## 追加説明及び具体的状況の説明\r## 1 ・全てにおいて悪化している。\r## 2 ・様々な問題で悪化している。\r## 3 ・客は、先行きが不安で買い控えをしている。\r## 4 ・景気が悪化している。\r## 5 ・原油の値上がりが懸念材料になってくる。\r## 6 ・先行きが見通せない。\r## 7 ・先行きが見通せない。\r## 8 ・為替相場や株安により、先行きに不安がある。\r## 9 ・客の業績が悪化している。\r## 10 ・海外情勢への不安要素が一層顕著に表れてきている。\r こちらも良さそうです。\n文書分類結果 boxplotでデータ全体を見てみましょう。\n# 結果の可視化 library(ggplot2) ggplot(sample_with_score_lss, aes(x=景気の現状判断,y=score_lss)) + geom_boxplot() 先ほどとは、様相が異なっています。 上図では、横軸に「景気の現状判断」をとっていますが、景気が良い/悪いでセンチメントスコアがあまり変わらない結果となっています。サンプル全体でみると、文書分類を行える特徴量といえます。 念のため、ロジスティック回帰を行ってみます。\ndataset \u0026lt;- sample_with_score_lss %\u0026gt;% dplyr::mutate(condition=as.factor(dplyr::if_else(景気の現状判断==\u0026#34;○\u0026#34;|景気の現状判断==\u0026#34;◎\u0026#34;,1,0))) %\u0026gt;% dplyr::select(condition, score_lss) sample_recipe \u0026lt;- recipes::recipe(condition~., dataset) %\u0026gt;% recipes::step_center(recipes::all_predictors()) %\u0026gt;% recipes::step_scale(recipes::all_predictors()) %\u0026gt;% recipes::prep(dataset) lr_fitted \u0026lt;- parsnip::logistic_reg() %\u0026gt;% parsnip::set_engine(\u0026#34;glm\u0026#34;) %\u0026gt;% parsnip::fit(condition~., data=sample_recipe %\u0026gt;% recipes::juice()) lr_fitted %\u0026gt;% purrr::pluck(\u0026#34;fit\u0026#34;) %\u0026gt;% gtsummary::tbl_regression() %\u0026gt;% gtsummary::add_n() html {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#wtlwlowqss .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n#wtlwlowqss .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n#wtlwlowqss .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n#wtlwlowqss .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n#wtlwlowqss .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n#wtlwlowqss .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n#wtlwlowqss .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n#wtlwlowqss .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n#wtlwlowqss .gt_column_spanner_outer:first-child { padding-left: 0; }\n#wtlwlowqss .gt_column_spanner_outer:last-child { padding-right: 0; }\n#wtlwlowqss .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n#wtlwlowqss .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; }\n#wtlwlowqss .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n#wtlwlowqss .gt_from_md \u0026gt; :first-child { margin-top: 0; }\n#wtlwlowqss .gt_from_md \u0026gt; :last-child { margin-bottom: 0; }\n#wtlwlowqss .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n#wtlwlowqss .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n#wtlwlowqss .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n#wtlwlowqss .gt_row_group_first td { border-top-width: 2px; }\n#wtlwlowqss .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; }\n#wtlwlowqss .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; }\n#wtlwlowqss .gt_first_summary_row.thick { border-top-width: 2px; }\n#wtlwlowqss .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n#wtlwlowqss .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; }\n#wtlwlowqss .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; }\n#wtlwlowqss .gt_striped { background-color: rgba(128, 128, 128, 0.05); }\n#wtlwlowqss .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n#wtlwlowqss .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n#wtlwlowqss .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; }\n#wtlwlowqss .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n#wtlwlowqss .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; }\n#wtlwlowqss .gt_left { text-align: left; }\n#wtlwlowqss .gt_center { text-align: center; }\n#wtlwlowqss .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n#wtlwlowqss .gt_font_normal { font-weight: normal; }\n#wtlwlowqss .gt_font_bold { font-weight: bold; }\n#wtlwlowqss .gt_font_italic { font-style: italic; }\n#wtlwlowqss .gt_super { font-size: 65%; }\n#wtlwlowqss .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; }\n#wtlwlowqss .gt_asterisk { font-size: 100%; vertical-align: 0; }\n#wtlwlowqss .gt_slash_mark { font-size: 0.7em; line-height: 0.7em; vertical-align: 0.15em; }\n#wtlwlowqss .gt_fraction_numerator { font-size: 0.6em; line-height: 0.6em; vertical-align: 0.45em; }\n#wtlwlowqss .gt_fraction_denominator { font-size: 0.6em; line-height: 0.6em; vertical-align: -0.05em; } \n\rCharacteristic\rN\rlog(OR)1\r95% CI1\rp-value\r\r\rscore_lss\r44,944\r0.55\r0.53, 0.57\r\r\r\r1 OR = Odds Ratio, CI = Confidence Interval\r\r\r\r\r推定したモデルの精度を確認するため、データセットの予測分類を計算し、正解ラベルと突合します。\nlr_pred \u0026lt;- lr_fitted %\u0026gt;% predict(dataset) %\u0026gt;% dplyr::mutate(truth = dataset %\u0026gt;% dplyr::select(condition) %\u0026gt;% dplyr::pull()) 正解率は約60%となりました。\nlr_pred %\u0026gt;% yardstick::metrics(truth = truth, estimate = .pred_class) ## # A tibble: 2 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.612\r## 2 kap binary 0\r 混合行列(Confusion Matrix)を描画します。\nlr_pred %\u0026gt;% yardstick::conf_mat(truth = truth, estimate = .pred_class) %\u0026gt;% ggplot2::autoplot(type = \u0026#34;heatmap\u0026#34;) 全く文書分類できていませんね。全てのサンプルの予測値が0(=ネガティブ)となってしまっています。データセットは不均衡ですが、それにしても1(=ポジティブ)の予測がまったくないのは、そもそもの特徴量がいけていないためだと思われます。\n上手くいかない仮説としては、センチメントスコアがゼロ周りに多く分布しているためという理由が考えられます(つまり、センチメントスコアが大きい単語はあまり文章に含まれていない or 様々な言葉で言い換えられている)。 可視化を行い、確認してみます。\ntextplot_terms(lss, names(ja_seedwords)) 上記の図は縦軸が頻度(対数)、横軸が極性(Polarity、センチメントスコアと解釈)となっています。極性が大きく、頻度が高い単語は種語が多く、頻度の高い単語はゼロ周りに分布していることが確認できました。 語調の強い単語以外は極性の絶対値が低いと出るようで、日本語のように強い表現があまり取られない言語では機能しずらいのかもしれません。\n3. 終わりに 辞書ベース手法の時と同じく、教師あり学習以外の手法で文書分類を行うことは難しいという印象です。\n次は、深層学習ベースの手法を用いて、語順や文脈を加味した分析を行います！\n","date":1648857600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1648857600,"objectID":"82d1564dffa95cb75fdb9b7cdaccf49a","permalink":"/post/post29/","publishdate":"2022-04-02T00:00:00Z","relpermalink":"/post/post29/","section":"post","summary":"単語埋め込み(word embedding)を用いたセンチメントスコアの算出を実践します！","tags":["Python","前処理","機械学習","テキスト解析"],"title":"【徹底比較】センチメントスコア算出手法！！ - 第4回","type":"post"},{"authors":null,"categories":["単発"],"content":"1. 単語埋め込み(Word Embedding)手法について 前回までのpostで、教師なし学習である辞書ベース分類法と教師あり学習であるナイーブベイズ分類器を紹介しました。教師あり学習のほうが精度は出そうだが、教師データの準備にコストがかかり、教師なし学習は教師データは不要である一方、精度に課題があります。どちらも一長一短がある手法であり、この両者を埋め合わせる良い手法はないのかと調べていると準教師学習を用いた手法であるLatent Semantic Scalingを発見しました。\nLatent Semantic Scaling: A Semisupervised Text Analysis Technique for New Domains and Languages(watanabe2020?)\nこの手法は単語埋め込み(word embedding)を用いているため、まずはそちらを紹介します。\n単語埋め込み(Word Embedding)とは この論文では単語埋め込み(Word Embedding)を使用しています。単語埋め込み(Word Embedding)とは、自然言語処理における言語モデリング手法の一種であり、単語や語句を固定長の実数ベクトルとして表現する手法のことです。\nWorks Applicationsが提供しているモデルChiveを例とすると、以下のように各単語にその単語の特徴を表現する300次元の実数ベクトル(=Embeddingベクトル)が与えられています。\nchive_path = r\u0026#34;C:\\Users\\hogehoge\\Watcher\\chive-1.2-mc15_gensim\\chive-1.2-mc15.kv\u0026#34; import gensim vectors = gensim.models.KeyedVectors.load(chive_path) print(\u0026#34;Embeddingベクトルの次元数：\u0026#34;, vectors[\u0026#34;王様\u0026#34;].shape[0]) ## Embeddingベクトルの次元数： 300\r print(\u0026#34;Embeddingベクトルの値：\u0026#34;, vectors[\u0026#34;王様\u0026#34;][1:10]) ## Embeddingベクトルの値： [ 0.08844764 -0.02172066 0.11218461 0.00986984 -0.26948628 -0.14956778\r## 0.08493619 0.16736646 0.03643996]\r このEmbeddingベクトルを用いて、単語同士の演算を行うことも可能です。例えば、「王様 - 男 + 女 = 女王」のようなもので、具体的には「王様」のEmbeddingベクトルから「男」のEmbeddingベクトルを減算、「女」のEmbeddingベクトルを加算し、計算結果と最も値が近い単語を返す処理をします。\nprint(\u0026#34;王様 - 男 - 女：\u0026#34;, vectors.most_similar(positive=[\u0026#34;王様\u0026#34;,\u0026#34;女\u0026#34;], negative=[\u0026#34;男\u0026#34;], topn=3)) ## 王様 - 男 - 女： [('王女', 0.5806534886360168), ('女王', 0.5754266381263733), ('王妃', 0.5491030216217041)]\r 上記では「王女」が最も近い単語となっていますが、2番目が「女王」であり、意図した結果が返ったことが分かります。\n以下のように、各単語のEmbeddingベクトルのコサイン類似度を測ることによって、或る単語と意味が近い単語を調べることも可能です。\nprint(\u0026#34;関西学院大学と意味の近い単語上位3位：\u0026#34;, vectors.most_similar([\u0026#34;関西学院大学\u0026#34;],topn=3)) ## 関西学院大学と意味の近い単語上位3位： [('関西大学', 0.798370361328125), ('立命館大学', 0.7937482595443726), ('同志社大学', 0.7829927206039429)]\r やはり関関同立というだけあって想定通りの結果です。また、その順番も多くの人のイメージ通りかも知れません(同じキリスト教の大学と言うことであれば、同志社大学の近いという見方もあるかも知れません)。\nでは、この実数ベクトルをどのように計算しているのでしょうか。Word Embeddingの手法としては、Word2Vec(Efficient Estimation of Word Representations in Vector Space (arxiv.org))が有名です。\nLatent Semantic Scalingとは 政治学者である渡辺耕平さんによって提案されたWord Embeddingを利用してセンチメントスコアを単語へ付与する手法です。種語(Seed Words)と呼ばれる抽象的な意味を持つ幾つかの単語からの近さでセンチメントスコアの付与対象とする単語群の実数ベクトルを計算する準教師学習となっており、教師あり学習、教師なし学習のメリデメを補完しています。\n  教師あり学習\nメリット - 学習のスピードが速く、精度も高い\nデメリット - 教師データを整備するのが大変\n  教師なし学習\nメリット - 教師データを用意する必要がないので、分析を始めやすい\nデメリット - 学習のスピードが遅く、精度も低い\n  Word Embeddingの算出には特異値分解を用いています。各要素がその文書(行)におけるその単語(列)の出現頻度が入力されている行列である文書行列に対して特異値分解を行い、各単語のEmbeddingベクトルを計算します。ここで文書行列を$D$、文書行列$D$における単語(列)数を$n$とすると、\n$$ D \\approx U \\Sigma V' $$\nと分解でき、$V$が$k×n$行列となるので各単語の特徴量ベクトルと見なすことができます($k$は分析者が決定)。$V$のうち$i$列目のベクトルと$V_i$と表すと、センチメントスコアは種語$s \\in S$の特徴量ベクトル$V_s$と各単語の特徴量ベクトル$V_j$のコサイン類似度から算出します。単語$f$のセンチメントスコアを$g_f$とすると、算出式は\n$$ g_f = \\frac{1}{|S|}\\sum_{s\\in S} \\cos(v_s,v_f)p_s $$\nとなります。ここで、$p_s$は分析者が定める種語のセンチメントスコアです。種語が複数個存在する場合には平均値を使用します。。\n次に、単語のセンチメントスコアを用いて文書のセンチメントスコアを算出します。文書$d$のセンチメントスコアを$y_d$とすると、その文書に含まれる単語$f\\in F$を用いて\n$$ y_d = \\frac{1}{N}\\sum_{f\\in F}g_fh_f $$\nで計算します。ここで、$h_f$は各単語$f$のその文書内での出現回数、$N$はその文書に含まれる単語の総数です($V$に含まれない単語は除く)。\n2. Latent Semantic Scalingの実践 Latent Semantic Scalingによるセンチメントスコアの算出を行います。\n前処理 Rで行っていきます。まず、サンプルデータを読み込みます。\nfilepath \u0026lt;- r\u0026#34;(C:\\Users\\hogehoge\\Watcher\\RawData)\u0026#34; library(magrittr) files \u0026lt;- stringr::str_c(filepath, list.files(filepath, pattern = \u0026#34;*.csv\u0026#34;)) sample \u0026lt;- readr::read_csv(files,locale=readr::locale(encoding=\u0026#34;Shift-JIS\u0026#34;),show_col_types = FALSE) sample \u0026lt;- sample[sample$追加説明及び具体的状況の説明!=\u0026#34;−\u0026#34;|sample$追加説明及び具体的状況の説明!=\u0026#34;＊\u0026#34;,] sample \u0026lt;- sample[sample$景気の現状判断!=\u0026#34;□\u0026#34;,] sample$景気の現状判断 \u0026lt;- factor(sample$景気の現状判断, levels=c(\u0026#34;◎\u0026#34;,\u0026#34;○\u0026#34;,\u0026#34;▲\u0026#34;,\u0026#34;×\u0026#34;)) 次に、sudachiを用いてテキストデータのToken化を行います。Rにはsudachiを使えるパッケージが存在しないので、Pythonパッケージのsudachipyをreticulateで呼び出して使用します。\n# sudachiによる形態素解析→Token化 # コーパス生成 corp \u0026lt;- quanteda::corpus(sample,text_field=\u0026#34;追加説明及び具体的状況の説明\u0026#34;) # sudachipy呼び出し→インスタンス化 sudachipy \u0026lt;- reticulate::import(\u0026#34;sudachipy\u0026#34;) # 正規化関数定義 sudachi_normalize \u0026lt;- function(tokens){ res \u0026lt;- c() for(i in 0:(length(tokens)-1)){ res \u0026lt;- append(res,tokens[i]$normalized_form()) } return(res) } # tokenize関数定義 sudachi_tokenize \u0026lt;- function(sentence, mode = \u0026#34;A\u0026#34;){ tokenizer_obj \u0026lt;- sudachipy$dictionary$Dictionary()$create() mode \u0026lt;- switch(mode, \u0026#34;A\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$A, \u0026#34;B\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$B, \u0026#34;C\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$C, stop(\u0026#34;Only Can Use A, B, C\u0026#34;)) res \u0026lt;- purrr::map(sentence, ~sudachi_normalize(tokenizer_obj$tokenize(., mode))) return(res) } stopwords \u0026lt;- c(\u0026#34;の\u0026#34;,\u0026#34;に\u0026#34;,\u0026#34;は\u0026#34;,\u0026#34;を\u0026#34;,\u0026#34;た\u0026#34;,\u0026#34;が\u0026#34;,\u0026#34;で\u0026#34;,\u0026#34;て\u0026#34;,\u0026#34;と\u0026#34;,\u0026#34;し\u0026#34;,\u0026#34;れ\u0026#34;,\u0026#34;さ\u0026#34;,\u0026#34;ある\u0026#34;,\u0026#34;いる\u0026#34;, \u0026#34;も\u0026#34;,\u0026#34;する\u0026#34;,\u0026#34;から\u0026#34;,\u0026#34;な\u0026#34;,\u0026#34;こと\u0026#34;,\u0026#34;い\u0026#34;,\u0026#34;や\u0026#34;,\u0026#34;れる\u0026#34;,\u0026#34;など\u0026#34;,\u0026#34;なっ\u0026#34;,\u0026#34;ない\u0026#34;,\u0026#34;この\u0026#34;, \u0026#34;ため\u0026#34;,\u0026#34;その\u0026#34;,\u0026#34;あっ\u0026#34;,\u0026#34;よう\u0026#34;,\u0026#34;また\u0026#34;,\u0026#34;もの\u0026#34;,\u0026#34;あり\u0026#34;,\u0026#34;まで\u0026#34;,\u0026#34;られ\u0026#34;,\u0026#34;なる\u0026#34;,\u0026#34;へ\u0026#34;, \u0026#34;か\u0026#34;,\u0026#34;だ\u0026#34;,\u0026#34;これ\u0026#34;,\u0026#34;おり\u0026#34;,\u0026#34;より\u0026#34;,\u0026#34;ず\u0026#34;,\u0026#34;なり\u0026#34;,\u0026#34;られる\u0026#34;,\u0026#34;ば\u0026#34;,\u0026#34;なかっ\u0026#34;,\u0026#34;なく\u0026#34;, \u0026#34;しかし\u0026#34;,\u0026#34;せ\u0026#34;,\u0026#34;だっ\u0026#34;,\u0026#34;できる\u0026#34;,\u0026#34;それ\u0026#34;,\u0026#34;う\u0026#34;,\u0026#34;なお\u0026#34;,\u0026#34;のみ\u0026#34;,\u0026#34;でき\u0026#34;,\u0026#34;き\u0026#34;,\u0026#34;つ\u0026#34;, \u0026#34;および\u0026#34;,\u0026#34;いう\u0026#34;,\u0026#34;さらに\u0026#34;,\u0026#34;ら\u0026#34;,\u0026#34;たり\u0026#34;,\u0026#34;たち\u0026#34;,\u0026#34;ます\u0026#34;,\u0026#34;ん\u0026#34;,\u0026#34;なら\u0026#34;,\u0026#34;特に\u0026#34;, \u0026#34;せる\u0026#34;,\u0026#34;及び\u0026#34;,\u0026#34;とき\u0026#34;,\u0026#34;にて\u0026#34;,\u0026#34;ほか\u0026#34;,\u0026#34;ながら\u0026#34;,\u0026#34;うち\u0026#34;,\u0026#34;そして\u0026#34;,\u0026#34;ただし\u0026#34;, \u0026#34;かつて\u0026#34;,\u0026#34;それぞれ\u0026#34;,\u0026#34;お\u0026#34;,\u0026#34;ほど\u0026#34;,\u0026#34;ほとんど\u0026#34;,\u0026#34;です\u0026#34;,\u0026#34;とも\u0026#34;,\u0026#34;ところ\u0026#34;,\u0026#34;ここ\u0026#34;, \u0026#34;居る\u0026#34;,\u0026#34;為る\u0026#34;,\u0026#34;有る\u0026#34;,\u0026#34;成る\u0026#34;,\u0026#34;来る\u0026#34;,\u0026#34;よる\u0026#34;,\u0026#34;おる\u0026#34;) # Token化実行 toks_sent \u0026lt;- corp %\u0026gt;% sudachi_tokenize() %\u0026gt;% quanteda::as.tokens() %\u0026gt;% quanteda::tokens_remove(quanteda::stopwords(\u0026#34;ja\u0026#34;, source = \u0026#34;marimo\u0026#34;)) %\u0026gt;% quanteda::tokens_remove(c(\u0026#34;、\u0026#34;, \u0026#34;。\u0026#34;, \u0026#34;・\u0026#34;)) %\u0026gt;% quanteda::tokens_remove(stopwords) # メタデータをTokenに再付与 quanteda::docvars(toks_sent) \u0026lt;- quanteda::docvars(corp) このtoks_sentから文書行列を生成します。文書行列とは、単語と文書の関係を表す行列で、各行が単語(token)、各列が文書を表し、各要素は文書中の単語の出現回数となっています。\n# 文書行列の生成 dfmt_sent \u0026lt;- toks_sent %\u0026gt;% quanteda::dfm() %\u0026gt;% quanteda::dfm_remove(pattern=\u0026#34;\u0026#34;) %\u0026gt;% quanteda::dfm_trim(min_termfreq = 10) 出現回数Top20は以下のようになっています。\nprint(quanteda::topfeatures(dfmt_sent, n = 20)) ## 数 客 前年 売り上げ 影響 来客 減少 新型 ## 11912 11752 11238 9461 9247 7908 7656 6100 ## コロナ % ウイルス 販売 良い 為 増える 動き ## 5928 5893 5865 5114 5045 5037 4832 4602 ## 状況 3 増加 量 ## 4590 4587 4483 4402\r センチメントスコアの算出 次に、種語として以下の単語を定義します。\nlibrary(magrittr) library(LSX) ja_seedwords \u0026lt;- list(positive = c(\u0026#34;安心\u0026#34;,\u0026#34;好調\u0026#34;,\u0026#34;秀逸\u0026#34;,\u0026#34;改善\u0026#34;,\u0026#34;良好\u0026#34;,\u0026#34;適切\u0026#34;,\u0026#34;安堵\u0026#34;), negative=c(\u0026#34;不安\u0026#34;,\u0026#34;不調\u0026#34;,\u0026#34;稚拙\u0026#34;,\u0026#34;悪化\u0026#34;,\u0026#34;不良\u0026#34;,\u0026#34;過剰\u0026#34;,\u0026#34;過少\u0026#34;,\u0026#34;懸念\u0026#34;)) %\u0026gt;% as.seedwords() print(ja_seedwords) ## 安心 好調 秀逸 改善 良好 適切 安堵 不安 不調 稚拙 悪化 不良 過剰 過少 懸念 ## 1 1 1 1 1 1 1 -1 -1 -1 -1 -1 -1 -1 -1\r 上記のように、種語のスコアはポジティブ/ネガティブの最大値となっています。\n文書行列に対して特異値分解を行い、センチメントスコアを算出します。Word Embeddingのサイズ$k$は300とします。\nlss \u0026lt;- textmodel_lss(dfmt_sent, seeds = ja_seedwords, k = 300) センチメントスコア上位・下位20単語を抜き出します。LSX::coef()で抜き出すことができ、降順で並んでいます。\nprint(head(coef(lss), 20)) # most positive words ## 安心 改善 好調 重点 米飯 前期 ## 0.15075889 0.14272040 0.12926093 0.12218111 0.11418503 0.11297516 ## di コンクリート デザート 誕生 フード 調理 ## 0.10698720 0.10633358 0.09668659 0.09597283 0.09563199 0.09549041 ## 努める ファースト 政権 占める 良好 景況 ## 0.09523555 0.09523126 0.09446756 0.09153319 0.09054534 0.09019622 ## 今期 有り難い ## 0.09003611 0.08986984\r print(tail(coef(lss), 20)) # most negative words ## 贈与 老後 ローン 繰り 不良 減税 ## -0.09561125 -0.09683069 -0.09875152 -0.09982278 -0.10067741 -0.10109826 ## 韓国 益々 情勢 煽る 不調 日韓 ## -0.10140439 -0.10567625 -0.10567861 -0.10844798 -0.11014713 -0.11054370 ## 将来 要素 先行き 辿る 一途 不安 ## -0.11360784 -0.11364341 -0.11439845 -0.11930636 -0.11956281 -0.13344996 ## 悪化 懸念 ## -0.15213723 -0.15636390\r ポジティブな単語に「米飯」、「コンクリート」、「デザート」、「フード」が入っていたり、ネガティブな単語に「贈与」、「老後」が入っているなどツッコミどころはあるものの、7割はそれっぽい単語がリストアップされているのではないでしょうか。\n次に、このスコアを用いて、辞書ベース分類法と同様のやり方で文書のセンチメントスコアを算出します。\n# 文書のセンチメントスコア算出 coef_df \u0026lt;- data.frame(word=names(coef(lss)), values=as.numeric(coef(lss))) dfmt_sent_arranged \u0026lt;- quanteda::dfm_match(dfmt_sent, coef_df$word) n \u0026lt;- unname(quanteda::rowSums(dfmt_sent_arranged)) score_lss \u0026lt;- ifelse(n\u0026gt;0, quanteda::rowSums(dfmt_sent_arranged %*% coef_df$values)/n, NA) sample_with_score_lss \u0026lt;- sample %\u0026gt;% cbind(score_lss) センチメントスコア上位10位のサンプルを確認します。\nsample_with_score_lss %\u0026gt;% dplyr::slice_max(n=10, order_by = score_lss) %\u0026gt;% dplyr::select(追加説明及び具体的状況の説明, score_lss) ## 追加説明及び具体的状況の説明\r## 1 ・景況感が良好である。\r## 2 ・好調に推移している。\r## 3 ・受注が好調である。\r## 4 ・主要商品である米飯、調理パンやファストフードが好調に推移している。企画商品のデリカも好調である。\r## 5 ・県内企業の景況感は改善している。業況判断ＤＩが製造業で前期に比べて改善し、プラス水準を維持している。\r## 6 ・客先の決算が好調である。納税額も増えている。\r## 7 ・ファーストフード、デザートの陳列を増やしたので売上が増加した。\r## 8 ・ファーストフード、デザートの陳列を増やしたので売上が増加した。\r## 9 ・受注量が好調である。\r## 10 ・受注量は好調である。\r## score_lss\r## 1 0.08265024\r## 2 0.07564103\r## 3 0.07239461\r## 4 0.05859545\r## 5 0.05651628\r## 6 0.05048285\r## 7 0.04896476\r## 8 0.04896476\r## 9 0.04726825\r## 10 0.04726825\r 非常に良い結果となっているように思えます。 「米飯」、「デザート」、「フード」の謎が解けましたね。「好調」、「改善」、「増加」など肯定的な単語に共起しているため、これらの単語も肯定的と捉えられたようです。サンプルにこれらの単語を含む否定的(または中立)な文章が含まれれば、これらの単語のスコアもあるべき値に収斂すると思われます。\nセンチメントスコア下位10位のサンプルも確認しましょう。\nsample_with_score_lss %\u0026gt;% dplyr::slice_min(n=10, order_by = score_lss) %\u0026gt;% dplyr::select(追加説明及び具体的状況の説明) ## 追加説明及び具体的状況の説明\r## 1 ・全てにおいて悪化している。\r## 2 ・様々な問題で悪化している。\r## 3 ・客は、先行きが不安で買い控えをしている。\r## 4 ・景気が悪化している。\r## 5 ・原油の値上がりが懸念材料になってくる。\r## 6 ・先行きが見通せない。\r## 7 ・先行きが見通せない。\r## 8 ・為替相場や株安により、先行きに不安がある。\r## 9 ・客の業績が悪化している。\r## 10 ・海外情勢への不安要素が一層顕著に表れてきている。\r こちらも良さそうです。\n文書分類結果 boxplotでデータ全体を見てみましょう。\n# 結果の可視化 library(ggplot2) ggplot(sample_with_score_lss, aes(x=景気の現状判断,y=score_lss)) + geom_boxplot() 先ほどとは、様相が異なっています。 上図では、横軸に「景気の現状判断」をとっていますが、景気が良い/悪いでセンチメントスコアがあまり変わらない結果となっています。サンプル全体でみると、文書分類を行える特徴量といえます。 念のため、ロジスティック回帰を行ってみます。\ndataset \u0026lt;- sample_with_score_lss %\u0026gt;% dplyr::mutate(condition=as.factor(dplyr::if_else(景気の現状判断==\u0026#34;○\u0026#34;|景気の現状判断==\u0026#34;◎\u0026#34;,1,0))) %\u0026gt;% dplyr::select(condition, score_lss) sample_recipe \u0026lt;- recipes::recipe(condition~., dataset) %\u0026gt;% recipes::step_center(recipes::all_predictors()) %\u0026gt;% recipes::step_scale(recipes::all_predictors()) %\u0026gt;% recipes::prep(dataset) lr_fitted \u0026lt;- parsnip::logistic_reg() %\u0026gt;% parsnip::set_engine(\u0026#34;glm\u0026#34;) %\u0026gt;% parsnip::fit(condition~., data=sample_recipe %\u0026gt;% recipes::juice()) lr_fitted %\u0026gt;% purrr::pluck(\u0026#34;fit\u0026#34;) %\u0026gt;% gtsummary::tbl_regression() %\u0026gt;% gtsummary::add_n() html {\rfont-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\r}\r#xtzoqsvsqi .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n#xtzoqsvsqi .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n#xtzoqsvsqi .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n#xtzoqsvsqi .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n#xtzoqsvsqi .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n#xtzoqsvsqi .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n#xtzoqsvsqi .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n#xtzoqsvsqi .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n#xtzoqsvsqi .gt_column_spanner_outer:first-child { padding-left: 0; }\n#xtzoqsvsqi .gt_column_spanner_outer:last-child { padding-right: 0; }\n#xtzoqsvsqi .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n#xtzoqsvsqi .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; }\n#xtzoqsvsqi .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n#xtzoqsvsqi .gt_from_md \u0026gt; :first-child { margin-top: 0; }\n#xtzoqsvsqi .gt_from_md \u0026gt; :last-child { margin-bottom: 0; }\n#xtzoqsvsqi .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n#xtzoqsvsqi .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n#xtzoqsvsqi .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n#xtzoqsvsqi .gt_row_group_first td { border-top-width: 2px; }\n#xtzoqsvsqi .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; }\n#xtzoqsvsqi .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; }\n#xtzoqsvsqi .gt_first_summary_row.thick { border-top-width: 2px; }\n#xtzoqsvsqi .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n#xtzoqsvsqi .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; }\n#xtzoqsvsqi .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; }\n#xtzoqsvsqi .gt_striped { background-color: rgba(128, 128, 128, 0.05); }\n#xtzoqsvsqi .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n#xtzoqsvsqi .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n#xtzoqsvsqi .gt_footnote { margin: 0px; font-size: 90%; padding-left: 4px; padding-right: 4px; padding-left: 5px; padding-right: 5px; }\n#xtzoqsvsqi .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n#xtzoqsvsqi .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; }\n#xtzoqsvsqi .gt_left { text-align: left; }\n#xtzoqsvsqi .gt_center { text-align: center; }\n#xtzoqsvsqi .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n#xtzoqsvsqi .gt_font_normal { font-weight: normal; }\n#xtzoqsvsqi .gt_font_bold { font-weight: bold; }\n#xtzoqsvsqi .gt_font_italic { font-style: italic; }\n#xtzoqsvsqi .gt_super { font-size: 65%; }\n#xtzoqsvsqi .gt_footnote_marks { font-style: italic; font-weight: normal; font-size: 75%; vertical-align: 0.4em; }\n#xtzoqsvsqi .gt_asterisk { font-size: 100%; vertical-align: 0; }\n#xtzoqsvsqi .gt_slash_mark { font-size: 0.7em; line-height: 0.7em; vertical-align: 0.15em; }\n#xtzoqsvsqi .gt_fraction_numerator { font-size: 0.6em; line-height: 0.6em; vertical-align: 0.45em; }\n#xtzoqsvsqi .gt_fraction_denominator { font-size: 0.6em; line-height: 0.6em; vertical-align: -0.05em; } \n\rCharacteristic\rN\rlog(OR)1\r95% CI1\rp-value\r\r\rscore_lss\r44,944\r0.55\r0.53, 0.57\r\r\r\r1 OR = Odds Ratio, CI = Confidence Interval\r\r\r\r\r推定したモデルの精度を確認するため、データセットの予測分類を計算し、正解ラベルと突合します。\nlr_pred \u0026lt;- lr_fitted %\u0026gt;% predict(dataset) %\u0026gt;% dplyr::mutate(truth = dataset %\u0026gt;% dplyr::select(condition) %\u0026gt;% dplyr::pull()) 正解率は約60%となりました。\nlr_pred %\u0026gt;% yardstick::metrics(truth = truth, estimate = .pred_class) ## # A tibble: 2 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.612\r## 2 kap binary 0\r 混合行列(Confusion Matrix)を描画します。\nlr_pred %\u0026gt;% yardstick::conf_mat(truth = truth, estimate = .pred_class) %\u0026gt;% ggplot2::autoplot(type = \u0026#34;heatmap\u0026#34;) 全く文書分類できていませんね。全てのサンプルの予測値が0(=ネガティブ)となってしまっています。データセットは不均衡ですが、それにしても1(=ポジティブ)の予測がまったくないのは、そもそもの特徴量がいけていないためだと思われます。\n上手くいかない仮説としては、センチメントスコアがゼロ周りに多く分布しているためという理由が考えられます(つまり、センチメントスコアが大きい単語はあまり文章に含まれていない or 様々な言葉で言い換えられている)。 可視化を行い、確認してみます。\ntextplot_terms(lss, names(ja_seedwords)) 上記の図は縦軸が頻度(対数)、横軸が極性(Polarity、センチメントスコアと解釈)となっています。極性が大きく、頻度が高い単語は種語が多く、頻度の高い単語はゼロ周りに分布していることが確認できました。 語調の強い単語以外は極性の絶対値が低いと出るようで、日本語のように強い表現があまり取られない言語では機能しずらいのかもしれません。\n3. 終わりに 辞書ベース手法の時と同じく、教師あり学習以外の手法で文書分類を行うことは難しいという印象です。\n次は、深層学習ベースの手法を用いて、語順や文脈を加味した分析を行います！\n","date":1648857600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1648857600,"objectID":"e5ebee69168d0f9a145ba6b9288dec22","permalink":"/post/post31/","publishdate":"2022-04-02T00:00:00Z","relpermalink":"/post/post31/","section":"post","summary":"単語埋め込み(word embedding)を用いたセンチメントスコアの算出を実践します！","tags":["Python","前処理","機械学習","テキスト解析"],"title":"【徹底比較】センチメントスコア算出手法！！ - 第4回","type":"post"},{"authors":null,"categories":["単発"],"content":"おはこんばんにちは。センチメントスコア算出企画の第3弾です。 今回はナイーブベイズ分類器のセンチメントスコア算出方法を実践します。\n1. 分析手法の説明 ナイーブベイズ分類器とは ナイーブベイズ分類器、または単純ベイズ分類器と呼ばれる手法です。分類問題をベイズの定理を用いて解く手法を指します。まず、ベイズの定理から説明します。入力情報$X$が与えられた時に、出力$Y$が得られる確率は以下で表すことができます。\n$$ P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)} $$\nこれがベイズの定理で、$P(Y)$は事前確率、$P(Y|X)$は事後確率、$P(X|Y)$は尤度と呼ばれます。ナイーブベイズ分類器が利用される典型的な問題に迷惑メールの分類問題があります。この問題だと、$X$は受信メールに含まれている単語、$Y$は迷惑メールかどうかを表す2値(0,1)のデータとなります。受信メールに$\\hat{X}$という単語が含まれている際に、そのメールが迷惑メール($Y=1$)である確率は$P(Y=1|X=\\hat{X})$であり、それは以下の3つで求められます。\n 迷惑メールが発生する確率$P(Y=1)$ そのメールが迷惑メールだった際に単語$\\hat{X}$が含まれている確率$P(X=\\hat{X}|Y=1)$ 単語$\\hat{X}$が発生する確率$P(X=\\hat{X})$  ただし、事後確率によって文書分類を行う際には、上式の確率を計算する必要はありません。知りたいのは確率の値ではなく、ある$X$が与えられた時に$P(Y=1|X)$と$P(Y=0|X)$のどちらが大きいかですので、共通している分母は計算から除外することができ、分子$P(X|Y)P(Y)$をそれぞれ計算し、その大小関係によって迷惑メールかどうかを割り振ればよいことになります。\nまた、上記は迷惑メールの分類に限らず、今回行おうとしているセンチメント情報、つまりその文書内容がポジティブかネガティブかを分類する問題に対しても使えることがわかると思います。\n事後確率の推定方法 求めたい事後確率の分子を求める方法について説明します。その計算のためには事前確率$P(Y)$と尤度$P(Y|X)$を求める必要がありますが、基本的にそれぞれにパラメトリックな確率分布を仮定するため、確率分布のパラメータを推定する必要があります。なお、事前確率が従う確率分布を事前分布と呼びます。\n入力単語ベクトルを$X={x_1,x_2,\u0026hellip;,x_N}$とします。ここで、各$x_n$は$n$番目の単語を表します。各$x_n$が生起する条件付確率は語順や周辺の単語に依存しないと仮定すると、$P(X|Y)$は以下のように書き替えることができます。\n$$ P(X|Y) = \\Pi_{i=1}^N P(x_i|Y) $$\nでは、上記の条件付確率をどのように求めるかに話を移しましょう。今、$P(・)$のそれぞれにとある確率分布を仮定すると、$P(X|Y)P(Y)$は\n$$ P(Y;\\Theta,\\Phi)P(X;\\Theta|Y) \\tag{1} $$ と書けます。ここで、$\\Theta={\\theta_1,\\theta_2,\u0026hellip;,\\theta_M}, \\Phi={\\phi_1,\\phi_2,\u0026hellip;,\\phi_L}$は確率分布のパラメータです。このパラメータを求めることができれば、(1)を計算でき、$X$が与えられた際の$Y={0,1}$のそれぞれの事後確率を求めることができます。 このようにして、事後確率の推定問題を確率分布のパラメータの推定問題に帰着させることができました。具体的に、どのようにしてパラメータを推定するのかについて、見ていきましょう。\n学習データセットとして、\n$$ {(S,T)} = (Y=S_1,X=T_1), (Y=S_2,X=T_2), \u0026hellip;,(Y=S_D,X=T_D) $$\nが得られているとします。ここで、$D$はサンプルサイズです。また、$Y$は$1$の時にポジティブ、$0$の時にネガティブとし、独立を仮定します(つまり$S$は0,1の2値を取る)。最も望ましいパラメータ$\\Phi$の値は、学習データセットの(1)の同時確率を最大にするパラメータを推定値として選択することにし、$\\Theta$は事前にわかっている知識を用いて何らかの値を定めます。つまり、以下を最大にするということです。\n$$ M(\\Theta,\\Phi) = \\Pi_{j=1}^{D}P(S_j;\\Theta,\\Phi)P(T_j;\\Theta|S_j) \\tag{2} $$\n(2)は以下のように変形することができます。\n\r$$\r\\begin{eqnarray*}\rM(\\Theta,\\Phi) \u0026=\u0026 P(T_1;\\Theta|0)P(0;\\Theta,\\Phi)×P(T_2;\\Theta|0)P(0;\\Theta,\\Phi)×...\\\\\r\u0026\u0026×P(T_Q;\\Theta|0)P(0;\\Theta,\\Phi)×P(T_{Q+1};\\Theta|1)P(1;\\Theta,\\Phi) \\\\\r\u0026=\u0026 \\Pi_{k=0}^1P(k;\\Theta,\\Phi)\\Pi_{i=1}^{Q_k}P(T_{Q_i};\\Theta|k) \\tag{3}\r\\end{eqnarray*}\r$$\r\r\rここで、$Q_k$は$Y=k$となる学習データの個数を表しています。 \r確率分布のパラメータの推定方法 次に気になるのは$\\Theta, \\Phi$を具体的にどのようにして計算するかです。これは確率分布を特定しないと進めないため、尤度には多項分布、事前分布にはディリクレ分布を仮定します。一般的な仮定だと思います。多項分布とは確率密度関数が以下のような確率分布です。\n$$ P(n_1,\u0026hellip;,n_k) = \\frac{n!}{n_1!\u0026hellip;n_k!}p_1^{n_1}\u0026hellip;p_k^{n_k} $$\nここで、$n_i$は非負で$n=n_1+\u0026hellip;+n_k$です。また、$p_1+\u0026hellip;p_k=1$を満たします。多項分布は名前から分かるとおり、二項分布の拡張版です。二項分布はコイン投げが例として使われますが、多項分布はサイコロ投げが従う分布です($n=6, p_i=1/6$)。テキスト解析の文脈では、$n_i$は文章に含まれる単語$i$の出現回数になります。\nディリクレ分布は同時確率密度関数が以下のような確率分布です。\n$$ f(x_1,\u0026hellip;,x_n) = \\frac{\\Gamma(\\alpha)}{\\Gamma(\\alpha_1)\u0026hellip;\\Gamma(\\alpha_n)}x_1^{\\alpha_1-1}\u0026hellip;x_n^{\\alpha_n-1} $$\nここで、$x_i$は非負で$x_1+\u0026hellip;+x_n=1$、$\\alpha_1+\u0026hellip;+\\alpha_n=\\alpha$はパラメータです。今、$\\alpha_i$を正の整数とすると$\\Gamma(\\alpha_i)=(\\alpha_{i-1})!$より、\n$$ f(x_1,\u0026hellip;,x_n) = \\frac{(\\alpha-1)!}{(\\alpha_{1}-1)!\u0026hellip;(\\alpha_n-1)!}x_1^{\\alpha_1-1}\u0026hellip;x_n^{\\alpha_n-1} $$\nとなります。多項分布とディリクレ分布の確率分布を見比べると、とても似た形であることがわかります。両者は形は似ていますが、関数の入力が異なります。多項分布は各単語の生起確率がパラメータ、指数部分の出現回数が入力である一方、ディリクレ分布は生起確率が入力、出現回数がパラメータになります。また、ディリクレ分布は多項分布の共役事前分布となっています。 これらの確率分布を仮定すると、(3)は以下のようになります。\n\r$$\r\\begin{eqnarray*}\rM(\\vec{p};\\vec{\\alpha}) \u0026\\propto\u0026 \\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k=0}^1\\Pi_{i=1}^Vp_{k_i}^{\\alpha_{i}-1}\\Pi_{j=1}^V p_{k_{j}}^{\\tau_{k_{j}}} \\\\\r\u0026=\u0026 \\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k=0}^1 \\Pi_{j=1}^V p_{k_{j}}^{\\tau_{k_{j}}+\\alpha_j-1} \\tag{4}\r\\end{eqnarray*}\r$$\r\rここで、$p_{k_j}$はポジティブ(またはネガティブ)である文書($k={1,0}$)で単語$j$が出現する確率、$\\tau_{k_j}=\\sum_{i=1}^{Q_k}c_{kij}$はポジティブ(またはネガティブ)である教師データに含まれる単語$j$の総数、$\\alpha_j$は単語$j$のディリクレ分布のパラメータ、$V$は単語数の上限を表しています。$Z(\\vec{\\alpha})$は確率の総和を1とするための正規化定数で、ベイズ統計学では分配関数と呼ばれます。(3)に対応させると、$\\Theta=\\vec{p}={p_{k_j}}_{k_j=0_1}^{1_V}$、$\\Phi=\\vec{\\alpha}={\\alpha_j}_{j=1}^V$となります。ここから、パラメータ$p_{k_{j}}$の推定値を$M(\\vec{\\alpha};\\vec{p})$を最大にする値として求めます。ただし、以下の制約条件があります。\n$$ \\sum_{j=1}^V p_{k_{j}}=1 $$\nつまり、以下の最適化問題として定式化できます。\n\r$$\r\\begin{eqnarray*}\r\\hat{p}_{k_j} \u0026=\u0026 \\underset{{p_{k_j}}}{\\arg\\max}~M(\\vec{p};\\vec{\\alpha})~~s.t.\\sum_{j=1}^V p_{k_{j}}=1\\\\\r\u0026=\u0026\\underset{{p_{k_j}}}{\\arg\\max}~\\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k=0}^1 \\Pi_{j=1}^V p_{k_{j}}^{\\tau_{k_{j}}+\\alpha_j-1}~~s.t.\\sum_{j=1}^V p_{k_{j}}=1\r\\end{eqnarray*}\r$$\r\rこの問題を解くためにラグランジュ未定乗数法を使用します。\n\r$$\r\\begin{eqnarray*}\rL \u0026=\u0026 (\\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k=0}^1 \\Pi_{j=1}^V p_{k_{j}}^{\\tau_{k_{j}}+\\alpha_j-1}-\\lambda(1-\\sum_{k=1}^V p_{k_{j}})) \\\\\r\\frac{\\partial L}{\\partial p_{l_m}}\r\u0026=\u0026 (\\tau_{l_m}+\\alpha_m-1)p_{l_m}^{\\tau_{l_m}+\\alpha_m-2}\\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k\\neq l}^1 \\Pi_{j\\neq m}^V p_{k_{j}}^{\\tau_{k_{j}}+\\alpha_j-1}-\\lambda \\\\\r\u0026=\u0026 (\\tau_{l_m}+\\alpha_m-1)p_{l_m}^{-1}\\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k=0}^1 \\Pi_{j=1}^V p_{k_{j}}^{\\tau_{k_{j}}+\\alpha_j-1}-\\lambda \\\\\rp_{l_m} \u0026=\u0026 \\frac{(\\tau_{l_m}+\\alpha_m-1)}{\\lambda}\\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k=0}^1 \\Pi_{j=1}^V p_{k_{j}}^{\\tau_{k_{j}}+\\alpha_j-1} \\tag{5}\r\\end{eqnarray*}\r$$\r\r上記のように解けます。ここで、制約条件を用いて上式の両辺で総和を取ると、\r\r$$\r\\begin{eqnarray*}\r1 \u0026=\u0026 \\sum_{m=1}^V \\frac{(\\tau_{l_m}+\\alpha_m-1)}{\\lambda}\\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k=0}^1 \\Pi_{j=1}^V p_{k_{j}}^{\\tau_{k_{j}}+\\alpha_j-1} \\\\\r\\lambda \u0026=\u0026 \\sum_{m=1}^V (\\tau_{l_m}+\\alpha_m-1)\\frac{1}{Z(\\vec{\\alpha})}\\Pi_{k=0}^1 \\Pi_{j=1}^V p_{k_{j}}^{\\tau_{k_{j}}+\\alpha_j-1}\r\\end{eqnarray*}\r$$\r\rとなるので、$\\lambda$を(5)式に代入すると、\n\r$$\r\\hat{p}_{l_m} = \\frac{(\\tau_{l_m}+\\alpha_m-1)}{\\sum_{m=1}^V (\\tau_{l_m}+\\alpha_m-1)} \\tag{6}\r$$\r\rと、事後分布を最大にするパラメータを求めることができます。$p_{l_m}$の直感的な解釈は、ポジティブ(ネガティブ)な文脈での単語$m$の出現頻度が全ての単語の出現頻度に占める割合です。ただ、それだけだと学習データに含まれない単語の$p_{l_m}$が０となってしまい、事後確率も0となってしまいます。上記では、事前分布にディリクレ分布を使用しているため、$p_{l_m}$が0にならないことがわかります($+\\alpha_m$の部分)。所謂、頻度ゼロ問題を解決しています。\nところで、ディリクレ分布の$\\alpha_m$は事前にわかっている知識を用いて何らかの値を定めると言っていましたが、どのようにして求めるのでしょうか？推定値を見ればわかる通り、$\\alpha_m$は文書内における単語$m$の主観的な出現頻度を表していると解釈できます。ですが、正直$\\alpha_m$に意味を持たせている人は少なく、先述した頻度ゼロ問題を解決できればよいと考えている人が大半ではないでしょうか。つまり、$\\alpha_m=\\alpha+1$($\\alpha$は定数、例えば1とか)としてしまうということです。これは加算スクリーニングと呼ばれます。\nさて、このようにして求めた分類器で分類を行います。思考実験として、1つの単語を除いて他の単語がニュートラルかつ各単語の出現頻度が全て同じ文書を考えます。この場合、その1つの単語が両文脈(ポジティブ/ネガティブ)のうち相対的に頻出である文脈へ分類されることになります。\nナイーブベイズ分類器は教師あり学習です。つまり、正解ラベルデータを用意する必要があります。これがナイーブベイズ分類器の欠点ですが、一方でデータがある場合には特殊な文脈でも分類が可能です。\nナイーブベイズ分類器の利点・欠点 ナイーブベイズ分類器の利点・欠点は以下のようなものだと思います。\n\u0026lt;利点👍\u0026gt;\n 辞書が不要 未知語が出現しても分類できる 分類過程がわかりやすい  \u0026lt;欠点👎\u0026gt;\n 学習のための教師ありデータセットが必要 文章の語順を考慮できない(bag of words)  欠点の文章の語順を考慮できないという点は辞書ベース手法にも当てはまりますが、文章中の単語の発生回数を使用しているため、語順の関係性や係り受けといった情報を活用することができません。そのため、例えば「良くない」のような肯定語を否定するような言い回しは、ポジティブと分類されやすい可能性があります。\n2. ナイーブベイズ分類器の実践 ここからは、ナイーブベイズ分類器を用いて実際に分析をしていきます。学習データセットの景気ウオッチャー調査では、タクシー運転手や小売店の店主、旅館の経営者など景気に敏感な方々(景気ウオッチャー)に対して、5段階の景況感とその理由を毎月アンケートしています。5段階は、「良い(◎)」、「やや良い(○)」、「変わらない(■)」、「やや悪い(▲)」、「悪い(×)」となっています。\nまずサンプルデータを読み込みます。\nfilePath = r\u0026#34;C:\\Users\\hogehoge\\Watchder\\RawData\\*.csv\u0026#34; import pandas as pd import glob files = glob.glob(filePath) lists = [] for file in files: df = pd.read_csv(file, encoding=\u0026#34;shift-jis\u0026#34;) lists.append(df) sample = pd.concat(lists, axis=0, ignore_index=True) sample = sample[((sample.追加説明及び具体的状況の説明==\u0026#39;−\u0026#39;)|(sample.追加説明及び具体的状況の説明==\u0026#39;＊\u0026#39;)==False)] sample = sample[sample.景気の現状判断!=\u0026#39;□\u0026#39;] corpus = sample.追加説明及び具体的状況の説明.str.replace(\u0026#34;・\u0026#34;,\u0026#34;\u0026#34;) corpus.head() ## 0 店頭の取扱額が前年比約120％と好調であった。\r## 1 当施設の利用乗降客数は１月26日時点で前年比130.1％となっており、１月としては過去最高の...\r## 2 年末の消費の反動もあってか、客の動きがやや鈍い。ただ、相変わらず高額商材が売れているというこ...\r## 3 外国人観光客による売上が前年比152％と好調を継続しているほか、来客数が前年比102％と好調...\r## 4 積極的に景気が上向きにあるとまではいいづらいものの、３か月前との比較では改善している。\r## Name: 追加説明及び具体的状況の説明, dtype: object\rなお、自由記述部分(追加説明及び具体的状況の説明)が「－」は回答が存在しない、「＊」は主だった回答等が存在しないため、サンプルから除外しています。 また、景気の現状判断が「変わらない(■)」のデータは分類対象外とするため、こちらもサンプルから除外しています。\n次に、分類対象となるラベル(景気の現状判断)を数値へ加工します。「良い(◎)」と「やや良い(○)」を1、「やや悪い(▲)」、「悪い(×)」を0とする2値分類です。\nfrom sklearn.preprocessing import LabelEncoder score_mapping = {\u0026#39;◎\u0026#39;: 1, \u0026#39;○\u0026#39;: 1, \u0026#39;▲\u0026#39;: 0, \u0026#39;×\u0026#39;:0} labels = sample.景気の現状判断.map(score_mapping) 自由記述部分の処理を行います。前節で説明したとおり、ナイーブベイズ分類器は入力変数として、各文章に含まれる単語の出現回数が必要です。このデータを作成するためには、\n サンプルデータの文章を形態素に分解 各文中に形態素が出現する回数をカウント  の処理が必要です。英語であれば、この処理はsklearn.feature_extraction.CountVectorizerが行ってくれますが、日本語の場合はそうはいかず、形態素解析のための関数を渡してやる必要があります。よって、入力：文章ベクトル、出力：形態素のリストとなる関数tokenizer_sudachiを定義し、それをsklearn.feature_extraction.CountVectorizerの引数analyzerに渡しています。\nfrom sudachipy import tokenizer from sudachipy import dictionary from sklearn.feature_extraction.text import CountVectorizer tokenizer_obj = dictionary.Dictionary().create() mode = tokenizer.Tokenizer.SplitMode.A tokenizer_sudachi = lambda t: [m.normalized_form() for m in tokenizer_obj.tokenize(t, mode)] vectorizer = CountVectorizer(analyzer=tokenizer_sudachi) counts = vectorizer.fit_transform(corpus) countsはサンプルサイズ×出現単語総数となるsparse matrixで、これがナイーブベイズ分類器の入力変数となります。\n入力countsと出力labelsが準備できたので、sklearn.model_selection.train_test_splitを使って学習データとテストデータにランダムに分割します。\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(counts, labels, test_size=0.5, random_state=42) 前回の辞書ベース手法ではサンプル全体を用いて予測を行いました。それは辞書に含まれる極性スコアが景気ウオッチャー調査のサンプルデータをもとに作成したものではなく、過学習しないためです。ただ、今回のナイーブベイズ分類器はそのデータを用いてパラメータの学習を行う教師あり学習のため、その性能を適切に評価するためには学習データ以外のデータを検証用に残しておく必要があります。\nいよいよ、学習パートに入ります。学習にはsklearn.naive_bayes.MultinomialNBを使用します。これは前節で説明した(6)でパラメータの推計を行い、予測モデルを構築するクラスとなっています。引数としてalphaを指定する必要がありますが、これは事前分布であるディリクレ分布の$\\alpha$です。デフォルト値である1を指定すると、加算スクリーニングとなります。\nfrom sklearn.naive_bayes import MultinomialNB clf = MultinomialNB(alpha=1.0) clfとしてインスタンス化することができました。 では、fit()メソッドを用いて、学習を行います。\nclf.fit(X_train, y_train) ## MultinomialNB()\r学習が完了しました。score()メソッドで学習データに対する正解率を確認してみましょう。\nclf.score(X_train, y_train) ## 0.874935083329399\rおよそ87.5%ということで、高い正解率を得ることが出来ました。\nただ、学習データに対して過学習している可能性があるため、テストデータでの精度も確認してみましょう。テストデータの予測値を計算するためにはpredictメソッドを使用します。\ny_pred = clf.predict(X_test) 次に、混同行列(ConfusionMatrix)を作成します。これは分類モデルの性能を評価するために用いられるもので、横軸に予測値、縦軸に実際の値をとり、(1,1)と(2,2)要素に多くサンプルが集まるほど正しい予測ができています。\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay import matplotlib.pyplot as plt ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot() ## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000000EC7CA48\u0026gt;\rplt.show() たまに、正解率が高くてもほとんど0を予測するだけのモデルになっており、正解データも0が多いデータであるため、見かけ上上手く分類することができていることもありますが、今回に限ってはしっかりと識別できているようです。正例よりも負例の予測精度が高いようです。単純に負例のサンプルサイズが大きいのが原因かも知れません。\n最後に、性能評価のための分類指標を記載します。各指標の意味はここでは説明しませんが、正解率は学習データと変わらず、過学習はしていないようです。\nprint(classification_report(y_test, y_pred)) ## precision recall f1-score support\r## ## 0 0.87 0.88 0.87 13223\r## 1 0.79 0.78 0.78 7958\r## ## accuracy 0.84 21181\r## macro avg 0.83 0.83 0.83 21181\r## weighted avg 0.84 0.84 0.84 21181\r3. 終わりに このpostではナイーブベイズ分類器を用いたセンチメント情報の抽出ならびに文書分類問題の実践を行いました。 sklearn.naive_bayes.MultinomialNBクラスを用いれば、比較的容易に分析を行うことが出来ます。テストデータの精度も高く、教師あり学習の威力を思い知りました。いきなり深層学習を行うのではなく、まずはナイーブベイズ分類器から開始するのが良さそうです。\n次回は、準教師あり学習である単語埋め込み(Word Embedding)を用いた方法について紹介します！\n","date":1648166400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1648166400,"objectID":"85e9818128d6ca566d054b79c5c25eca","permalink":"/post/post28/","publishdate":"2022-03-25T00:00:00Z","relpermalink":"/post/post28/","section":"post","summary":"ナイーブベイズ分類器を用いたセンチメントスコアの算出を実践します！","tags":["Python","前処理","機械学習","テキスト解析"],"title":"【徹底比較】センチメントスコア算出手法！！ - 第3回","type":"post"},{"authors":null,"categories":["単発"],"content":"\r\r\r\r\r\rおはこんばんにちは。センチメントスコア算出企画の第2弾です。 今回は辞書ベースのセンチメントスコア算出方法を実践します。\n1. 分析手法の説明 辞書ベース手法とは 辞書ベース手法はその名の通り、辞書を用いてセンチメントスコアを算出する手法です。各単語が持つ感情極性(ここでは、ポジティブな意味かネガティブな意味かを表す)をスコア化したものをあらかじめ辞書として保存し、文書内で出現した単語とそれぞれの辞書スコアを紐付け、(何らかの方法で)集計することで、その文書全体のセンチメントスコアを算出します。\nセンチメントスコアを参照する辞書 上記においては、スコアをどのように算出するのかが重要になります。現在公開されている日本語辞書の中で有名な東京工業大学の高村研究室が作成した辞書(Takamura, Inui, and Okumura 2005)では以下のような方法で算出を行っているようです(原典はこちら)。\n まず，辞書，シソーラス（類義語辞典），コーパスデータを用いて，極性が同じになりやすい単語ペアを抽出し，そしてそれらのペアを連結することにより巨大な語彙ネットワークを構築します．たとえば，「良い」と「良好」が類義語関係にあるので，この二単語を結ぶなどの作業を行います．ここで，単語の感情極性を電子スピンの方向とみなし，語彙ネットワークをスピン系とみなして，語彙ネットワークの状態（各スピンがどの方向を向いているか）を計算します．この計算結果を見ることにより，単語の感情極性がわかるのです．\n こちらの辞書はHPで公開されており、非営利目的であれば利用が可能のようです。ちょっと覗いてみましょう。\n## 単語 読み仮名 品詞 感情極性スコア\r## 1 優れる すぐれる 動詞 1.000000\r## 2 良い よい 形容詞 0.999995\r## 3 喜ぶ よろこぶ 動詞 0.999979\r## 4 褒める ほめる 動詞 0.999979\r## 5 めでたい めでたい 形容詞 0.999645\r## 6 賢い かしこい 形容詞 0.999486\r センチメントスコアは1から-1までを取り、1に近いほどポジティブを表します。最もポジティブな単語は「優れる」となっていることがわかります。\n辞書ベース手法の利点・欠点 \u0026lt;利点\u0026gt;手法のわかりやすさ/教師データが不要👍\n辞書を利用する良い点は、手法のわかりやすさと教師データを必要としない点です。特に後者は辞書ベースならではの長所です。通常、文書分類器を作成するためには、その文書がポジティブ・ネガティブのどちらであるかという正解ラベルの付いた教師データが必要です。必要なデータは文脈にもよると思いますが、少なくとも100件は必要でしょう。これは分析者が少なくとも100件以上の文書を読んで、手作業でラベルを付与しなければならないことを意味します。\n\u0026lt;欠点\u0026gt;辞書の性能に結果が大きく依存する👎\n短所としては、特殊な文脈におけるセンチメントスコアを参集する場合は専用の辞書が必要になるという点です。辞書ベースの分類法の場合、辞書に含まれない単語はセンチメントスコアへ影響を全く与えません。また、一般に公開されている辞書はwikipedia等のオールジャンルの文書を学習することによって開発されていることが多いため、感情極性が文脈に対してニュートラルになっています。よって、例えば金融等の文脈におけるセンチメントスコアを算出する場合、専門用語の感性極性が0となったり、文脈を考慮した感性極性が得られないことから、直感と異なる結果が出る可能性があります。\n2. 辞書ベース手法の実践 使用データ では、実際に辞書ベース手法を用いて、景気ウォッチャー調査の文章からセンチメントスコアの抽出を試みます。景気ウオッチャー調査では、タクシー運転手や小売店の店主、旅館の経営者など景気に敏感な方々(景気ウオッチャー)に対して、5段階の景況感とその理由を毎月アンケートしています。5段階は、「良い(◎)」、「やや良い(○)」、「変わらない(■)」、「やや悪い(▲)」、「悪い(×)」となっています。\nデータを覗いてみると、以下のようになっています。\n\r\r基準年\r\r基準日\r\r景気の現状判断\r\r業種・職種\r\r判断の理由\r\r追加説明及び具体的状況の説明\r\r\r\r\r\r2016\r\r1月31日\r\r◎\r\r旅行代理店（従業員）\r\r販売量の動き\r\r・店頭の取扱額が前年比約120％と好調であった。\r\r\r\r2016\r\r1月31日\r\r◎\r\r観光名所（従業員）\r\r来客数の動き\r\r・当施設の利用乗降客数は１月26日時点で前年比130.1％となっており、１月としては過去最高の利用乗降客数になることが確定したほどの入込状況にある。\r\r\r\r2016\r\r1月31日\r\r○\r\r一般小売店［酒］（経営者）\r\r単価の動き\r\r・年末の消費の反動もあってか、客の動きがやや鈍い。ただ、相変わらず高額商材が売れているということもあり、売上はそれなりの金額をキープしている。\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（売場主任）\r\rお客様の様子\r\r・外国人観光客による売上が前年比152％と好調を継続しているほか、来客数が前年比102％と好調を維持している。月半ばに停滞した売上も下旬に入ってから回復傾向にあり、定価品、バーゲン品とも前年を上回っている。\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（担当者）\r\r来客数の動き\r\r・積極的に景気が上向きにあるとまではいいづらいものの、３か月前との比較では改善している。\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（販売促進担当）\r\rそれ以外\r\r・気温が平年並みとなり、これまでの温暖、少雪の状態がみられなくなってきたことで、防寒衣料、雑貨商材を中心に多少改善の傾向がみられる。\r\r\r\r\r「景気の現状判断」が景況感を表しており、その判断の理由が「追加説明及び具体的状況の説明」にある形です。よって、前者を教師データ、後者を説明データとする教師あり学習データとして利用することができます。\nセンチメントスコアの抽出 では、実践です。quantedaというパッケージを用いるため、Rで行っていきます。\nまず、前処理です。サンプルデータを読み込みます。\nfilepath \u0026lt;- r\u0026#34;(C:\\Users\\hogehoge\\Watcher\\RawData)\u0026#34; library(magrittr) files \u0026lt;- stringr::str_c(filepath, list.files(filepath, pattern = \u0026#34;*.csv\u0026#34;)) sample \u0026lt;- readr::read_csv(files,locale=readr::locale(encoding=\u0026#34;Shift-JIS\u0026#34;),show_col_types = FALSE) sample \u0026lt;- sample[sample$追加説明及び具体的状況の説明!=\u0026#34;−\u0026#34;|sample$追加説明及び具体的状況の説明!=\u0026#34;＊\u0026#34;,] sample \u0026lt;- sample[sample$景気の現状判断!=\u0026#34;□\u0026#34;,] sample$景気の現状判断 \u0026lt;- factor(sample$景気の現状判断, levels=c(\u0026#34;◎\u0026#34;,\u0026#34;○\u0026#34;,\u0026#34;▲\u0026#34;,\u0026#34;×\u0026#34;)) 次に、sudachiを用いてテキストデータのToken化を行います。Rにはsudachiを使えるパッケージが存在しないので、Pythonパッケージのsudachipyをreticulateで呼び出して使用します。\nstopwords_path \u0026lt;- r\u0026#34;(C:\\Users\\hogehoge\\Watcher\\marimo-master\\yaml\\stopwords_ja.yml)\u0026#34; # sudachiによる形態素解析→Token化 # stopwordsはmarimoを使用 ja_stopwords \u0026lt;- quanteda::dictionary(yaml::read_yaml(stopwords_path)) # コーパス生成 corp \u0026lt;- quanteda::corpus(sample,text_field=\u0026#34;追加説明及び具体的状況の説明\u0026#34;) # sudachipy呼び出し→インスタンス化 sudachipy \u0026lt;- reticulate::import(\u0026#34;sudachipy\u0026#34;) # 正規化関数定義 sudachi_normalize \u0026lt;- function(tokens){ res \u0026lt;- c() for(i in 0:(length(tokens)-1)){ res \u0026lt;- append(res,tokens[i]$normalized_form()) } return(res) } # tokenize関数定義 sudachi_tokenize \u0026lt;- function(sentence, mode = \u0026#34;A\u0026#34;){ tokenizer_obj \u0026lt;- sudachipy$dictionary$Dictionary()$create() mode \u0026lt;- switch(mode, \u0026#34;A\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$A, \u0026#34;B\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$B, \u0026#34;C\u0026#34; = sudachipy$tokenizer$Tokenizer$SplitMode$C, stop(\u0026#34;Only Can Use A, B, C\u0026#34;)) res \u0026lt;- purrr::map(sentence, ~sudachi_normalize(tokenizer_obj$tokenize(., mode))) return(res) } # Token化実行 toks_sent \u0026lt;- corp %\u0026gt;% sudachi_tokenize() %\u0026gt;% quanteda::as.tokens() %\u0026gt;% quanteda::tokens_remove(ja_stopwords, padding = TRUE) # メタデータをTokenに再付与 quanteda::docvars(toks_sent) \u0026lt;- quanteda::docvars(corp) このtoks_sentから文書行列を生成します。文書行列とは、単語と文書の関係を表す行列で、各行が単語(token)、各列が文書を表し、各要素は文書中の単語の出現回数となっています。\n# 文書行列の生成 dfmt_sent \u0026lt;- toks_sent %\u0026gt;% quanteda::dfm() %\u0026gt;% quanteda::dfm_remove(pattern=\u0026#34;\u0026#34;) %\u0026gt;% quanteda::dfm_trim(min_termfreq = 10) 文書行列dfmt_sentと各単語のスコアベクトルの内積を取り、文書のセンチメントスコアを算出します。\n# 単語感情極性対応表 PNdict \u0026lt;- read.delim(\u0026#34;http://www.lr.pi.titech.ac.jp/~takamura/pubs/pn_ja.dic\u0026#34;,header=FALSE,sep=\u0026#34;:\u0026#34;) # 辞書の並びに合わせて文書行列の列を入れ替える。 dfmt_sent_arranged \u0026lt;- quanteda::dfm_match(dfmt_sent, PNdict$V1) # 文書のセンチメントスコア算出 n \u0026lt;- unname(quanteda::rowSums(dfmt_sent_arranged)) score_dict \u0026lt;- scale(ifelse(n\u0026gt;0, quanteda::rowSums(dfmt_sent_arranged %*% PNdict$V4)/n, NA)) sample_with_score_dict \u0026lt;- sample %\u0026gt;% cbind(score_dict) サンプルデータを抜き出して、センチメントスコアを観察してみましょう。\nsample_with_score_dict %\u0026gt;% head() %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r基準年\r\r基準日\r\r景気の現状判断\r\r業種・職種\r\r判断の理由\r\r追加説明及び具体的状況の説明\r\rscore_dict\r\r\r\r\r\r2016\r\r1月31日\r\r◎\r\r旅行代理店（従業員）\r\r販売量の動き\r\r・店頭の取扱額が前年比約120％と好調であった。\r\r1.1372119\r\r\r\r2016\r\r1月31日\r\r◎\r\r観光名所（従業員）\r\r来客数の動き\r\r・当施設の利用乗降客数は１月26日時点で前年比130.1％となっており、１月としては過去最高の利用乗降客数になることが確定したほどの入込状況にある。\r\r-0.7233374\r\r\r\r2016\r\r1月31日\r\r○\r\r一般小売店［酒］（経営者）\r\r単価の動き\r\r・年末の消費の反動もあってか、客の動きがやや鈍い。ただ、相変わらず高額商材が売れているということもあり、売上はそれなりの金額をキープしている。\r\r-0.9988214\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（売場主任）\r\rお客様の様子\r\r・外国人観光客による売上が前年比152％と好調を継続しているほか、来客数が前年比102％と好調を維持している。月半ばに停滞した売上も下旬に入ってから回復傾向にあり、定価品、バーゲン品とも前年を上回っている。\r\r1.1159495\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（担当者）\r\r来客数の動き\r\r・積極的に景気が上向きにあるとまではいいづらいものの、３か月前との比較では改善している。\r\r3.2269249\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（販売促進担当）\r\rそれ以外\r\r・気温が平年並みとなり、これまでの温暖、少雪の状態がみられなくなってきたことで、防寒衣料、雑貨商材を中心に多少改善の傾向がみられる。\r\r0.0416559\r\r\r\r\r2行目の文章は明らかにポジティブですが、なにが影響したのかネガティブ寄りの判定になっていますね。また、顕著なのが3行目で「鈍い」という単語に引っ張られ、ネガティブなセンチメントを抽出しやすくなっています。この文章の本意は「ただ・・・」という逆接以降の文章ですが、文脈や語順が捉えられないために正しい情報を引き出すことができていません。5行目は「積極的」、「上向き」、「改善」といった単語を検知し、かなりポジティブなセンチメントがあるという判定をしていますが、「積極的に景気が上向きとはいいづらい」というネガティブな文脈を捉え切れておらず、過大評価となっています。\n# 結果の可視化 library(ggplot2) ggplot(sample_with_score_dict, aes(x=景気の現状判断,y=score_dict)) + geom_boxplot() 「景気の現状判断」毎にboxplotを書いてみましたが、ぱっと見はあまり変わらない結果になりました。 ほんの少し緩やかに右下がりのような気もしますが、文書分類可能な結果ではないことがわかります。\n(追加検証)tf-idfを用いた算出 上記の分析では単語の出現回数を利用していましたが、tf-idfを用いた分析も行うことにします。tf-idfとは、term frequency-inverse document frequencyの略で、情報検索やテキストマイニングでよく使われる重みのことです。この重みは、ある単語がコーパス1の中の文書にとってどれだけ重要かを評価するために使われる統計的尺度で、その特徴量は文書中の単語の出現回数が増加するほど上昇しますが、コーパス中の単語の頻度によって相殺されます。例えば、迷惑メールを分類することを意図した電子メールのコーパスにおいて、「お知らせ」や「ニュース」などは出現頻度が高い一方でどの文書にも出現するため、迷惑メールを特定するという点においては重要度は高くありません。一方で、「当選」や「今だけ！」などはコーパス上にはそこまで出現頻度が高くない単語であるものの、迷惑メールの文章中には頻度が高く出現すると考えられます。tf-idfでは後者の単語の特徴量が高くなるように定義されています。\ntf-idfの計算方法はtfとidfに分かれます。\n tf: Term frequency 単語が文書中にどれだけ頻繁に出現しているかを測定するもの。文書の長さはそれぞれ異なるため、長い文書では短い文書よりはるかに多くの用語が出現する可能性があります。そのため、正規化の方法として、単語頻度を文書の長さ（文書内の単語総数）で割ることがよくあります。  $$ tf(t) = \\frac{文章内の単語tの出現回数}{文章内の単語総数} $$\n-idf: Inverse Document Frequency ある単語がコーパス内でどの程度希少であるかを測定するもの。「です」、「は」、「が」などの頻出語を減らして、希少語を増やす意図があります。\n$$ idf(t) = \\log(\\frac{コーパス内の文書総数}{単語tを含む文書数}) $$\nrで実装してみます。tf-idfはquanteda::dfm_tfidf()で計算できます。引数には、文書行列を渡します。そのほかは上記の分析と同じです。\n# tf-idf版文書行列の生成 dfmt_tfidf_sent \u0026lt;- dfmt_sent %\u0026gt;% quanteda::dfm_tfidf() # 辞書の並びに合わせて文書行列の列を入れ替える。 dfmt_tfidf_sent_arranged \u0026lt;- quanteda::dfm_match(dfmt_tfidf_sent, PNdict$V1) # 文書のセンチメントスコア算出 n \u0026lt;- unname(quanteda::rowSums(dfmt_tfidf_sent_arranged)) score_dict \u0026lt;- scale(ifelse(n\u0026gt;0, quanteda::rowSums(dfmt_tfidf_sent_arranged %*% PNdict$V4)/n, NA)) sample_with_score_dict \u0026lt;- sample %\u0026gt;% cbind(score_dict) 先ほどと同じようにサンプルを抜き出してみます。\nsample_with_score_dict %\u0026gt;% head() %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r基準年\r\r基準日\r\r景気の現状判断\r\r業種・職種\r\r判断の理由\r\r追加説明及び具体的状況の説明\r\rscore_dict\r\r\r\r\r\r2016\r\r1月31日\r\r◎\r\r旅行代理店（従業員）\r\r販売量の動き\r\r・店頭の取扱額が前年比約120％と好調であった。\r\r0.1641589\r\r\r\r2016\r\r1月31日\r\r◎\r\r観光名所（従業員）\r\r来客数の動き\r\r・当施設の利用乗降客数は１月26日時点で前年比130.1％となっており、１月としては過去最高の利用乗降客数になることが確定したほどの入込状況にある。\r\r-0.0770581\r\r\r\r2016\r\r1月31日\r\r○\r\r一般小売店［酒］（経営者）\r\r単価の動き\r\r・年末の消費の反動もあってか、客の動きがやや鈍い。ただ、相変わらず高額商材が売れているということもあり、売上はそれなりの金額をキープしている。\r\r-0.8631460\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（売場主任）\r\rお客様の様子\r\r・外国人観光客による売上が前年比152％と好調を継続しているほか、来客数が前年比102％と好調を維持している。月半ばに停滞した売上も下旬に入ってから回復傾向にあり、定価品、バーゲン品とも前年を上回っている。\r\r1.0510362\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（担当者）\r\r来客数の動き\r\r・積極的に景気が上向きにあるとまではいいづらいものの、３か月前との比較では改善している。\r\r2.3897946\r\r\r\r2016\r\r1月31日\r\r○\r\r百貨店（販売促進担当）\r\rそれ以外\r\r・気温が平年並みとなり、これまでの温暖、少雪の状態がみられなくなってきたことで、防寒衣料、雑貨商材を中心に多少改善の傾向がみられる。\r\r0.8152262\r\r\r\r\r傾向は先ほどの分析とさほど変わらないように見受けられます。 boxplotの結果も見てみましょう。\n# 結果の可視化 library(ggplot2) ggplot(sample_with_score_dict, aes(x=景気の現状判断,y=score_dict)) + geom_boxplot() 右下がりの傾向が強くなったかもしれません。ただ、依然として景気判断別に見たスコアの分布は差がなく、文書分類ができる精度ではないことがわかります。\n3. 終わりに 最も簡易な手法ということで予想はしていましたが、辞書ベース手法ではセンチメントスコアの抽出はできませんでした。もちろん、私のやり方が悪いのが理由で上手くやれば文書の分類は可能かもしれません。ただ、経済に特化した極性辞書を準備しなければ難しいのではないかというのが、私の感想です。 次はナイーブベイズ分類器を用いた文書分類に取り組みます。 ここまで、読み進めて頂きましてありがとうございました。次回の記事も期待していてください。\nTakamura, Hiroya, Takashi Inui, and Manabu Okumura. 2005. “The 43rd Annual Meeting.” In. Association for Computational Linguistics. https://doi.org/10.3115/1219840.1219857.\n\r\r  文章を構造化し大規模に集積したもの \u0026#x21a9;\u0026#xfe0e;\n   ","date":1648080000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1648080000,"objectID":"33a9e17152e4b9767e936af3103bf76b","permalink":"/post/post27/","publishdate":"2022-03-24T00:00:00Z","relpermalink":"/post/post27/","section":"post","summary":"辞書ベース手法を用いたセンチメントスコアの算出を実践します。","tags":["Python","前処理","機械学習","テキスト解析"],"title":"【徹底比較】センチメントスコア算出手法！！ - 第2回","type":"post"},{"authors":null,"categories":["単発"],"content":"\r\r\r\r\r\r\r\r\r\r\r\r\r\rおはこんばんにちは。テキスト解析はデータ分析界隈ではもうかなり当たり前になってきています。テキストの感性情報(センチメント)を抽出する手法には、どのようなものがあるかあるのか調べたところかなり時代は進んでいると実感しました。これら数回にわたって、実際にRやPythonで作成した機械学習モデルの精度を比較してみたいと思います。 今回はその第１回目ということで、データセットや全体感のお話です。\n1. 解きたい問題 今回解きたい問題は「景況感を表す文書表現を学習し、実際の景気判断を予測する」というタスクです。 データセットには、内閣府が出している景気ウオッチャー調査を使用します。\n景気ウオッチャー調査とは 景気ウオッチャー調査とは、内閣府が月次で公表している景気動向判断のための統計情報です。調査の目的は以下の通りです(内閣府Webサイトより)。\n \u0026lt;調査の目的\u0026gt;  地域の景気に関連の深い動きを観察できる立場にある人々の協力を得て、地域ごとの景気動向を的確かつ迅速に把握し、景気動向判断の基礎資料とすることを目的とする。\n  では、この「地域の景気に関連の深い動きを観察できる立場にある人々」、「地域ごとの景気動向」、「的確かつ迅速に」という部分について少し深掘ってみます。\n調査の対象範囲 調査の対象範囲は47都道府県となっています。また、調査対象者は「家計動向、企業動向、雇用等、代表的な経済活動項目の動向を敏感に反映する現象を観察できる業種の適当な職種の中から選定した2,050人」となっており、例えば小売店や飲食店の店員・店長、タクシー運転手、製造業・非製造業企業の経営者・従業員、人材派遣会社社員などを対象に行われています。\n調査期日および期間 調査は月次で行われ、毎月25日から月末にかけて実施されます。当月の景況感について調査が行われ、翌月に公表されます。\n調査の実施方法 アンケート調査を電話・Webサイト・電子メールのいずれかで回答することになっています。質問項目は以下の通りです(調査票)。\n  景気の現状に対する判断（方向性）\n  1.の理由\n  2.の追加説明及び具体的状況の説明(自由記述)\n  景気の先行きに対する判断(方向性)\n  (4)の理由\n  (参考)景気の現状に対する判断(水準)\n  ご参考までに2016年1月調査のサンプルデータをお見せします。\nfilePath \u0026lt;- r\u0026#34;(C:\\Users\\hogehoge\\Watcher\\RawData\\watcher_2016.csv)\u0026#34; library(magrittr) sample \u0026lt;- readr::read_csv(filePath,locale=readr::locale(encoding=\u0026#34;SHIFT-JIS\u0026#34;), show_col_types=FALSE) options(kableExtra.html.bsTable = TRUE) knitr::kable(head(sample)) %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r基準年 \r基準日 \r景気の現状判断 \r業種・職種 \r判断の理由 \r追加説明及び具体的状況の説明 \r\r\r\r\r2016 \r1月31日 \r◎ \r旅行代理店（従業員） \r販売量の動き \r・店頭の取扱額が前年比約120％と好調であった。 \r\r\r2016 \r1月31日 \r◎ \r観光名所（従業員） \r来客数の動き \r・当施設の利用乗降客数は１月26日時点で前年比130.1％となっており、１月としては過去最高の利用乗降客数になることが確定したほどの入込状況にある。 \r\r\r2016 \r1月31日 \r○ \r一般小売店［酒］（経営者） \r単価の動き \r・年末の消費の反動もあってか、客の動きがやや鈍い。ただ、相変わらず高額商材が売れているということもあり、売上はそれなりの金額をキープしている。 \r\r\r2016 \r1月31日 \r○ \r百貨店（売場主任） \rお客様の様子 \r・外国人観光客による売上が前年比152％と好調を継続しているほか、来客数が前年比102％と好調を維持している。月半ばに停滞した売上も下旬に入ってから回復傾向にあり、定価品、バーゲン品とも前年を上回っている。 \r\r\r2016 \r1月31日 \r○ \r百貨店（担当者） \r来客数の動き \r・積極的に景気が上向きにあるとまではいいづらいものの、３か月前との比較では改善している。 \r\r\r2016 \r1月31日 \r○ \r百貨店（販売促進担当） \rそれ以外 \r・気温が平年並みとなり、これまでの温暖、少雪の状態がみられなくなってきたことで、防寒衣料、雑貨商材を中心に多少改善の傾向がみられる。 \r\r\r\r問題設定 今回の問題設定は、「追加説明及び具体的状況の説明」に記載されているテキスト情報をもとに、「景気の現状判断」を予測するタスクになります。 「景気の現状判断」は「良い(◎)」、「やや良い(○)」、「変わらない(■)」、「やや悪い(▲)」、「悪い(×)」となっており、これを2値分類問題にして学習を行います(変わらない(■)はサンプルから除外)。\n2. テキスト解析手法の進化の譜系 上記問題設定を解くためには、テキスト情報から感性情報(センチメント)を抽出する必要があります。私が調べた浅い知識によると、感性情報の抽出方法には以下のような手法があります。\n 辞書ベース ナイーブベイズ分類器 単語埋め込み(Word Embedding)を用いた方法 Recurrent neural network(RNN) Transformer  おそらく、下に行くにつれてより最新の手法になっており、精度も高いのではないかと推察されます。 また、1,2,3はやり方にもよるんでしょうが文章が与えられた際にその語順関係を考慮しません。 今後、各手法について解析結果の記事を投稿する予定ですので、詳しい中身についてはそれぞれの回で触れたいと思います。\n3. サンプルデータ概観 サンプルデータの読み込み 今回使用する景気ウオッチャー調査のサンプルは2016年~2020年のものとなっています。現状判断の調査結果を格のしたcsvファイル(景気判断理由集)を月ごとに内閣府のWebページからダウンロードし、整形したものを年ごとに連結しています。\nfilepath = r\u0026#34;(C:\\Users\\hogehoge\\Watcher\\RawData)\u0026#34; files \u0026lt;- stringr::str_c(filepath, \u0026#34;\\\\\u0026#34;,list.files(filepath, pattern = \u0026#34;*.csv\u0026#34;)) sample \u0026lt;- readr::read_csv(files, locale=readr::locale(encoding=\u0026#34;SHIFT-JIS\u0026#34;), show_col_types = FALSE) # 「－」は回答が存在しない、「＊」は主だった回答等が存在しないため、除外 sample \u0026lt;- sample[sample$追加説明及び具体的状況の説明 != \u0026#34;－\u0026#34;,] sample \u0026lt;- sample[sample$追加説明及び具体的状況の説明 != \u0026#34;＊\u0026#34;,] 知らなかったんですが、readr::read_csvって2.0.0からファイルパスのリストを引数としてcsvを読み込み、その結果を連結してくれるようになったんですね。地味に便利です。\nサンプルサイズ サンプルサイズは76800でした。サンプルサイズとしては申し分ないかと思います。\nNROW(sample) ## [1] 76800\r念のため、各年ごとにも見てみます。\nlibrary(magrittr) sample %\u0026gt;% dplyr::group_by(基準年) %\u0026gt;% dplyr::summarise(サンプルサイズ=dplyr::n()) %\u0026gt;% dplyr::mutate(`割合(%)`=round(サンプルサイズ/sum(サンプルサイズ)*100, digits=1)) %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r基準年 \rサンプルサイズ \r割合(%) \r\r\r\r\r2016 \r15168 \r19.8 \r\r\r2017 \r16724 \r21.8 \r\r\r2018 \r14756 \r19.2 \r\r\r2019 \r14588 \r19.0 \r\r\r2020 \r15564 \r20.3 \r\r\r\r各年およそ14,000~16,000サンプルほど存在するようです。大きなバラツキもありません。\n景気の現状判断の分布 これから予測を行う景気の現状判断はどのような分布になっているでしょうか。\nsample$景気の現状判断 \u0026lt;- factor(sample$景気の現状判断, levels=c(\u0026#34;◎\u0026#34;,\u0026#34;○\u0026#34;,\u0026#34;□\u0026#34;,\u0026#34;▲\u0026#34;,\u0026#34;×\u0026#34;)) sample %\u0026gt;% dplyr::group_by(景気の現状判断) %\u0026gt;% dplyr::summarise(サンプルサイズ=dplyr::n()) %\u0026gt;% dplyr::mutate(`割合(%)`=round(サンプルサイズ/sum(サンプルサイズ)*100, digits=1)) %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r景気の現状判断 \rサンプルサイズ \r割合(%) \r\r\r\r\r◎ \r1569 \r2.0 \r\r\r○ \r14370 \r18.7 \r\r\r□ \r34438 \r44.8 \r\r\r▲ \r18642 \r24.3 \r\r\r× \r7781 \r10.1 \r\r\r\r思ったよりキレイな分布をしている印象です。このサンプル期間では、「どちらとも言えない」が最も多く、また、比較的「やや悪い」や「悪い」が多いことがわかります。このあたりは2019年度以降のコロナ禍の影響を反映している可能性があるので、新たに時間軸を追加し、クロス集計をしてみましょう。\nsample %\u0026gt;% dplyr::group_by(基準年, 景気の現状判断) %\u0026gt;% dplyr::tally() %\u0026gt;% tidyr::spread(景気の現状判断, n) %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r基準年 \r◎ \r○ \r□ \r▲ \r× \r\r\r\r\r2016 \r235 \r2455 \r7546 \r4129 \r803 \r\r\r2017 \r375 \r4031 \r8681 \r3053 \r584 \r\r\r2018 \r349 \r3105 \r7340 \r3336 \r626 \r\r\r2019 \r237 \r2050 \r6743 \r4516 \r1042 \r\r\r2020 \r373 \r2729 \r4128 \r3608 \r4726 \r\r\r\rやはり、2019年以降「悪い」が多く出現していることがわかります。一方で、「やや悪い」はそれ以前の年でも恒常的に多いようです。\n回答者の属性 次に回答者の属性を確認しましょう。回答者の属性は業種・職種から取得できます。まず、どういった業種が多く回答しているのか確認してみます。回答数上位10番目までの業種を表示させます。\nsample %\u0026gt;% dplyr::mutate(業種 = stringr::str_remove(sample$`業種・職種`, \u0026#34;（.+）\u0026#34;) %\u0026gt;% stringr::str_remove(\u0026#34;［.+］\u0026#34;)) %\u0026gt;% dplyr::group_by(業種) %\u0026gt;% dplyr::summarise(サンプルサイズ=dplyr::n()) %\u0026gt;% dplyr::arrange(desc(サンプルサイズ)) %\u0026gt;% dplyr::top_n(10, サンプルサイズ) %\u0026gt;% dplyr::mutate(`割合(%)`=round(サンプルサイズ/sum(サンプルサイズ)*100, digits=1)) %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r業種 \rサンプルサイズ \r割合(%) \r\r\r\r\r百貨店 \r4959 \r15.1 \r\r\rスーパー \r4738 \r14.4 \r\r\r一般小売店 \r3851 \r11.7 \r\r\rコンビニ \r3772 \r11.5 \r\r\r乗用車販売店 \r2967 \r9.0 \r\r\r通信会社 \r2866 \r8.7 \r\r\r商店街 \r2734 \r8.3 \r\r\r人材派遣会社 \r2700 \r8.2 \r\r\r衣料品専門店 \r2128 \r6.5 \r\r\r職業安定所 \r2088 \r6.4 \r\r\r\r回答数上位には、百貨店やスーパーなど小売店が目立ちます。また、車のディーラーの方や人材派遣会社など生産・雇用に関連する業種の回答者も多いようです。\n次に、経営者や職員など職種の分布を確認します。こちらも上位10位までを表示します。\nsample %\u0026gt;% dplyr::mutate(職種 = stringr::str_extract(sample$`業種・職種`, \u0026#34;（.+）\u0026#34;) %\u0026gt;% stringr::str_remove(\u0026#34;（\u0026#34;) %\u0026gt;% stringr::str_remove(\u0026#34;）\u0026#34;)) %\u0026gt;% dplyr::group_by(職種) %\u0026gt;% dplyr::summarise(サンプルサイズ=dplyr::n()) %\u0026gt;% dplyr::arrange(desc(サンプルサイズ)) %\u0026gt;% dplyr::top_n(10, サンプルサイズ) %\u0026gt;% dplyr::mutate(`割合(%)`=round(サンプルサイズ/sum(サンプルサイズ)*100, digits=1)) %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r職種 \rサンプルサイズ \r割合(%) \r\r\r\r\r経営者 \r20612 \r40.0 \r\r\r営業担当 \r7057 \r13.7 \r\r\r従業員 \r4371 \r8.5 \r\r\r店長 \r4343 \r8.4 \r\r\rNA \r3145 \r6.1 \r\r\r職員 \r3118 \r6.1 \r\r\r総務担当 \r2930 \r5.7 \r\r\r代表者 \r2615 \r5.1 \r\r\r社員 \r1796 \r3.5 \r\r\r役員 \r1518 \r2.9 \r\r\r\r職種別では経営者がダントツで多い結果となりました。代表者など表記揺れもありますので、実態はもう少し多いと思います。想像では、現場の職員の方の回答が多いと思っていましたが予想とは異なる結果となりました。\n(景気)判断の理由の分布 次に(景気)判断の理由の分布を見ます。\nresults \u0026lt;- sample %\u0026gt;% dplyr::group_by(判断の理由) %\u0026gt;% dplyr::summarise(サンプルサイズ=dplyr::n()) %\u0026gt;% dplyr::arrange(desc(サンプルサイズ)) %\u0026gt;% dplyr::mutate(`割合(%)`=round(サンプルサイズ/sum(サンプルサイズ)*100, digits=1)) results %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling(bootstrap_options = c(\u0026#34;striped\u0026#34;)) \r\r判断の理由 \rサンプルサイズ \r割合(%) \r\r\r\r\r来客数の動き \r17432 \r22.7 \r\r\r販売量の動き \r16153 \r21.0 \r\r\rお客様の様子 \r11511 \r15.0 \r\r\r受注量や販売量の動き \r9033 \r11.8 \r\r\r取引先の様子 \r5034 \r6.6 \r\r\r求人数の動き \r4155 \r5.4 \r\r\rそれ以外 \r4007 \r5.2 \r\r\r単価の動き \r3654 \r4.8 \r\r\r競争相手の様子 \r1603 \r2.1 \r\r\r周辺企業の様子 \r1161 \r1.5 \r\r\r受注価格や販売価格の動き \r1099 \r1.4 \r\r\r求職者数の動き \r930 \r1.2 \r\r\r採用者数の動き \r666 \r0.9 \r\r\r雇用形態の様子 \r361 \r0.5 \r\r\r販売量の動き \r1 \r0.0 \r\r\r\r「来客数の動き」と「販売量の動き」が多く、「お客様の様子」や「受注量や販売量の動き」まで含めると7割を占めます。回答業種に小売店が多いことから、このような項目が上位にきているのだと推察されます。\n景気判断の理由に関するテキストマイニング 景気の現状判断に対する理由の追加説明及び具体的状況の説明を解析し、どのような単語が頻繁に使用されているかをWordCloudを用いて確認します。\nまず、文書のトークン化を行います。日本語のような言語では英語などと異なり単語ごとの間にスペースがないため、別途区切りを入れてやる必要があります。区切られた各語は形態素と呼ばれ、言葉が意味を持つまとまりの単語の最小単位を指します。また、文章を形態素へ分割することを形態素解析と言います(英語のような場合単にTokenizationと言ったりします)。\n形態素解析を行うツールは以下のようなものが存在します。\n MeCab JUMAN JANOME Ginza Sudachi  おそらく最も使用されているものはMecabではないかと思いますが、標準装備されている辞書(ipadic)の更新がストップしており、最近の新語に対応できないという問題があります。この点については新語に強いNEologd辞書を加えることで、対処可能であることを別記事で紹介していますが、今回はワークスアプリケーションズが提供しているSudachi[@TAKAOKA18.8884]を使用することにしたいと思います。公式GithubからSudachiの特長を引用します。\n  複数の分割単位の併用\n 必要に応じて切り替え 形態素解析と固有表現抽出の融合    多数の収録語彙\n UniDic と NEologd をベースに調整    機能のプラグイン化\n 文字正規化や未知語処理に機能追加が可能    同義語辞書との連携\n 後日公開予定    特質すべきは「複数の分割単位の併用」でしょう。Sudachiでは短い方から A, B, Cの3つの分割モードを提供しています。AはUniDic短単位相当、Cは固有表現相当、BはA, Cの中間的な単位となっており、以下のように同じ「選挙管理委員会」という単語でも形態素が異なることが確認できます。これはSudachi特有の特長になります。\nfrom sudachipy import tokenizer from sudachipy import dictionary tokenizer_obj = dictionary.Dictionary().create() mode = tokenizer.Tokenizer.SplitMode.A [m.normalized_form() for m in tokenizer_obj.tokenize(\u0026#34;選挙管理委員会\u0026#34;, mode)] ## ['選挙', '管理', '委員', '会']\rmode = tokenizer.Tokenizer.SplitMode.B [m.normalized_form() for m in tokenizer_obj.tokenize(\u0026#34;選挙管理委員会\u0026#34;, mode)] ## ['選挙', '管理', '委員会']\rmode = tokenizer.Tokenizer.SplitMode.C [m.normalized_form() for m in tokenizer_obj.tokenize(\u0026#34;選挙管理委員会\u0026#34;, mode)] ## ['選挙管理委員会']\rまた、辞書についてもUniDicとNEologdをベースとして更新が続けられており、新語にも対応できます。\nmode = tokenizer.Tokenizer.SplitMode.C [m.normalized_form() for m in tokenizer_obj.tokenize(\u0026#34;新型コロナウイルス\u0026#34;, mode)] ## ['新型コロナウイルス']\r個人的に素晴らしいと思うポイントは表記正規化や文字正規化ができると言うことです。以下のように旧字等で同じ意味だが表記が異なる単語や英語/日本語、書き間違え等を正規化する機能があります。\ntokenizer_obj.tokenize(\u0026#34;附属\u0026#34;, mode)[0].normalized_form() ## '付属'\rtokenizer_obj.tokenize(\u0026#34;SUMMER\u0026#34;, mode)[0].normalized_form() ## 'サマー'\rtokenizer_obj.tokenize(\u0026#34;シュミレーション\u0026#34;, mode)[0].normalized_form() ## 'シミュレーション'\rこのような高性能な形態素解析ツールが無償でしかも商用利用も可というところに驚きを隠せません。次回以降で説明しますが、このSudachiで形態素解析を行ったWord2vecモデルであるChiveも提供されています。\n説明が長くなりましたがSudachiでTokenizationを行いましょう。SudachiはPythonで使用することができます。説明が前後していますが、上記コードで行っているようにtokenizerオブジェクトを作成し、tokenize()メソッドで文章をTokenizeします。Tokenizeされた結果はMorphemeListオブジェクトに格納されます。MorphemeListオブジェクトの各要素に対してnormalized_form()メソッドを実行することで正規化された形態素を取得することができます。ここまでをやってみます。\nまず、サンプルデータを読み込みます。\nfrom sudachipy import tokenizer from sudachipy import dictionary from sklearn.preprocessing import LabelEncoder import pandas as pd sample = pd.read_csv(r\u0026#34;C:\\Users\\hogehoge\\Watcher\\RawData\\watcher_2016.csv\u0026#34;,encoding=\u0026#34;shift-jis\u0026#34;) sample = sample[(sample.追加説明及び具体的状況の説明==\u0026#39;−\u0026#39;)|(sample.追加説明及び具体的状況の説明==\u0026#39;＊\u0026#39;)==False|(sample.景気の現状判断==\u0026#39;□\u0026#39;)] 読み込んだsampleをtokenizerでToken化していきます。\ntokenizer_obj = dictionary.Dictionary().create() mode = tokenizer.Tokenizer.SplitMode.C tokenizer_sudachi = lambda t: [m.normalized_form() for m in tokenizer_obj.tokenize(t, mode)] tokens = sample.追加説明及び具体的状況の説明.map(tokenizer_sudachi) tokens.head() ## 0 [・, 店頭, の, 取り扱い, 額, が, 前年, 比, 約, 120, %, と, 好調...\r## 1 [・, 当, 施設, の, 利用, 乗降, 客数, は, 1, 月, 26, 日, 時点, ...\r## 2 [・, 年, 末, の, 消費, の, 反動, も, 有る, て, か, 、, 客, の, ...\r## 3 [・, 外国人, 観光客, に, よる, 売り上げ, が, 前年, 比, 152, %, と...\r## 4 [・, 積極的, だ, 景気, が, 上向き, に, 有る, と, まで, は, 言う, 辛...\r## Name: 追加説明及び具体的状況の説明, dtype: object\rTokenizeすることができました。 WordCloudを作成するためにPythonのwordcloudを用います。これは引数にTokenをスペースで区切った文字列を与える必要があるため、その整形を行います。\nword_lists = [] for token in tokens: for word in token: word_lists.append(word) word_chain = \u0026#39; \u0026#39;.join(word_lists) では、wordcloudを作成していきます。日本語表示では文字化けが発生するため、こちらでフォント「IPAexゴシック」を予めダウンロードし、フォルダに保存したものを読み込んでいます。\nfont_path = r\u0026#39;C:\\Users\\hogehoge\\Watcher\\ipaexg.ttf\u0026#39; wordcloud.WordCloud.genetate()でWordCloudを作成します。\nfrom wordcloud import WordCloud import matplotlib.pyplot as plt wc = WordCloud(background_color=\u0026#34;white\u0026#34;, font_path=font_path, max_words=100, max_font_size=100, contour_width=1, contour_color=\u0026#39;steelblue\u0026#39;) wc.generate(word_chain) plt.figure(figsize=(12,10)) plt.imshow(wc, cmap=plt.cm.gray, interpolation=\u0026#34;bilinear\u0026#34;) plt.axis(\u0026#34;off\u0026#34;) plt.show() 助詞や「為る」、「有る」、「居る」、「成る」など特徴のない語句が頻出しており、意味のある可視化になっていません。これらをword_chainから除き、もう一度実行してみます。\nstopwords = [\u0026#34;見る\u0026#34;,\u0026#34;為る\u0026#34;,\u0026#34;有る\u0026#34;,\u0026#34;居る\u0026#34;,\u0026#34;成る\u0026#34;,\u0026#34;は\u0026#34;,\u0026#34;に\u0026#34;,\u0026#34;よる\u0026#34;,\u0026#34;た\u0026#34;,\u0026#34;が\u0026#34;,\u0026#34;て\u0026#34;,\u0026#34;だ\u0026#34;,\u0026#34;など\u0026#34;,\u0026#34;と\u0026#34;,\u0026#34;も\u0026#34;,\u0026#34;の\u0026#34;,\u0026#34;や\u0026#34;,\u0026#34;で\u0026#34;,\u0026#34;から\u0026#34;,\u0026#34;を\u0026#34;,\u0026#34;おる\u0026#34;,\u0026#34;より\u0026#34;,\u0026#34;等\u0026#34;] for stopword in stopwords: word_chain = word_chain.replace(stopword, \u0026#34;\u0026#34;) wc.generate(word_chain) plt.figure(figsize=(12,10)) plt.imshow(wc, cmap=plt.cm.gray, interpolation=\u0026#34;bilinear\u0026#34;) plt.axis(\u0026#34;off\u0026#34;) plt.show() 先ほどよりは意味のある可視化になっています。「来客数」、「売り上げ」、「販売量」などやはり小売店関連のワードが頻出しているようです。これは回答者属性とも整合的です。\n4. 終わりに これから各回で以下の手法を用いた予測モデルの構築を行っていきます。次回は、辞書ベースに基づくセンチメントの抽出を行い、文書のポジネガ分類を行います。\n 辞書ベース ナイーブベイズ分類器 単語埋め込み(Word Embedding)を用いた方法 Recurrent neural network(RNN) Transformer  実は、解析自体は全て完了しており、文書を書くだけのフェーズになっています。ちゃんと結果も出ていますので、乞うご期待です！\n","date":1647820800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1647820800,"objectID":"a0f5d68fe530347abecf8979041ce8c6","permalink":"/post/post25/","publishdate":"2022-03-21T00:00:00Z","relpermalink":"/post/post25/","section":"post","summary":"テキストからセンチメント情報を抽出する手法を数回にわたって紹介します。","tags":["Python","前処理","機械学習","テキスト解析"],"title":"【徹底比較】センチメントスコア算出手法！！ - 第1回","type":"post"},{"authors":null,"categories":["キャリア"],"content":"1. 退職しました おはこんばんにちは。かなり久しぶりの更新になってしまいました。この間、2021年7月7日に結婚し、既婚者になりました。また、2021年4月に同じ部署内のチームを移動し、BPR・業務インフラ整備・金融規制対応などロジ周りの企画に役割が変わりました。そして、2022年2月28日に現職を退社し(最終出社は1月28日)、3月1日から関西の電子部品メーカーで働くことになりました。\n2. 転職理由 入社してエコノミストが求められていないことに気づいた そもそも、現職の資産運用会社にはエコノミストを志望して入社しました。金銭的問題から博士課程後期の進学を断念し、大学院時代の研究内容に近い職種に就きたかったためです。バイサイドのエコノミストを志望したのは、単にファンドマネージャー相手の方が高度な分析をしても許されるんじゃないかという短絡的な理由からでした。\nしかし、入社してみると、希望の部署に配属になることは出来ませんでした。最初は機関投資家営業に配属になり、機関投資家向け(年金基金や金融法人)の顧客報告書をひたすら作成していました。社内DBから取得したファンドのパフォーマンスや残高を定められたフォーマットに転記し、ファンドマネージャーにコメントをもらう仕事です。2部署目(3年目)で運用企画に異動になりましたが、こちらでも(社内)資料を作成していました。企画の議論は秘匿性が高く基本的にシニア層のみで行われるため、若手は目的もはっきりとしないまま(ざっくりとした説明はありますが詳細は聞けない)、手を動かすことが求められていました。このように、資料作成要員として3年ほど過ごし、自分の成長として感じられたのは、ExcelやPowerPointに習熟していくことのみでした。次第に「市場価値など全くない人材になってるな」と危機感を覚えましたが、上司からは「まだ若いんだから」と真面目に受け止めてもらえませんでした。\n同時に、エコノミストの言動を参考にしているファンドマネージャーなどほぼ存在しないことも分かりました。エコノミストの付加価値はセミナーでの講演で、その講演でも自分で分析した内容を話すと言うよりはシンクタンクなどのレポートを要約して資料を再構成し、説明するという非生産的な仕事をしているようでした。自分の就職活動時の短絡的な発想を反省しつつ、「やはりマクロ経済学ではお金にならないんだな」ということを再確認しました。\n転職理由の半分は結婚 先述の通り、2021年7月7日に結婚しました。妻とは大学生以来の付き合いであり、7年の交際期間がありました。どちらも地元が関西であり、子供には関西弁をしゃべらせたいという最大の目的から今後を考えて生活拠点を東京から関西に移したいと思うようになりました(他にもお互いの身寄りがいるとか理由はあります)。その時点では、エコノミスト志望でもなかったので、「もはや東京にいる意味もないな」と感じていました。\n2021年4月から配属になった新しい部署では、プロジェクトマネジメントを行う立場になり、ノウハウもない中で業務コンサルのような業務を行っていました。コンサル会社であるケンブリッジの書籍などを読みあさり、トライアンドエラーを繰り返す中で、プロジェクトが前進することに達成感を感じ、現場の方からの信頼も次第に得ることができました。自分たちの考えた企画案が具現化していくことにやりがいを感じ、「この仕事だったら業種も選ばないし、関西でも働けるな」と転職を考え出したのが8月頃でした。\n残りの半分は現職でのキャリアパスが想像できなかったから 生活拠点を関西に移し、プロマネとして働くのは、現職でも遠隔地勤務という勤務形態を利用すれば可能でした。現に、同じ部署にその制度を利用している方がいたし、少なくとも2年はほぼリモートワークだったので、うちの部署なら可能だろうなというイメージも湧いていました。その選択をしなかったのは、現職ではこの職種でキャリアパスを歩んでいく事が想像できなかったからです。\n資産運用会社の花形はファンドマネージャーです。新卒入社の大半がファンドマネージャー志望で入社します。業界内で有名な方も複数存在し、運用者のキャリアパスはそれなりにイメージできると思います1。一方で、運用者以外のキャリアパスとなると、かなり不明瞭で各分野でプロフェッショナルとして仕事をしている方は限定的である印象を受けました。私が所属していた部署は、若手にとっては運用への通過点という位置づけで、この部署でのキャリアパスをイメージ出来る人は上司含めて存在していなかったと思います。部署にノウハウの蓄積がされておらず、シニアの仕事の進め方に疑問を抱くこともしばしばあり、目指したいと思える人物も存在しませんでした2。部署柄、他の部署の方と仕事をする機会も多かったですが、状況は他の部署でも同じようで、運用者志望ではなく社内コンサル業を志望する自分にとっては、周りの支援・理解なく1人前になる想像ができず、「転職するなら早いほうがよいな」と思いました[^3]。\n運用者志望の希望が全員叶うわけではなく、若手の早期退職者に悩む人事は多いと思いますが、一方で運用者以外の受け皿が用意されているとは言い難いと思います。少なくとも、フロントとそれ以外では温度感が全く違います(後者はシニアまでほぼオペレーション担当の印象でした)。運用会社には金融専門職(ファンドマネージャー、エコノミスト、アナリスト)はいても、ビジネスパーソンはいない。この問題は根が深いですね。\n3. 次の会社・仕事 上記のような理由から、9月頃に転職活動をはじめました。複数のエージェントサービスを利用し、書類審査を本格的に出し始めたのが10月初旬ごろ、面接を受け出したのが10月半ばでした。そして、11月に内定を頂き、次の会社へ入社を決めました。この会社では、自部署でのキャリアステップの典型例を示してくださり、その中に自分の目指したいものがあったので入社を決めました。\n次の会社は関西の電子部品メーカーです。業務内容は現職で行っている業務とさほど変わらないものと認識しています。ただ、現職よりもデータを起点にした企画を行うということで、BIツールや統計処理なども守備範囲に入ってきそうです。これまでは必要な材料は現場へのヒアリングのみをベースとしており、Excelをこねくり回して出た信頼性がわからない結果をPowerPointスライドにするというやり方だったので、ひとつステップアップかなと思います。また、扱うデータも売上情報や受注情報などこれまでの金融時系列データとは異なってきそうです。ただ、ビジネスへの深い理解が必要という点は変わらないと思うので、その点は早くキャッチアップできるように頑張りたいと思います。\n4. 今後のブログ更新について これまでの投稿は金融・経済に関連するものでしたが、今後は可視化・プロジェクトマネジメントなどに内容が移ってきそうです。これまでよりも、気軽に投稿することは増えてくると思うので、更新頻度を上げていきたいと思います！\n  アシスタント→アナリスト→FMやオペレーション担当→投資判断者などでしょうか \u0026#x21a9;\u0026#xfe0e;\n 会議のファシリテーションにはかなり悩んでいました。本当であれば、会議が終わった後に細かいフィードバックを頂きたかったし、自分の課題はどこにあるのか指摘して頂きたかったですが、そのようなことが出来る人がいませんでした。「今のやり方のまま仕事を進めてくれたらいいから」しか言われなかった。 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1643673600,"objectID":"bb4f9ea995b516ba867dd3fb189d9773","permalink":"/post/post26/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/post/post26/","section":"post","summary":"今後に関するご報告です。","tags":[],"title":"転職します","type":"post"},{"authors":null,"categories":["単発"],"content":"おはこんばんにちは。最近会社のPCにAnacondaを入れてもらいました。業務で使用することはないのですが、ワークショップで使用するので色々勉強しています。以前、Googleが提供しているEarth Engineから衛星画像を取得して解析した際にPythonを使用しましたが、今回はPythonから様々なデータが取得できるpandas_datareaderを使用したいと思います。pandas_datareaderでは以下のようなデータソースからデータが取得できます。\n  Tiingo\n  IEX\n  Alpha Vantage\n  Enigma\n  Quandl\n  St.Louis FED\n  Kenneth French\u0026rsquo;s data library\n  World Bank\n  OECD\n  Eurostat\n  Thrift Saving Plan\n  Nasdaq Trader symbol definitions\n  Stooq\n  MOEX\n  Naver Finance\n  なお、このブログではRstuioとblogdownパッケージ、gitを組み合わせてgithub上に記事を投稿しています。ですが、Rstudioとreticulateパッケージのおかげで、pythonを使用した記事もrmdで作成し、htmlとして出力できています。ここでまず、reticulateパッケージを用いてconda仮想環境へ接続する方法を紹介しておきます。\nlibrary(reticulate) conda_path \u0026lt;- \u0026#34;C:\\\\Users\\\\hoge\\\\Anaconda3\\\\envs\\\\環境名\u0026#34; use_condaenv(conda_path) これで接続できます。conda_pathには仮想環境へのパスを入力してください。\nimport sys sys.version ## '3.7.7 (default, May 6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]'\r1. ECONDBからのデータ取得 pandas_datareaderでは、ECOMDBからマクロ経済関連のデータを取得することができます。\nECONDBとは？ ECONDBは各国の主要マクロ経済データをdashboard形式で提供してくれるWebサイトで、またAPIをサポートしており、PythonやExcelにシームレスにデータを連係してくれます。\nデータ取得方法 pandas_datareaderを用いた使用方法は以下の通りです。\n基本的な使用方法 pandas_datareaderからデータモジュールをインポートすることから始めます。\nimport pandas_datareader.data as web ## C:\\Users\\aashi\\Documents\\R\\win-library\\4.1\\reticulate\\python\\rpytools\\loader.py:44: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\r## level=level\rEconDBからデータを取得するには、DataReaderメソッドを呼び出し、以下のようにdata_source引数に'econdb'と適当なqueryを渡せばよいです。\ndf = web.DataReader(query, data_source=\u0026#39;econdb\u0026#39;, **kwargs) クエリパラメータの形式は、取得するデータの種類によって異なります。\nクエリ指定方法 データはいくつかのデータセットに分割されます。データセットには、トピック、頻度、調査方法などの共通の特徴を抽出できるティッカーが付与されています。ユーザーは検索機能を使用してデータセットを探すことができます。UST_MSPDデータセットを例にしてみます。\nページに入ると、いくつかのフィルターがあり、特定のシリーズと特定のタイムフレームに選択を絞り込むことができます。適切なフィルタが設定された状態で、Exportドロップダウンボタンをクリックすると、選択したデータをエクスポートするための多くのオプションとフォーマットが表示されます。その中でも、Export to Pythonは、事前にフォーマットされたパラメータを持つコードの重要な部分を表示します。これをそのまま貼り付けてしまえばデータを取得できます。\nquery = \u0026#34;\u0026amp;\u0026#34;.join([ \u0026#34;dataset=UST_MSPD\u0026#34;, \u0026#34;v=Category\u0026#34;, \u0026#34;h=TIME\u0026#34;, \u0026#34;from=2018-01-01\u0026#34;, \u0026#34;to=2019-12-31\u0026#34; ]) df = web.DataReader(query, \u0026#39;econdb\u0026#39;) df.head() ## Category Bills ... United States Savings Securities\r## Holder Intragovernmental Holdings ... Totals\r## TIME_PERIOD ... ## 2018-01-01 3830.0 ... 159902\r## 2018-02-01 3748.0 ... 159475\r## 2018-03-01 4552.0 ... 159040\r## 2018-04-01 2641.0 ... 158606\r## 2018-05-01 577.0 ... 158233\r## ## [5 rows x 51 columns]\r実践的な取得コード こんなこともできます。\nimport pandas as pd from matplotlib import pyplot as plt import pandas_datareader.data as web from datetime import datetime import seaborn as sns start = datetime(1980,1,1) end = datetime(2019,12,31) # parameters for data from econdb country = [\u0026#39;US\u0026#39;,\u0026#39;UK\u0026#39;,\u0026#39;JP\u0026#39;,\u0026#39;EU\u0026#39;] indicator = [\u0026#39;RGDP\u0026#39;,\u0026#39;CPI\u0026#39;,\u0026#39;URATE\u0026#39;,\u0026#39;CA\u0026#39;,\u0026#39;HOU\u0026#39;,\u0026#39;POP\u0026#39;,\u0026#39;RETA\u0026#39;,\u0026#39;IP\u0026#39;] # Parse API from econdb econ = pd.DataFrame() for cnty in country: temp2 = pd.DataFrame() for idctr in indicator: temp = web.DataReader(\u0026#39;ticker=\u0026#39; + idctr + cnty,\u0026#39;econdb\u0026#39;,start,end) temp.columns = [idctr] temp2 = pd.concat([temp2,temp],join=\u0026#39;outer\u0026#39;,axis=1) temp2 = temp2.assign(kuni=cnty,kijyundate=temp2.index) econ = pd.concat([econ,temp2],join=\u0026#39;outer\u0026#39;) econ = econ.reset_index(drop=True) econ.head() # Plot CPI for example ## RGDP CPI URATE CA HOU POP RETA IP kuni kijyundate\r## 0 6842024.0 78.0 6.3 -10666.0 NaN 226554.0 NaN 52.17 US 1980-01-01\r## 1 NaN 79.0 6.3 NaN NaN 226753.0 NaN 52.20 US 1980-02-01\r## 2 NaN 80.1 6.3 NaN NaN 226955.0 NaN 51.98 US 1980-03-01\r## 3 6701046.0 80.9 6.9 9844.0 NaN 227156.0 NaN 50.97 US 1980-04-01\r## 4 NaN 81.7 7.5 NaN NaN 227387.0 NaN 49.71 US 1980-05-01\rsns.set ## \u0026lt;function set at 0x0000000031ED0B88\u0026gt;\rsns.relplot(data=econ,x=\u0026#39;kijyundate\u0026#39;,y=\u0026#39;CPI\u0026#39;,hue=\u0026#39;kuni\u0026#39;,kind=\u0026#39;line\u0026#39;) plt.show() 2. World Bankからのデータ取得方法 世界銀行から取得できるデータとは？ 世界銀行は前身が国際復興開発銀行(IBRD)、国際開発協会(IDA)であることからもわかるように開発系のデータが取得できます。最近ではCOVID-19関連のデータも取得することができます。 pandas_datareaderでは、wb関数を使用することで、World Bank\u0026rsquo;s World Development Indicatorsと呼ばれる世界銀行の数千ものパネルデータに簡単にアクセスできます。\nデータの検索方法 例えば、北米地域の国々の一人当たりの国内総生産をドルベースで比較したい場合は、search関数を使用します。\nfrom pandas_datareader import wb matches = wb.search(\u0026#39;gdp.*capita.*const\u0026#39;) print(matches.loc[:,[\u0026#39;id\u0026#39;,\u0026#39;name\u0026#39;]]) ## id name\r## 716 6.0.GDPpc_constant GDP per capita, PPP (constant 2011 internation...\r## 10411 NY.GDP.PCAP.KD GDP per capita (constant 2015 US$)\r## 10413 NY.GDP.PCAP.KN GDP per capita (constant LCU)\r## 10415 NY.GDP.PCAP.PP.KD GDP per capita, PPP (constant 2017 internation...\r## 10416 NY.GDP.PCAP.PP.KD.87 GDP per capita, PPP (constant 1987 internation...\rNY.GDP.PCAP.KDがそれに当たることがわかります。2010年のUSドルベースで実質化されているようです。\nデータの取得方法 download関数でデータを取得します。\ndat = wb.download(indicator=\u0026#39;NY.GDP.PCAP.KD\u0026#39;, country=[\u0026#39;US\u0026#39;, \u0026#39;CA\u0026#39;, \u0026#39;MX\u0026#39;], start=2010, end=2018) print(dat) ## NY.GDP.PCAP.KD\r## country year ## Canada 2018 44917.369814\r## 2017 44325.416776\r## 2016 43536.913403\r## 2015 43596.135537\r## 2014 43635.095481\r## 2013 42846.284196\r## 2012 42315.807389\r## 2011 42036.997844\r## 2010 41155.323638\r## Mexico 2018 9945.776845\r## 2017 9842.400712\r## 2016 9751.569083\r## 2015 9616.645558\r## 2014 9426.324588\r## 2013 9282.991933\r## 2012 9280.258638\r## 2011 9076.301453\r## 2010 8878.561377\r## United States 2018 59821.592274\r## 2017 58387.775808\r## 2016 57418.933846\r## 2015 56863.371496\r## 2014 55574.356825\r## 2013 54604.130054\r## 2012 53989.248340\r## 2011 53190.231121\r## 2010 52759.998081\rpandasのdataframe形式でデータを取得できていることが分かります。年と国がindexになっていますね。\n3. Fama/French Data Libraryからのデータ取得方法 Fama/French Data Libraryで取れるデータとは 金融関連データになりますが、有名なFama/Frechの3 Factor modelのデータセットがFama/French Data Libraryから取得できます。get_available_datasets関数は、利用可能なすべてのデータセットのリストを返します。\nデータ取得方法 from pandas_datareader.famafrench import get_available_datasets len(get_available_datasets()) ## 297\r利用可能なデータセットは297です。 データセットにどんなものがあるか、20個ほどサンプリングしてみます。\nimport random print(random.sample(get_available_datasets(),20)) ## ['Japan_3_Factors_Daily', 'Developed_ex_US_Mom_Factor_Daily', 'Portfolios_Formed_on_ME', '25_Portfolios_OP_INV_5x5', 'North_America_6_Portfolios_ME_OP', 'Europe_6_Portfolios_ME_Prior_250_20_daily', 'Portfolios_Formed_on_INV', '10_Industry_Portfolios_Wout_Div', '5_Industry_Portfolios', '32_Portfolios_ME_OP_INV_2x4x4', 'Europe_32_Portfolios_ME_INV(TA)_OP_2x4x4', 'North_America_6_Portfolios_ME_Prior_12_2', 'Europe_25_Portfolios_ME_Prior_12_2', 'Developed_ex_US_25_Portfolios_ME_OP_Daily', 'Europe_3_Factors', 'Portfolios_Formed_on_E-P', 'Asia_Pacific_ex_Japan_25_Portfolios_ME_Prior_12_2', 'Europe_25_Portfolios_ME_OP', 'Developed_25_Portfolios_ME_BE-ME', '38_Industry_Portfolios']\r日本株のポートフォリオも存在します。\nds = web.DataReader(\u0026#39;5_Industry_Portfolios\u0026#39;, \u0026#39;famafrench\u0026#39;) print(ds[\u0026#39;DESCR\u0026#39;]) ## 5 Industry Portfolios\r## ---------------------\r## ## This file was created by CMPT_IND_RETS using the 202201 CRSP database. It contains value- and equal-weighted returns for 5 industry portfolios. The portfolios are constructed at the end of June. The annual returns are from January to December. Missing data are indicated by -99.99 or -999. Copyright 2022 Kenneth R. French\r## ## 0 : Average Value Weighted Returns -- Monthly (59 rows x 5 cols)\r## 1 : Average Equal Weighted Returns -- Monthly (59 rows x 5 cols)\r## 2 : Average Value Weighted Returns -- Annual (5 rows x 5 cols)\r## 3 : Average Equal Weighted Returns -- Annual (5 rows x 5 cols)\r## 4 : Number of Firms in Portfolios (59 rows x 5 cols)\r## 5 : Average Firm Size (59 rows x 5 cols)\r## 6 : Sum of BE / Sum of ME (5 rows x 5 cols)\r## 7 : Value-Weighted Average of BE/ME (5 rows x 5 cols)\r5つ目がポートフォリオに含まれる銘柄数、1つ目がvalue weightedポートフォリオの月次リターンです。\nds[4].head() ## Cnsmr Manuf HiTec Hlth Other\r## Date ## 2017-03 519 592 663 577 1025\r## 2017-04 517 586 658 574 1018\r## 2017-05 515 583 655 572 1009\r## 2017-06 511 580 651 570 1000\r## 2017-07 525 615 684 613 1056\rds[0].head() ## Cnsmr Manuf HiTec Hlth Other\r## Date ## 2017-03 0.79 -0.20 1.91 0.03 -1.69\r## 2017-04 1.82 0.36 2.19 0.91 0.23\r## 2017-05 2.02 0.35 3.12 -0.25 -0.45\r## 2017-06 -1.19 0.32 -2.12 5.54 4.22\r## 2017-07 -0.07 2.31 4.02 0.70 1.49\r4. FERDからのデータ取得方法 FREDで取得できるデータとは FREDでは多種多様な経済統計データを取得することができます。サイトへ行くと、以下のように統計毎にページが存在します。この統計名の横についているCPIAUCSLがTickerになっており、これを渡すことで、データを取得することができます。\nデータ取得方法 先ほど見たTickerをDataReader関数に渡し、データソースをfredとすることで、データを取得することができます。\nimport datetime start = datetime.datetime(2010, 1, 1) end = datetime.datetime(2013, 1, 27) gdp = web.DataReader(\u0026#39;GDP\u0026#39;, \u0026#39;fred\u0026#39;, start, end) inflation = web.DataReader([\u0026#39;CPIAUCSL\u0026#39;, \u0026#39;CPILFESL\u0026#39;], \u0026#39;fred\u0026#39;, start, end) gdp.head() ## GDP\r## DATE ## 2010-01-01 14764.611\r## 2010-04-01 14980.193\r## 2010-07-01 15141.605\r## 2010-10-01 15309.471\r## 2011-01-01 15351.444\rinflation.head() ## CPIAUCSL CPILFESL\r## DATE ## 2010-01-01 217.488 220.633\r## 2010-02-01 217.281 220.731\r## 2010-03-01 217.353 220.783\r## 2010-04-01 217.403 220.822\r## 2010-05-01 217.290 220.962\r5. OECDからのデータ取得方法 OECDは以前以下の記事で紹介しましたが、pandas_datareaderでも取得することができます。\nOECD.orgからマクロパネルデータをAPIで取得する\nただ、OECD dataset codeを指定するだけ1なので、pandasdmxよりは自由度が低いです。 あと、前回取得したMEI_ARCHIVEとか指定するとデータが多すぎて、エラーが出ます。OECDデータを取得するときには、国や期間など細かい指定のできるpandasdmxのほうが良いと個人的に思います。\nなお、使用方法はFREDと同様で、データソースにoecdを指定します。\ndf = web.DataReader(\u0026#39;TUD\u0026#39;, \u0026#39;oecd\u0026#39;) df.head() ## Country Australia ... Slovenia\r## Frequency Annual ... Annual\r## Measure Percentage of employees ... Percentage of employees\r## Time ... ## 2018-01-01 NaN ... NaN\r## 2019-01-01 NaN ... NaN\r## 2020-01-01 NaN ... NaN\r## ## [3 rows x 39 columns]\r6. Eurostatからのデータ取得方法 Eurostatから取得できるデータとは Eurostatは欧州連合の統計局で、主にEU地域のデータを取得することができます。データは以下のように多岐にわたっており、経済金融だけでなく農業や人口動態、輸送、環境等々多種多様なデータを取得することができます。\nIDをどのように取得すればよいのかですが、以下のページにて、取得したいデータを順々に掘り進めていくと黄色で色を付けたようなIDコードが出てきます。これで取得データのIDを特定します。\nただ、eurostatもOECDと同じくsdmxに対応しているため、pandasdmxのほうが使いやすいかもしれません。\nデータ取得方法 一例として、 先ほど見たEmployment and activity by sex and age - annual dataを取得してみます。\ndf = web.DataReader(\u0026#39;lfsi_emp_a\u0026#39;,\u0026#39;eurostat\u0026#39;).unstack() df.head() ## UNIT INDIC_EM SEX AGE GEO FREQ TIME_PERIOD\r## Percentage of total population Persons in the labour force (former name: active persons) Females From 15 to 24 years Austria Annual 2018-01-01 54.0\r## 2019-01-01 52.7\r## 2020-01-01 52.7\r## Belgium Annual 2018-01-01 27.8\r## 2019-01-01 29.5\r## dtype: float64\r最後に pandas_datareaderを使用して、様々なソースから多種多様なデータを取得しました。資産運用会社などで働いている方はbloombergやEIKONからデータを取得できるため、あまり魅力的に感じないかもしれませんが、個人で分析をしている方や定期的にデータを取得したい方は非常によいパッケージだと思います。自分自身、この新しいWebサイトにリニューアルしてから、週次や月次単位で経済分析を上げようかなと思っており、これらを使用して経済の定点観測をしたいなと思っているところです。皆さんも興味あるデータをpandas_datareaderで自動収集してみてください！\n  サイトで統計を選び、export \u0026gt;- SDMX Queryとするとその統計のコードが見れます。 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1604361600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1604361600,"objectID":"a65e93e58ebc8f614f34b02e0a573c06","permalink":"/post/post15/","publishdate":"2020-11-03T00:00:00Z","relpermalink":"/post/post15/","section":"post","summary":"pandas_datareaderを使って色々なデータを取得してみました。","tags":["Python","前処理"],"title":"Pythonのpandas_datareaderから色々なデータを取得してみる","type":"post"},{"authors":null,"categories":["マクロ経済学","単発"],"content":"おはこんばんにちは。マクロ経済データを集める方法はいくつかありますが、各国のデータを集めるとなると一苦労です。ですが、OECDからAPI経由でデータ取得すれば面倒な処理を自動化できます。今日はその方法をご紹介します。\n1.OECD.Stat Web API OECD.orgではOECD.Statというサービスを提供しており、OECD加盟国と特定の非加盟国の様々な経済データが提供されています。WEBサイトに行けば手動でcsvデータをダウンロードすることもできますが、定期的にデータを取得し、分析する必要があるならばデータ取得処理を自動化したい衝動に駆られます。OECDはWeb APIを提供しているので、PythonやRさえ使えればこれを実現できます。\n\u0026lt;OECD実施の具体的な内容\u0026gt;\n以下は、現時点での特定のOECD REST SDMXインターフェースの実装詳細のリストです。\n  匿名クエリのみがサポートされ、認証はありません。\n  各レスポンスは1,000,000件のオブザベーションに制限されています。\n  リクエストURLの最大長は1000文字です。\n  クロスオリジンリクエストは、CORS ヘッダでサポートされています (CORS についての詳細は こちらを参照)。\n  エラーは結果には返されませんが、HTTP ステータスコードとメッセージは Web サービスガイドラインに従って設定されます。\n  存在しないデータセットが要求された場合は、401 Unauthorizedが返されます。\n  REST クエリの source (または Agency ID) パラメータは必須ですが、「ALL」キーワードはサポートされています。\n  バージョニングはサポートされていません: 常に最新の実装バージョンが使用されます。\n  データの並べ替えはサポートされていません。\n  lastNObservationsパラメータはサポートされていません。\n  dimensionAtObservation=AllDimensions が使用されている場合でも、観測は時系列 (またはインポート固有) の順序に従います。\n  現時点では、参照メタデータの検索はサポートされていません。\n  2.pandasdmx Web APIはsdmx-jsonという形式で提供されます。Pythonではこれを使用するための便利なパッケージが存在します。それが**pandasdmx**です。データをダウンロードする方法は以下の通りです。\n pandasdmxをimportし、Requestメソッドに引数として\u0026rsquo;OECD\u0026rsquo;を渡し、api.Requestオブジェクトを作成する。 作成したapi.Requestオブジェクトのdataメソッドにクエリ条件を渡し、OECD.orgからsdmx-json形式のデータをダウンロードする。 ダウンロードしたデータをto_pandas()メソッドでpandasデータフレームへ整形する。  3.実装 では、実際にやってみましょう。取得するのは、「**Revisions Analysis Dataset -- Infra-annual Economic Indicators**」というデータセットです。OECDのデータセットの一つであるMonthly Ecnomic Indicator(MEI)の修正を含む全てのデータにアクセスしているので、主要な経済変数(国内総生産とその支出項目、鉱工業生産と建設生産指数、国際収支、複合主要指標、消費者物価指数、小売取引高、失業率、就業者数、時間当たり賃金、貨マネーサプライ、貿易統計など)について、初出時の速報データから修正が加えられた確報データまで確認することができます。このデータセットでは、1999年2月から毎月の間隔で、過去に主要経済指標データベースで分析可能だったデータのスナップショットが提供されています。つまり、各時点で入手可能なデータに基づく、予測モデルの構築ができるデータセットになっています。最新のデータは有用ですが速報値なので不確実性がつきまといます。バックテストを行う際にはこの状況が再現できず実際の運用よりも良い環境で分析してしまうことが問題になったりします。いわゆるJagged edge問題です。このデータセットでは実運用の状況が再現できるため非常に有用であると思います。今回は以下のデータ項目を取得します。\n   統計概要 統計ID 頻度     GDP 101 四半期   鉱工業生産指数 201 月次   小売業取引高 202 月次   マネーサプライ - 広義流動性 601 月次   貿易統計 702+703 月次   経常収支 701 四半期   就業者数 502 月次   失業率 501 月次   時間当たり賃金（製造業） 503 月次   単位あたり労働コスト 504 四半期   建築生産指数 203 月次    まず、関数を定義します。引数はデータベースID、その他ID(国IDや統計ID)、開始地点、終了地点です。\nimport pandasdmx as sdmx ## C:\\Users\\aashi\\ANACON~1\\lib\\site-packages\\pandasdmx\\remote.py:13: RuntimeWarning: optional dependency requests_cache is not installed; cache options to Session() have no effect\r## RuntimeWarning,\roecd = sdmx.Request(\u0026#39;OECD\u0026#39;) def resp_OECD(dsname,dimensions,start,end): dim_args = [\u0026#39;+\u0026#39;.join(d) for d in dimensions] dim_str = \u0026#39;.\u0026#39;.join(dim_args) resp = oecd.data(resource_id=dsname, key=dim_str + \u0026#34;/all?startTime=\u0026#34; + start + \u0026#34;\u0026amp;endTime=\u0026#34; + end) df = resp.to_pandas().reset_index() return(df) データを取得する次元を指定します。以下では、①国、②統計項目、③入手時点、④頻度をタプルで指定しています。\ndimensions = ((\u0026#39;USA\u0026#39;,\u0026#39;JPN\u0026#39;,\u0026#39;GBR\u0026#39;,\u0026#39;FRA\u0026#39;,\u0026#39;DEU\u0026#39;,\u0026#39;ITA\u0026#39;,\u0026#39;CAN\u0026#39;,\u0026#39;NLD\u0026#39;,\u0026#39;BEL\u0026#39;,\u0026#39;SWE\u0026#39;,\u0026#39;CHE\u0026#39;),(\u0026#39;201\u0026#39;,\u0026#39;202\u0026#39;,\u0026#39;601\u0026#39;,\u0026#39;702\u0026#39;,\u0026#39;703\u0026#39;,\u0026#39;701\u0026#39;,\u0026#39;502\u0026#39;,\u0026#39;503\u0026#39;,\u0026#39;504\u0026#39;,\u0026#39;203\u0026#39;),(\u0026#34;202001\u0026#34;,\u0026#34;202002\u0026#34;,\u0026#34;202003\u0026#34;,\u0026#34;202004\u0026#34;,\u0026#34;202005\u0026#34;,\u0026#34;202006\u0026#34;,\u0026#34;202007\u0026#34;,\u0026#34;202008\u0026#34;),(\u0026#34;M\u0026#34;,\u0026#34;Q\u0026#34;)) 関数を実行します。\nresult = resp_OECD(\u0026#39;MEI_ARCHIVE\u0026#39;,dimensions,\u0026#39;2019-Q1\u0026#39;,\u0026#39;2020-Q2\u0026#39;) result.count() ## LOCATION 8266\r## VAR 8266\r## EDI 8266\r## FREQUENCY 8266\r## TIME_PERIOD 8266\r## value 8266\r## dtype: int64\rデータの最初数件を見てみます。\nresult.head() ## LOCATION VAR EDI FREQUENCY TIME_PERIOD value\r## 0 BEL 201 202001 M 2019-01 112.5\r## 1 BEL 201 202001 M 2019-02 111.8\r## 2 BEL 201 202001 M 2019-03 109.9\r## 3 BEL 201 202001 M 2019-04 113.5\r## 4 BEL 201 202001 M 2019-05 112.1\rデータがTidyな形(Long型)で入っているのがわかります。一番右側のvalueが値として格納されており、その他インデックスは\n  LOCATION - 国\n  VAR - 統計項目\n  EDI - 入手時点(MEI_ARCHIVEの場合)\n  FREQUENCY - 頻度(月次、四半期等)\n  TIME_PERIOD - 統計の基準時点\n  となっています。よって、EDIが異なる行で同じTIME_PERIODが存在します。例えば、上ではベルギー(BEL)の鉱工業生産指数(201)の2020/01時点で利用可能な2019-01~2019-05のデータが表示されています。可視化や回帰も行いやすいLongフォーマットでの提供なので非常にありがたいですね。鉱工業生産指数がアップデートされていく様子を可視化してみました。\nimport seaborn as sns import matplotlib.pyplot as plt import pandas as pd result = result[result[\u0026#39;FREQUENCY\u0026#39;]==\u0026#39;M\u0026#39;] result[\u0026#39;TIME_PERIOD\u0026#39;] = pd.to_datetime(result[\u0026#39;TIME_PERIOD\u0026#39;],format=\u0026#39;%Y-%m\u0026#39;) sns.relplot(data=result[lambda df: (df.VAR==\u0026#39;201\u0026#39;) \u0026amp; (pd.to_numeric(df.EDI) \u0026gt; 202004)],x=\u0026#39;TIME_PERIOD\u0026#39;,y=\u0026#39;value\u0026#39;,hue=\u0026#39;LOCATION\u0026#39;,kind=\u0026#39;line\u0026#39;,col=\u0026#39;EDI\u0026#39;) plt.show() コロナの経済的な被害が大きくなるにつれて折れ線グラフが落ち込んでいく様子が見て取れる一方、微妙にですが過去値についても速報値→確報値へと修正が行われています。また、国によって統計データの公表にラグがあることも分かります。ベルギーは最も公表が遅いようです。時間があるときに、このデータを使った簡単な予測モデルの分析を追記したいと思います。\n4.別件ですが。。。 Python 3 エンジニア認定データ分析試験に合格しました。合格率70%だけあって、かなり簡単でしたがPythonを基礎から見返すいい機会になりました。今やっている業務ではデータ分析はおろかPythonやRを使う機会すらないので、転職も含めた可能性を考えています。とりあえず、以下の資格を今年度中に取得する予定で、金融にこだわらずにスキルを活かせるポストを探していこうと思います。ダイエットと同じで宣言して自分を追い込まないと。。。\n G検定 Oracle Database Master Silver SQL Linuc レベル 1 基本情報技術者 AWS 認定ソリューションアーキテクト - アソシエイト  合格状況は都度ブログで報告していきたいと思います。\n","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1603065600,"objectID":"c0bed9051b28d59784db7575607fe5fe","permalink":"/post/post22/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/post/post22/","section":"post","summary":"OECD.orgのAPIを使って、各国のマクロ経済データを取得してみました。","tags":["Python","ガウス過程","前処理","Webスクレイピング","API"],"title":"OECD.orgからマクロパネルデータをAPIで取得する","type":"post"},{"authors":null,"categories":["単発","統計","プログラミング"],"content":"0. やりたいこと 今回お見せするのは前述の通り、為替のTickデータを使った前処理(と解析)になります。主眼をRcppを用いた効率化に置いていますので詳しくは踏み入りませんが、やりたいことをざっくりと先に示しておきます。\nやりたいのは、JPY/USDレートの5分刻みリターンからJumpを検知することです。ここでのJumpとはそれまでと比べて為替レートがガクッと上昇(下落)した点です。日中為替レートは小刻みに動きますが、なにかイベントがあると大きく上昇(下落)します。どんなイベントがJumpを引き起こすのかは非常に興味深い点です。これを検証するにはまずJumpを検知する必要があるのです。 参考とするのは以下の論文です。\nSuzanne S. Lee \u0026amp; Per A. Mykland, 2008. \u0026ldquo;Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,\u0026rdquo; Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.\nCitationが204もある非常に評価されている論文です。推定方法を掻い摘んで説明します。まず、連続複利リターンを$d\\log S(t)$ for $t\u0026gt;0$とします。ここで、$S(t)$は$t$時点での資産価格です。市場にJumpがない場合、$S(t)$は以下の確率過程に従うと仮定します。 $$ d\\log S(t) = \\mu(t)dt + \\sigma(t)dW(t) \\tag{1} $$ ここで、$W(t)$は標準ブラウン運動、$\\mu(t)$はドリフト項、$\\sigma(t)$はスポットボラティリティです。また、Jumpがあるとき、$S(t)$は $$ d\\log S(t) = \\mu(t)dt + \\sigma(t)dW(t) + Y(t)dJ(t) \\tag{2} $$ に従うと仮定します。ここで、$J(t)$は$W(t)$とは独立したカウント過程です。1$Y(t)$はジャンプのサイズを表現しており、予測可能な過程であるとします。\n次に、$S(t)$の対数リターンを考えます。それはつまり$\\log S(t_i)/S(t_{i-1})$ですが、これは正規分布$N(0,\\sigma(t_i))$に従います。2ここで、$t_{i-1}$から$t_{i}$にJumpがあった際の$t_i$時点の統計量$\\mathcal{L(i)}$を以下で定義します。\n$$ \\mathcal{L(i)} \\equiv \\frac{|\\log S(t_i)/S(t_{i-1})|}{\\hat{\\sigma}_{t_i}} \\tag{3} $$ 上記は対数リターンの絶対値を単純に標準化したものですが、標準偏差の推定量には以下で定義される\u0026quot;Realized Bipower Variation\u0026quot;を使用しています。 $$ \\hat{\\sigma}_{t_i} = \\frac{1}{K-2}\\sum_{j=i-K+2}^{i-2}|\\log S(t_j)/\\log S(t_{j-1})||\\log S(t_{j-1})/\\log S(t_{j-2})| \\tag{4} $$ $K$はWindowに含まれるサンプルサイズの数です。仮に5min刻みリターンを用い、2020/9/10 10:00にJumpが発生した場合、$K=270$としている場合は前日2020/9/9 11:30から2020/9/11 09:55までのサンプルを用いて計算することになります。やっていることは、リターンの絶対値をかけたものを足し合わせるということですが、これでJumpが生じた次の瞬間(つまり$t_{i+1}$とか）の推定値がJumpに影響されにくいようです。ちなみに$K=270$は5min刻みリターンの場合の推奨値と別の文献で紹介されています。\nこうして計算されたJump統計量$\\mathcal{L(i)}$をどのように統計的検定に用いてJumpを検出するかに話を移しましょう。これは確率変数である$\\mathcal{L(i)}$の最大値(こちらも確率変数)を考え、その分布から大きく逸脱した値を取った場合(95%点とか)、そのリターンをJumpとします。 期間$[t_{i-1},t_{i}]$にJumpがないとした場合、この期間の長さ$\\Delta=t_{i}-t_{i-1}$を$0$に近づけると、つまり$\\Delta\\rightarrow0$とすると、標準正規変数の絶対値の最大値は、ガンベル分布に収束します。皆さん大好き極値統計ですね。よって、Jumpは以下の条件が満たされた際に帰無仮説が棄却され、検出することができます。 $$ \\mathcal{L(i)} \u0026gt; G^{-1}(1-\\alpha)S_{n} + C_{n} \\tag{5} $$ ここで、$G^{-1}(1-\\alpha)$は標準ガンベル分布の$(1-\\alpha)$分位関数です。$\\alpha=10%$だと2.25になります。また、 $$ S_{n} = \\frac{1}{c(2\\log n)^{0.5}},~ \\\nC_{n} = \\frac{(2\\log n)^{0.5}}{c}-\\frac{\\log \\pi+\\log(\\log n)}{2c(2\\log n)^{0.5}} $$ です(導出はしませんが、1式と2式を使って証明できます)。ここで、$c=(2/\\pi)^{0.5}$で、$n$は推定に使用する総サンプルサイズです。 最終的に、$Jump_{t_i}$は $$ Jump_{t_i} = \\log\\frac{S(t_i)}{S(t_{i-1})}×I(\\mathcal{L(i)} - G^{-1}(1-\\alpha)S_{n} + C_{n})\\tag{6} $$ で求められることになります。ここで、$I(・)$は中身が0より大きいと1、それ以外は0を返すIndicator関数です。\n1. データの読み込み では、推定方法がわかったのでまずTickデータの読み込みをしましょう。データはQuantDataManagerからcsvを取得し、それを作業ディレクトリに保存しています。\nlibrary(magrittr) # Tick dataの読み込み strPath \u0026lt;- r\u0026#34;(C:\\Users\\hogehoge\\JPYUSD_Tick_2011.csv)\u0026#34; JPYUSD \u0026lt;- readr::read_csv(strPath) 関係ないんですが、最近Rを4.0.2へ上げました。4.0以上ではPythonでできた文字列のEscapeができるとうことで今までのストレスが解消されてかなりうれしいです。 データは以下のような感じで、日付の他にBid値、Ask値と取引量が格納されています。なお、ここでは2011年のTickを使用しています。東日本大震災の時のドル円を対象とするためです。\nsummary(JPYUSD) ## DateTime Bid Ask Volume ## Min. :2011-01-03 07:00:00 Min. :75.57 Min. :75.58 Min. : 1.00 ## 1st Qu.:2011-03-30 15:09:23 1st Qu.:77.43 1st Qu.:77.44 1st Qu.: 2.00 ## Median :2011-06-15 14:00:09 Median :80.40 Median :80.42 Median : 2.00 ## Mean :2011-06-22 05:43:11 Mean :79.91 Mean :79.92 Mean : 2.55 ## 3rd Qu.:2011-09-09 13:54:51 3rd Qu.:81.93 3rd Qu.:81.94 3rd Qu.: 3.00 ## Max. :2011-12-30 06:59:59 Max. :85.52 Max. :85.54 Max. :90.00\rちなみに、DateTimeはUTC基準で日本時間だと2011/1/3 07:00:00から2011-12-30 06:59::59(米国時間2011-12-30 16:59:59)までを含んでいます。サンプルサイズは約1200万件です。\nNROW(JPYUSD) ## [1] 11946621\r2. 前処理 では次にBidとAskから仲値を計算し、後でリターンを算出するために対数を取っておきます。\n# AskとBidの仲値を計算し、対数化(対数リターン算出用) JPYUSD \u0026lt;- JPYUSD %\u0026gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %\u0026gt;% dplyr::mutate(logMid = log(Mid)) 現状不規則に並んでいる取引データを5min刻みのリターンに整形します。やり方は、\n 1年間を5min毎に刻んだPOSIXctベクトルを作る。 1.を引数として渡すと、その5minのWindowのうち、最初と最後のサンプルから対数リターンを順々に計算する関数を作成する。 実行。 という計画です。まず、1.のベクトルを作成します。  # 5min刻みでのリターンを算出するためのPOSIXベクトルを作成(288×日数) start \u0026lt;- as.POSIXct(\u0026#34;2011-01-02 22:00:00\u0026#34;,tz=\u0026#34;UTC\u0026#34;) end \u0026lt;- as.POSIXct(\u0026#34;2011-12-31 21:55:00\u0026#34;,tz=\u0026#34;UTC\u0026#34;) from \u0026lt;- seq(from=start,to=end,by=5*60) では、2.に移ろうということなんですが、データが1200万件もあるとRでpurrr::mapとかapply属を使用したとしても、関数呼び出しに時間がかかって結構非効率だったりします。。。sapplyでやってみましたがなかなか処理が完了せず、強制終了しました。こういうときには、Rccpが便利です。Rはグラフや統計処理のための非常に便利な関数が多数ありますが、ユーザーで定義した関数の呼び出しを含む、大量の繰り返し処理を苦手とします(スクリプト言語なのでコンパイル言語よりはという意味です)。なので、繰り返し処理の部分だけ、C++で書いてしまって、それをRcppをつかってRの関数としてコンパイルし、実行。結果の集計や可視化、執筆はRで行うというフローが非常に効率的です。 また、RccpはRに似た違和感の少ない記述方法でC++を記述するのを助けてくれます。詳しいことは以下を見れば問題ないと思います。かなりまとまっていて控えめに言って神です。\nみんなのRcpp\nでは、2.にあたるコードを書いていきます。コーディングに当たってはネット上の記事を参考にしました。C++はRよりも歴史があるし、使用者も多いので知りたい情報はすぐ見つけられます。\n#include \u0026lt;Rcpp.h\u0026gt;#include \u0026lt;algorithm\u0026gt; using namespace Rcpp; //[[Rcpp::plugins(cpp11)]]  // [[Rcpp::export]] DataFrame Rolling_r_cpp( DataFrame input, //（計測時刻time, 計測値data）のデータフレーム  newDatetimeVector from, //計算するタイミングの始点ベクトル  double time_window = 5*60) //計算するwindow幅（秒） { // 計測時刻と計測値をベクトルとして取り出す  newDatetimeVector time = input[\u0026#34;DateTime\u0026#34;]; // 今回は time は昇順にソートされているのが前提です。  NumericVector data = input[\u0026#34;logMid\u0026#34;]; // 計算するタイミングの終点ベクトル  newDatetimeVector to = from + time_window; // 計算する数  R_xlen_t N = from.length(); // 格納するベクトル  NumericVector value(N); // ベクトル要素の位置をあらわすオブジェクト  newDatetimeVector::iterator begin = time.begin(); newDatetimeVector::iterator end = time.end(); newDatetimeVector::iterator p1 = begin; newDatetimeVector::iterator p2 = begin; // window i についてループ  for(R_xlen_t i = 0; i \u0026lt; N; ++i){ // Rcout \u0026lt;\u0026lt; \u0026#34;i=\u0026#34; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;;  double f = from[i]; //windowの始点の時刻  double t = f + time_window; //windowの終点の時刻  // windowの終点が最初の計測時刻以前の時はNA、または  // windowの始点が最後の計測時刻のより後の時はNA  if(t \u0026lt;= *begin || f \u0026gt; *(end-1)){ value[i] = NA_REAL; continue;//次のループへ  } // ベクトル time の位置 p1 以降の要素xから  // 時刻がwindowの始点f「以降」である「最初の要素」の位置を p1 とする  p1 = std::find_if(p1, end, [\u0026amp;f](double x){return f\u0026lt;=x;}); // p1 = std::lower_bound(p1, end, f); //上と同義  // ベクトル time の位置 p1 以降の要素xから  // 時刻がwindowの終点t「より前」である「最後の要素」の位置を p2 とする  // （下では、時刻がwindowの終点t「以降」である「最初の要素」の１つ前の位置、にすることで実現している’）  p2 = std::find_if(p1, end, [\u0026amp;t](double x){return t\u0026lt;=x;}) - 1 ; // p2 = std::lower_bound(p1, end, t) - 1 ;//上と同義  // 要素の位置p1,p2を、要素番号i1, i2に変換する  R_xlen_t i1 = p1 - begin; R_xlen_t i2 = p2 - begin; // 要素番号の確認  // C++は要素番号が0から始まるのでRに合わせるために1を足している  // Rcout \u0026lt;\u0026lt; \u0026#34;i1 = \u0026#34; \u0026lt;\u0026lt; i1+1 \u0026lt;\u0026lt; \u0026#34; i2 = \u0026#34; \u0026lt;\u0026lt; i2+1 \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;;  // 該当する範囲のデータについて計算する  if(i1\u0026gt;i2) { value[i] = NA_REAL; // window内にデータがない場合  } else { value[i] = data[i2] - data[i1]; } // ↑を変更することで様々なwindow関数を作成できる  } // 計算した時間と、値をデータフレームとして出力する  DataFrame out = DataFrame::create( Named(\u0026#34;from\u0026#34;, from), Named(\u0026#34;r\u0026#34;, value*100)); return out; } Rcpp::sourceCppでコンパイルしたら、以下のようにRの関数として実行します。\nsystem.time(results \u0026lt;- Rolling_r_cpp(JPYUSD,from)) ## ユーザ システム 経過 ## 0.06 0.00 0.07\rはい。1200万件のデータの処理に1秒かかりません。便利ー。\nsummary(results) ## from r ## Min. :2011-01-02 22:00:00 Min. :-1.823 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 ## Median :2011-07-03 09:57:30 Median : 0.000 ## Mean :2011-07-03 09:57:30 Mean : 0.000 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 ## Max. :2011-12-31 21:55:00 Max. : 2.880 ## NA's :29977\r問題なく、リターンが計算されています。では、Realized Bipower Variationの計算に移りましょう。5min刻みの場合はWindowの長さは270が推奨でしたが、そこも引数として柔軟を持たせた作りにします。また、NAの処理についても丁寧に行います。\n#include \u0026lt;Rcpp.h\u0026gt;#include \u0026lt;cmath\u0026gt; using namespace Rcpp; //[[Rcpp::plugins(cpp11)]]  // [[Rcpp::export]] float rbv_cpp( NumericVector x, // rbvを計算するリターンベクトル  bool na_rm = true) // xにNAが含まれている場合、取り除いて計算するか { // 計算回数を取得  R_xlen_t N = x.length(); // 計算結果を入れる変数を定義  float out = 0; // xの欠損有無を確認  LogicalVector lg_NA = is_na(x); // xにNAが存在した場合、そのNAを除いて計算するかどうか  if(any(lg_NA).is_true() and na_rm==FALSE){ out = NA_REAL; // NAを計算結果として出力  } else { // NAを除く場合  if (any(lg_NA).is_true() and na_rm==TRUE){ x[is_na(x)==TRUE] = 0.00; // NAに0を埋め、実質的に計算から除外する  } // rbvの分子(総和)を計算  for(R_xlen_t i = 1; i \u0026lt; N; ++i){ out = out + std::abs(x[i])*std::abs(x[i-1]); } // 平均値を計算し、ルートをとる  long denomi; //分母  if(N-sum(lg_NA)-2\u0026gt;0){ denomi = N-sum(lg_NA)-2; } else { denomi = 1; } out = out/denomi; out = std::sqrt(out); } return out; } // [[Rcpp::export]] DataFrame Rolling_rbv_cpp( DataFrame input, //（計測時刻time, 計測値data）のデータフレーム  int K = 270, // 計算するRolling Window幅  bool na_pad = false, // Window幅が足りないときにNAを返すか  bool na_remove = false // Window幅の中にNAが存在した場合、除いて計算を行うか ){ // リターンベクトルとサンプル数を取り出す  NumericVector data = input[\u0026#34;r\u0026#34;]; R_xlen_t T = data.length(); // 計算結果を格納するベクトルを準備  NumericVector value(T); // Windows幅毎にRBVを計算し、格納する  if(na_pad==TRUE){ value[0] = NA_REAL; // NAを返す  value[1] = NA_REAL; // NAを返す  value[2] = NA_REAL; // NAを返す  } else { value[0] = 0; // 0を返す  value[1] = 0; // 0を返す  value[2] = 0; // NAを返す  } for(R_xlen_t t = 3; t \u0026lt; T; ++t){ // Windows幅が足りるかどうかで処理を分岐  if (t-K\u0026gt;=0){ value[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // 通常計算を実行  } else if(na_pad==FALSE) { value[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Kに満たない不完全なWidnows幅で計算を実行  } else { value[t] = NA_REAL; // NAを返す  } } // 計算した時間と値をデータフレームとして出力する  DataFrame out = DataFrame::create( Named(\u0026#34;from\u0026#34;, input[\u0026#34;from\u0026#34;]), Named(\u0026#34;r\u0026#34;, data), Named(\u0026#34;rbv\u0026#34;,value)); return out; } では、これもコンパイルし、Rで実行します。\nsystem.time(results \u0026lt;- results %\u0026gt;% Rolling_rbv_cpp(na_remove = FALSE)) ## ユーザ システム 経過 ## 1.00 0.38 1.45\rこちらも一瞬ですね。\n3. Jump統計量の計算 では、次に今計算したリターンと標準偏差から統計量$\\mathcal{L}_{t_i}$を計算しましょう。\n# 対数リターンの絶対値を標準化=Jump統計量 results \u0026lt;- results %\u0026gt;% dplyr::mutate(J=ifelse(rbv\u0026gt;0,abs(r)/rbv,NA)) 今こんな感じです。\nsummary(results) ## from r rbv J ## Min. :2011-01-02 22:00:00 Min. :-1.823 Min. :0.00 Min. : 0.00 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 1st Qu.:0.02 1st Qu.: 0.28 ## Median :2011-07-03 09:57:30 Median : 0.000 Median :0.02 Median : 0.64 ## Mean :2011-07-03 09:57:30 Mean : 0.000 Mean :0.03 Mean : 0.93 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 3rd Qu.:0.03 3rd Qu.: 1.23 ## Max. :2011-12-31 21:55:00 Max. : 2.880 Max. :0.16 Max. :58.60 ## NA's :29977 NA's :44367 NA's :44423\rでは、Jump検定に移りましょう。まず、必要な関数を定義しておきます。\n# Jump検定を計算するための定数\u0026amp;関数を準備 c \u0026lt;- (2/pi)^0.5 Cn \u0026lt;- function(n){ return((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5)) } Sn \u0026lt;- function(n){ 1/(c*(2*log(n))^0.5) } では検定を行います。棄却されたサンプルは1、それ以外は0を返します。\n# Jump検定(10%)を実行(返り値はlogical) N \u0026lt;- NROW(results$J) results \u0026lt;- results %\u0026gt;% dplyr::mutate(Jump = J \u0026gt; 2.25*Sn(N) + Cn(N)) summary(results) ## from r rbv J ## Min. :2011-01-02 22:00:00 Min. :-1.823 Min. :0.00 Min. : 0.00 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 1st Qu.:0.02 1st Qu.: 0.28 ## Median :2011-07-03 09:57:30 Median : 0.000 Median :0.02 Median : 0.64 ## Mean :2011-07-03 09:57:30 Mean : 0.000 Mean :0.03 Mean : 0.93 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 3rd Qu.:0.03 3rd Qu.: 1.23 ## Max. :2011-12-31 21:55:00 Max. : 2.880 Max. :0.16 Max. :58.60 ## NA's :29977 NA's :44367 NA's :44423 ## Jump ## Mode :logical ## FALSE:59864 ## TRUE :257 ## NA's :44423 ## ## ## 4. ggplot2を用いた可視化 数値が計算できましたので可視化しましょう。2011/03/11の日中のJPY/USDの5min刻み対数リターンの推移とJumpを重ねてPlotします。ちなみに横軸は日本時間に修正しています。\n# 2011/03/11の東日本大震災発生時のJumpについてPlot results %\u0026gt;% dplyr::filter(from \u0026gt;= as.POSIXct(\u0026#34;2011-03-11 00:00:00\u0026#34;,tz=\u0026#34;UTC\u0026#34;),from \u0026lt; as.POSIXct(\u0026#34;2011-03-12 00:00:00\u0026#34;,tz=\u0026#34;UTC\u0026#34;)) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x=from,y=r)) + ggplot2::geom_path(linetype=3) + ggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=\u0026#34;red\u0026#34;)) + ggplot2::scale_x_datetime(date_breaks = \u0026#34;2 hours\u0026#34;, labels = scales::date_format(format=\u0026#34;%H:%M\u0026#34;,tz=\u0026#34;Asia/Tokyo\u0026#34;)) + ggplot2::ggtitle(\u0026#34;JPY/USD Jumps within Tohoku earthquake on 2011-3-11\u0026#34;) ## Warning: Removed 36 row(s) containing missing values (geom_path).\r## Warning: Removed 36 row(s) containing missing values (geom_path).\rここまで執筆するのに結構時間使っていて、今23:37なんで深い考察は控えますが、震災が発生したのが14:46:18ですから市場は震災直後即座に円安に反応したことが分かります。その後なぜか円高方向へ進み19:00にはピークになっています。安全資産の円とか言われますが、この時ばかりは不確実性の高まりからして安全じゃないだろと思いますが。。。\n5. まとめ Rcppを使ったR分析の効率化について紹介しました。C++は愚直にコードを書いてもRより格段に処理が早いのでコーディングミスしにくい印象です。学術的な実装をやるときは内容が複雑になるのでこれはありがたいです。また、コンパイルエラーが起こってもRStudioを使っていればどこでコンパイルエラーが起こっているか手がかりをくれますのでその点でもストレスはないのでお勧めです。\n  非負、整数、非減少の値を持つ確率過程のこと。 \u0026#x21a9;\u0026#xfe0e;\n 平均はドリフト項の形状により必ずしも0にはなりませんが、今ドリフト項は十分小さい値を想定しているのでこの書き方にさせてください。論文ではより厳密に定義しています。 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1599696000,"objectID":"6264c9dd2374f6071851bf483d6c2fee","permalink":"/post/post21/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/post/post21/","section":"post","summary":"おはこんばんにちは。為替のTickデータを使った解析を行っているんですが、サンプルサイズが1年間のデータで1200万件にも及びます。メモリには乗るんですが、一つ一つに対して少し複雑な処理を行おうとすると処理にかなり時間がかかったりして非常に非効率です。今回はRcppパッケージを用いて、C++で書いた関数をR上の関数としてコンパイルし処理速度を高めるという方法を紹介したいと思います。","tags":["R","C++","前処理","金融"],"title":"Rcppでデータハンドリングを高速に行う(Tickデータの処理を事例に)","type":"post"},{"authors":null,"categories":["競馬"],"content":"おはこんばんにちは。前回、競走馬の馬体写真からCNNを用いて順位を予想するモデルを構築しました。結果は芳しくなく、特にshap値を用いた要因分析を行うと馬体よりも背景の厩舎に反応している様子が見えたりと分析の精緻化が必要となりました。今回はPytorchのPre-trainedモデルを用いて馬体写真から背景を切り出し、馬体のみとなった写真で再分析を行いたいと思います。\n1. Pre-trainedモデルのダウンロード コードはこちらのものを参考にしています。まず、パッケージをインストールします。\nimport numpy as np import cv2 import matplotlib.pyplot as plt import torch import torchvision from torchvision import transforms import glob from PIL import Image import PIL import os 学習済みモデルのインストールを行います。\n#学習済みモデルをインストール device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True) model = model.to(device) model.eval() どうやら全てのPre-trainedモデルは、同じ方法で正規化された形状$（N, 3, H, W）$の3チャンネルRGB画像のミニバッチを想定しているようです。ここで$N$は画像の数、$H$と$W$は少なくとも224ピクセルであることが想定されています。画像は、[0, 1]の範囲にスケーリングされ、その後、平均値＝[0.485, 0.456, 0.406]と標準値＝[0.229, 0.224, 0.225]を使用して正規化される必要があります。ということで、前処理を行う関数を定義します。\n#前処理 preprocess = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) 2. 背景削除処理の実行 では、前回記事のseleniumを用いたコードで収集した画像を読み込み、1枚1枚背景削除処理を行っていきます。\n#フォルダを指定 folders = os.listdir(r\u0026#34;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026#34;) #それぞれのフォルダから画像を読み込み、Image関数を使用してRGB値ベクトル(numpy array)へ変換 for i, folder in enumerate(folders): files = glob.glob(\u0026#34;C:/Users/aashi/umanalytics/photo/image/\u0026#34; + folder + \u0026#34;/*.jpg\u0026#34;) index = i for k, file in enumerate(files): img_array = np.fromfile(file, dtype=np.uint8) img = cv2.imdecode(img_array, cv2.IMREAD_COLOR) h,w,_ = img.shape input_tensor = preprocess(img) input_batch = input_tensor.unsqueeze(0).to(device) with torch.no_grad(): output = model(input_batch)[\u0026#39;out\u0026#39;][0] output_predictions = output.argmax(0) mask_array = output_predictions.byte().cpu().numpy() Image.fromarray(mask_array*255).save(r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\\mask.jpg\u0026#39;) mask = cv2.imread(r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\\mask.jpg\u0026#39;) bg = np.full_like(img,255) img = cv2.multiply(img.astype(float), mask.astype(float)/255) bg = cv2.multiply(bg.astype(float), 1.0 - mask.astype(float)/255) outImage = cv2.add(img, bg) Image.fromarray(outImage.astype(np.uint8)).convert(\u0026#39;L\u0026#39;).save(file) 行っている処理はPre-trainedモデルで以下のようなmask画像を出力し、実際の画像のnumpy配列とmask画像を統合して、背景削除画像を生成しています。出力例は以下のような感じです。\nplt.gray() plt.figure(figsize=(20,20)) plt.subplot(1,3,1) plt.imshow(img) plt.subplot(1,3,2) plt.imshow(mask) plt.subplot(1,3,3) plt.imshow(outImage) plt.show() plt.close() フォルダはこんな感じです。うまく処理できているものもあれば調教師の方が映ってしまっているのもありますね。物体を識別して、馬だけをmaskする方法もあるとは思いますがこのモデルでは物体のラベリングまではできないのでこのまま進みます。\n3. CNNを用いた分析 ここからは前回記事と同じ内容です。結果のみ掲載します。\n## Test accuracy: 0.711864406779661\r## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000003694DFC8\u0026gt;\rまったく識別できていません。全ての馬が負値の予想になっています。馬体写真には順位を予測するような特徴量はないんでしょうか。それともG1の出走馬ではバラツキがなく、識別不可能なのでしょうか。いずれいにせよ、ちょっと厳しそうです。\n4. Shap値を用いた結果解釈 前回同様、どのように失敗したのかshap値を使って検証してみましょう。この画像を例として使います。\nplt.imshow(X_test[4]) plt.show() plt.close() import shap background = X_resampled[np.random.choice(X_resampled.shape[0],100,replace=False)] e = shap.GradientExplainer(model,background) shap_values = e.shap_values(X_test[[4]]) shap.image_plot(shap_values[1],X_test[[4]]) 前足から顔にかけてを評価しているようです。意外に臀部を評価している様子はありません。\n5.まとめ 厩舎背景を削除し、再実行してみましたが結果変わらずでした。PyTorchを使ったり、背景削除を行ういい経験にはなりましたが結果は伴わずということで馬体写真はいったんここでストップです。\n","date":1597190400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1597190400,"objectID":"cc3bd2ae9c58d0a37a0f6159c997c020","permalink":"/post/post20/","publishdate":"2020-08-12T00:00:00Z","relpermalink":"/post/post20/","section":"post","summary":"前回の馬体モデルでは厩舎の背景が邪魔だったのでトリミングしました。","tags":["Python","機械学習","前処理"],"title":"PytorchのPre-trainedモデルで馬体写真の背景を自動トリミングする","type":"post"},{"authors":null,"categories":["単発","仕事関連","統計"],"content":"1. 今回のテーマ「バックテスト」とは？ バックテストは、アルゴリズムによる投資戦略のヒストリカルシミュレーションです。バックテストは、立案した投資戦略がある期間にわたって実行されていた場合に発生したであろう利益と損失をアルゴリズムを用いて計算します。その際、シャープレシオやインフォメーションレシオなどの投資戦略のパフォーマンスを評価する一般的な統計量が使用されています。投資家は通常、これらのバックテストの統計量を調査し、最高のパフォーマンスを発揮する投資(運用)戦略に資産配分を決定するため、資産運用会社は良好なパフォーマンスを血のにじむような回数のバックテストを試行錯誤し、資料を作ってプレゼンしたりするわけです。\n投資家の立場に立つなら、バックテストされた投資戦略のパフォーマンスについては、インサンプル(IS)とアウトオブサンプル(OOS)を区別することが重要です。ISのパフォーマンスは、投資戦略の設計に使用したサンプル（機械学習の文献では「学習期間」や「訓練セット」と呼ばれる物です）でシミュレートしたものです。一方、OOSパフォーマンスは、投資戦略の設計に使用されなかったサンプル（別名「テストセット」）でシミュレーションされたものです。バックテストは、そのパフォーマンスを持ってその投資戦略の有効性を占う物ですので、ISのパフォーマンスがOOSのパフォーマンスと一致している場合に再現性が担保され、現実的であるということができます。ただ、アウトサンプルの結果はこれからの結果であるので、バックテストを受け取った時点でそのバックテストが信頼に足るものか判断することは難しいです。hold-out法などで、以下のように学習データとテストデータを分け、OOSでのテストを行っているものもありますが、OOSの結果をフィードバックして戦略の改善ができる以上、純粋なアウトサンプルとは呼べません。\nですので、ファンドマネージャーから良い結果のバックテストを受け取った場合、そのシミュレーションがどれだけ現実的であるかをなんとかして評価することが非常に重要となります。また、ファンドマネージャーも自身のバックテスト結果が持つ不確実性を理解しておくことが重要です。今回はバックテストのシミュレーションの現実性をどのようにして評価するのか、再現性のあるバックテストを行うためには何に注意すれば良いのかを調べてみたいと思います。\n2. バックテストはオーバーフィットする Bailey, Borwein, López de Prado and Zhu(2015)は、どのような金融時系列でも、バックテストのシミュレーションをオーバーフィット(過学習)させることが(比較的)簡単にできると主張しています。ここで、オーバーフィットとは、機械学習の概念であり，モデルが一般的な構造よりも特定の観察データ(ISデータ)にフォーカスしてしまう状況を表します。\nBailey et. al.(2015)では、この主張の一例として株式戦略のバックテスト結果が芳しくない状況が挙げられています。バックテストではその名の通り過去データを使用しているので、具体的に損失が発生している銘柄を特定することが可能で、その銘柄の推奨を削除するためにいくつかのパラメータを追加し、取引システムを設計することで、パフォーマンスを向上させることができるというわけです（「データ・スヌーピング」として知られているテクニック）。数回シミュレーションを繰り返えせば、特定のサンプルに存在するが、母集団の中では稀であるかもしれない特徴から利益を得る「最適なパラメータ」を導くことができます。\n機械学習の文献では、オーバーフィッティングの問題を対処するための膨大な研究の蓄積があります。ですが、Bailey et. al.(2015)は、機械学習の文脈で提案されている手法は一般的に複数の投資問題には適用できないと主張します。その理由は以下4点のようです。\n  機械学習でオーバーフィッティングを防ぐ手法は、予測の説明力や質を評価するために、その事象が定義される領域において明示的な点推定と信頼区間を必要としますが、このような明確な予測を行う投資戦略はほとんどないため。\n 例えば、「E-mini S\u0026amp;P500は、金曜日の終値で1標準偏差5ポイントで1,600前後になると予測されています」とはあまり言われず、むしろ「買い」または「強い買い」といった定性的な推奨が提供されることが一般的です。しかも、この予想は予測の有効期限も明示されず、なにか予期せぬ事象が発生した際に変更がなされます。一方、定量予測では金曜日の終値と明記されています。    仮に特定の投資戦略が予測式に依存していたとしても、投資戦略の他の構成要素がオーバーフィットされている可能性がある。\n 言い換えれば、単に予測式を調整する以外にも、投資戦略をオーバーフィットさせる方法はたくさんあるということです。    回帰のオーバーフィットの方法はパラメトリックであり、金融の場合観察不可能なデータに関する多くの仮定を含むため。\n  いくつかの手法は試行回数をコントロールしていないため。\n  Bailey et. al.(2015)では、バックテストのパフォーマンスが比較的低い投資戦略を特定するためには、比較的少ない試行回数が必要であることを示しています。ここでの試行回数とは試行錯誤の回数だと思ってください。また、試行回数に応じて必要とされるバックテストの期間であるthe minimum backtest length（MinBTL）を計算しています。この論文では、パフォーマンスを評価するために常にシャープレシオが使用されていますが、他のパフォーマンス指標にも応用できるそうです。その内容を見てみましょう。\n3. シャープレシオが従う分布とは MinBTLを導出するために、まずシャープレシオの(漸近)分布を導出します。そもそも、投資戦略の設計は、通常、特定のパターンが金融変数の将来値を予測するのに役立つかもしれないという事前知識または信念から始まります。例えば、さまざまな満期の債券の間にリードラグ効果を認識している場合は、イールドカーブが上昇した場合に均衡値への回帰に賭ける戦略を設計することができます。このモデルは、cointegration equation、ベクトル誤差補正モデル、確率微分方程式のシステムなどの形をとることが考えられます。\nこのようなモデル構成（または試行）の数は膨大であり、ファンドマネージャーは当然、戦略のパフォーマンスを最大化するものを選択したいと考え、そのためにヒストリカルシミュレーション（バックテスト）を行います(前述)。バックテストでは、最適なサンプルサイズ、シグナルの更新頻度、リスクサイジング、ストップロス、最大保有期間などなどを他の変数との兼ね合いの中で評価します。\nこの論文中でパフォーマンス評価の尺度として使用されるシャープレシオは、過去のリターンのサンプルに基づいて、戦略のパフォーマンスを評価する統計量で、BMに対する平均超過リターン/標準偏差(リスク)として定義されます。通常には、「リスク1標準偏差に対するリターン」と解釈され、資産クラスにもよりますが1を上回っていると非常に良い戦略であると見なせます。以下では、ある戦略の超過リターン$r_t$がi.i.d.の確率変数であり、正規分布に従うと仮定します。つまり、$r_t$の分布は$r_s(t\\neq s)$と独立であることを仮定しています。あまり現実的な仮定ではありませんが。。。\n$$ r_t \\sim \\mathcal{N}(\\mu,\\sigma^2) $$ ここで、$\\mathcal{N}$は平均$\\mu$、分散$\\sigma^2$の正規分布を表しています。今、時点t~t-q+1の超過リターン$r_{t}(q)$を\n$$ r_{t}(q) \\equiv r_{t} + r_{t-1} + \u0026hellip; + r_{t-q+1} $$ と定義すると(複利部分を無視してます)、年率化されたシャープレシオは\n\r$$\r\\begin{eqnarray}\rSR(q) \u0026=\u0026 \\frac{E[r_{t}(q)]}{\\sqrt{Var(r_{t}(q))}}\\\\\r\u0026=\u0026 \\frac{q\\mu}{\\sqrt{q}\\sigma}\\\\\r\u0026=\u0026 \\frac{\\mu}{\\sigma}\\sqrt{q}\r\\end{eqnarray}\r$$\r\rと表すことができます。ここで、$q$は年毎のリターンの数(頻度)です。例えば、日次リターンの場合$q=365$となります(閏年を除く)。 $\\mu$と$\\sigma$は一般に未知ですので、$SR$の真値を知ることはできません。なので、$R_t$を標本リターン、リスクフリーレート$R^f$(定数)とすると、標本平均$\\hat{\\mu}=1/T\\sum_{t=1}^T R_{t}-R^f$と標本標準偏差$\\hat{\\sigma}=\\sqrt{1/T\\sum_{t=1}^{T}(R_{t}-\\hat{\\mu})}$を用いてシャープレシオの推定値を計算することになります($T$はバックテストを行うサンプルサイズ)。\n$$ \\hat{SR}(q) = \\frac{\\hat{\\mu}}{\\hat{\\sigma}}\\sqrt{q} $$ 必然的な結果として、$SR$の計算はかなりの推定誤差が伴う可能性が高くなります。では、本節の本題、$\\hat{SR}$の漸近分布を導出してみましょう。まず、$\\hat{\\mu}$と$\\hat{\\sigma}^2$の漸近分布はi.i.d.と$\\mu, \\sigma$が有限な値をとることから中心極限定理を適用することにより、\n$$ \\sqrt{T}\\hat{\\mu}\\sim^{a}\\mathcal{N}(\\mu,\\sigma^2), \\\n\\sqrt{T}\\hat{\\sigma}^2\\sim^a\\mathcal{N}(\\sigma^2,2\\sigma^4) $$ となります。シャープレシオはこの$\\hat{\\mu}$と$\\hat{\\sigma}^2$から計算される確率変数であるので、この関数を$g(\\hat{{\\boldsymbol \\theta}})$と表しましょう。ここで、$\\hat{{\\boldsymbol \\theta}}=(\\hat{\\mu},\\hat{\\sigma}^2)'$です。今、i.i.d.であるので$\\hat{{\\boldsymbol \\theta}}$は互いに独立となり、上記の議論から漸近同時分布は\n$$ \\sqrt{T}\\hat{{\\boldsymbol \\theta}} \\sim^a \\mathcal{N}({\\boldsymbol \\theta},{\\boldsymbol V_{\\boldsymbol \\theta}}) $$ と書けます。ここで、${\\boldsymbol V_{\\boldsymbol \\theta}}$は\n$$ {\\boldsymbol V_{\\boldsymbol \\theta}} = \\left( \\begin{array}{cccc} \\sigma^2 \u0026amp; 0\\\n0 \u0026amp; 2\\sigma^4\\\n\\end{array} \\right) $$ です。シャープレシオの推定値は今$g(\\hat{{\\boldsymbol \\theta}})$と$\\hat{{\\boldsymbol \\theta}}$だけの関数になっていますのでデルタ法より、\n$$ \\hat{SR} = g(\\hat{{\\boldsymbol \\theta}}) \\sim^a \\mathcal{N}(g({\\boldsymbol \\theta}),\\boldsymbol V_g) $$ と漸近的に正規分布に従います。ここで、$\\boldsymbol V_g$は\n$$ \\boldsymbol V_g=\\frac{\\partial g}{\\partial{\\boldsymbol \\theta}}{\\boldsymbol V_{\\boldsymbol \\theta}}\\frac{\\partial g}{\\partial{\\boldsymbol \\theta}'} $$ です。$g({\\boldsymbol \\theta})=\\mu/\\sigma$なので、\n$$ \\frac{\\partial g}{\\partial{\\boldsymbol \\theta}'} = \\left[ \\begin{array}{cccc} \\frac{\\partial g}{\\partial \\mu}\\\n\\frac{\\partial g}{\\partial \\sigma^2}\\\n\\end{array} \\right] = \\left[ \\begin{array}{cccc} \\frac{1}{\\sigma}\\\n-\\frac{\\mu}{2\\sigma^3}\\\n\\end{array} \\right] $$ よって、\n\r$$\r\\begin{eqnarray}\r\\boldsymbol V_g \u0026=\u0026 \\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}, \\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right)\r\\left( \\begin{array}{cccc}\r\\sigma^2 \u0026 0\\\\\r0 \u0026 2\\sigma^4\\\\\r\\end{array}\r\\right)\r\\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\\\\r\\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right) \\\\\r\u0026=\u0026 \\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\sigma^2, \\frac{\\partial g}{\\partial \\sigma}2\\sigma^4\\\\\r\\end{array}\r\\right)\r\\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\\\\r\\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right) \\\\\r\u0026=\u0026 (\\frac{\\partial g}{\\partial \\mu})^2\\sigma^2 + (\\frac{\\partial g}{\\partial \\sigma})^2\\sigma^4 \\\\\r\u0026=\u0026 1 + \\frac{\\mu^2}{2\\sigma^2} \\\\\r\u0026=\u0026 1 + \\frac{1}{2}SR^2\r\\end{eqnarray}\r$$\r\rと導出することができます。シャープレシオの絶対値が大きくなるほど指数的に分散が大きくなる傾向があるので良いパフォーマンスを見た時には注意が必要かもしれません。年率化されたシャープレシオの推定値$\\hat{SR}(q)$が従う分布はここから\n$$ \\hat{SR}(q)\\sim^a \\mathcal{N}(\\sqrt{q}SR,\\frac{V(q)}{T}) \\\nV(q) = q{\\boldsymbol V}_g = q(1 + \\frac{1}{2}SR^2) $$ となります。今、$y$をバックテストを行う年数とすると$T=yq$と書け、これを用いて上式を以下のように書き換えることができます(日次リターンで3年計測の場合、サンプルサイズ$T$は$T=3×365=1095$)。\n$$ \\hat{SR}(q)\\sim^a \\mathcal{N}(\\sqrt{q}SR,\\frac{1+\\frac{1}{2}SR^2}{y}) \\tag{1} $$ 頻度$q$はシャープレシオの平均には影響しますが分散には影響を及ぼしません。これでシャープレシオの推定値の漸近分布を導出することができました。さて、これを使ってなにをしたかったのかということですが、私たちは今バックテストの信頼性について考えていたのでした。つまり、FMが新商品を開発するために頭をひねって考え出した$N$個の投資戦略案のバックテストをした際に、それらのシャープレシオの真値がどれも0であるにも関わらず、非常に高い(良い)値が出る確率はいかほどなのかということです。Bailey et. al.(2015)では以下のように記述されていました。\nHow high is the expected maximum Sharpe ratio IS among a set of strategy configurations where the true Sharpe ratio is zero?\nまた、期待最大シャープレシオの値を小さくするためには、いったいどれほどの期間バックテストをすべきなのかも知りたいわけです。\n4. the minimum backtest lengthを導出してみる 今考えている状況は、$\\mu=0$で$y$を簡単化のために1年とすると(1)式より$\\hat{SR}(q)$は標準正規分布$\\mathcal{N}(0,1)$に従います。さて、今から私たちは$\\hat{SR}_n(n=1,2,\u0026hellip;N)$の最大値$\\max[\\hat{SR}]_N$の期待値について考えていくのですが、勘の良い人ならお気づきの通り、議論は極値統計の文脈に入っていくことになります。$\\hat{SR}_n\\sim\\mathcal{N}(0,1)$はi.i.d.なので、その最大統計量の極値分布はFisher-Tippett-Gnedenko定理よりガンベル分布になります(証明追えてないです、ごめんなさい)。\n$$ \\lim_{N\\rightarrow\\infty}prob[\\frac{\\max[\\hat{SR}]_N-\\alpha}{\\beta}\\leq x] = G(x) = e^{-e^{-x}} $$ ここで、$\\alpha=Z(x)^{-1}[1-1/N], \\beta=Z(x)^{-1}[1-1/Ne^{-1}]-\\alpha$で、$Z(x)$は標準正規分布の累積分布関数を表しています。ガンベル分布のモーメント母関数$M_x(t)$は\n\r$$\r\\begin{eqnarray}\rM_x(t) \u0026=\u0026 E[e^{tx}] = \\int_{-\\infty}^\\infty e^{tx}e^{-x}e^{-e^{-x}}dx \\\\\r\\end{eqnarray}\r$$\r\rと書け、$x=-\\log(y)$と変数変換すると$dx/dy=-1/y=-(e^{-x})^{-1}$なので、\n\r$$\r\\begin{eqnarray}\rM_x(t) \u0026=\u0026 \\int_{\\infty}^0-e^{-t\\log(y)}e^{-y}dy \\\\\r\u0026=\u0026 \\int_{0}^\\infty y^{-t}e^{-y}dy \\\\\r\u0026=\u0026 \\Gamma(1-t)\r\\end{eqnarray}\r$$\r\rとなります。$\\Gamma(x)$はガンマ関数です。ここから、標準化された最大統計量の期待値(平均)は\n\r$$\r\\begin{eqnarray}\r\\lim_{N\\rightarrow\\infty} E[\\frac{\\max[\\hat{SR}]_N-\\alpha}{\\beta}] \u0026=\u0026 M_x'(t)|_{t=0} \\\\\r\u0026=\u0026 (-1)\\Gamma'(1) \\\\\r\u0026=\u0026 (-1)(-\\gamma) = \\gamma\r\\end{eqnarray}\r$$\r\rとなります。ここで、$\\gamma\\approx0.5772156649\u0026hellip;$はEuler-Mascheroni定数です。よって、$N$が大きいとき、i.i.d.の標準正規分布の最大統計量の期待値は\n$$ E[\\max[\\hat{SR}]] \\approx \\alpha + \\gamma\\beta = (1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}] \\tag{2} $$ と近似できます($N\u0026gt;1$)。これがBailey et. al.(2015)のProposition 1.になります。$E[\\max[\\hat{SR}]]$を戦略数(試行錯誤数)$N$の関数としてプロットしたのが以下になります。\nlibrary(ggplot2) ## Warning: パッケージ 'ggplot2' はバージョン 4.1.3 の R の下で造られました\rExMaxSR = function(N){ gamma_ct = -digamma(1) Z = qnorm(0.99) return((1-gamma_ct)*Z*(1-1/N) + gamma_ct*Z*(1-1/N*exp(1)^{-1})) } N = list(0:100) result = purrr::map(N,ExMaxSR) ggplot2::ggplot(data.frame(ExpMaxSR = unlist(result),N = unlist(N)),aes(x=N,y=ExpMaxSR)) + geom_line(size=1) + ylim(0,3) 小さい$N$に対して急激に$\\max[\\hat{SR}]$の期待値が上昇していることがわかると思います。$N=10$の時、$\\max[\\hat{SR}]=1.54$となっており、全ての戦略のシャープレシオの真値が0にも拘わらず、少なくとも1つは見かけ上かなり良いパフォーマンスの戦略が見つかることが期待されます。金融ではhold-out法でのバックテストはしばしば使用されるかと思いますが、この方法は試行(錯誤)回数を考慮に入れていないため、$N$が大きいときには信頼に足る結果を返してくれないわけです。バックテストの結果を向上させるため、闇雲にあれやこれやとシミュレーションを行うことは非常に危険だと思いませんか？最終的にプレゼン資料に上がってくるのは$N$個の戦略のうち、最もパフォーマンスが良いもののみですから、今回の例のように10個戦略を考えただけでもどれかはシャープレシオが1.87付近に分布しているわけです。試行錯誤数なんてもちろん資料には記載しませんから、非常にミスリーディングなわけです。こういった資料を評価する際にはまず偽陽性を疑ってかかった方がいいかもしれません。\nでは、どうすれば良いのかという話ですが、Bailey et. al.(2015)では、Minimum Backtest Lengthを計算しています。要は試行(錯誤)数$N$を増やすにつれて、バックテストの年数$y$も伸ばしていけよと戒めているわけです。$N$とMinimum Backtest Lengthの関係性を示していきましょう。先ほどと同じく$\\mu=0$を仮定しますが、$y\\neq 1$であるケースを考えます。年率化シャープレシオの最大統計量の期待値は(2)式より、\n$$ E[\\max[\\hat{SR}(q)]_N] \\approx y^{-1/2}((1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}]) $$ となります。これを$y$に対して解いてやることでMinBTLが求まります。\n$$ MinBTL \\approx (\\frac{(1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}]}{\\bar{E[\\max[\\hat{SR}(q)]_N]}})^2 $$ ここで、$\\bar{E[\\max[\\hat{SR}(q)]_N]}$は$E[\\max[\\hat{SR}(q)]_N]$の上限値で、シャープレシオの真値が0である$N$戦略でシャープレシオの最大統計量が取りうる値を抑えます。その際に、必要なバックテスト年数$y$がMinBTLとして導出されるのです。$\\bar{E[\\max[\\hat{SR}(q)]_N]}=1$として、MinBTLを$N$の関数としてプロットしたものが以下です。\nMinBTL \u0026lt;- function(N,MaxSR){ return((ExMaxSR(N)/MaxSR)^2) } N = list(1:100) result = purrr::map2(N,1,MinBTL) ggplot2::ggplot(data.frame(MinBTL = unlist(result),N = unlist(N)),aes(x=N,y=MinBTL)) + geom_line(size=1) + ylim(0,6) simSR \u0026lt;- function(T1){ r = rnorm(T1) return(mean(r)/sd(r)) } 仮にバックテスト年数が3年以内しかできない場合は試行(錯誤)回数$N$はほぼ1回に抑えないといけないことになります。3年以内の場合は一発で当ててねという厳しめの制約です。注意しないといけないのは、MinBTLの範囲内でバックテストを行っていたとしてもオーバーフィットすることは考えられるということです。つまり、MinBTLは必要条件であって十分条件でないというわけです。\n4. 終わりに López de Prado(2018)では、オーバーフィッティングを防ぐ汎用的な手段として以下が挙げられています。\n  Develop models for entire asset classes or investment universes, rather than for specific securities. Investors diversify, hence they do not make mistake X only on security Y. If you find mistake X only on security Y, no matter how apparently profitable, it is likely a false discovery. (拙訳：特定の有価証券ではなく、アセットクラス全体またはユニバース全体のモデルを開発すること。投資家はリスクを分散させているので、彼らはある証券Yだけに対してミスXをすることはありません。あなたが証券YだけにミスXを見つけた場合は、それがどんなに明らかに有益であっても、誤発見である可能性が高い。)\n  Apply bagging as a means to both prevent overfitting and reduce the variance of the forecasting error. If bagging deteriorates the performance of a strategy, it was likely overfit to a small number of observations or outliers. (拙訳：オーバーフィットを防ぎ、予測誤差の分散を減らすための手段として、バギングを適用すること。バギングが戦略のパフォーマンスを悪化させる場合、それは少数の観測値または外れ値にオーバーフィットした可能性が高い。)\n  Do not backtest until all your research is complete. (拙訳：すべてのリサーチが完了するまでバックテストをしないこと。)\n  Record every backtest conducted on a dataset so that the probability of backtest overfitting may be estimated on the final selected result (see Bailey, Borwein, López de Prado and Zhu(2017)), and the Sharpe ratio may be properly deflated by the number of trials carried out (Bailey and López de Prado(2014.b)). (拙訳：研究者が最終的に選択したバックテスト結果がオーバーフィットしている確率を推定できるように、単一の(同じ)データセットで実施されたバックテストをすべて記録すること（Bailey, Borwein, López de Prado and Zhu [2017]）、また、実施された試行数によってシャープレシオを適切にデフレーションできるようにすること（Bailey and López de Prado [2014]）。)\n  Simulate scenarios rather than history. A standard backtest is a historical simulation, which can be easily overfit. History is just the random path that was realized, and it could have been entirely different. Your strategy should be profitable under a wide range of scenarios, not just the anecdotal historical path. It is harder to overfit the outcome of thousands of “what if” scenarios. (拙訳：ヒストリカルではなくシナリオをシミュレーションすること。標準的なバックテストはヒストリカルシミュレーションであり、オーバーフィットしやすい。歴史(これまでの実績)はランダムなパスの実現値に過ぎず、全く違ったものになっていた可能性があります。あなたの戦略は、逸話的なヒストリカルパスではなく、様々なシナリオの下で利益を得ることができるものであるべきです。何千もの「もしも」のシナリオ結果をオーバーフィットさせるのは(ヒストリカルシミュレーションで過学習するよりも)より難しいことです。)\n  Do not research under the influence of a backtest. If the backtest fails to identify a profitable strategy, start from scratch. Resist the temptation of reusing those results. (拙訳：バックテストのフィードバックを受けてリサーチしないこと。バックテストが有益な戦略を見つけ出すことに失敗した場合は、ゼロからリサーチを再始動してください。それらの結果を再利用する誘惑に抗ってください。)\n  3と6は本日の論文と関係のある文脈だと思います。この分野は他にも研究の蓄積があるので、業務でバックテストを行うという人は運用手法の勉強もいいですが、そもそものお作法としてバックテストの正しい運用方法について学ぶことをお勧めします。\nさて、いつもとは違う観点で、少しメタ的なトピックに取り組んでみました。自分自身仕事柄バックテスト結果などを見ることも多いですし、このブログでもしばしばhold-out法でのバックテストをしています。得られた結果の不確実性を理解して、評価できるよう今後もこのトピックの研究を追っていきたいと思います。\n","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1594166400,"objectID":"229d2ed415199727e60335773ffb3171","permalink":"/post/post19/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/post/post19/","section":"post","summary":"金融であればクオンツの方は新規運用戦略の立案をする際に、バックテストを行ってパフォーマンスの確認をすることがあると思います。今回は、バックテストのオーバーフィッティングがアウトオブサンプル・パフォーマンスに及ぼす影響について調べたので備忘録をかねてまとめてみました。","tags":["機械学習","金融","時系列解析"],"title":"そのバックテスト本当に再現性ありますか？","type":"post"},{"authors":null,"categories":["競馬"],"content":"おはこんばんにちは。今回は競馬予想についての記事を書きたいと思います。前回、LightGBMを用いてyahoo競馬から取得したレース結果データ(テーブルデータ)を用いて、競馬順位予想モデルを作成しました。前回は構造データを用いましたが、このご時世ですからこんな分析は誰にでもできるわけです。時代は非構造データ、というわけで今回は馬体画像から特徴量を抽出し、順位予想を行う畳み込みニューラルネットワーク(Convolutional Neural Network, CNN)を作成してみました。画像解析はEarth Engineを用いた衛星画像の解析に続いて2回目、深層学習はこのブログでは初めてと言うことになります。なお、Pythonを使用しています。\n1. データ収集のためのクローリング まず、馬体画像をネットから収集することから始めます。1番良いのはレース当日のパドックの写真を使用することでしょう。ただ、パドックの写真をまとまった形で掲載してくれているサイトは調べた限りは存在しませんでした。もしかしたら、Youtubeに競馬ファンの方がパドック動画を上げていらっしゃるかも知れませんので、それを画像に切り抜いて使う or 動画としてCNN→RNNのEncoder-Decoderモデルに適用すると面白いかもしれません。しかし、そこまでの能力は今の自分にはありません。\nデータをどこから取得するか そこで、今回はデイリーのWebサイトからデータを取得しています。ここには直近1年間?のG1レースに出馬する競走馬のレース前の馬体写真が掲載されています。実際のレース場へ行けない馬券師さんたちはこの写真を見て馬の状態を分析していると思われます。\nなお、このサイトには出馬全頭の馬体写真が掲載されているわけではありません。また、G1の限られたレースのみですので、そもそも全ての馬が仕上がっている可能性もあり、差がつかないことも十分予想されます。ただ、手っ取り早くやってみることを優先し、今回はこのデータを使用することにしたいと思います。\nseleniumでクローリングを実行する クローリングにはseleniumを使用します。今回はCNNがメインなのでWebクローリングについては説明しません。使用したコードは以下です。\n【注意】以下のコードを使用される場合は自己責任でお願いします。\nfrom selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.support.select import Select from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.alert import Alert from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException from selenium.webdriver.common.action_chains import ActionChains from time import sleep from urllib import request import random # seleniumのオプション設定（おまじない） options = Options() options.add_argument(\u0026#39;--disable-gpu\u0026#39;); options.add_argument(\u0026#39;--disable-extensions\u0026#39;); options.add_argument(\u0026#39;--proxy-server=\u0026#34;direct://\u0026#34;\u0026#39;); options.add_argument(\u0026#39;--proxy-bypass-list=*\u0026#39;); options.add_argument(\u0026#39;--start-maximized\u0026#39;); # driver指定 DRIVER_PATH = r\u0026#39;C:/Users/aashi/Desktop/chromedriver_win32/chromedriver.exe\u0026#39; driver = webdriver.Chrome(executable_path=DRIVER_PATH, chrome_options=options) # urlを渡し、サイトへアクセス url = \u0026#39;https://www.daily.co.jp/horse/horsecheck/photo/\u0026#39; driver.get(url) driver.implicitly_wait(15) # オブジェクトのロード待ちの最大時間でこれを越えるとエラー sleep(5) # webページの遷移を行うので1秒sleep # 各レース毎に画像データ保存 selector0 = \u0026#34;body \u0026gt; div \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; div \u0026gt; section \u0026gt; a\u0026#34; elements = driver.find_elements_by_css_selector(selector0) for i in range(0,len(elements)): elements = driver.find_elements_by_css_selector(selector0) element = elements[i] element.click() sleep(5) # webページの遷移を行うので5秒sleep target = driver.find_element_by_link_text(\u0026#39;Ｇ１馬体診断写真集のTOP\u0026#39;) actions = ActionChains(driver) actions.move_to_element(target) actions.perform() sleep(5) # webページの遷移を行うので5秒sleep selector = \u0026#34;body \u0026gt; div.wrapper.horse.is-fixedHeader.is-fixedAnimation \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; article \u0026gt; div.photoDetail-wrapper \u0026gt; section \u0026gt; div \u0026gt; figure\u0026#34; figures = driver.find_elements_by_css_selector(selector) download_dir = r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026#39; selector = \u0026#34;body \u0026gt; div \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; article \u0026gt; div.photoDetail-wrapper \u0026gt; section \u0026gt; h1\u0026#34; race_name = driver.find_element_by_css_selector(selector).text for figure in figures: img_name = figure.find_element_by_tag_name(\u0026#39;figcaption\u0026#39;).text horse_src = figure.find_element_by_tag_name(\u0026#39;img\u0026#39;).get_attribute(\u0026#34;src\u0026#34;) save_name = download_dir + \u0026#39;/\u0026#39; + race_name + \u0026#39;_\u0026#39; + img_name + \u0026#39;.jpg\u0026#39; request.urlretrieve(horse_src,save_name) driver.back() 保存した画像を実際のレース結果と突合し、手作業で上位3位以内グループとそれ以外のグループに分けました。以下のような感じで画像が保存されています。\nこれで元データの収集が完了しました。\n2. Kerasを用いてCNNを学習させる Kerasとは？ さて、次にKerasを使ってCNNを学習させましょう。まず、KerasとはTensorflowやTheano上で動くNeural Networkライブラリの1つです。Kerasは比較的短いコードでモデルを組むことができ、また学習アルゴリズムが多いことが特徴のようです。\nCNNとは？ CNNは画像解析を行う際によく使用される(Deep) Neural Networkの1種で、その名の通りConvolution(畳み込み)を追加した物となっています。畳み込みとは以下のような処理のことを言います。\nここのインプットとは画像データのことです。画像解析では画像を数値として認識し、解析を行います。コンピュータ上の画像はRGB値という、赤(Red)、緑(Green)、青(Blue)の3色の0~255までの数値の強弱で表現されています。赤255、緑0、青0といった形で3層のベクトルになっており、この場合完全な赤が表現されます。上図の場合、a,b,cなどが各ピクセルのRGB値のいずれかを表していると考えることができます。畳み込みはこのRGB値をカーネルと呼ばれる行列との内積をとることで画像の特徴量を計算します。畳み込み層の意味は以下の動画がわかりやすいです。\n\rカーネルを上手くその画像の特徴的な部分を取得できるように学習することで、画像の識別が可能になります。畳み込み層はCNNの最重要部分だと思います。\n上図のようにCNNは畳み込み意外にももちろん入力層や出力層など通常のNeural Networkと同じ層も持っています。なお、MaxPooling層について知りたい人は以下の動画を参照されてください。\n\r深層学習の学習方法については最急下降法(勾配法)がオーソドックスなものとして知られていますが、Adamなど色々な拡張アルゴリズムが提案されています。基本的には、Adamはmomentumを使用することが多いでしょうか。\nコーディング では、実際にコーディングしていきます。\nfrom keras.utils import np_utils ## Using TensorFlow backend.\rfrom keras.models import Sequential from keras.layers.convolutional import MaxPooling2D from keras.layers import Activation, Conv2D, Flatten, Dense,Dropout from sklearn.model_selection import train_test_split from keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop, Nadam from PIL import Image import numpy as np import glob import matplotlib.pyplot as plt import time import os まず最初に収集してきた画像データを数値データに変換し学習データを作成します。 ディレクトリ構造は以下のようになっており、上位画像とその他画像が別ディレクトリに保存されています。各ディレクトリから画像を読み込む際に、上位画像には1、その他には0というカテゴリ変数を与えます。\n馬体写真\n 上位 その他  #フォルダを指定 folders = os.listdir(r\u0026#34;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026#34;) #画総数を指定(今回は50×50×3)。 image_size = 300 dense_size = len(folders) X = [] Y = [] #それぞれのフォルダから画像を読み込み、Image関数を使用してRGB値ベクトル(numpy array)へ変換 for i, folder in enumerate(folders): files = glob.glob(\u0026#34;C:/Users/aashi/umanalytics/photo/image/\u0026#34; + folder + \u0026#34;/*.jpg\u0026#34;) index = i for k, file in enumerate(files): image = Image.open(file) image = image.convert(\u0026#34;L\u0026#34;).convert(\u0026#34;RGB\u0026#34;) image = image.resize((image_size, image_size)) #画素数を落としている data = np.asarray(image) X.append(data) Y.append(index) X = np.array(X) Y = np.array(Y) X = X.astype(\u0026#39;float32\u0026#39;) X = X / 255.0 # 0~1へ変換 X.shape Y = np_utils.to_categorical(Y, dense_size) #訓練データとテストデータへ変換 X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20) 訓練データとテストデータの分割ができました。今考えているのは「上位」と「その他」の2値分類となっていますが、「上位」を3位以内と定義したので不均衡なデータとなっています(その他データが上位データの5倍くらい)。こういった場合、そのままのデータで学習をするとサンプルサイズが多い方のラベル(この場合「その他」)を予測しやすくなり、バイアスのあるモデルとなります。よって、学習データは2クラスそれぞれが同じサンプルサイズとなるよう調整してやる必要があります。\nindex_zero = np.random.choice(np.array(np.where(y_train[:,1]==0))[0,],np.count_nonzero(y_train[:,1]==1),replace=False) index_one = np.array(np.where(y_train[:,1]==1))[0] y_resampled = y_train[np.hstack((index_one,index_zero))] X_resampled = X_train[np.hstack((index_one,index_zero))] 学習データにはこのy_resampledとX_resampledを使用します。次に、CNNを構築していきます。Kerasでは、sequential modelを指定し、addメソッドで層を追加して行くことでモデルを定義します。\nmodel = Sequential() model.add(Conv2D(32, (3, 3), padding=\u0026#39;same\u0026#39;,input_shape=X_train.shape[1:])) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(Conv2D(32, (3, 3))) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), padding=\u0026#39;same\u0026#39;)) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(Conv2D(64, (3, 3))) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512)) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(Dropout(0.5)) model.add(Dense(dense_size)) model.add(Activation(\u0026#39;softmax\u0026#39;)) model.summary() ## Model: \u0026quot;sequential_1\u0026quot;\r## _________________________________________________________________\r## Layer (type) Output Shape Param # ## =================================================================\r## conv2d_1 (Conv2D) (None, 300, 300, 32) 896 ## _________________________________________________________________\r## activation_1 (Activation) (None, 300, 300, 32) 0 ## _________________________________________________________________\r## conv2d_2 (Conv2D) (None, 298, 298, 32) 9248 ## _________________________________________________________________\r## activation_2 (Activation) (None, 298, 298, 32) 0 ## _________________________________________________________________\r## max_pooling2d_1 (MaxPooling2 (None, 149, 149, 32) 0 ## _________________________________________________________________\r## dropout_1 (Dropout) (None, 149, 149, 32) 0 ## _________________________________________________________________\r## conv2d_3 (Conv2D) (None, 149, 149, 64) 18496 ## _________________________________________________________________\r## activation_3 (Activation) (None, 149, 149, 64) 0 ## _________________________________________________________________\r## conv2d_4 (Conv2D) (None, 147, 147, 64) 36928 ## _________________________________________________________________\r## activation_4 (Activation) (None, 147, 147, 64) 0 ## _________________________________________________________________\r## max_pooling2d_2 (MaxPooling2 (None, 73, 73, 64) 0 ## _________________________________________________________________\r## dropout_2 (Dropout) (None, 73, 73, 64) 0 ## _________________________________________________________________\r## flatten_1 (Flatten) (None, 341056) 0 ## _________________________________________________________________\r## dense_1 (Dense) (None, 512) 174621184 ## _________________________________________________________________\r## activation_5 (Activation) (None, 512) 0 ## _________________________________________________________________\r## dropout_3 (Dropout) (None, 512) 0 ## _________________________________________________________________\r## dense_2 (Dense) (None, 2) 1026 ## _________________________________________________________________\r## activation_6 (Activation) (None, 2) 0 ## =================================================================\r## Total params: 174,687,778\r## Trainable params: 174,687,778\r## Non-trainable params: 0\r## _________________________________________________________________\rでは、学習パートに入ります。アルゴリズムにはAdadeltaを使用します。よくわかってないんですけどね。。。\noptimizers =\u0026#34;Adadelta\u0026#34; results = {} epochs = 50 model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=optimizers, metrics=[\u0026#39;accuracy\u0026#39;]) results = model.fit(X_resampled, y_resampled, validation_split=0.2, epochs=epochs) 不均衡データ調整のためのアンダーサンプリング ここから、Testデータで2値分類を行うのですが、学習データをアンダーサンプリングしているので、予測確率を計算する際にアンダーサンプリングを行ったサンプル選択バイアスが生じてしまいます。論文はこちら。 よって、補正が必要になるのですがこの部分の定式化をここでしておきたいと思います。現在行っている2値分類問題を説明千数$X$から2値を取る目的変数$Y$を予測する問題と表現することにします。データセット$(X,Y)$は正例が負例よりもかなり少なく、負例のサンプルサイズを正例に合わせたデータセットを$(X_s,Y_s)$とします。ここで、$(X,Y)$のサンプル組が$(X_s,Y_s)$にも含まれる場合に1を取り、含まれない場合に0をとるカテゴリ変数$s$を定義します。 データセット$(X,Y)$を用いて構築したモデルに説明変数$x$を与えた時、正例と予測する条件付き確率は$P(y=1|x)$で表すことができます。一方、$(X_s,Y_s)$を用いて構築したモデルで正例を予測する条件付き確率はベイズの定理とカテゴリ変数$s$を用いて、\n$$ P(y=1|x,s=1) = \\frac{P(s=1|y=1)P(y=1|x)}{P(s=1|y=1)P(y=1|x) + P(s=1|y=0)P(y=0|x)} $$ と書けます。$(X_s,Y_s)$は負例のサンプルサイズを正例に合わせているため、$P(s=1,y=1)=1$であるので上式は\n$$ P(y=1|x,s=1) = \\frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)P(y=0|x)} = \\frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)(1-P(y=1|x))} $$ と書き換えることができます。$P(s=1|y=0)\\neq0$であることは$(X_s,Y_s)$の定義より自明です(0だと正例しかない不均衡データになる)。よって、$P(y=0,x)\\neq0$である限り、アンダーサンプリングのモデルが正例とはじき出す確率は元のデータセットが出す確率に対して正のバイアスがあることがわかります。求めたいのはバイアスのない$P(y=1|x)$なので$P=P(y=1|x),P_s=P(y|x,s=1),\\beta=P(s=1,y=0)$とすると、\n$$ P = \\frac{\\beta P_s}{\\beta P_s-P_s+1} $$ とかけ、この関係式を用いてバイアスを補正することができます。 今確認したことを関数として定義しましょう。\ndef calibration(y_proba, beta): return y_proba / (y_proba + (1 - y_proba) / beta) sampling_rate = sum(y_train[:,1]) / sum(1-y_train[:,1]) y_proba_calib = calibration(model.predict(X_test), sampling_rate) y_pred = np_utils.to_categorical(np.argmax(y_proba_calib,axis=1), dense_size) from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score score = accuracy_score(y_test, y_pred) print(\u0026#39;Test accuracy:\u0026#39;, score) ## Test accuracy: 0.22033898305084745\rまったく良くない結果です。ConfusionMatrixを出してみたところどうやらうまくいっていないことがわかりました。\nConfusionMatrixDisplay(confusion_matrix(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))).plot() ## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000005474F108\u0026gt;\rplt.show() plt.close() 不均衡データのバイアス修正はしたんですが、それでもなお正値を予測しやすいモデルとなっています。これでは使えないですね。\n3. Shap値を用いた結果解釈 今学習したモデルのshap値を考え、結果の解釈をしたいと思います。shap値については時間があれば、説明を追記したいと思います。簡単に言えば、CNNが画像のどの部分に特徴を捉え、馬が上位に入るかを予想したかを可視化で捉えることができます。この馬の解析をすることにします。\nplt.imshow(X_test[0]) plt.show() plt.close() import shap background = X_train[np.random.choice(X_train.shape[0],100,replace=False)] e = shap.GradientExplainer(model,background) shap_values = e.shap_values(X_test[[0]]) shap.image_plot(shap_values[1],X_test[[0]]) 非常に微妙ですが、足や臀部などを評価しているようにみえます。ですが、背景に反応しているようにも見えるので馬体のみ取り出すトリミングをやる必要がありますね。これは物体検知のモデルを構築する必要がありそうです。また、今度の機会に考えます。\n4. 最後に 正直まったく上手くいっていません。やはり馬体から順位予測をするのは難しいのでしょうか。ほかの変数と掛け合わせると結果が変わったりするのでしょうか。今のままだとよい特徴量を抽出することができていないように思います。 Youtubeからパドック動画を取得して、Encoder-Decoderモデルで解析するところまでやらないとうまくいかないんですかね。自分の実力が十分上がれば是非やってみたいと思います(いつになることやら)。それまでには、PCのスペックを上げないといけません。定額給付金を使うかな。。。\n","date":1593907200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1593907200,"objectID":"6dc680f46f0d2b9d9526697571eabba2","permalink":"/post/post18/","publishdate":"2020-07-05T00:00:00Z","relpermalink":"/post/post18/","section":"post","summary":"馬体写真という新しい観点から競馬予測モデルを作りました。","tags":["Python","Webスクレイピング","機械学習"],"title":"CNNを使って馬体写真から順位予想してみた","type":"post"},{"authors":null,"categories":["単発","Tips"],"content":"皆さんおはこんばんにちは。 最近在宅勤務で運動不足ですが、平日休日問わず研究活動をしています。 学術誌投稿を目指したBlog記事には書けない内容なので、更新はストップしてしまっていますがちゃんと活動はしています。\n今回はその研究の中で利用しているMeCabにまつわるTipsのご紹介です。MeCabというと形態素解析ソフトということはこのブログを読まれている方はお分かりかと思いますが、その辞書にNEologd辞書を使用できるようにしてみたというのが内容です。MeCabってなに？という方もいらっしゃるかもしれませんので、かなり簡単にご紹介します。\n1. Mecab(RMeCab)とは？ MeCabは日本語テキストマイニングで使用される形態素解析ソフトです。英語などの言語とは異なり、日本語は単語毎にスペースを置かないため、テキストマイニングを行う際、そのままでは文章を単語単位に区切って集計するといったことができません。例えば、「これはペンです。」という文章はそのままでは「これはペンです。」という1つの単語として認識されてしまいます。ですが、単語の頻度分析を行う際などは、「これ/は/ペン/です/。」という風に文章を単語レベルにまで分割し、「ペン」という特徴量を取得したいわけです。それができるのがRMeCabで、この処理は形態素解析と呼ばれます。1そして、RMeCabとはこのMeCabのラッパーになります。RからMeCabを使用するにはRMeCabを使用する必要があります。使用方法は簡単で、RMeCabC関数に文章を渡すだけです。\nlibrary(magrittr) library(RMeCab) RMeCabC(\u0026#34;これはペンです。\u0026#34;) %\u0026gt;% unlist() ## 名詞 助詞 名詞 助動詞 記号 ## \u0026quot;これ\u0026quot; \u0026quot;は\u0026quot; \u0026quot;ペン\u0026quot; \u0026quot;です\u0026quot; \u0026quot;。\u0026quot;\rRMeCabC関数は結果をリストで返してくるので、unlistでcharacterに変換しています。\n固有名詞に弱いデフォルトのMeCab 便利なように思えるのですが、デフォルトのMeCabは固有名詞に弱いという弱点があります。例えば、「欅坂46が赤いきつねを食べている。」という文章を形態素解析してみましょう。\nRMeCabC(\u0026#34;欅坂46が赤いきつねを食べている。\u0026#34;) %\u0026gt;% unlist() ## 名詞 名詞 名詞 助詞 形容詞 名詞 助詞 動詞 ## \u0026quot;欅\u0026quot; \u0026quot;坂\u0026quot; \u0026quot;46\u0026quot; \u0026quot;が\u0026quot; \u0026quot;赤い\u0026quot; \u0026quot;きつね\u0026quot; \u0026quot;を\u0026quot; \u0026quot;食べ\u0026quot; ## 助詞 動詞 記号 ## \u0026quot;て\u0026quot; \u0026quot;いる\u0026quot; \u0026quot;。\u0026quot;\r予想していた結果は「欅坂46/が/赤いきつね/を/食べ/て/いる/。」でしょう。ただ、固有名詞を上手く形態素解析することができていないため、不必要なところで分割がなされており、「赤い/きつね/を/食べ」という部分については「赤い（動物の）きつね」を食べているかのような解析結果になっています。赤いきつねの「きつね」と動物の「きつね」を同等に扱ってしまうので、問題があります。また、経済においても以下のように日経平均株価が分割され、新聞社の「日経」と株価の「日経」が、大統領の「トランプ」とカードの「トランプ」が区別できないといったことが想定されます。\nRMeCabC(\u0026#34;今週の日経平均株価は、上値抵抗線を突破して上昇！\u0026#34;) %\u0026gt;% unlist() ## 名詞 助詞 名詞 名詞 名詞 助詞 記号 名詞 名詞 名詞 助詞 ## \u0026quot;今週\u0026quot; \u0026quot;の\u0026quot; \u0026quot;日経\u0026quot; \u0026quot;平均\u0026quot; \u0026quot;株価\u0026quot; \u0026quot;は\u0026quot; \u0026quot;、\u0026quot; \u0026quot;上値\u0026quot; \u0026quot;抵抗\u0026quot; \u0026quot;線\u0026quot; \u0026quot;を\u0026quot; ## 名詞 動詞 助詞 名詞 記号 ## \u0026quot;突破\u0026quot; \u0026quot;し\u0026quot; \u0026quot;て\u0026quot; \u0026quot;上昇\u0026quot; \u0026quot;！\u0026quot;\rRMeCabC(\u0026#34;トランプ政権による経済活動再開の指針発表が評価される\u0026#34;) %\u0026gt;% unlist() ## 名詞 名詞 助詞 名詞 名詞 名詞 助詞 ## \u0026quot;トランプ\u0026quot; \u0026quot;政権\u0026quot; \u0026quot;による\u0026quot; \u0026quot;経済\u0026quot; \u0026quot;活動\u0026quot; \u0026quot;再開\u0026quot; \u0026quot;の\u0026quot; ## 名詞 名詞 助詞 名詞 動詞 動詞 ## \u0026quot;指針\u0026quot; \u0026quot;発表\u0026quot; \u0026quot;が\u0026quot; \u0026quot;評価\u0026quot; \u0026quot;さ\u0026quot; \u0026quot;れる\u0026quot;\rこれはデフォルトでMeCabが使用している辞書に原因があります。IPA辞書(ipadic)と呼ばれる物で、奈良先端科学技術大学院大学が公開している茶筌と呼ばれる形態素解析ソフト用に作られました。こちらを見ると辞書の更新は2007年でストップしており、新語や流行語がアップデートされていないために上記の固有名詞が上手く形態素解析できないことがわかります。\n新語に強いNEologd辞書 最近MeCabでよく使用されている辞書にNEologd辞書というものがあります。NEologd辞書とは、Web上から得た新語に対応しており、頻繁に更新されるMeCab用のシステム辞書です。2Twitterのアカウントには、\n 特色は語彙の多さと更新頻度、新語の採録の速さ、読み仮名の正確さ、表記揺れへの対応。おもな解析対象はWeb上のニュース記事や流行した出来事。 おもな用途は文書分類、文書ベクトル作成、単語埋め込みベクトル作成、読み仮名付与。\n と記載されており、上述したIPA辞書の弱点を補完する辞書となっています。今回の記事はその辞書のインストール方法についてですが、インストールした辞書の威力を先にお見せしておきます。実行するには、RMeCab関数に辞書ファイル(.dicファイル)のパスを渡してやれば良いです。\ndic_directory \u0026lt;- \u0026#34;C:\\\\hogehoge\\\\mecab-user-dict-seed.yyyymmdd.dic\u0026#34; RMeCabC(\u0026#34;欅坂46が赤いきつねを食べている。\u0026#34;,dic=dic_directory) %\u0026gt;% unlist() ## 名詞 助詞 名詞 助詞 動詞 助詞 ## \u0026quot;欅坂46\u0026quot; \u0026quot;が\u0026quot; \u0026quot;赤いきつね\u0026quot; \u0026quot;を\u0026quot; \u0026quot;食べ\u0026quot; \u0026quot;て\u0026quot; ## 動詞 記号 ## \u0026quot;いる\u0026quot; \u0026quot;。\u0026quot;\rRMeCabC(\u0026#34;今週の日経平均株価は、上値抵抗線を突破して上昇！\u0026#34;,dic=dic_directory) %\u0026gt;% unlist() ## 名詞 助詞 名詞 助詞 記号 ## \u0026quot;今週\u0026quot; \u0026quot;の\u0026quot; \u0026quot;日経平均株価\u0026quot; \u0026quot;は\u0026quot; \u0026quot;、\u0026quot; ## 名詞 名詞 名詞 助詞 名詞 ## \u0026quot;上値\u0026quot; \u0026quot;抵抗\u0026quot; \u0026quot;線\u0026quot; \u0026quot;を\u0026quot; \u0026quot;突破\u0026quot; ## 動詞 助詞 名詞 記号 ## \u0026quot;し\u0026quot; \u0026quot;て\u0026quot; \u0026quot;上昇\u0026quot; \u0026quot;！\u0026quot;\rRMeCabC(\u0026#34;トランプ政権による経済活動再開の指針発表が評価される\u0026#34;,dic=dic_directory) %\u0026gt;% unlist() ## 名詞 助詞 名詞 名詞 助詞 ## \u0026quot;トランプ政権\u0026quot; \u0026quot;による\u0026quot; \u0026quot;経済活動\u0026quot; \u0026quot;再開\u0026quot; \u0026quot;の\u0026quot; ## 名詞 名詞 助詞 名詞 動詞 ## \u0026quot;指針\u0026quot; \u0026quot;発表\u0026quot; \u0026quot;が\u0026quot; \u0026quot;評価\u0026quot; \u0026quot;さ\u0026quot; ## 動詞 ## \u0026quot;れる\u0026quot;\r上値抵抗線は1語カウントしてほしいところではありますが、それ以外は上手く形態素解析できていそうです。 このNEologd辞書ですが、Windowsのインストールが想定されていません。つまり、Windowsユーザーは直接インストールすることができないのです。Windowsユーザーは少々ややこしい手順を踏まなければなりません。今回はそのややこしい手順を解説する記事です。Linuxでインストールを行い、それをWindows環境にコピー、その後辞書ファイルを作成します。\n2. インストール手順 A. Windows Subsystem for Linux(WSL)のインストール Windows Power Shellを管理者権限で開き、\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux を実行する。WSLをインストールすることができます。\n{width=100%}\nB. Ubuntu Linuxのインストール Microsoft Storeよりubuntuをダウンロードする。\n{width=100%}\nインストールが完了したらubuntuを起動し、初期設定を完了させる(ID、パスワード)。\nC. Ubuntu LinuxにMeCabをインストール ubuntuのコマンドプロンプトで、\nsudo apt-get update sudo apt install mecab sudo apt install libmecab-dev sudo apt install mecab-ipadic-utf8 を入力し、MeCabをインストール。\n{width=100%}\nD. Ubuntu for LinuxにNEologd辞書をインストール ubuntsuのコマンドプロンプトで\nsudo apt install make git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git cd mecab-ipadic-neologd sudo bin/install-mecab-ipadic-neologd -n -a を実行。NEologd辞書ファイルが(/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/)にインストールできます。\nE. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー ubuntuのコマンドプロンプトで\nexplorer.exe . を入力。エクスプローラーが立ち上がるので、/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/(ディレクトリ)をWindowsの任意のディレクトリにコピーする。完了したらUbuntuは閉じる。\n{width=100%}\nF. Windows内で辞書ファイルのコンパイル(SHIFT-JIS)を行う。 コピーしたディレクトリ内の以下のcsvを辞書ファイル(.dic)にコンパイルします。(yyyymmddは辞書の最終更新日なので変更してください)\n-mecab-ipadic-neologd-buildmecab-ipadic-2.7.0-20070801-neologd-yyyymmdd-mecab-user-dict-seed.yyyymmdd.csv その際、元ファイルはエンコーディングがUTF-8となっているので、SHIFT-JISへ変換することに注意です。これをしないと、RMeCabを実行したときに結果が文字化けします。コンパイルにはMeCabのmecab-dict-indexというバイナリファイルを使用します。自分は以下のディレクトリに存在しました。\nC: -Program Files (x86) -MeCab -bin -mecab-dict-index.exe\n{width=100%}\nコマンドプロンプトを立ち上げ、\nC:\\Program Files (x86)\\MeCab\\bin\\mecab-dict-index.exe -d .../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmdd(ファイルの保存場所) -u NEologd.yyyymmdd.dic -f utf-8 -t shift-jis mecab-ipadic-neologd\\buildmecab\\ipadic-2.7.0-20070801-neologd-yyyymmdd\\mecab-user-dict-seed.yyyymmdd.csv を適宜変更の上、入力。.../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmddに辞書がコンパイルされ、NEologd.yyyymmdd.dicができます。\n3. まとめ 以上で、NEologd辞書が使用できるようになります。非常に強力なツールなので使用してみてください。なお、辞書がアップデートされた際は同じ手続きを行う必要があります。\n\u0026lt;後日談\u0026gt; Ubuntu for Linuxを使用しなくてもダウンロードできそうな方法ありました。\nhttps://qiita.com/zincjp/items/c61c441426b9482b5a48\nただ、自分は実行していないので実際にできるかはわかりません。辞書が更新されたら、やってみたいと思います。\n  MeCab内部の仕組みについてはこちらの記事が参考になります。 \u0026#x21a9;\u0026#xfe0e;\n 2020/4/19時点の最近版は2020/3/15更新の辞書です。 \u0026#x21a9;\u0026#xfe0e;\n   ","date":1587254400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1587254400,"objectID":"76148b2112439101151e4d7e3e7f6986","permalink":"/post/post17/","publishdate":"2020-04-19T00:00:00Z","relpermalink":"/post/post17/","section":"post","summary":"新語に強いNEologd辞書をMecabにインストールしてみました。","tags":["R","テキストマイニング"],"title":"WindowsにNEologd辞書をインストールして、RMeCabを実行する方法","type":"post"},{"authors":null,"categories":["競馬"],"content":"おはこんばんにちは。かなり久しぶりではありますが、Pythonの勉強をかねて以前yahoo.keibaで収集した競馬のレース結果データから、レース結果を予想するモデルを作成したいと思います。\n1.データインポート まず、前回sqliteに保存したレース結果データをpandasデータフレームへ保存します。\nconn = sqlite3.connect(r\u0026#39;C:\\hogehoge\\horse_data.db\u0026#39;) sql = r\u0026#39;SELECT * FROM race_result\u0026#39; df = pd.read_sql(con=conn,sql=sql) データの中身を確認してみましょう。列は以下のようになっています。orderが着順となっています。\ndf.columns ## Index(['order', 'frame_number', 'horse_number', 'trainer', 'passing_rank',\r## 'last_3F', 'time', 'margin', 'horse_name', 'horse_age', 'horse_sex',\r## 'horse_weight', 'horse_weight_change', 'brinker', 'jockey',\r## 'jockey_weight', 'jockey_weight_change', 'odds', 'popularity',\r## 'race_date', 'race_course', 'race_name', 'race_distance', 'type',\r## 'race_turn', 'race_condition', 'race_weather', 'colour', 'owner',\r## 'farm', 'locality', 'horse_birthday', 'father', 'mother', 'prize',\r## 'http'],\r## dtype='object')\rorderの中身を確認してみると、括弧（）がついている物が多く、また取消や中止、失格などが存在するため、文字型に認識されていることがわかります。ちなみに括弧（）内の順位は入線順位というやつで、他馬の走行を妨害したりして順位が降着させられたことを意味します（http://www.jra.go.jp/judge/）。\ndf.loc[:,\u0026#39;order\u0026#39;].unique() ## array(['1', '7', '2', '8', '5', '15', '6', '12', '11', '14', '3', '13',\r## '4', '16', '9', '10', '取消', '中止', '除外', '17', '18', '4(3)', '2(1)',\r## '3(2)', '6(4)', '失格', '9(8)', '16(6)', '12(12)', '13(9)', '6(3)',\r## '10(7)', '6(5)', '9(3)', '11(8)', '13(2)', '12(9)', '14(7)',\r## '10(1)', '16(8)', '14(6)', '10(3)', '12(1)', '13(6)', '7(1)',\r## '12(6)', '6(2)', '11(2)', '15(6)', '13(10)', '14(4)', '7(5)',\r## '17(4)', '9(7)', '16(14)', '12(11)', '14(2)', '8(2)', '9(5)',\r## '11(5)', '12(7)', '11(1)', '12(8)', '7(4)', '5(4)', '13(12)',\r## '14(3)', '10(2)', '11(10)', '18(3)', '10(4)', '15(8)', '8(3)',\r## '5(1)', '10(5)', '7(3)', '5(2)', '9(1)', '13(3)', '16(11)',\r## '11(3)', '18(15)', '11(6)', '10(6)', '14(12)', '12(5)', '15(14)',\r## '17(8)', '18(6)', '4(2)', '18(10)', '16(7)', '13(1)', '16(10)',\r## '15(7)', '9(4)', '15(5)', '12(3)', '8(7)', '15(2)', '12(10)',\r## '14(9)', '3(1)', '6(1)', '14(5)', '15(4)', '11(4)', '12(4)',\r## '16(4)', '9(2)', '13(5)', '12(2)', '15(1)', '4(1)', '14(13)',\r## '14(1)', '13(7)', '5(3)', '8(6)', '15(13)', '7(2)', '15(11)',\r## '10(9)', '11(9)', '8(4)', '15(3)', '13(4)', '16(12)', '16(5)',\r## '18(11)', '10(8)', '18(8)', '14(8)', '16(9)', '8(5)', '8(1)',\r## '14(11)', '9(6)', '16(13)', '16(15)', '11(11)', '15(10)', '7(6)'],\r## dtype=object)\rまずここを修正しましょう。括弧を除去してint型に型変更し、入線順位は新たな列arriving orderとして追加します。\ndf[\u0026#39;arriving order\u0026#39;] = df[df.order.str.contains(r\u0026#39;\\d*\\(\\d*\\)\u0026#39;,regex=True)][\u0026#39;order\u0026#39;].replace(r\u0026#39;\\d+\\(\u0026#39;,r\u0026#39;\u0026#39;,regex=True).replace(r\u0026#39;\\)\u0026#39;,r\u0026#39;\u0026#39;,regex=True).astype(\u0026#39;float64\u0026#39;) df[\u0026#39;arriving order\u0026#39;].unique() ## array([nan, 3., 1., 2., 4., 8., 6., 12., 9., 7., 5., 10., 14.,\r## 11., 15., 13.])\rdf[\u0026#39;order\u0026#39;] = df[\u0026#39;order\u0026#39;].replace(r\u0026#39;\\(\\d+\\)\u0026#39;,r\u0026#39;\u0026#39;,regex=True) df = df[lambda df: ~df.order.str.contains(r\u0026#39;(取消|中止|除外|失格)\u0026#39;,regex=True)] ## C:\\Users\\aashi\\ANACON~1\\lib\\site-packages\\pandas\\core\\strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\r## return func(self, *args, **kwargs)\rdf[\u0026#39;order\u0026#39;] = df[\u0026#39;order\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;order\u0026#39;].unique() ## array([ 1., 7., 2., 8., 5., 15., 6., 12., 11., 14., 3., 13., 4.,\r## 16., 9., 10., 17., 18.])\rきれいなfloat型に処理することができました。では、次にラスト3Fのタイムの前処理に移ります。前走のラスト3Fのタイムを予測に使用します。\nimport numpy as np df[\u0026#39;last_3F\u0026#39;] = df[\u0026#39;last_3F\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False).astype(\u0026#39;float64\u0026#39;) df[\u0026#39;last_3F\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;last_3F\u0026#39;].shift(-1) 前走のレースと順位、追加順位もデータセットへ含めましょう。\ndf[\u0026#39;prerace\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;race_name\u0026#39;].shift(-1) df[\u0026#39;preorder\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;order\u0026#39;].shift(-1) df[\u0026#39;prepassing\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;passing_rank\u0026#39;].shift(-1) 出走時点で獲得している累積賞金額も追加します。\ndf[\u0026#39;preprize\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;prize\u0026#39;].shift(-1) df[\u0026#39;preprize\u0026#39;] = df[\u0026#39;preprize\u0026#39;].fillna(0) df[\u0026#39;margin\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;margin\u0026#39;].shift(-1) その他、欠損値やデータ型の修正、カテゴリデータのラベルエンコーディングです。\ndf[\u0026#39;horse_weight\u0026#39;] = df[\u0026#39;horse_weight\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;margin\u0026#39;] = df[\u0026#39;margin\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False) df[\u0026#39;horse_age\u0026#39;] = df[\u0026#39;horse_age\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;horse_weight_change\u0026#39;] = df[\u0026#39;horse_weight_change\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;jockey_weight\u0026#39;] = df[\u0026#39;jockey_weight\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;race_distance\u0026#39;] = df[\u0026#39;race_distance\u0026#39;].replace(r\u0026#39;m\u0026#39;,r\u0026#39;\u0026#39;,regex=True).astype(\u0026#39;float64\u0026#39;) df[\u0026#39;race_turn\u0026#39;] = df[\u0026#39;race_turn\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False) df.loc[df[\u0026#39;order\u0026#39;]!=1,\u0026#39;order\u0026#39;] = 0 df[\u0026#39;race_turn\u0026#39;] = df[\u0026#39;race_turn\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;colour\u0026#39;] = df[\u0026#39;colour\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;prepassing\u0026#39;] = df[\u0026#39;prepassing\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;prerace\u0026#39;] = df[\u0026#39;prerace\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;father\u0026#39;] = df[\u0026#39;father\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;mother\u0026#39;] = df[\u0026#39;mother\u0026#39;].fillna(\u0026#39;missing\u0026#39;) from sklearn import preprocessing cat_list = [\u0026#39;trainer\u0026#39;, \u0026#39;horse_name\u0026#39;, \u0026#39;horse_sex\u0026#39;, \u0026#39;brinker\u0026#39;, \u0026#39;jockey\u0026#39;, \u0026#39;race_course\u0026#39;, \u0026#39;race_name\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;race_turn\u0026#39;, \u0026#39;race_condition\u0026#39;, \u0026#39;race_weather\u0026#39;, \u0026#39;colour\u0026#39;, \u0026#39;father\u0026#39;, \u0026#39;mother\u0026#39;, \u0026#39;prerace\u0026#39;, \u0026#39;prepassing\u0026#39;] for column in cat_list: target_column = df[column] le = preprocessing.LabelEncoder() le.fit(target_column) label_encoded_column = le.transform(target_column) df[column] = pd.Series(label_encoded_column).astype(\u0026#39;category\u0026#39;) import pandas_profiling as pdq profile = pdq.ProfileReport(df) profile 2. 予測モデルの作成 ではLightGBMで予測モデルを作ってみます。optunaのLightGBMを使用して、ハイパーパラメータチューニングを行い、学習したモデルを用いて計算したテストデータの予測値と実績値のconfusion matrixならびに正解率を算出します。\nimport optuna.integration.lightgbm as lgb from sklearn.model_selection import train_test_split y = df[\u0026#39;order\u0026#39;] x = df.drop([\u0026#39;order\u0026#39;,\u0026#39;passing_rank\u0026#39;,\u0026#39;time\u0026#39;,\u0026#39;odds\u0026#39;,\u0026#39;popularity\u0026#39;,\u0026#39;owner\u0026#39;,\u0026#39;farm\u0026#39;,\u0026#39;locality\u0026#39;,\u0026#39;horse_birthday\u0026#39;,\u0026#39;http\u0026#39;,\u0026#39;prize\u0026#39;,\u0026#39;race_date\u0026#39;,\u0026#39;margin\u0026#39;],axis=1) X_train, X_test, y_train, y_test = train_test_split(x, y) X_train, x_val, y_train, y_val = train_test_split(X_train, y_train) lgb_train = lgb.Dataset(X_train, y_train) lgb_eval = lgb.Dataset(x_val, y_val) lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train) lgbm_params = { \u0026#39;objective\u0026#39;: \u0026#39;binary\u0026#39;, \u0026#39;boost_from_average\u0026#39;: False } model = lgb.train(lgbm_params, lgb_train, categorical_feature = cat_list, valid_sets = lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=False) def calibration(y_proba, beta): return y_proba / (y_proba + (1 - y_proba) / beta) sampling_rate = y_train.sum() / len(y_train) y_proba = model.predict(X_test, num_iteration=model.best_iteration) y_proba_calib = calibration(y_proba, sampling_rate) y_pred = np.vectorize(lambda x: 1 if x \u0026gt; 0.49 else 0)(y_proba_calib) 可視化パートです。\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score import matplotlib.pyplot as plt # Confusion Matrixを生成 ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot() ## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000000C3E2F7C8\u0026gt;\rplt.show() plt.close() accuracy_score(y_test, y_pred) ## 0.9299209299125913\rprecision_score(y_test, y_pred) ## 0.9401709401709402\raccuracy_score（予測精度）が90%を超え、precision_Score（適合率、陽=1着と予想したデータの正解率）もいい感じです。\nrecall_score(y_test, y_pred) ## 0.009727052997700878\rf1_score(y_test, y_pred) ## 0.019254893952212852\r一方、recall_score(再現性、陽=1着のサンプルのうち実際に正解した割合)が低く偽陰性が高いことが確認できます。その結果、F1値も低くなっていますね。競馬予測モデルの場合、偽陰性が高いことは偽陽性が高いことよりはましなのですが、回収率を上げるためには偽陰性を下げることを頑張らなければいけません。これは今後の課題ですね。次節ではshapley値を使って要因分解をしたいと思います。。\n3. shapでの結果解釈 import shap shap.initjs() ## \u0026lt;IPython.core.display.HTML object\u0026gt;\rexplainer = shap.TreeExplainer(model) ## Setting feature_perturbation = \u0026quot;tree_path_dependent\u0026quot; because no background data was given.\rshap_values = explainer.shap_values(X_test) ## LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\rまず、各特徴量の重要度を見ることにします。summary_plotメソッドを使用します。\nshap.summary_plot(shap_values, X_test) 横軸は各特徴量の平均的な重要度を表しています(shap値の絶対値)。preprize(前走までの賞金獲得金額)やhorse_age、preorder(前走の着順)などが予測に重要であることが分かります。特にpreprizeの重要度は1着の予測、1着以外の予測どちらに対しても大きいです。horse_ageも同様です。ただ、これでは重要というだけで定性的な評価はできません。例えば、preprizeが大きい→1位になる確率が上昇といった関係が確認できれば、それは重要な情報になり得ます。次にそれを確認します。summary_plotメソッドを使用します。\nshap.summary_plot(shap_values[1], X_test) 上図も各特徴量の重要度を表しています(今回は絶対値ではありません)。今回はそれぞれの特徴量の重要度がバイオリンプロットによって表されており、かつ特徴量の値の大きさで色分けがされています。例えば、preprizeだと横軸が0以上の部分でのみ赤色の分布が発生しており、ここからpreprizeの特徴量が大きい、つまり前走までの獲得賞金額が多いと平均的に1着の確率が上がるという当たり前の解釈をすることができます。 他にも、horse_age,preorder,last_3Fは特徴量が小さくなるほど1着になる確率があがることも読み取れます。horse_weight, jokey_weightは大きくなるほど1着になる確率が上がるようです。一方、その他は特に定性的な関係を読み取ることはできません。\n次に、特徴量と確率の関係をより詳しく確認してみましょう。先ほど、preprizeは特徴量が大きくなるほど1着になる確率が上昇するということがわかりました。ただ、その確率の上昇は1次関数的に増加するのか、指数的に増大するのか、それとも$\\log x$のように逓減していくのか、わかりません。dependence_plotを使用してそれを確認してみましょう。\nshap.dependence_plot(ind=\u0026#34;preprize\u0026#34;, shap_values=shap_values[1], features=X_test) 上図は学習したLightGBMをpreprizeの関数として見たときの概形をplotしたものです。先に確認したとおり、やはり特徴量が大きくなるにつれ、1着になる確率が上昇していきます。ただ、その上昇は徐々に逓減していき、2000万円を超えるところでほぼ頭打ちとなります。また、上図ではhorse_ageでの色分けを行っており、preprizeとの関係性も確認できるようになっています。やはり、直感と同じく、preprizeが高い馬の中でもhorse_ageが若い馬の1着確率が高くなることが見て取れます。\npreorderのdependence_plotも確認してみましょう。\nshap.dependence_plot(ind=\u0026#34;preorder\u0026#34;, shap_values=shap_values[1], features=X_test) やはり、前走の着順が上位になるほど1着確率が高まることがここからも分かります。また、その確率は6着以上とそれ以外で水準感が変わることも分かります。last_3Fのタイムとの関係性も確認していますが、こちらはあまり関連性はなさそうです。\n4. 最後に LightGBMを使用し、競馬の予測モデルを作成してみました。さすがLightGBMといった感じで、予測精度は高かったです。また、shap値を使用した重要特徴量の検出も上手くいきました。これによって、LightGBMの気持ちを理解し、より良い特徴量の発見を進めていくことでモデリングの精度を高めていこうと思います。\n","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1582934400,"objectID":"ab5059d2582cbfad9b39f58a8285b701","permalink":"/post/post16/","publishdate":"2020-02-29T00:00:00Z","relpermalink":"/post/post16/","section":"post","summary":"Kagglerが大好きなLightGBMで競馬の順位予想してみました。","tags":["Python","前処理","機械学習"],"title":"LightGBMを使用して競馬結果を予想してみる","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"おはこんばんにちは。とある理由で10年物長期金利のフィッティングを行いたいと思いました。というわけで、USデータを用いて解析していきます。まず、データを収集しましょう。quantmodパッケージを用いて、FREDからデータを落とします。getsymbols(キー,from=開始日,src=\u0026quot;FRED\u0026quot;, auto.assign=TRUE)で簡単にできちゃいます。ちなみにキーはFREDのHPで確認できます。\n1. データ収集 library(quantmod) # data name collected symbols.name \u0026lt;- c(\u0026#34;10-Year Treasury Constant Maturity Rate\u0026#34;,\u0026#34;Effective Federal Funds Rate\u0026#34;,\u0026#34; Consumer Price Index for All Urban Consumers: All Items\u0026#34;,\u0026#34;Civilian Unemployment Rate\u0026#34;,\u0026#34;3-Month Treasury Bill: Secondary Market Rate\u0026#34;,\u0026#34;Industrial Production Index\u0026#34;,\u0026#34; 10-Year Breakeven Inflation Rate\u0026#34;,\u0026#34;Trade Weighted U.S. Dollar Index: Broad, Goods\u0026#34;,\u0026#34; Smoothed U.S. Recession Probabilities\u0026#34;,\u0026#34;Moody\u0026#39;s Seasoned Baa Corporate Bond Yield\u0026#34;,\u0026#34;5-Year, 5-Year Forward Inflation Expectation Rate\u0026#34;,\u0026#34;Personal Consumption Expenditures\u0026#34;) # Collect economic data symbols \u0026lt;- c(\u0026#34;GS10\u0026#34;,\u0026#34;FEDFUNDS\u0026#34;,\u0026#34;CPIAUCSL\u0026#34;,\u0026#34;UNRATE\u0026#34;,\u0026#34;TB3MS\u0026#34;,\u0026#34;INDPRO\u0026#34;,\u0026#34;T10YIEM\u0026#34;,\u0026#34;TWEXBMTH\u0026#34;,\u0026#34;RECPROUSM156N\u0026#34;,\u0026#34;BAA\u0026#34;,\u0026#34;T5YIFRM\u0026#34;,\u0026#34;PCE\u0026#34;) getSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, src = \u0026#34;FRED\u0026#34;, auto.assign = TRUE) ## [1] \u0026quot;GS10\u0026quot; \u0026quot;FEDFUNDS\u0026quot; \u0026quot;CPIAUCSL\u0026quot; \u0026quot;UNRATE\u0026quot; ## [5] \u0026quot;TB3MS\u0026quot; \u0026quot;INDPRO\u0026quot; \u0026quot;T10YIEM\u0026quot; \u0026quot;TWEXBMTH\u0026quot; ## [9] \u0026quot;RECPROUSM156N\u0026quot; \u0026quot;BAA\u0026quot; \u0026quot;T5YIFRM\u0026quot; \u0026quot;PCE\u0026quot;\rmacro_indicator \u0026lt;- merge(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE) rm(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE,USEPUINDXD) 2. 月次解析パート データは こちら から参照できます。では、推計用のデータセットを作成していきます。被説明変数は10-Year Treasury Constant Maturity Rate(GS10)です。説明変数は以下の通りです。\n   説明変数名 キー 代理変数     Federal Funds Rate FEDFUNDS 短期金利   Consumer Price Index CPIAUCSL 物価   Unemployment Rate UNRATE 雇用関連   3-Month Treasury Bill TB3MS 短期金利   Industrial Production Index INDPRO 景気   Breakeven Inflation Rate T10YIEM 物価   Trade Weighted Dollar Index TWEXBMTH 為替   Recession Probabilities RECPROUSM156N 景気   Moody\u0026rsquo;s Seasoned Baa Corporate Bond Yield BAA リスクプレミアム   Inflation Expectation Rate T5YIFRM 物価   Personal Consumption Expenditures PCE 景気   Economic Policy Uncertainty Index USEPUINDXD 政治    かなり適当な変数選択ではあるんですが、マクロモデリング的に長期金利ってどうやってモデル化するかというとちゃんとやってない場合が多いです。DSGEでは効率市場仮説に従って10年先までの短期金利のパスをリンクしたものと長期金利が等しくなると定式化するのが院生時代のモデリングでした（マクロファイナンスの界隈ではちゃんとやってそう）。そういうわけで、短期金利を説明変数に加えています。そして、短期金利に影響を与えるであろう物価にかかる指標も3つ追加しました。加えて、景気との相関が強いことはよく知られているので景気に関するデータも追加しました。これらはそもそもマクロモデルでは短期金利は以下のようなテイラールールに従うとモデリングすることが一般的であることが背景にあります。\n$$ r_t = \\rho r_{t-1} + \\alpha \\pi_{t} + \\beta y_{t} $$ ここで、$r_t$は政策金利（短期金利）、$\\pi_t$はインフレ率、$y_t$はoutputです。$\\rho, \\alpha, \\beta$はdeep parameterと呼ばれるもので、それぞれ慣性、インフレ率への金利の感応度、outputに対する感応度を表しています。$\\rho=0,\\beta=0$の時、$\\alpha\u0026gt;=1$でなければ合理的期待均衡解が得られないことは「テイラーの原理」として有名です。 その他、Corporate bondとの裁定関係も存在しそうなMoody's Seasoned Baa Corporate Bond Yieldも説明変数に追加しています。また、欲を言えばVIX指数と財政に関する指標を追加したいところです。財政に関する指数はQuateryかAnnualyなので今回のようなmonthlyの推計には使用することができません。この部分は最もネックなところです。なにか考え付いたら再推計します。\nでは、推計に入ります。今回は説明変数が多いのでlasso回帰を行い、有効な変数を絞り込みたいと思います。また、比較のためにOLSもやります。説明変数は被説明変数の1期前の値を使用します。おそらく、1期前でもデータの公表時期によっては翌月の推計に間に合わない可能性もありますが、とりあえずこれでやってみます。\n# make dataset traindata \u0026lt;- na.omit(merge(macro_indicator[\u0026#34;2003-01-01::2015-12-31\u0026#34;][,1],stats::lag(macro_indicator[\u0026#34;2003-01-01::2015-12-31\u0026#34;][,-1],1))) testdata \u0026lt;- na.omit(merge(macro_indicator[\u0026#34;2016-01-01::\u0026#34;][,1],stats::lag(macro_indicator[\u0026#34;2016-01-01::\u0026#34;][,-1],1))) # fitting OLS trial1 \u0026lt;- lm(GS10~.,data = traindata) summary(trial1) ## ## Call:\r## lm(formula = GS10 ~ ., data = traindata)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.7593 -0.2182 0.0041 0.2143 0.7051 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 15.0576773 4.3419245 3.468 0.000693 ***\r## FEDFUNDS -0.2075752 0.1413832 -1.468 0.144253 ## CPIAUCSL -0.0750111 0.0204871 -3.661 0.000352 ***\r## UNRATE -0.2183796 0.0784608 -2.783 0.006109 ** ## TB3MS 0.3031085 0.1393904 2.175 0.031310 * ## INDPRO -0.0705997 0.0263855 -2.676 0.008328 ** ## T10YIEM 1.1476564 0.1758964 6.525 1.10e-09 ***\r## TWEXBMTH -0.0313911 0.0117338 -2.675 0.008338 ** ## RECPROUSM156N -0.0103854 0.0021233 -4.891 2.66e-06 ***\r## BAA 0.7802368 0.0858538 9.088 7.60e-16 ***\r## T5YIFRM -0.4529529 0.1881510 -2.407 0.017342 * ## PCE 0.0009815 0.0002477 3.963 0.000116 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.2953 on 143 degrees of freedom\r## Multiple R-squared: 0.9218,\tAdjusted R-squared: 0.9158 ## F-statistic: 153.2 on 11 and 143 DF, p-value: \u0026lt; 2.2e-16\r自由度修正済み決定係数高めですね。2015/12/31までのモデルを使って、アウトサンプルのデータ(2016/01/01~)を予測し、平均二乗誤差を計算します。\nest.OLS.Y \u0026lt;- predict(trial1,testdata[,-1]) Y \u0026lt;- as.matrix(testdata[,1]) mse.OLS \u0026lt;- sum((Y - est.OLS.Y)^2) / length(Y) mse.OLS ## [1] 0.2422673\r次にlasso回帰です。Cross Validationを行い、$\\lambda$を決めるglmnetパッケージのcv.glmnet関数を使用します。\n# fitting lasso regression library(glmnet) trial2 \u0026lt;- cv.glmnet(as.matrix(traindata[,-1]),as.matrix(traindata[,1]),family=\u0026#34;gaussian\u0026#34;,alpha=1) plot(trial2) trial2$lambda.min ## [1] 0.001463052\rcoef(trial2,s=trial2$lambda.min) ## 12 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## s1\r## (Intercept) 8.1170282595\r## FEDFUNDS . ## CPIAUCSL -0.0370371311\r## UNRATE -0.1675017594\r## TB3MS 0.1244417351\r## INDPRO -0.0554153857\r## T10YIEM 1.1916137480\r## TWEXBMTH -0.0124760531\r## RECPROUSM156N -0.0094119322\r## BAA 0.7423502867\r## T5YIFRM -0.4664569086\r## PCE 0.0004936828\rUnemployment Rate、3-Month Treasury Bill、Breakeven Inflation Rate、Moody's Seasoned Baa Corporate Bond Yield、Inflation Expectation Rateの回帰係数が大きくなるという結果ですね。失業率以外は想定内の結果です。ただ、今回の結果を見る限り景気との相関は低そうです（逆向きにしか効かない？）。MSEを計算します。\nest.lasso.Y \u0026lt;- predict(trial2, newx = as.matrix(testdata[,-1]), s = trial2$lambda.min, type = \u0026#39;response\u0026#39;) mse.lasso \u0026lt;- sum((Y - est.lasso.Y)^2) / length(Y) mse.lasso ## [1] 0.1590115\rlasso回帰のほうが良い結果になりました。lasso回帰で計算した予測値と実績値を時系列プロットしてみます。\nlibrary(tidyverse) ggplot(gather(data.frame(actual=Y[,1],lasso_prediction=est.lasso.Y[,1],OLS_prediction=est.OLS.Y,date=as.POSIXct(rownames(Y))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) + geom_line() + scale_x_datetime(breaks = \u0026#34;6 month\u0026#34;,date_labels = \u0026#34;%Y-%m\u0026#34;) + scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,3.5)) 方向感はいい感じです。一方で、2016年1月からや2018年12月以降の急激な金利低下は予測できていません。この部分については何か変数を考えるorローリング推計を実施する、のいずれかをやってみないと精度が上がらなそうです。\n3. 日次解析パート 月次での解析に加えて日次での解析もやりたいとおもいます。日次データであればデータの公表は市場が閉まり次第の場合が多いので、いわゆるjagged edgeの問題が起こりにくいと思います。まずは日次データの収集から始めます。\n# data name collected symbols.name \u0026lt;- c(\u0026#34;10-Year Treasury Constant Maturity Rate\u0026#34;,\u0026#34;Effective Federal Funds Rate\u0026#34;,\u0026#34;NASDAQ Composite Index\u0026#34;,\u0026#34;3-Month Treasury Bill: Secondary Market Rate\u0026#34;,\u0026#34;Economic Policy Uncertainty Index for United States\u0026#34;,\u0026#34; 10-Year Breakeven Inflation Rate\u0026#34;,\u0026#34;Trade Weighted U.S. Dollar Index: Broad, Goods\u0026#34;,\u0026#34;Moody\u0026#39;s Seasoned Baa Corporate Bond Yield\u0026#34;,\u0026#34;5-Year, 5-Year Forward Inflation Expectation Rate\u0026#34;) # Collect economic data symbols \u0026lt;- c(\u0026#34;DGS10\u0026#34;,\u0026#34;DFF\u0026#34;,\u0026#34;NASDAQCOM\u0026#34;,\u0026#34;DTB3\u0026#34;,\u0026#34;USEPUINDXD\u0026#34;,\u0026#34;T10YIE\u0026#34;,\u0026#34;DTWEXB\u0026#34;,\u0026#34;DBAA\u0026#34;,\u0026#34;T5YIFR\u0026#34;) getSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, src = \u0026#34;FRED\u0026#34;, auto.assign = TRUE) ## [1] \u0026quot;DGS10\u0026quot; \u0026quot;DFF\u0026quot; \u0026quot;NASDAQCOM\u0026quot; \u0026quot;DTB3\u0026quot; \u0026quot;USEPUINDXD\u0026quot;\r## [6] \u0026quot;T10YIE\u0026quot; \u0026quot;DTWEXB\u0026quot; \u0026quot;DBAA\u0026quot; \u0026quot;T5YIFR\u0026quot;\rNASDAQCOM.r \u0026lt;- ROC(na.omit(NASDAQCOM)) macro_indicator.d \u0026lt;- merge(DGS10,DFF,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR) rm(DGS10,DFF,NASDAQCOM,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR) 次にデータセットを構築します。学習用と訓練用にデータを分けます。実際の予測プロセスを考え、2営業日前のデータを説明変数に用いています。\n# make dataset traindata.d \u0026lt;- na.omit(merge(macro_indicator.d[\u0026#34;1980-01-01::2010-12-31\u0026#34;][,1],stats::lag(macro_indicator.d[\u0026#34;1980-01-01::2010-12-31\u0026#34;][,-1],2))) testdata.d \u0026lt;- na.omit(merge(macro_indicator.d[\u0026#34;2010-01-01::\u0026#34;][,1],stats::lag(macro_indicator.d[\u0026#34;2010-01-01::\u0026#34;][,-1],2))) # fitting OLS trial1.d \u0026lt;- lm(DGS10~.,data = traindata.d) summary(trial1.d) ## ## Call:\r## lm(formula = DGS10 ~ ., data = traindata.d)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.81961 -0.12380 0.00509 0.14514 0.72712 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -3.5168438 0.1664198 -21.132 \u0026lt; 2e-16 ***\r## DFF 0.0770736 0.0217208 3.548 0.000403 ***\r## NASDAQCOM -0.4301865 0.4280568 -1.005 0.315121 ## DTB3 0.1137128 0.0238678 4.764 2.14e-06 ***\r## USEPUINDXD -0.0006591 0.0001073 -6.144 1.11e-09 ***\r## T10YIE 0.6957403 0.0351666 19.784 \u0026lt; 2e-16 ***\r## DTWEXB 0.0276208 0.0010896 25.350 \u0026lt; 2e-16 ***\r## DBAA 0.2879946 0.0131335 21.928 \u0026lt; 2e-16 ***\r## T5YIFR 0.3433321 0.0376058 9.130 \u0026lt; 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.2167 on 1148 degrees of freedom\r## Multiple R-squared: 0.8902,\tAdjusted R-squared: 0.8894 ## F-statistic: 1163 on 8 and 1148 DF, p-value: \u0026lt; 2.2e-16\r依然決定係数は高めです。\nest.OLS.Y.d \u0026lt;- predict(trial1.d,testdata.d[,-1]) Y.d \u0026lt;- as.matrix(testdata.d[,1]) mse.OLS.d \u0026lt;- sum((Y.d - est.OLS.Y.d)^2) / length(Y.d) mse.OLS.d ## [1] 0.8350614\r次にlasso回帰です。CVで$\\lambda$を決定。\n# fitting lasso regression trial2.d \u0026lt;- cv.glmnet(as.matrix(traindata.d[,-1]),as.matrix(traindata.d[,1]),family=\u0026#34;gaussian\u0026#34;,alpha=1) plot(trial2.d) trial2.d$lambda.min ## [1] 0.001339007\rcoef(trial2.d,s=trial2.d$lambda.min) ## 9 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## s1\r## (Intercept) -3.4297225594\r## DFF 0.0707532975\r## NASDAQCOM -0.3512778327\r## DTB3 0.1204390843\r## USEPUINDXD -0.0006453775\r## T10YIE 0.6894098221\r## DTWEXB 0.0272773903\r## DBAA 0.2831306290\r## T5YIFR 0.3411269206\rliborの係数値が0になりました。MSEはOLSの方が高い結果に。\nest.lasso.Y.d \u0026lt;- predict(trial2.d, newx = as.matrix(testdata.d[,-1]), s = trial2.d$lambda.min, type = \u0026#39;response\u0026#39;) mse.lasso.d \u0026lt;- sum((Y.d - est.lasso.Y.d)^2) / length(Y.d) mse.lasso.d ## [1] 0.8492769\r予測値をプロットします。\nggplot(gather(data.frame(actual=Y.d[,1],lasso_prediction=est.lasso.Y.d[,1],OLS_prediction=est.OLS.Y.d,date=as.POSIXct(rownames(Y.d))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) + geom_line() + scale_x_datetime(breaks = \u0026#34;2 year\u0026#34;,date_labels = \u0026#34;%Y-%m\u0026#34;) + scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,5)) 月次と同じく、OLSとlassoで予測値に差はほとんどありません。なかなかいい感じに変動を捉えることができていますが、2011年のUnited States federal government credit-rating downgradesによる金利低下や2013年の景気回復に伴う金利上昇は捉えることができていません。日次の景気指標はPOSデータくらいしかないんですが、強いて言うなら最近使用した夜間光の衛星画像データなんかは使えるかもしれません。時間があればやってみます。とりあえず、いったんこれでこの記事は終わりたいと思います。ここまで読み進めていただき、ありがとうございました。\n","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1563235200,"objectID":"8bd91b4fb1550ed802c88f7dcf9db6b7","permalink":"/post/post14/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/post/post14/","section":"post","summary":"単なる思い付きです。","tags":["R","時系列解析","金融"],"title":"10年物長期金利をフィッティングしてみる","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"皆さんおはこんばんにちわ。前回、GPLVMモデルを用いたGDP予測モデルを構築しました。ただ、ナウキャスティングというからにはオルタナティブデータを用いた解析を行いたいところではあります。ふと、以下の記事を見つけました。\n焦点：ナウキャストのＧＤＰ推計、世界初の衛星画像利用　利用拡大も\nこちらは東京大学の渡辺努先生が人工衛星画像を用いてGDP予測モデルを開発したというものです。記事には\n 米国の海洋大気庁が運営する気象衛星「スオミＮＰＰ」が日本上空を通過する毎日午前１時３０分時点の画像を購入し、縦、横７２０メートル四方のマス目ごとの明るさを計測する。同じ明るさでも、農地、商業用地、工業用地など土地の用途によって経済活動の大きさが異なるため、国土地理院の土地利用調査を参照。土地の用途と、明るさが示す経済活動の相関を弾き出し、この結果を考慮した上で、明るさから経済活動の大きさを試算する。 （中略）衛星画像のように誰もが入手可能な公表データであれば、政府、民間の区別なく分析が可能であるため、渡辺氏はこれを「統計の民主化」と呼び、世界的な潮流になると予想している。\n と書かれており、衛星写真を用いた分析に興味を惹かれました。 衛星写真って誰でも利用可能か？というところですが、GoogleがEarth Engineというサービスを提供していることがわかりました。\nhttps://earthengine.google.com/\n （拙訳）Google Earth Engineは、数ペタバイトの衛星画像群と地理空間データセットを惑星規模の解析機能と組み合わせ、科学者、研究者、開発者が変化を検出し、傾向を射影し、地球の変容を定量化することを可能にします。\n 研究・教育・非営利目的ならば、なんと無料で衛星写真データを解析することができます。具体的に何ができるのかは以下の動画を見てください。\n\r今回はそんなEath Engineのpython APIを用いて衛星画像データを取得し、解析していきたいと思います。\n1. Earth Engineを使うための事前準備 Earth Engineを使用するためには、Google Accountを使って申請を行う必要があります。先ほどの画像の右上の「Sign Up」からできます。申請を行って、Gmailに以下のようなメールが来るととりあえずEarth Engineは使用できるようになります。\nとりあえずというのはWEB上のEarth Engine コードエディタは使用できるということです。コードエディタというのは以下のようなもので、ブラウザ上でデータを取得したり、解析をしたり、解析結果をMAPに投影したりすることができる便利ツールです。Earth Engineの本体はむしろこいつで、APIは副次的なものと考えています。\n真ん中のコードエディタにコードを打っていきますが、言語はjavascriptです(APIはpythonとjavascript両方あるんですけどね)。解析結果をMAPに投影したり、reference（左）を参照したり、Consoleに吐き出したデータを確認することができるのでかなり便利です。が、データを落とした後で高度な解析を行いたい場合はpythonを使ったほうが慣れているので今回はAPIを使用しています。 話が脱線しました。さて、Earth Engineの承認を得たら、pipでearthengine-apiをインストールしておきます。そして、コマンドプロンプト上で、earthengine authenticateと打ちます。そうすると、勝手にブラウザが立ち上がり、以下のようにpython apiのauthenticationを行う画面がでますので「次へ」を押下します。\n次に以下のような画面にいきますので、そのまま承認します。これでauthenticationの完成です。pythonからAPIが使えます。\n2. Python APIを用いた衛星画像データの取得 Python APIを使用する準備ができました。ここからは衛星画像データを取得していきます。以下にあるようにEarth Engineにはたくさんのデータセットが存在します。\nhttps://developers.google.com/earth-engine/datasets/\n今回はVIIRS Stray Light Corrected Nighttime Day/Night Band Composites Version 1というデータセットを使用します。このデータセットは世界中の夜間光の光量を月次単位で平均し、提供するものです。サンプル期間は2014-01~現在です。\nEarth Engineにはいくつかの固有なデータ型が存在します。覚えておくべきものは以下の3つです。\nImage… ある１時点におけるrasterデータです。imageオブジェクトはいくつかのbandで構成されています。このbandはデータによって異なりますが、おおよそのデータはbandそれぞれがRGB値を表していたりします。Earth Engineを使用する上で最も基本的なデータです。\nImageCollection… Imageオブジェクトを時系列に並べたオブジェクトです。今回は時系列解析をするのでこのデータを使用します。\nFeatureCollection… GeoJSON Featureです。地理情報を表すGeometryオブジェクトやそのデータのプロパティ（国名等）が格納されています。今回は日本の位置情報を取得する際に使用しています。\nではコーディングしていきます。まず、日本の地理情報のFeatureCollectionオブジェクトを取得します。地理情報はFusion Tablesに格納されていますので、IDで引っ張りCountryがJapanのものを抽出します。ee.FeatureCollection()の引数にIDを入力すれば簡単に取得できます。\nimport ee from dateutil.parser import parse ee.Initialize() # get Japan geometory as FeatureCollection from fusion table japan = ee.FeatureCollection(\u0026#39;ft:1tdSwUL7MVpOauSgRzqVTOwdfy17KDbw-1d9omPw\u0026#39;).filter(ee.Filter.eq(\u0026#39;Country\u0026#39;, \u0026#39;Japan\u0026#39;)) 次に夜間光の衛星画像を取得してみます。こちらもee.ImageCollection()にデータセットのIDを渡すと取得できます。なお、ここではbandを月次の平均光量であるavg_radに抽出しています。\n# get night-light data from earth engine from 2014-01-01 to 2019-01-01 dataset = ee.ImageCollection(\u0026#39;NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\u0026#39;).filter(ee.Filter.date(\u0026#39;2014-01-01\u0026#39;,\u0026#39;2019-01-01\u0026#39;)).select(\u0026#39;avg_rad\u0026#39;) 取得した衛星画像を日本周辺に切り出し、画像ファイルとして出力してみましょう。画像ファイルの出力はimageオブジェクトで可能です（そうでないと画像がたくさん出てきてしまいますからね。。。）。今取得したのはImageCollectionオブジェクトですからImageオブジェクトへ圧縮してやる必要があります（上がImageCollectionオブジェクト、下が圧縮されたImageオブジェクト）。\nここでは、ImageCollectionオブジェクトの中にあるのImageオブジェクトの平均値をとってサンプル期間の平均的な画像を出力してみたいと思います。ImageCollection.mean()でできます。また、.visualize({min:0.5})でピクセル値が0.5以上でフィルターをかけています。こうしないと雲と思われるものやゴミ？みたいなものがついてしまいます。次に、ここまで加工した画像データをダウンロードするurlを.getDownloadURLメソッドで取得しています。その際、regionで切り出す範囲をポリゴン値で指定し、scaleでデータの解像度を指定しています（scaleが小さすぎると処理が重すぎるらしくエラーが出て処理できません）。\ndataset.mean().visualize(min=0.5).getDownloadURL(dict(name=\u0026#39;thumbnail\u0026#39;,region=[[[120.3345348936478, 46.853488838010854],[119.8071911436478, 24.598157870729043],[148.6353161436478, 24.75788466523463],[149.3384411436478, 46.61252884462868]]],scale=5000)) 取得した画像が以下です。\nやはり、東京を中心とした関東圏、大阪を中心とした関西圏、愛知、福岡、北海道（札幌周辺）の光量が多く、経済活動が活発であることがわかります。また、陸内よりも沿岸部で光量が多い地域があることがわかります。これは経済活動とは直接関係しない現象のような気もします。今回は分析対象外ですが、北緯38度を境に北側が真っ暗になるのが印象的です。これは言うまでもなく北朝鮮と韓国の境界線ですから、両国の経済活動水準の差が視覚的にコントラストされているのでしょう。今回使用したデータセットは2014年からのものですが、他のデータセットでは1990年代からのデータが取得できるものもあります（その代わり最近のデータは取れませんが）。それらを用いて朝鮮半島や中国の経済発展を観察するのも面白いかもしれません。\nさて、画像は取得できましたがこのままでは解析ができません。ここからは夜間光をピクセル値にマッピングしたデータを取得し、数値的な解析を試みます。ただ、先ほどとはデータ取得の手続きが少し変わります。というのも、今度は日本各地で各ピクセル単位ごとにさまざまな値をとる夜間光を集約し、1つの代用値にしなければならないからです。ピクセルごとの数値を手に入れたところで解析するには手に余ってしまいますからね。イメージは以下のような感じです（Earth Engineサイトから引用）。\n先ほど取得した夜間光のImageCollectionのある1時点の衛星画像が左です。その中に日本というRegionが存在し、それをee.Reducerによって定量的に集約（aggregate）します。Earth Engine APIには.reduceRegions()メソッドが用意されていますのでそれを用いればいいです。引数は、reducer=集約方法（ここでは合計値）、collection=集約をかけるregion（FeatureCollectionオブジェクト）、scale=解像度、です。以下では、ImageCollection（dataset）の中にある1番目のImageオブジェクトに.reduceRegions()メソッドをかけています。\n# initialize output box time0 = dataset.first().get(\u0026#39;system:time_start\u0026#39;); first = dataset.first().reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(\u0026#39;time_start\u0026#39;, time0) 我々は時系列データが欲しいわけですから、ImageCollection内にあるImageそれぞれに対して同じ処理を行う必要があります。Earth Engineにはiterateという便利な関数があり、引数に処理したい関数を渡せばfor文いらずでこの処理を行ってくれます。ここではImageオブジェクトにreduceRegionsメソッドを処理したComputed Objectを以前に処理したものとmergeするmyfuncという関数を定義し、それをiterateに渡しています。最後に、先ほどと同じく生成したデータをgetDownloadURLメソッドを用いてurlを取得しています（ファイル形式はcsv）。\n# define reduceRegions function for iteration def myfunc(image,first): added = image.reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(\u0026#39;time_start\u0026#39;, image.get(\u0026#39;system:time_start\u0026#39;)) return ee.FeatureCollection(first).merge(added) # implement iteration nightjp = dataset.filter(ee.Filter.date(\u0026#39;2014-02-01\u0026#39;,\u0026#39;2019-01-01\u0026#39;)).iterate(myfunc,first) # get url to download ee.FeatureCollection(nightjp).getDownloadURL(filetype=\u0026#39;csv\u0026#39;,selectors=ee.FeatureCollection(nightjp).first().propertyNames().getInfo()) CSVファイルのurlが取得できました。この時系列をプロットして今日は終わりにしたいと思います。 データを読み込むとこんな感じです。\nimport pandas as pd import matplotlib.pyplot as plt plt.style.use(\u0026#39;ggplot\u0026#39;) nightjp_csv.head() ## system:index sum Country Unnamed: 3 Unnamed: 4\r## 0 2014/1/1 881512.4572 Japan NaN NaN\r## 1 2014/2/1 827345.3551 Japan NaN NaN\r## 2 2014/3/1 729110.4619 Japan NaN NaN\r## 3 2014/4/1 612665.8866 Japan NaN NaN\r## 4 2014/5/1 661434.5027 Japan NaN NaN\rplt.plot(pd.to_datetime(nightjp_csv[\u0026#39;system:index\u0026#39;]),nightjp_csv[\u0026#39;sum\u0026#39;]) かなり季節性がありますね。冬場は日照時間が少ないこともあって光量が増えているみたいです。それにしても急激な増え方ですが。次回はこのデータと景況感の代理変数となる経済統計を元に統計解析を行いたいと思います。おたのしみに。\n","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1563235200,"objectID":"933532735edaadd769f2e019e917078f","permalink":"/post/post12/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/post/post12/","section":"post","summary":"オルタナティブデータを用いた解析やってみました。","tags":["Python","前処理","Earth Engine"],"title":"Google Earth Engine APIで衛星画像データを取得し、景況感をナウキャスティングしてみる","type":"post"},{"authors":null,"categories":["競馬"],"content":"2回目になりますが、またrvestで過去のレース結果を落としてみたいと思います。過去の記事を見てないという人は先にそちらをご覧になられることをお勧めします。\n今回データを取り直そうと思ったのは、競馬の分析をした際により多くの項目を説明変数に加えて、分析をしたいと思ったからです。なので、今回は前回のRスクリプトに追記を行う形でプログラムを作成しました。新たに追加したデータ項目は以下の14個です。\n 芝かダートか 右回りか左回りか レースコンディション（良や稍重など） 天候 馬の毛色（栗毛、鹿毛など） 馬主 生産者 産地 生年月日 父馬 母馬 そのレースまでの獲得賞金（2003年から入手可能） ジョッキーの体重 ジョッキーの体重の増減  実はまだデータ収集は終わっていなくて、Rのプログラムがずっと実行中になっています（3日くらい回しています）。しかし、プログラム自体はきっちり回っているのでスクリプトの紹介をしていこうと思います。もしかしたら追記で結果を書くかもしれません。\n1. スクリプトの中身 まずはパッケージの呼び出しです。\n# rvestによる競馬データのwebスクレイピング #install.packages(\u0026#34;rvest\u0026#34;) #if (!require(\u0026#34;pacman\u0026#34;)) install.packages(\u0026#34;pacman\u0026#34;) #install.packages(\u0026#34;beepr\u0026#34;) #install.packages(\u0026#34;RSQLite\u0026#34;) pacman::p_load(qdapRegex) library(rvest) library(stringr) library(dplyr) library(beepr) library(RSQLite) かなりwarnningが出るのでそれを禁止し、SQLiteに接続しています\n# warnning禁止 options(warn=-1) # SQLiteへの接続 con = dbConnect(SQLite(), \u0026#34;horse_data.db\u0026#34;, synchronous=\u0026#34;off\u0026#34;) 1994年からしかオッズが取れないので、1994年から直近までのデータを取得します。yahoo競馬では月ごとにレースがまとめられているので、それを変数として使用しながらデータをとっていきます。基本的には、該当年、該当月のレース結果一覧へアクセスし、そのページ上の各日の個々の競馬場ごとのタイムテーブルへのリンクを取得します。個々の競馬場でレースはだいたい12ほどあるので、そのリンクを取得し、各レースのレース結果ページにアクセスします。そして、レース結果を取得していきます。まず、各日の個々の競馬場ごとのタイムテーブルへのリンクの取得方法です。\nfor(year in 1994:2019){ start.time \u0026lt;- Sys.time() # 計算時間を図る # yahoo競馬のレース結果一覧ページの取得 for (k in 1:12){ # kは月を表す tryCatch( { keiba.yahoo \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp/schedule/list/\u0026#34;, year,\u0026#34;/?month=\u0026#34;,k)) # 該当年、該当月のレース結果一覧にアクセス Sys.sleep(2) race_lists \u0026lt;- keiba.yahoo %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) # 全urlを取得 # 競馬場ごとの各日のレースリストを取得 race_lists \u0026lt;- race_lists[str_detect(race_lists, pattern=\u0026#34;race/list/\\\\d+/\u0026#34;)==1] # 「result」が含まれるurlを抽出 } , error = function(e){signal \u0026lt;- 1} ) ここでは、取得したリンクのurlにresultという文字が含まれているものだけを抽出しています。要はそれが各競馬場のレーステーブルへのリンクとなります。ここからは取得した競馬場のレーステーブルのリンクを用いて、そのページにアクセスし、全12レースそれぞれのレース結果が掲載されているページのリンクを取得していきます。\nfor (j in 1:length(race_lists)){ # jは当該年月にあったレーステーブルへのリンクを表す tryCatch( { race_list \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,race_lists[j])) race_url \u0026lt;- race_list %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) # 全urlを取得 # レース結果のurlを取得 race_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026#34;result\u0026#34;)==1] # 「result」が含まれるurlを抽出 } , error = function(e){signal \u0026lt;- 1} ) 各レース結果へのリンクが取得できたので、ここからはいよいよレース結果の取得とその整形パートに入ります。かなり長ったらしく複雑なコードになってしまいました。レース結果は以下のようなテーブル属性に格納されているので、まずそれを単純に引っ張ってきます。\nfor (i in 1:length(race_url)){ # iは当該年月当該競馬場で開催されたレースを表す print(str_c(\u0026#34;現在、\u0026#34;, year, \u0026#34;年\u0026#34;, k, \u0026#34;月\u0026#34;,j, \u0026#34;グループ、\u0026#34;, i,\u0026#34;番目のレースの保存中です\u0026#34;)) tryCatch( { race1 \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,race_url[i])) # レース結果のurlを取得 signal \u0026lt;- 0 Sys.sleep(2) } , error = function(e){signal \u0026lt;- 1} ) # レースが中止orこれまでの過程でエラーでなければ処理を実行 if (identical(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@class = \u0026#39;resultAtt mgnBL fntSS\u0026#39;]\u0026#34;) %\u0026gt;% html_text(),character(0)) == TRUE \u0026amp;\u0026amp; signal == 0){ # レース結果をスクレイピング race_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026#34;) %\u0026gt;% html_table() race_result \u0026lt;- do.call(\u0026#34;data.frame\u0026#34;,race_result) # リストをデータフレームに変更 colnames(race_result) \u0026lt;- c(\u0026#34;order\u0026#34;,\u0026#34;frame_number\u0026#34;,\u0026#34;horse_number\u0026#34;,\u0026#34;horse_name/age\u0026#34;,\u0026#34;time/margin\u0026#34;,\u0026#34;passing_rank/last_3F\u0026#34;,\u0026#34;jockey/weight\u0026#34;,\u0026#34;popularity/odds\u0026#34;,\u0026#34;trainer\u0026#34;) #　列名変更 tableをただ取得しただけでは以下のように\\nが入っていたり、一つのセルに複数の情報が入っていたりと分析には使えないデータとなっています。なので、これを成型する必要が出てきます。\n# 通過順位と上り3Fのタイム race_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026#34;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026#34;\\\\d{2}\\\\.\\\\d\u0026#34;))) race_result \u0026lt;- race_result[-6] # タイムと着差 race_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026#34;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026#34;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026#34;))) race_result$margin[race_result$order==1] \u0026lt;- \u0026#34;トップ\u0026#34; race_result$margin[race_result$margin==\u0026#34;character(0)\u0026#34;] \u0026lt;- \u0026#34;大差\u0026#34; race_result$margin[race_result$order==0] \u0026lt;- NA race_result \u0026lt;- race_result[-5] # 馬名、馬齢、馬体重 race_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;[ァ-ヴー・]+\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026#34;))) race_result$horse_sex \u0026lt;- str_extract(race_result$horse_age, pattern = \u0026#34;牡|牝|せん\u0026#34;) race_result$horse_age \u0026lt;- str_extract(race_result$horse_age, pattern = \u0026#34;\\\\d\u0026#34;) race_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;\\\\d{3}\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026#34;))) race_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026#34;\u0026#34;) race_result \u0026lt;- dplyr::mutate(race_result,brinker=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;B\u0026#34;))) race_result$brinker[race_result$brinker!=\u0026#34;B\u0026#34;] \u0026lt;- \u0026#34;N\u0026#34; race_result \u0026lt;- race_result[-4] # ジョッキー race_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026#34;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,jockey_weight=as.character(str_extract_all(race_result$`jockey/weight`,\u0026#34;\\\\d{2}\u0026#34;))) race_result$jockey_weight_change \u0026lt;- 0 race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026#34;☆\u0026#34;)==1] \u0026lt;- 1 race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026#34;△\u0026#34;)==1] \u0026lt;- 2 race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026#34;△\u0026#34;)==1] \u0026lt;- 3 race_result \u0026lt;- race_result[-4] # オッズと人気 race_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026#34;\\\\(.+\\\\)\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026#34;\\\\d+[^(\\\\d+.\\\\d)]\u0026#34;))) race_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026#34;\u0026#34;) race_result \u0026lt;- race_result[-4] 次に、今取得したtable以外の情報も取り込むことにします。具体的には、レース名や天候、馬場状態、日付、競馬場などです。これらの情報はレース結果ページの上部に掲載されています。\n# レース情報 race_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_distance \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//p[@id = \u0026#39;raceTitMeta\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026#34;\\\\d+年\\\\d+月\\\\d+日\u0026#34;))) race_result$race_date \u0026lt;- str_replace_all(race_result$race_date,\u0026#34;年\u0026#34;,\u0026#34;/\u0026#34;) race_result$race_date \u0026lt;- str_replace_all(race_result$race_date,\u0026#34;月\u0026#34;,\u0026#34;/\u0026#34;) race_result$race_date \u0026lt;- as.Date(race_result$race_date) race_course \u0026lt;- as.character(str_extract_all(race_date,pattern = \u0026#34;札幌|函館|福島|新潟|東京|中山|中京|京都|阪神|小倉\u0026#34;)) race_result$race_course \u0026lt;- race_course race_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026#34;\\\\s\u0026#34;,\u0026#34;\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,race_distance=as.character(str_extract_all(race_distance,\u0026#34;\\\\d+m\u0026#34;))) race_type=as.character(str_extract_all(race_distance,pattern = \u0026#34;芝|ダート\u0026#34;)) race_result$type \u0026lt;- race_type race_turn \u0026lt;- as.character(str_extract_all(race_distance,pattern = \u0026#34;右|左\u0026#34;)) race_result$race_turn \u0026lt;- race_turn if(length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg ryou\u0026#39;]\u0026#34;)) == 1){ race_result$race_condition \u0026lt;- \u0026#34;良\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg yayaomo\u0026#39;]\u0026#34;)) == 1){ race_result$race_condition \u0026lt;- \u0026#34;稍重\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg omo\u0026#39;]\u0026#34;)) == 1){ race_result$race_condition \u0026lt;- \u0026#34;重\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg furyou\u0026#39;]\u0026#34;)) == 1){ race_result$race_condition \u0026lt;- \u0026#34;不良\u0026#34; } else race_result$race_condition \u0026lt;- \u0026#34;NA\u0026#34; if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg hare\u0026#39;]\u0026#34;)) == 1){ race_result$race_weather \u0026lt;- \u0026#34;晴れ\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg ame\u0026#39;]\u0026#34;)) == 1){ race_result$race_weather \u0026lt;- \u0026#34;曇り\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg kumori\u0026#39;]\u0026#34;)) == 1){ race_result$race_weather \u0026lt;- \u0026#34;雨\u0026#34; } else race_result$race_weather \u0026lt;- \u0026#34;その他\u0026#34; 次は各馬の情報です。 実はさきほど取得したtableの馬名はリンクになっており、そのリンクをたどると各馬の情報が取得できます（毛色や生年月日など）。\nhorse_url \u0026lt;- race1 %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) horse_url \u0026lt;- horse_url[str_detect(horse_url, pattern=\u0026#34;directory/horse\u0026#34;)==1] # 馬情報のリンクだけ抽出する for (l in 1:length(horse_url)){ tryCatch( { horse1 \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,horse_url[l])) Sys.sleep(0.5) horse_name \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;dirTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026#34;) %\u0026gt;% html_text() horse \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;dirTitName\u0026#39;]/ul\u0026#34;) %\u0026gt;% html_text() race_result$colour[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;毛色：.+\u0026#34;)) race_result$owner[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;馬主：.+\u0026#34;)) race_result$farm[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;生産者：.+\u0026#34;)) race_result$locality[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;産地：.+\u0026#34;)) race_result$horse_birthday[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;\\\\d+年\\\\d+月\\\\d+日\u0026#34;)) race_result$father[race_result$horse_name==horse_name] \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026#34;//td[@class = \u0026#39;bloodM\u0026#39;][@rowspan = \u0026#39;4\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_result$mother[race_result$horse_name==horse_name] \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026#34;//td[@class = \u0026#39;bloodF\u0026#39;][@rowspan = \u0026#39;4\u0026#39;]\u0026#34;) %\u0026gt;% html_text() } , error = function(e){ race_result$colour[race_result$horse_name==horse_name] \u0026lt;- NA race_result$owner[race_result$horse_name==horse_name] \u0026lt;- NA race_result$farm[race_result$horse_name==horse_name] \u0026lt;- NA race_result$locality[race_result$horse_name==horse_name] \u0026lt;- NA race_result$horse_birthday[race_result$horse_name==horse_name] \u0026lt;- NA race_result$father[race_result$horse_name==horse_name] \u0026lt;- NA race_result$mother[race_result$horse_name==horse_name] \u0026lt;- NA } ) } race_result$colour \u0026lt;- str_replace_all(race_result$colour,\u0026#34;毛色：\u0026#34;,\u0026#34;\u0026#34;) race_result$owner \u0026lt;- str_replace_all(race_result$owner,\u0026#34;馬主：\u0026#34;,\u0026#34;\u0026#34;) race_result$farm \u0026lt;- str_replace_all(race_result$farm,\u0026#34;生産者：\u0026#34;,\u0026#34;\u0026#34;) race_result$locality \u0026lt;- str_replace_all(race_result$locality,\u0026#34;産地：\u0026#34;,\u0026#34;\u0026#34;) #race_result$horse_birthday \u0026lt;- str_replace_all(race_result$horse_birthday,\u0026#34;年\u0026#34;,\u0026#34;/\u0026#34;) #race_result$horse_birthday \u0026lt;- str_replace_all(race_result$horse_birthday,\u0026#34;月\u0026#34;,\u0026#34;/\u0026#34;) #race_result$horse_birthday \u0026lt;- as.Date(race_result$horse_birthday) race_result \u0026lt;- dplyr::arrange(race_result,horse_number) # 馬番順に並べる 次にそのレースまでに獲得した賞金額を落としに行きます。これはレース結果のページの出馬表と書かれたリンクをたどるとアクセスできます。ここに賞金があるのでそれを取得します。\nyosou_url \u0026lt;- race1 %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) yosou_url \u0026lt;- yosou_url[str_detect(yosou_url, pattern=\u0026#34;denma\u0026#34;)==1] if (length(yosou_url)==1){ yosou1 \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,yosou_url)) Sys.sleep(2) yosou \u0026lt;- yosou1 %\u0026gt;% html_nodes(xpath = \u0026#34;//td[@class = \u0026#39;txC\u0026#39;]\u0026#34;) %\u0026gt;% as.character() prize \u0026lt;- yosou[grepl(\u0026#34;万\u0026#34;,yosou)==TRUE] %\u0026gt;% str_extract_all(\u0026#34;\\\\d+万\u0026#34;) prize \u0026lt;- t(do.call(\u0026#34;data.frame\u0026#34;,prize)) %\u0026gt;% as.character() race_result$prize \u0026lt;- prize race_result$prize \u0026lt;- str_replace_all(race_result$prize,\u0026#34;万\u0026#34;,\u0026#34;\u0026#34;) %\u0026gt;% as.numeric() } else race_result$prize \u0026lt;- NA 取得した各レース結果を格納するdatasetというデータフレームを作成し、データを格納していきます。1年ごとにそれをSQLite へ保存していきます。\n## ファイル貯めるのかく if (k == 1 \u0026amp;\u0026amp; i == 1 \u0026amp;\u0026amp; j == 1){ dataset \u0026lt;- race_result } else { dataset \u0026lt;- rbind(dataset,race_result) } # if文2の終わり }else { print(\u0026#34;保存できませんでした\u0026#34;) }# if文1の終わり } # iループの終わり } # jループ終わり } # kループの終わり beep(3) write.csv(dataset,\u0026#34;race_result2.csv\u0026#34;, row.names = FALSE) if (year == 1994){ dbWriteTable(con, \u0026#34;race_result\u0026#34;, dataset) } else { dbWriteTable(con, \u0026#34;temp\u0026#34;, dataset) dbSendQuery(con, \u0026#34;INSERT INTO race_result select * from temp\u0026#34;) dbSendQuery(con, \u0026#34;DROP TABLE temp\u0026#34;) } # ifの終わり } # yearループの終わり end.time \u0026lt;- Sys.time() print(str_c(\u0026#34;処理時間は\u0026#34;,end.time-start.time,\u0026#34;です。\u0026#34;)) beep(5) options(warn = 1) dbDisconnect(con) 以上です。取れたデータは以下のようになりました。\nhead(race_result) ## order frame_number horse_number trainer passing_rank last_3F time\r## 1 10 1 1 田中 剛 09-09 39.0 1.14.3\r## 2 16 1 2 天間 昭一 11-11 40.3 1.15.7\r## 3 15 2 3 田中 清隆 14-14 39.4 1.15.1\r## 4 9 2 4 中舘 英二 08-08 39.1 1.14.3\r## 5 12 3 5 根本 康広 11-11 39.0 1.14.4\r## 6 4 3 6 杉浦 宏昭 04-04 38.4 1.13.2\r## margin horse_name horse_age horse_sex horse_weight\r## 1 アタマ サトノジョニー 3 牡 512\r## 2 3 1/2馬身 ツギノイッテ 3 牡 464\r## 3 3馬身 ギュウホ 3 牡 444\r## 4 2 1/2馬身 セイウンメラビリア 3 牝 466\r## 5 クビ サバイバルトリック 3 牝 450\r## 6 アタマ ステイホット 3 牝 474\r## horse_weight_change brinker jockey jockey_weight jockey_weight_change\r## 1 +30 N 松岡 正海 56 0\r## 2 +8 N 西田 雄一郎 56 0\r## 3 +8 N 杉原 誠人 56 0\r## 4 +10 N 村田 一誠 54 0\r## 5 -2 N 野中 悠太郎 51 0\r## 6 -2 N 大野 拓弥 54 0\r## odds popularity race_date race_course race_name race_distance type\r## 1 40.3 9 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 2 340.9 16 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 3 283.1 14 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 4 299.7 15 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 5 26.7 8 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 6 2.4 1 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## race_turn race_condition race_weather colour owner\r## 1 右 良 晴れ 栗毛 株式会社 サトミホースカンパニー\r## 2 右 良 晴れ 黒鹿毛 西村 新一郎\r## 3 右 良 晴れ 鹿毛 有限会社 ミルファーム\r## 4 右 良 晴れ 青鹿毛 西山 茂行\r## 5 右 良 晴れ 黒鹿毛 福田 光博\r## 6 右 良 晴れ 栗毛 小林 善一\r## farm locality horse_birthday father\r## 1 千代田牧場 新ひだか町 2016年1月29日 オルフェーヴル\r## 2 織笠 時男 青森県 2016年4月17日 スクワートルスクワート\r## 3 神垣 道弘 新ひだか町 2016年4月19日 ジャングルポケット\r## 4 石郷岡 雅樹 新冠町 2016年4月21日 キンシャサノキセキ\r## 5 原田牧場 日高町 2016年4月30日 リーチザクラウン\r## 6 社台ファーム 千歳市 2016年3月13日 キャプテントゥーレ\r## mother prize\r## 1 スパークルジュエル 0\r## 2 エプソムアイリス 0\r## 3 デライトシーン 0\r## 4 ドリームシップ 0\r## 5 フリーダムガール 180\r## 6 ステイアライヴ 455\r","date":1562976000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1562976000,"objectID":"290827b2bff5fc58ceb8d3e3c7aa86ac","permalink":"/post/post11/","publishdate":"2019-07-13T00:00:00Z","relpermalink":"/post/post11/","section":"post","summary":"前回のデータ収集では足りないデータがあったので再度スクレイピングしてみました。","tags":["R","Webスクレイピング","前処理","SQL"],"title":"rvestでyahoo競馬にある過去のレース結果をスクレイピングしてみた（2回目）","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"おはこんばんにちは。 ずいぶん前にGianonne et al (2008)のマルチファクターモデルで四半期GDPの予想を行いました。 結果としては、ある程度は予測精度が出ていたものの彼らの論文ほどは満足のいくものではありませんでした。原因としてはクロスセクショナルなデータ不足が大きいと思われ、現在収集方法についてもEXCELを用いて改修中です。しかし一方で、マルチファクターモデルの改善も考えたいと思っています。前回は月次経済統計を主成分分析（実際にはカルマンフィルタ）を用いて次元削減を行い、主成分得点を説明変数としてGDPに回帰しました。今回はこの主成分分析のド発展版であるGaussian Process Latent Variable Model(GPLVM)を用いてファクターを計算し、それをGDPに回帰したいと思います。\n1. GPLVMとは GPLVMとは、Gaussian Process Modelの一種です。以前、Gaussian Process Regressionの記事を書きました。\n最も基本的なGaussian Process Modelは上の記事のようなモデルで、非説明変数$Y=(y_{1},y_{2},\u0026hellip;,y_{n})$と説明変数$X=(\\textbf{x}_{1},\\textbf{x}_{2},\u0026hellip;,\\textbf{x}_{n})$があり、以下のような関係式で表される際にそのモデルを直接推定することなしに新たな説明変数$X$の入力に対し、非説明変数$Y$の予測値をはじき出すというものでした。\n$$ \\displaystyle y_{i} = \\textbf{w}^{T}\\phi(\\textbf{x}_{i}) $$\n\rここで、$\\textbf{x}_{i}$は$i$番目の説明変数ベクトル、$\\phi(・)$は非線形関数、 $\\textbf{w}^{T}$は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、$\\phi(\\textbf{x}_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)$を想定しています（$x_{1,i}$は$i$番目の入力データ$\\textbf{x}_{i}$の１番目の変数）。詳しくは過去記事を参照してください。\r\r今回やるGPLVMは説明変数ベクトルが観測できない潜在変数（Latent Variable）であるところが特徴です。以下のスライドが非常にわかりやすいですが、GP-LVMは確率的主成分分析（PPCA）の非線形版という位置付けになっています。\n資料\nでは具体的な説明に移ります。GPLVMは主成分分析の発展版ですので、主に次元削減のために行われることを想定しています。つまり、データセットがあったとして、サンプルサイズ$n$よりも変数の次元$p$が大きいような場合を想定しています。\n2. 最もPrimitiveなGP-LVM \r先述したようにGPLVMはPPCAの非線形版です。なので、GPLVMを説明するスタートはPPCAになります。観測可能な$D$次元データセットを$\\{\\textbf{y}_{n}\\}_{n=1}^{N}$とします。そして、潜在変数を$\\textbf{x}_{n}$とおきます。今、データセットと潜在変数の間には以下のような関係があるとします。\r\r\r$$\r\\textbf{y}_{n} = \\textbf{W}\\textbf{x}_{n} + \\epsilon_{n}\r$$\r\r\rここで、$\\textbf{W}$はウェイト行列、$\\epsilon_{n}$はかく乱項で$N(0,\\beta^{-1}\\textbf{I})$に従います（被説明変数が多次元になることに注意）。また、$\\textbf{x}_{n}$は$N(0,\\textbf{I})$に従います。このとき、$\\textbf{y}_{n}$の尤度を$\\textbf{x}_{n}$を周辺化することで表現すると、\r\r\r$$\r\\begin{eqnarray*}\r\\displaystyle p(\\textbf{y}_{n}|\\textbf{W},\\beta) \u0026=\u0026 \\int p(\\textbf{y}_{n}|\\textbf{x}_{n},\\textbf{W},\\beta)N(0,\\textbf{I})d\\textbf{x}_{n} \\\\\r\\displaystyle \u0026=\u0026 \\int N(\\textbf{W}\\textbf{x}_{n},\\beta^{-1}\\textbf{I})N(0,\\textbf{I})d\\textbf{x}_{n} \\\\\r\u0026=\u0026 N(0,\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}) \\\\\r\\displaystyle \u0026=\u0026 \\frac{1}{(2\\pi)^{DN/2}|\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}|^{N/2}}\\exp(\\frac{1}{2}\\textbf{tr}( (\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{YY}^{T}))\r\\end{eqnarray*}\r$$\r\r\rとなります。ここで、$p(\\textbf{y}_{n}|\\textbf{x}_{n},\\textbf{W},\\beta)=N(\\textbf{W}\\textbf{x}_{n},\\beta^{-1}\\textbf{I})$です。平均と分散は以下から求めました。\r\r\r$$\r\\begin{eqnarray*}\rE(\\textbf{y}_{n}|\\textbf{W},\\beta) \u0026=\u0026 E(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n}) \\\\\r\u0026=\u0026 E(\\textbf{W}\\textbf{x}_{n}) + E(\\epsilon_{n}) \\\\\r\u0026=\u0026 \\textbf{W}E(\\textbf{x}_{n}) + E(\\epsilon_{n}) = 0\r\\end{eqnarray*}\r$$\r\r\r$$\r\\begin{eqnarray*}\rE[(\\textbf{y}_{n}|\\textbf{W},\\beta)(\\textbf{y}_{n}|\\textbf{W},\\beta)^{T}] \u0026=\u0026 E[ (\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n} - 0)(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n} - 0)^{T} ] \\\\\r\u0026=\u0026 E[ (\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n})(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n})^{T} ] \\\\\r\u0026=\u0026 E[ \\textbf{W}\\textbf{x}_{n}(\\textbf{W}\\textbf{x}_{n})^{T} + \\textbf{W}\\textbf{x}_{n}\\epsilon_{n}^{T} + \\epsilon_{n}\\textbf{W}\\textbf{x}_{n}^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026=\u0026 E[ \\textbf{W}\\textbf{x}_{n}(\\textbf{W}\\textbf{x}_{n})^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026=\u0026 E[ \\textbf{W}\\textbf{x}_{n}\\textbf{x}_{n}^{T}\\textbf{W}^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026=\u0026 E[ \\textbf{W}\\textbf{x}_{n}\\textbf{x}_{n}^{T}\\textbf{W}^{T}] + E[\\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026=\u0026 \\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}\r\\end{eqnarray*}\r$$\r\r$\\textbf{W}$を求めるためには$\\textbf{y}_{n}$がi.i.d.と仮定し、以下のようなデータセット全体の尤度を最大化すれば良いことになります。\n$$ \\displaystyle p(\\textbf{Y}|\\textbf{W},\\beta) = \\prod_{n=1}^{N}p(\\textbf{y}_{n}|\\textbf{W},\\beta) $$\n\rここで、$\\textbf{Y}$は$N×D$の計画行列です。このように、PPCAでは$\\textbf{x}_{n}$を周辺化し、$\\textbf{W}$を最適化します。逆に、Lawrence(2004)では$\\textbf{W}$を周辺化し、$\\textbf{x}_{n}$します（理由は後述）。$\\textbf{W}$を周辺化するために、$\\textbf{W}$に事前分布を与えましょう。\r\r$$ \\displaystyle p(\\textbf{W}) = \\prod_{i=1}^{D}N(\\textbf{w}_{i}|0,\\alpha^{-1}\\textbf{I}) $$\n\rここで、$\\textbf{w}_{i}$はウェイト行列$\\textbf{W}$の$i$番目の列です。では、$\\textbf{W}$を周辺化して$\\textbf{Y}$の尤度関数を導出してみます。やり方はさっきとほぼ同じなので省略します。\r\r$$ \\displaystyle p(\\textbf{Y}|\\textbf{X},\\beta) = \\frac{1}{(2\\pi)^{DN/2}|K|^{D/2}}\\exp(\\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T})) $$\n\rここで、$\\textbf{K}=\\alpha^2\\textbf{X}\\textbf{X}^{T} + \\beta^{-1}\\textbf{I}$は$p(\\textbf{Y}|\\textbf{X},\\beta)$の分散共分散行列で、$\\textbf{X}=(\\textbf{x}_{1},\\textbf{x}_{2},...,\\textbf{x}_{N})^{T}$は入力ベクトルです。対数尤度は\r\r$$ \\displaystyle L = - \\frac{DN}{2}\\ln{2\\pi} - \\frac{1}{2}\\ln{|\\textbf{K}|} - \\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T}) $$\n周辺化のおかげでウェイト$\\textbf{W}$が消えたのでこれを$X$で微分してみましょう。\n$$ \\displaystyle\\frac{\\partial L}{ \\partial \\textbf{X}} = \\alpha^2 \\textbf{K}^{-1}\\textbf{Y}\\textbf{Y}^{T}\\textbf{K}^{-1}\\textbf{X} - \\alpha^2 D\\textbf{K}^{-1}\\textbf{X} $$\nここから、\n$$ \\displaystyle \\frac{1}{D}\\textbf{Y}\\textbf{Y}^{T}\\textbf{K}^{-1}\\textbf{X} = \\textbf{X} $$\nここで、特異値分解を用いると\n$$ \\textbf{X} = \\textbf{ULV}^{T} $$\n\rとなります。$\\textbf{U} = (\\textbf{u}_{1},\\textbf{u}_{2},...,\\textbf{u}_{q})$は$N×q$直交行列、$\\textbf{L} = diag(l_{1},l_{2},..., l_{q})$は$q×q$の特異値を対角成分に並べた行列、$\\textbf{V}$は$q×q$直交行列です。これを先ほどの式に代入すると、\r\r\r$$\r\\begin{eqnarray*}\r\\textbf{K}^{-1}\\textbf{X} \u0026=\u0026 (\\alpha^2\\textbf{X}\\textbf{X}^{T} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{X} \\\\\r\u0026=\u0026 \\textbf{X}(\\alpha^2\\textbf{X}^{T}\\textbf{X} + \\beta^{-1}\\textbf{I})^{-1} \\\\\r\u0026=\u0026 \\textbf{ULV}^{T}(\\alpha^2\\textbf{VLU}^{T}\\textbf{ULV}^{T} + \\beta^{-1}\\textbf{I})^{-1} \\\\\r\u0026=\u0026 \\textbf{ULV}^{T}\\textbf{V}(\\alpha^2\\textbf{LU}^{T}\\textbf{UL} + \\beta^{-1}\\textbf{I}^{-1})\\textbf{V}^{T} \\\\\r\u0026=\u0026 \\textbf{UL}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{V}^{T}\r\\end{eqnarray*}\r$$\r\rなので、\n\r$$\r\\begin{eqnarray*}\r\\displaystyle \\frac{1}{D}\\textbf{Y}\\textbf{Y}^{T}\\textbf{UL}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1})\\textbf{V}^{T} \u0026=\u0026 \\textbf{ULV}^{T}\\\\\r\\displaystyle \\textbf{Y}\\textbf{Y}^{T}\\textbf{UL} \u0026=\u0026 D\\textbf{U}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{L} \\\\\r\\end{eqnarray*}\r$$\r\rとなります。$l_{j}$が0でなければ、$\\textbf{Y}\\textbf{Y}^{T}\\textbf{u}_{j} = D(\\alpha^2 l_{j}^{2} + \\beta^{-1})\\textbf{u}_{j}$となり、$\\textbf{U}$のそれぞれの列は$\\textbf{Y}\\textbf{Y}^{T}$の固有ベクトルであり、対応する固有値$\\lambda_{j}$は$D(\\alpha^2 l_{j}^{2} + \\beta^{-1})$となります。つまり、未知であった$X=ULV$が実は$\\textbf{Y}\\textbf{Y}^{T}$の固有値問題から求めることが出来るというわけです。$l_{j}$は上式を利用して、\n$$ \\displaystyle l_{j} = (\\frac{\\lambda_{j}}{D\\alpha^2} - \\frac{1}{\\beta\\alpha^2})^{1/2} $$\nと$\\textbf{Y}\\textbf{Y}^{T}$の固有値$\\lambda_{j}$とパラメータから求められることがわかります。よって、$X=ULV$は\n$$ \\textbf{X} = \\textbf{U}_{q}\\textbf{L}\\textbf{V}^{T} $$\nとなります。ここで、$\\textbf{U}_{q}$は$\\textbf{Y}\\textbf{Y}^{T}$の固有ベクトルを$q$個取り出したものです。$\\beta$が限りなく大きければ（=観測誤差が限りなく小さければ）通常のPCAと一致します。\n以上がPPCAです。GPLVMはPPCAで確率モデルとして想定していた以下のモデルを拡張します。\n\r$$\r\\textbf{y}_{n} = \\textbf{W}^{T}\\textbf{x}_{n} + \\epsilon_{n}\r$$\r\r具体的には、通常のガウス過程と同様、\n\r$$\r\\displaystyle \\textbf{y}_{n} = \\textbf{W}^{T}\\phi(\\textbf{x}_{n})+ \\epsilon_{n}\r$$\r\r\rという風に基底関数$\\phi(\\textbf{x}_{n})$をかませて拡張します。$\\phi(・)$は平均$\\textbf{0}$、分散共分散行列$\\textbf{K}_{\\textbf{x}}$のガウス過程と仮定します。分散共分散行列$\\textbf{K}_{\\textbf{x}}$は\r\r$$ \\textbf{K}_{\\textbf{x}} = \\alpha^2\\phi(\\textbf{x})\\phi(\\textbf{x})^T $$\n\rであり、入力ベクトル$\\textbf{X}$を$\\phi(\\textbf{・})$で非線形変換した特徴量$\\phi(\\textbf{x})$が近いほど、出力値$\\textbf{Y}$も近くなりやすいという性質があることになります。GPLVMではこの性質を逆に利用しています。つまり、出力値$Y_i$と$Y_j$が近い→$\\phi(\\textbf{x}_i)$と$\\phi(\\textbf{x}_j)$が近い（内積が大きい）→$\\textbf{K}_{x,ij}$が大きい→観測不可能なデータ$X_{i}$と$X_{j}$は近い値（or同じようなパターン）をとる。\rこの議論からもわかるように、$\\textbf{K}_{\\textbf{x}}$は入力ベクトル$\\textbf{X}$それぞれの距離を表したものになります。分散共分散行列の計算には入力ベクトル$\\textbf{X}$を基底関数$\\phi(\\textbf{・})$で非線形変換した後、内積を求めるといったことをする必要はなく、カーネル関数を計算するのみでOKです。今回は王道中の王道RBFカーネルを使用していますので、これを例説明します。\r\rRBFカーネル（スカラーに対する） $$ \\theta_{1}\\exp(-\\frac{1}{\\theta_{2}}(x-x^T)^2) $$\nこのRBFカーネルは以下の基底関数と対応しています。\n$$ \\phi(x)_h = \\tau\\exp(-\\frac{1}{r}(x-h)^2) $$\n例えば、この基底関数で入力$x$を変換したものを$2H^2+1$個並べた関数を\n\r$$\r\\phi(x) = (\\phi(x)_{-H^2}, ..., \\phi(x)_{0},...,\\phi(x)_{H^2})\r$$\r\r入力$x$の特徴量だとすると$x'$との共分散$K_{x}(x,x')$は内積の和なので\n$$ K_{x}(x,x') = \\sum_{h=-H^2}^{H^2}\\phi_{h}(x)\\phi_{h}(x') $$\nとなります。ここで、$H \\to \\infty$とし、グリッドを極限まで細かくしてみます。\n\r$$\r\\begin{eqnarray*}\rK_{x}(x,x') \u0026=\u0026 \\lim_{H \\to \\infty}\\sum_{h=-H^2}^{H^2}\\phi_{h}(x)\\phi_{h}(x') \\\\\r\u0026\\to\u0026\\int_{-\\infty}^{\\infty}\\tau\\exp(-\\frac{1}{r}(x-h)^2)\\tau\\exp(-\\frac{1}{r}(x'-h)^2)dh \\\\\r\u0026=\u0026 \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{r}\\{(x-h)^2+(x'-h)^2\\})dh \\\\\r\u0026=\u0026 \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{r}\\{2(h-\\frac{x+x'}{2})^2+\\frac{1}{2}(x-x')^2\\})dh \\\\\r\\end{eqnarray*}\r$$\r\rとなります。$h$に関係のない部分を積分の外に出します。\n\r$$\r\\begin{eqnarray*}\r\u0026=\u0026 \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{2}{r}(h-\\frac{x+x'}{2})^2)dh\\exp(-\\frac{1}{2r}(x-x')^2) \\\\\r\\end{eqnarray*}\r$$\r\r残った積分を見ると、正規分布の正規化定数と等しいことがわかります。\n\r$$\r\\begin{eqnarray*}\r\\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{2\\sigma}(h-\\frac{x+x'}{2})^2)dh \u0026=\u0026 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{2}{r}(h-\\frac{x+x'}{2})^2)dh\\\\\r\\sigma \u0026=\u0026 \\frac{r}{4}\r\\end{eqnarray*}\r$$\r\rとなるので、ガウス積分の公式を用いて\n\r$$\r\\begin{eqnarray*}\r\u0026=\u0026 \\tau^2 \\sqrt{\\frac{\\pi r}{2}}\\exp(-\\frac{1}{2r}(x-x')^2)　\\\\\r\u0026=\u0026 \\theta_{1}\\exp(-\\frac{1}{\\theta_{2}}(x-x’)^2)\r\\end{eqnarray*}\r$$\r\rとなり、RBFカーネルと等しくなることがわかります。よって、RBFカーネルで計算した共分散は上述した基底関数で入力$x$を無限次元へ拡張した特徴量ベクトルの内積から計算した共分散と同値になることがわかります。つまり、入力$x$と$x'$のスカラーの計算のみで$K_{x}(x,x')$ができてしまうという夢のような計算効率化が可能になるわけです。無限次元特徴量ベクトルの回帰問題なんて普通計算できませんからね。。。カーネル関数は偉大です。 前の記事にも載せましたが、RBFカーネルで分散共分散行列を計算したガウス過程のサンプルパスは以下通りです（$\\theta_1=1,\\theta_2=0.5$）。\n# Define Kernel function Kernel_Mat \u0026lt;- function(X,sigma,beta){ N \u0026lt;- NROW(X) K \u0026lt;- matrix(0,N,N) for (i in 1:N) { for (k in 1:N) { if(i==k) kdelta = 1 else kdelta = 0 K[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta } } return(K) } N \u0026lt;- 10 # max value of X M \u0026lt;- 1000 # sample size X \u0026lt;- matrix(seq(1,N,length=M),M,1) # create X testK \u0026lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix library(MASS) P \u0026lt;- 6 # num of sample path Y \u0026lt;- matrix(0,M,P) # define Y for(i in 1:P){ Y[,i] \u0026lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y } # Plot matplot(x=X,y=Y,type = \u0026#34;l\u0026#34;,lwd = 2) 非常に滑らかな関数となっていることがわかります。RBFのほかにもカーネル関数は存在します。カーネル関数を変えると基底関数が変わりますから、サンプルパスは大きく変わることになります。\nGPLVMの推定方法に話を進めましょう。PPCAの時と同じく、以下の尤度関数を最大化する観測不能な入力$x$を推定値とします。\n$$ \\displaystyle L = - \\frac{DN}{2}\\ln{2\\pi} - \\frac{1}{2}\\ln{|\\textbf{K}|} - \\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T}) $$\nただ、PPCAとは異なり、その値は解析的に求めることができません。尤度関数の導関数は今や複雑な関数であり、展開することができないからです。よって、共役勾配法を用いて数値的に計算するのが主流なようです（自分は準ニュートン法で実装）。解析的、数値的のどちらにせよ導関数を求めておくことは必要なので、導関数を求めてみます。\n$$ \\frac{\\partial L}{\\partial \\textbf{K}_x} = \\frac{1}{2}(\\textbf{K}_x^{-1}\\textbf{Y}\\textbf{Y}^T\\textbf{K}_x^{-1}-D\\textbf{K}_x^{-1}) $$\nなので、チェーンルールから\n$$ \\frac{\\partial L}{\\partial \\textbf{x}} = \\frac{\\partial L}{\\partial \\textbf{K}_x}\\frac{\\partial \\textbf{K}_x}{\\partial \\textbf{x}} $$\n\rなので、カーネル関数を決め、$\\textbf{x}$に初期値を与えてやれば勾配法によって尤度$L$が最大となる点を探索することができます。今回使用するRBFカーネルで$\\frac{\\partial \\textbf{K}_x}{\\partial x_{nj}}$を計算してみます。\r\r\r$$\r\\begin{eqnarray*}\r\\frac{\\partial \\textbf{K}_x(\\textbf{x}_n,\\textbf{x}_n')}{x_{nj}}\u0026=\u0026\\frac{\\partial\\theta_{1}\\exp(-\\frac{|\\textbf{x}_n-\\textbf{x}_n'|^2}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026=\u0026 \\frac{\\partial\\theta_{1}\\exp(-\\frac{(\\textbf{x}_n-\\textbf{x}_n')^T(\\textbf{x}_n-\\textbf{x}_n')}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026=\u0026 \\frac{\\partial\\theta_{1}\\exp(-\\frac{-(\\textbf{x}_n^T\\textbf{x}_n-2\\textbf{x}_n'^T\\textbf{x}_n+\\textbf{x}_n'^T\\textbf{x}_n')}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026=\u0026 -2\\textbf{K}_x(\\textbf{x}_n\\textbf{x}_n')\\frac{(x_{nj}-x_{n'j})}{\\theta_2}\r\\end{eqnarray*}\r$$\r\r$(j)$番目の潜在変数の$n$番目のサンプルそれぞれに導関数を計算し、それを分散共分散行列と同じ行列に整理したものと$\\frac{\\partial L}{\\partial \\textbf{K}_x}$との要素ごとの積を足し合わせたものが勾配となります。\n3. Rでの実装 GPLVMをRで実装します。使用するデータは以前giannoneの記事で使用したものと同じものです。\nESTIMATE_GPLVM \u0026lt;- function(Y,P,sigma){ # 1. Set initial value Y \u0026lt;- as.matrix(Y) eigenvector \u0026lt;- eigen(cov(Y))$vectors X \u0026lt;- Y%*%eigenvector[,1:P] # initial value N \u0026lt;- NROW(Y) # Sample Size D \u0026lt;- NCOL(Y) # Dimention of dataset X0 \u0026lt;- c(as.vector(X)) sigma \u0026lt;- var(matrix(Y,dim(Y)[1]*dim(Y)[2],1)) # 2. Define log likelihood function loglik \u0026lt;- function(X0,Y,N,P,D,beta,sigma){ X \u0026lt;- matrix(X0,N,P) K \u0026lt;- matrix(0,N,N) scale \u0026lt;- diag(sqrt(3/((apply(X, 2, max) -apply(X, 2, min))^2))) X \u0026lt;- X%*%scale for (i in 1:N) { for (k in 1:N) { if(i==k) kdelta = 1 else kdelta = 0 K[i,k] \u0026lt;- K[k,i] \u0026lt;- sigma*exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1) } } L \u0026lt;- - D*N/2*log(2*pi) - D/2*log(det(K)) - 1/2*sum(diag(ginv(K)%*%Y%*%t(Y))) #loglikelihood return(L) } # 3. Define derivatives of log likelihood function dloglik \u0026lt;- function(X0,P,D,N,Y,beta,sigma){ X \u0026lt;- matrix(X0,N,P) K \u0026lt;- matrix(0,N,N) for (i in 1:N) { for (k in 1:N) { if(i==k) kdelta = 1 else kdelta = 0 K[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1) } } invK \u0026lt;- ginv(K) dLdK \u0026lt;- invK%*%Y%*%t(Y)%*%invK - D*invK dLdx \u0026lt;- matrix(0,N,P) for (j in 1:P){ for(i in 1:N){ dKdx \u0026lt;- matrix(0,N,N) for (k in 1:N){ dKdx[i,k] \u0026lt;- dKdx[k,i] \u0026lt;- -exp(-(t(X[i,]-X[k,])%*%(X[i,]-X[k,]))*0.5)*((X[i,j]-X[k,j])*0.5) } dLdx[i,j] \u0026lt;- sum(dLdK*dKdx) } } return(dLdx) } # 4. Optimization res \u0026lt;- optim(X0, loglik, dloglik, Y = Y, N=N, P=P, D=D, beta = exp(2), sigma = sigma, method = \u0026#34;BFGS\u0026#34;, control = list(fnscale = -1,trace=1000,maxit=10000)) output \u0026lt;- matrix(res$par,N,P) result \u0026lt;- list(output,res,P) names(result) \u0026lt;- c(\u0026#34;output\u0026#34;,\u0026#34;res\u0026#34;,\u0026#34;P\u0026#34;) return(result) } GPLVM_SELECT \u0026lt;- function(Y){ D \u0026lt;- NCOL(Y) library(stringr) for (i in 1:D){ if (i == 1){ result \u0026lt;- ESTIMATE_GPLVM(Y,i) P \u0026lt;- 2 print(str_c(\u0026#34;STEP\u0026#34;, i, \u0026#34; loglikelihood \u0026#34;, as.numeric(result$res$value))) }else{ temp \u0026lt;- ESTIMATE_GPLVM(Y,i) print(str_c(\u0026#34;STEP\u0026#34;, i, \u0026#34; loglikelihood \u0026#34;, as.numeric(temp$res$value))) if (result$res$value \u0026lt; temp$res$value){ result \u0026lt;- temp P \u0026lt;- i } } } print(str_c(\u0026#34;The optimal number of X is \u0026#34;, P)) print(str_c(\u0026#34;loglikelihood \u0026#34;, as.numeric(result$res$value))) return(result) } result \u0026lt;- ESTIMATE_GPLVM(scale(Y),5) ## initial value 1123.451566 ## final value 1111.732375 ## converged\rlibrary(tidyverse) ggplot(gather(as.data.frame(result$output),key = name,value = value), aes(x=rep(dataset1$publication,5),y=value,colour=name)) + geom_line(size=1) + xlab(\u0026#34;Date\u0026#34;) + ggtitle(\u0026#34;5 economic factors\u0026#34;) library(xts) X.xts \u0026lt;- xts(result$output,order.by = dataset1$publication) X.q.xts \u0026lt;- apply.quarterly(X.xts,mean) X.3m.xts \u0026lt;- X.xts[endpoints(X.xts,on=\u0026#34;quarters\u0026#34;),] if (months(index(X.q.xts)[NROW(X.q.xts)]) %in% c(\u0026#34;3月\u0026#34;,\u0026#34;6月\u0026#34;,\u0026#34;9月\u0026#34;,\u0026#34;12月\u0026#34;)){ } else X.q.xts \u0026lt;- X.q.xts[-NROW(X.q.xts),] if (months(index(X.3m.xts)[NROW(X.3m.xts)]) %in% c(\u0026#34;3月\u0026#34;,\u0026#34;6月\u0026#34;,\u0026#34;9月\u0026#34;,\u0026#34;12月\u0026#34;)){ } else X.3m.xts \u0026lt;- X.3m.xts[-NROW(X.3m.xts),] colnames(X.xts) \u0026lt;- c(\u0026#34;factor1\u0026#34;,\u0026#34;factor2\u0026#34;,\u0026#34;factor3\u0026#34;,\u0026#34;factor4\u0026#34;,\u0026#34;factor5\u0026#34;) GDP$publication \u0026lt;- GDP$publication + months(2) GDP.q \u0026lt;- GDP[GDP$publication\u0026gt;=index(X.q.xts)[1] \u0026amp; GDP$publication\u0026lt;=index(X.q.xts)[NROW(X.q.xts)],] rg \u0026lt;- lm(scale(GDP.q$GDP)~X.q.xts[-54]) rg2 \u0026lt;- lm(scale(GDP.q$GDP)~X.3m.xts[-54]) summary(rg) ## ## Call:\r## lm(formula = scale(GDP.q$GDP) ~ X.q.xts[-54])\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.48942 -0.13666 0.03985 0.14338 0.28562 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.02841 0.02795 1.017 0.3146 ## X.q.xts[-54]X.1 -0.31431 0.01090 -28.841 \u0026lt; 2e-16 ***\r## X.q.xts[-54]X.2 -0.19397 0.01247 -15.559 \u0026lt; 2e-16 ***\r## X.q.xts[-54]X.3 0.03879 0.01880 2.063 0.0447 * ## X.q.xts[-54]X.4 -0.31876 0.03288 -9.696 8.60e-13 ***\r## X.q.xts[-54]X.5 -0.21523 0.03486 -6.175 1.46e-07 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.2033 on 47 degrees of freedom\r## Multiple R-squared: 0.9626,\tAdjusted R-squared: 0.9587 ## F-statistic: 242.1 on 5 and 47 DF, p-value: \u0026lt; 2.2e-16\rsummary(rg2) ## ## Call:\r## lm(formula = scale(GDP.q$GDP) ~ X.3m.xts[-54])\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.71694 -0.16540 0.04863 0.20132 0.79584 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.06106 0.04843 1.261 0.2136 ## X.3m.xts[-54]1 -0.30018 0.01593 -18.843 \u0026lt; 2e-16 ***\r## X.3m.xts[-54]2 -0.18521 0.01859 -9.962 3.62e-13 ***\r## X.3m.xts[-54]3 0.05174 0.02803 1.846 0.0712 . ## X.3m.xts[-54]4 -0.29361 0.04235 -6.933 1.03e-08 ***\r## X.3m.xts[-54]5 -0.10650 0.04187 -2.544 0.0143 * ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.3039 on 47 degrees of freedom\r## Multiple R-squared: 0.9165,\tAdjusted R-squared: 0.9076 ## F-statistic: 103.2 on 5 and 47 DF, p-value: \u0026lt; 2.2e-16\r決定係数が大幅に改善しました。 3ヶ月の平均をとったファクターの方がパフォーマンスが良さそうなので、こちらで実際のGDPと予測値のプロットを行ってみます。\nggplot(gather(data.frame(fit=rg$fitted.values,actual=scale(GDP.q$GDP),Date=GDP.q$publication),key,value,-Date),aes(y=value,x=Date,colour=key)) + geom_line(size=1) + ggtitle(\u0026#34;fit v.s. actual GDP\u0026#34;) いかがでしょうか。個人的にはかなりフィッティングできている印象があります（もはや経済理論など不要なのでしょうか）。ただ、最も新しい値を除いては未来の値が情報量として加味された上で推計されていることになりますから、フェアではありません。正しく予測能力を検証するためには四半期ごとに逐次的に回帰を行う必要があります。\nというわけで、アウトサンプルの予測力がどれほどあるのかをテストしてみたいと思います。まず、2005年4月から2007年3月までの月次統計データでファクターを計算し、データ頻度を四半期に集約します。そして、2007年1Qのデータを除いて、GDPに回帰します。回帰したモデルの係数を用いて、2007年1Qのファクターデータで同時点のGDPの予測値を計算し、それを実績値と比較します。次は2005年4月から2007年6月までのデータを用いて･･･という感じでアウトサンプルの予測を行ってみます。\nlibrary(lubridate) test_df \u0026lt;- data.frame() for (i in as.list(seq(as.Date(\u0026#34;2015-04-01\u0026#34;),as.Date(\u0026#34;2019-03-01\u0026#34;),by=\u0026#34;quarter\u0026#34;))){ day(i) \u0026lt;- days_in_month(i) traindata \u0026lt;- dataset1[dataset1$publication\u0026lt;=i,] X_train \u0026lt;- ESTIMATE_GPLVM(scale(traindata[,-2]),5) X_train.xts \u0026lt;- xts(X_train$output,order.by = traindata$publication) X_train.q.xts \u0026lt;- apply.quarterly(X_train.xts,mean) if (months(index(X_train.q.xts)[NROW(X_train.q.xts)]) %in% c(\u0026#34;3月\u0026#34;,\u0026#34;6月\u0026#34;,\u0026#34;9月\u0026#34;,\u0026#34;12月\u0026#34;)){ } else X_train.q.xts \u0026lt;- X_train.q.xts[-NROW(X_train.q.xts),] colnames(X_train.q.xts) \u0026lt;- c(\u0026#34;factor1\u0026#34;,\u0026#34;factor2\u0026#34;,\u0026#34;factor3\u0026#34;,\u0026#34;factor4\u0026#34;,\u0026#34;factor5\u0026#34;) GDP_train.q \u0026lt;- scale(GDP[GDP$publication\u0026gt;=index(X_train.q.xts)[1] \u0026amp; GDP$publication\u0026lt;=index(X_train.q.xts)[NROW(X_train.q.xts)],2]) rg_train \u0026lt;- lm(GDP_train.q[-NROW(GDP_train.q)]~.,data=X_train.q.xts[-NROW(X_train.q.xts)]) summary(rg_train) test_df \u0026lt;- rbind(test_df,data.frame(predict(rg_train,X_train.q.xts[NROW(X_train.q.xts)],interval = \u0026#34;prediction\u0026#34;,level=0.90),GDP=GDP_train.q[NROW(GDP_train.q)])) } 計算できました。グラフにしてみましょう。先ほどのグラフよりは精度が悪くなりました。特にリーマンの後は予測値の信頼区間（90%）が大きく拡大しており、不確実性が増大していることもわかります。2010年以降に関しては実績値は信頼区間にほど入っており、予測モデルとしての性能はまあまあなのかなと思います。ただ、リーマンのような金融危機もズバッと当てるところにロマンがあると思うので元データの改善を図りたいと思います。この記事はいったんここで終了です。\nggplot(gather(data.frame(test_df,Date=as.Date(rownames(test_df))),,,-c(lwr,upr,Date)),aes(y=value,x=Date,colour=key)) + geom_ribbon(aes(ymax=upr,ymin=lwr,fill=\u0026#34;band\u0026#34;),alpha=0.1,linetype=\u0026#34;blank\u0026#34;) + geom_line(size=1) + ggtitle(\u0026#34;out-sample test\u0026#34;) ","date":1558828800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1558828800,"objectID":"eff061ee8a24f8f74005b0514c080a17","permalink":"/post/post8/","publishdate":"2019-05-26T00:00:00Z","relpermalink":"/post/post8/","section":"post","summary":"最近私が注目しているのがガウス過程。今回はそのガウス過程の中でも、主成分分析のようにデータセットの次元削減のために使用されるGaussian Process Latent Variable Model（GPLVM）の紹介をします。","tags":["R","ガウス過程","GPLVM"],"title":"GPLVMでマルチファクターモデルを構築してみた","type":"post"},{"authors":null,"categories":["仕事関連"],"content":"おはこんばんにちは。勤め先で、アセットアロケーションに関するワークショップに参加したので、この分野は完全なる専門外ですがシミュレーションをしてみたいと思います。今回は、最小分散ポートフォリオ(minimum variance portfolio)を基本ポートフォリオとしたうえで、その分散共分散行列（予測値）をどのように推計するのかという点について先行研究を参考にエクササイズしていきたいと思います。先行研究は以下の論文です（オペレーションリサーチのジャーナルでした）。\nAsset Allocation with Correlation: A Composite Trade-Off\n1. 最小分散ポートフォリオ 最小分散ポートフォリオの詳しい説明はここでは割愛しますが、要は各資産（内株、外株、内債、外債、オルタナ）のリターンの平均と分散を計算し、それらを縦軸平均値、横軸分散の二次平面にプロットしたうえで、投資可能範囲を計算し、その集合の中で最も分散が小さくなるポートフォリオの事らしいです（下図参照）。\n先行研究のCarroll et. al. (2017)では、\n this paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.\n と記載されており、現状でも期待リターンの推計は難しく、それを必要としない最小分散ポートフォリオは有益で実践的な手法であるといえます。最小分散ポートフォリオの目的関数は、その名の通り「分散を最小化すること」です。今、各資産のリターンを集めたベクトルを$r$、各資産の保有ウェイトを$\\theta$、ポートフォリオリターンを$R_{p}$で表すことにすると、ポートフォリオ全体の分散$var(R_{p})$は以下のように記述できます。\n$$ var(R_{p}) = var(r^{T}\\theta) = E( (r^{T}\\theta)(r^{T}\\theta)^{T}) = \\theta^{T}\\Sigma\\theta $$\nここで$\\Sigma$は$r$の分散共分散行列です。よって、最小化問題は以下になります。\n$$ \\min_{\\theta}(\\theta^{T}\\Sigma\\theta) \\\ns.t 1^{T}\\theta = 1 $$\nここでは、フルインベストメントを制約条件に加えています。ラグランジュ未定乗数法を用いてこの問題を解いてみましょう。ラグランジュ関数$L$は以下のようになります。\n$$ L = \\theta^{T}\\Sigma\\theta + \\lambda(1^{T}\\theta - 1) $$\n1階の条件は、\n$$ \\displaystyle\\frac{\\partial L}{\\partial \\theta} = 2\\Sigma\\theta + 1\\lambda = 0 \\\n\\displaystyle \\frac{\\partial L}{\\partial \\lambda} = 1^{T} \\theta = 1 $$\n1本目の式を$\\theta$について解くと、\n$$ \\theta = \\Sigma^{-1}1\\lambda^{*} $$\n\rとなります。ここで、$\\lambda^{*}=-1/2\\lambda$です。これを2本目の式に代入し、$\\lambda^{*}$について解きます。\r\r\r$$\r1^{T}\\Sigma1\\lambda^{*} = 1 \\\\\r\\displaystyle \\lambda^{*} = \\frac{1}{1^{T}\\Sigma^{-1}1}\r$$\r\r\r$\\theta = \\Sigma^{-1}1\\lambda^{*}$だったので、$\\lambda^{*}$を消去すると、\r\r$$ \\displaystyle \\theta_{gmv} = \\frac{\\Sigma^{-1}1}{1^{T}\\Sigma^{-1}1} $$\nとなり、最適なウェイトを求めることができました。とりあえず、これをRで実装しておきます。\ngmv \u0026lt;- function(r_dat,r_cov){ library(MASS) i \u0026lt;- matrix(1,NCOL(r_dat),1) r_weight \u0026lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i) wr_dat \u0026lt;- r_dat*as.numeric(r_weight) portfolio \u0026lt;- apply(wr_dat,1,sum) pr_dat \u0026lt;- data.frame(wr_dat,portfolio) sd \u0026lt;- sd(portfolio) result \u0026lt;- list(r_weight,pr_dat,sd) names(result) \u0026lt;- c(\u0026#34;weight\u0026#34;,\u0026#34;return\u0026#34;,\u0026#34;portfolio risk\u0026#34;) return(result) } nlgmv \u0026lt;- function(r_dat,r_cov){ qp.out \u0026lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))), bvec=c(1,rep(0,NCOL(r_dat))),meq=1) r_weight \u0026lt;- qp.out$solution wr_dat \u0026lt;- r_dat*r_weight portfolio \u0026lt;- apply(wr_dat,1,sum) pr_dat \u0026lt;- data.frame(wr_dat,portfolio) sd \u0026lt;- sd(portfolio) result \u0026lt;- list(r_weight,pr_dat,sd) names(result) \u0026lt;- c(\u0026#34;weight\u0026#34;,\u0026#34;return\u0026#34;,\u0026#34;portfolio risk\u0026#34;) return(result) } 入力は各資産のリターンと分散共分散行列になっています。出力はウェイト、リターン、リスクです。nlgmvは最小分散ポートフォリオの空売り制約バージョンです。解析的な解は得られないので、数値的に買いを求めています。\n2. 分散共分散行列をどのように求めるか 最小分散ポートフォリオの計算式は求めることができました。次は、その入力である分散共分散行列をどうやって求めるのかについて分析したいと思います。一番原始的な方法はその時点以前に利用可能なリターンデータを標本として分散共分散行列を求め、その値を固定して最小分散ポートフォリオを求めるというヒストリカルなアプローチかと思います（つまりウェイトも固定）。ただ、これはあくまで過去の平均値を将来の予想値に使用するため、いろいろ問題が出てくるかと思います。専門外の私が思いつくものとしては、前日ある資産Aのリターンが大きく下落したという場面で明日もこの資産の分散は大きくなることが予想されるにも関わらず、平均値を使用するため昨日の効果が薄められてしまうことでしょうか。それに、ウェイトを最初から変更しないというのも時間がたつにつれ、最適点から離れていく気がします。ただ、ではどう推計するのかついてはこの分野でも試行錯誤が行われているようです。Carroll et. al. (2017)でも、\n The estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.\n と述べられていました。この論文では以下のようなモデルを用いて推計が行われています。いずれも、分散共分散行列を時変としているところに特徴があります。\nA. Constant conditional correlation (CCC) model 元論文はこちら。\nまず、分散共分散行列と相関行列の関係性から、$\\Sigma_{t} = D_{t}R_{t}D_{t}$となります。ここで、$R_{t}$は相関行列、$D_{t}$は$diag(\\sigma_{1,t},\u0026hellip;,\\sigma_{N,t})$で各資産$tt$期の標準偏差$\\sigma_{i,t}$を対角成分に並べた行列です。ここから、$D_{t}$と$R_{t}$を分けて推計していきます。まず、$D_{t}$ですが、こちらは以下のような多変量GARCHモデル(1,1)で推計します。\n$$ r_{t} = \\mu + u_{t} \\\nu_{t} = \\sigma_{t}\\epsilon \\\n\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}u_{t-1}^{2} + \\alpha_{2}\\sigma_{t-1}^{2} \\\n\\epsilon_{t} = NID(0,1) \\\nE(u_{t}|u_{t-1}) = 0 $$\nここで、$\\mu$はリターンの標本平均です。$\\alpha_{i}$は推定すべきパラメータ。$D_{t}$をGARCHで推計しているので、リターンの分布が正規分布より裾野の厚い分布に従い、またリターンの変化は一定ではなく前日の分散に依存する関係をモデル化しているといえるのではないでしょうか。とりあえずこれで$D_{t}$の推計はできたということにします。次に$R_{t}$の推計ですが、このモデルではリターンを標本として求めるヒストリカルなアプローチを取ります。つまり、$R_{t}$は定数です。よって、リターン変動の大きさは時間によって変化するが、各資産の相対的な関係性は不変であるという仮定を置いていることになります。\nB. Dynamic Conditional Correlation (DCC) model 元論文はこちら。\nこちらのモデルでは、$D_{t}$を求めるところまでは①と同じですが、[$R_{t}$の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、$t$期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列$R_{t}$は、\n$$ R_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2} $$\nです。ここで、$Q_{t}$は$t$期での条件付分散共分散行列で以下のように定式化されます。\n$$ Q_{t} = \\bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\\epsilon_{i,t-1}\\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1} $$\nここで、$\\bar{Q}$はヒストリカルな方法で計算した分散共分散行列であり、$a,b$はパラメータです。この方法では、先ほどとは異なり、リターン変動の大きさが時間によって変化するだけでなく、各資産の相対的な関係性も通時的に変化していくという仮定を置いていることになります。金融危機時には全資産のリターンが下落し、各資産の相関が正になる事象も観測されていることから、この定式化は魅力的であるということができるのではないでしょうか。\nC. Dynamic Equicorrelation (DECO) model 元論文はこちら。\nこの論文はまだきっちり読めていないのですが、相関行列$R_{t}$の定義から\n$$ R_{t} = (1-\\rho_{t})I_{N} + \\rho_{t}1 $$\nとなるようです。ここで、$\\rho_{t}$はスカラーでequicorrelationの程度を表す係数です。equicorrelationとは平均的なペアワイズ相関の事であると理解しています。つまりは欠損値がなければ普通の相関と変わりないんじゃないかと。ただ、資産が増えればそのような問題にも対処する必要があるのでその点ではよい推定量のようです。$\\rho_{t}$は以下のように求めることができます。\n$$ \\displaystyle \\rho_{t} = \\frac{1}{N(N-1)}(\\iota^{T}R_{t}^{DCC}\\iota - N) = \\frac{2}{N(N-1)}\\sum_{i\u0026gt;j}\\frac{q_{ij,t}}{\\sqrt{q_{ii,t} q_{jj,t}}} $$\nここで、$\\iota$はN×1ベクトルで要素は全て1です。また、$q_{ij,t}$は$Q_{t}$のi,j要素です。\nさて、分散共分散行列のモデル化ができたところで、ここまでをRで実装しておきます。\ncarroll \u0026lt;- function(r_dat,FLG){ library(rmgarch) if(FLG == \u0026#34;benchmark\u0026#34;){ H \u0026lt;- cov(r_dat) }else{ #1. define variables N \u0026lt;- NCOL(r_dat) # the number of assets #2. estimate covariance matrix basic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = \u0026#39;sGARCH\u0026#39;), distribution.model = \u0026#39;norm\u0026#39;) multi_garch = multispec(replicate(N, basic_garch)) dcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = \u0026#34;mvnorm\u0026#34;,model = \u0026#34;DCC\u0026#34;) fit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE)) forecast_dcc_garch \u0026lt;- dccforecast(fit_dcc_garch) if (FLG == \u0026#34;CCC\u0026#34;){ #Constant conditional correlation (CCC) model D \u0026lt;- sigma(forecast_dcc_garch) R_ccc \u0026lt;- cor(r_dat) H \u0026lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1]) colnames(H) \u0026lt;- colnames(r_dat) rownames(H) \u0026lt;- colnames(r_dat) } else{ #Dynamic Conditional Correlation (DCC) model H \u0026lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1]) if (FLG == \u0026#34;DECO\u0026#34;){ #Dynamic Equicorrelation (DECO) model one \u0026lt;- matrix(1,N,N) iota \u0026lt;- rep(1,N) Q_dcc \u0026lt;- rcor(forecast_dcc_garch,type=\u0026#34;Q\u0026#34;)[[1]][,,1] rho \u0026lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N)) D \u0026lt;- sigma(forecast_dcc_garch) R_deco \u0026lt;- (1-rho)*diag(1,N,N) + rho*one H \u0026lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1]) colnames(H) \u0026lt;- colnames(r_dat) rownames(H) \u0026lt;- colnames(r_dat) } } } return(H) } 本来であれば、パッケージを使用するべきではないのですが、今日はエクササイズなので推計結果だけを追い求めたいと思います。GARCHについては再来週ぐらいに記事を書く予定です。 これで準備ができました。この関数にリターンデータを入れて、分散共分散行列を計算し、それを用いて最小分散ポートフォリオを計算することができるようになりました。\n3. テスト用データの収集 データは以下の記事を参考にしました。\n(Introduction to Asset Allocation)[https://www.r-bloggers.com/introduction-to-asset-allocation/]\n使用したのは、以下のインデックスに連動するETF(iShares)の基準価額データです。\n S\u0026amp;P500 NASDAQ100 MSCI Emerging Markets Russell 2000 MSCI EAFE US 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index) U.S. Real Estate(the Dow Jones US Real Estate Index) gold bullion market  まず、データ集めです。\nlibrary(quantmod) #************************** # ★8 ASSETS SIMULATION # SPY - S\u0026amp;P 500  # QQQ - Nasdaq 100 # EEM - Emerging Markets # IWM - Russell 2000 # EFA - EAFE # TLT - 20 Year Treasury # IYR - U.S. Real Estate # GLD - Gold #************************** # load historical prices from Yahoo Finance symbol.names = c(\u0026#34;S\u0026amp;P 500\u0026#34;,\u0026#34;Nasdaq 100\u0026#34;,\u0026#34;Emerging Markets\u0026#34;,\u0026#34;Russell 2000\u0026#34;,\u0026#34;EAFE\u0026#34;,\u0026#34;20 Year Treasury\u0026#34;,\u0026#34;U.S. Real Estate\u0026#34;,\u0026#34;Gold\u0026#34;) symbols = c(\u0026#34;SPY\u0026#34;,\u0026#34;QQQ\u0026#34;,\u0026#34;EEM\u0026#34;,\u0026#34;IWM\u0026#34;,\u0026#34;EFA\u0026#34;,\u0026#34;TLT\u0026#34;,\u0026#34;IYR\u0026#34;,\u0026#34;GLD\u0026#34;) getSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, auto.assign = TRUE) #gn dates for all symbols \u0026amp; convert to monthly hist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD) month.ends = endpoints(hist.prices, \u0026#39;day\u0026#39;) hist.prices = Cl(hist.prices)[month.ends, ] colnames(hist.prices) = symbols # remove any missing data hist.prices = na.omit(hist.prices[\u0026#39;1995::\u0026#39;]) # compute simple returns hist.returns = na.omit( ROC(hist.prices, type = \u0026#39;discrete\u0026#39;) ) # compute historical returns, risk, and correlation ia = list() ia$expected.return = apply(hist.returns, 2, mean, na.rm = T) ia$risk = apply(hist.returns, 2, sd, na.rm = T) ia$correlation = cor(hist.returns, use = \u0026#39;complete.obs\u0026#39;, method = \u0026#39;pearson\u0026#39;) ia$symbols = symbols ia$symbol.names = symbol.names ia$n = length(symbols) ia$hist.returns = hist.returns # convert to annual, year = 12 months annual.factor = 12 ia$expected.return = annual.factor * ia$expected.return ia$risk = sqrt(annual.factor) * ia$risk rm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD) リターンをプロットするとこんな感じです。\nPerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = \u0026#34;パフォーマンスサマリー\u0026#34;) 次に、バックテストのコーディングを行います。一気にコードを公開します。\n# BACK TEST backtest \u0026lt;- function(r_dat,FLG,start_date,span,learning_term,port){ #----------------------------------------- # BACKTEST # r_dat - return data(xts object)  # FLG - flag(CCC,DCC,DECO) # start_date - start date for backtest # span - rebalance frequency # learning_term - learning term (days) # port - method of portfolio optimization #----------------------------------------- library(stringi) initial_dat \u0026lt;- r_dat[stri_c(as.Date(start_date)-learning_term,\u0026#34;::\u0026#34;,as.Date(start_date))] for (i in NROW(initial_dat):NROW(r_dat)) { if (i == NROW(initial_dat)){ H \u0026lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG) if (port == \u0026#34;nlgmv\u0026#34;){ result \u0026lt;- nlgmv(initial_dat,H) }else if (port == \u0026#34;risk parity\u0026#34;){ result \u0026lt;- risk_parity(initial_dat,H) } weight \u0026lt;- t(result$weight) colnames(weight) \u0026lt;- colnames(initial_dat) p_return \u0026lt;- initial_dat[NROW(initial_dat),]*result$weight } else { if (i %in% endpoints(r_dat,span)){ H \u0026lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG) if (port == \u0026#34;nlgmv\u0026#34;){ result \u0026lt;- nlgmv(test_dat,H) }else if (port == \u0026#34;risk parity\u0026#34;){ result \u0026lt;- risk_parity(test_dat,H) } } weight \u0026lt;- rbind(weight,t(result$weight)) p_return \u0026lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight) } if (i != NROW(r_dat)){ term \u0026lt;- stri_c(index(r_dat[i+1,])-learning_term,\u0026#34;::\u0026#34;,index(r_dat[i+1,])) test_dat \u0026lt;- r_dat[term] } } p_return$portfolio \u0026lt;- xts(apply(p_return,1,sum),order.by = index(p_return)) weight.xts \u0026lt;- xts(weight,order.by = index(p_return)) result \u0026lt;- list(p_return,weight.xts) names(result) \u0026lt;- c(\u0026#34;return\u0026#34;,\u0026#34;weight\u0026#34;) return(result) } CCC \u0026lt;- backtest(hist.returns,\u0026#34;CCC\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) DCC \u0026lt;- backtest(hist.returns,\u0026#34;DCC\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) DECO \u0026lt;- backtest(hist.returns,\u0026#34;DECO\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) benchmark \u0026lt;- backtest(hist.returns,\u0026#34;benchmark\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) result \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio) colnames(result) \u0026lt;- c(\u0026#34;CCC\u0026#34;,\u0026#34;DCC\u0026#34;,\u0026#34;DECO\u0026#34;,\u0026#34;benchmark\u0026#34;) 計算結果をグラフにしてみます。\nPerformanceAnalytics::charts.PerformanceSummary(result,main = \u0026#34;BACKTEST\u0026#34;) 空売り制約を課したので、上で定義した最小分散ポートフォリオは使用していません。どうやらこれだけで解析的に解くのは難しいらしく、数値的に解くことにしています。リバランス期間を週次にしたので、自前のPCでは計算に時間がかかりましたが、結果が計算できました。\nリーマン以降はどうやらベンチマークである等ウェイトポートフォリオよりをアウトパフォームしているようです。特に、DECOはいい感じです。そもそもDECOとDCCはほぼ変わらないパフォーマンスであると思っていたのですが、どうやら自分の理解が足らないらしく、論文の読み返す必要があるようです。Equicorrelationの意味をもう一度考えてみたいと思います。それぞれの組入比率の推移は以下のようになりました。\n# plot allocation weighting d_allocation \u0026lt;- function(ggweight,title){ #install.packages(\u0026#34;tidyverse\u0026#34;) library(tidyverse) ggweight \u0026lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method) ggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) + geom_area(colour=\u0026#34;black\u0026#34;,size=.1) + scale_y_continuous(limits = c(0,1)) + labs(title=title) + facet_grid(method~.) } gmv_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026#34;CCC\u0026#34;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026#34;DCC\u0026#34;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026#34;DECO\u0026#34;,Date=index(DECO$weight))) # plot allocation weighting d_allocation(gmv_weight,\u0026#34;GMV Asset Allocation\u0026#34;) リーマンの際にTLT、つまり米国債への比率を増やしているようです。CCCとDCCはそれ以外の部分でも米国債への比率が高く、よく挙げられる最小分散ポートフォリオの問題点がここでも発生しているようです。一方、DECOがやはり個性的な組入比率の推移をしており、ここらを考えてももう一度論文を読み返してみる必要がありそうです。\n追記（2019/3/3） これまでは、最小分散ポートフォリオで分析をしていましたが、リスクパリティの結果も見たいなと言うことで、そのコードも書いてみました。\nrisk_parity \u0026lt;- function(r_dat,r_cov){ fn \u0026lt;- function(weight, r_cov) { N \u0026lt;- NROW(r_cov) risks \u0026lt;- weight * (r_cov %*% weight) g \u0026lt;- rep(risks, times = N) - rep(risks, each = N) return(sum(g^2)) } dfn \u0026lt;- function(weight,r_cov){ out \u0026lt;- weight for (i in 0:length(weight)) { up \u0026lt;- dn \u0026lt;- weight up[i] \u0026lt;- up[i]+.0001 dn[i] \u0026lt;- dn[i]-.0001 out[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002 } return(out) } std \u0026lt;- sqrt(diag(r_cov)) x0 \u0026lt;- 1/std/sum(1/std) res \u0026lt;- nloptr::nloptr(x0=x0, eval_f=fn, eval_grad_f=dfn, eval_g_eq=function(weight,r_cov) { sum(weight) - 1 }, eval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) }, lb=rep(0,length(std)),ub=rep(1,length(std)), opts = list(\u0026#34;algorithm\u0026#34;=\u0026#34;NLOPT_LD_SLSQP\u0026#34;,\u0026#34;print_level\u0026#34; = 0,\u0026#34;xtol_rel\u0026#34;=1.0e-8,\u0026#34;maxeval\u0026#34; = 1000), r_cov = r_cov) r_weight \u0026lt;- res$solution names(r_weight) \u0026lt;- colnames(r_cov) wr_dat \u0026lt;- r_dat*r_weight portfolio \u0026lt;- apply(wr_dat,1,sum) pr_dat \u0026lt;- data.frame(wr_dat,portfolio) sd \u0026lt;- sd(portfolio) result \u0026lt;- list(r_weight,pr_dat,sd) names(result) \u0026lt;- c(\u0026#34;weight\u0026#34;,\u0026#34;return\u0026#34;,\u0026#34;portfolio risk\u0026#34;) return(result) } CCC \u0026lt;- backtest(hist.returns,\u0026#34;CCC\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) DCC \u0026lt;- backtest(hist.returns,\u0026#34;DCC\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) DECO \u0026lt;- backtest(hist.returns,\u0026#34;DECO\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) benchmark \u0026lt;- backtest(hist.returns,\u0026#34;benchmark\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) result \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio) colnames(result) \u0026lt;- c(\u0026#34;CCC\u0026#34;,\u0026#34;DCC\u0026#34;,\u0026#34;DECO\u0026#34;,\u0026#34;benchmark\u0026#34;) 結果はこんな感じ。\nPerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026#34;BACKTEST COMPARISON\u0026#34;) library(plotly) # plot allocation weighting riskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026#34;CCC\u0026#34;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026#34;DCC\u0026#34;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026#34;DECO\u0026#34;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026#34;benchmark\u0026#34;,Date=index(benchmark$weight))) PerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026#34;BACKTEST COMPARISON\u0026#34;) riskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026#34;CCC\u0026#34;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026#34;DCC\u0026#34;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026#34;DECO\u0026#34;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026#34;benchmark\u0026#34;,Date=index(benchmark$weight))) # plot allocation weighting d_allocation(riskparity_weight, \u0026#34;Risk Parity Asset Allocation\u0026#34;) どの手法もbenchmarkをアウトパーフォームできているという好ましい結果になりました。 やはり、分散共分散行列の推計がうまくいっているようです。また、DECOのパフォーマンスがよいのは、相関行列に各資産ペアの相関係数の平均値を用いているため、他の手法よりもリスク資産の組み入れが多くなったからだと思われます。ウェイトは以下の通りです。\nとりあえず、今日はここまで。\n","date":1550361600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1551571200,"objectID":"0c6f6cbe96ac9b1239a629e09477d6d1","permalink":"/post/post2/","publishdate":"2019-02-17T00:00:00Z","relpermalink":"/post/post2/","section":"post","summary":"社内ワークショップがあり、休日は国会図書館に籠り、先行研究を漁った結果、微妙なアセットアロケーションモデルを構築できました。","tags":["R"],"title":"Asset Allocation ModelをRで組んでみた。","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"おはこんばんにちは。かなり久しぶりの投稿となってしまいました。決して研究をさぼっていたのではなく、BVARのコーディングに手こずっていました。あと少しで完成します。さて、今回はBVARやこの前のGiannnone et a (2008)のような分析でも大活躍のカルマンフィルタを実装してしまいたいと思います。このブログではパッケージソフトに頼らず、基本的に自分で一から実装を行い、研究することをポリシーとしていますので、これから頻繁に使用するであろうカルマンフィルタを関数として実装してしまうことは非常に有益であると考えます。今回はRで実装をしましたので、そのご報告をします。\n1. カルマンフィルタとは？ まず、カルマンフィルタに関する簡単な説明を行います。非常にわかりやすい記事があるので、こちらを読んでいただいたほうがより分かりやすいかと思います。\nカルマンフィルタとは、状態空間モデルを解くアルゴリズムの一種です。状態空間モデルとは、手元の観測可能な変数から観測できない変数を推定するモデルであり、以下のような形をしています。\n$$ Y_{t} = Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} \\\n\\alpha_{t} = T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t} $$\n\rここで、$Y_{t}$は$g×1$ベクトルの観測可能な変数(観測変数)、$Z_{t}$は$g×k$係数行列、$\\alpha_{t}$は$k×1$ベクトルの観測不可能な変数(状態変数)、$T_{t}$は$k×k$係数行列です。また、$\\epsilon_{t}$は観測変数の誤差項、$\\eta_{t}$は状態変数の誤差項です。これらの誤差項はそれぞれ$N(0,H_{t})$, `\\(N(0,Q_{t})\\)`に従います（$H_{t},Q_{t}$は分散共分散行列）。$d_{t}$, `\\(c_{t}\\)`は定数項です。1本目の式は観測方程式、2本目の式は遷移方程式と呼ばれます。\r状態空間モデルを使用する例として、しばしば池の魚の数を推定する問題が使用されます。今、池の中の魚の全数が知りたいとして、その推定を考えます。観測時点毎に池の中の魚をすべて捕まえてその数を調べるのは現実的に困難なので、一定期間釣りをして釣れた魚をサンプルに全数を推定することを考えます。ここで、釣れた魚は観測変数、池にいる魚の全数は状態変数と考えることができます。今、経験的に釣れた魚の数と全数の間に以下のような関係があるとします。\r\r$$ Y_{t} = 0.01\\alpha_{t} + 5 + \\epsilon_{t} $$ これが観測方程式になります。また、魚の全数は過去の値からそれほど急速には変化しないと考えられるため、以下のようなランダムウォークに従うと仮定します。\n$$ \\alpha_{t} = \\alpha_{t-1} + 500 + \\eta_{t} $$ これが遷移方程式になります。あとは、これをカルマンフィルタアルゴリズムを用いて計算すれば、観測できない魚の全数を推定することができます。 このように状態空間モデルは非常に便利なモデルであり、また応用範囲も広いです。例えば、販売額から潜在顧客数を推定したり、クレジットスプレッドやトービンのQ等経済モデル上の概念として存在する変数を推定する、BVARのようにVARや回帰式の時変パラメータ推定などにも使用されます。\n2. カルマンフィルタアルゴリズムの導出 さて、非常に便利な状態空間モデルの説明はこれくらいにして、カルマンフィルタの説明に移りたいと思います。カルマンフィルタは状態空間モデルを解くアルゴリズムの一種であると先述しました。つまり、他にも状態空間モデルを解くアルゴリズムは存在します。カルマンフィルタアルゴリズムは一般に誤差項の正規性の仮定を必要としないフィルタリングアルゴリズムであり、観測方程式と遷移方程式の線形性の仮定さえあれば、線形最小分散推定量となります。カルマンフィルタアルゴリズムの導出にはいくつかの方法がありますが、今回はこの線形最小分散推定量としての導出を行います。まず、以下の３つの仮定を置きます。\n 初期値$\\alpha_{0}$は正規分布$N(a_{0},\\Sigma_{0})$に従う確率ベクトルである($a_{t}$は$\\alpha_{t}$の推定値)。 誤差項$\\epsilon_{t}$,$\\eta_{s}$は全ての$t$,$s$で互いに独立で、初期値ベクトル$\\alpha_{0}$と無相関である（$E(\\epsilon_{t}\\eta_{s})=0$, $(E(\\epsilon_{t}\\alpha_{0})=0)$, $(E(\\eta_{t}\\alpha_{0})=0)$）。 2より、$E(\\epsilon_{t}\\alpha_{t}')=0$、$E(\\eta_{t}\\alpha_{t-1}')=0$。  まず、$t-1$期の情報集合$\\Omega_{t-1}$が既知の状態での$\\alpha_{t}$と$Y_{t}$の期待値（予測値）を求めてみましょう。上述した状態空間モデルと誤差項の期待値がどちらもゼロである事実を用いると、以下のように計算することができます。\n$$ E(\\alpha_{t}|\\Omega_{t-1}) = a_{t|t-1} = T_{t}a_{t-1|t-1} + c_{t} E(Y_{t}|\\Omega_{t-1}) = Y_{t|t-1} = Z_{t}a_{t|t-1} + d_{t} $$\nここで、次に、これらの分散を求めます。\n\r$$\r\\begin{eqnarray}\rE( (\\alpha_{t}-a_{t|t-1})(\\alpha_{t}-a_{t|t-1})'|\\Omega_{t-1}) \u0026=\u0026 E( (T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}-a_{t|t-1})(T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}-a_{t|t-1})'|\\Omega_{t-1}) \\\\ \u0026=\u0026 E(T_{t}\\alpha_{t-1}\\alpha_{t-1}'T_{t}' + R_{t}\\eta_{t}\\eta_{t}'R_{t}'|\\Omega_{t-1}) \\\\\r\u0026=\u0026 E(T_{t}\\alpha_{t-1}\\alpha_{t-1}'T_{t}'|\\Omega_{t-1}) + E(R_{t}\\eta_{t}\\eta_{t}'R_{t}'|\\Omega_{t-1}) \\\\\r\u0026=\u0026 T_{t}E(\\alpha_{t-1}\\alpha_{t-1}'|\\Omega_{t-1})T_{t}' + R_{t}E(\\eta_{t}\\eta_{t}'|\\Omega_{t-1})R_{t}' \\\\\r\u0026=\u0026 T_{t}\\Sigma_{t-1|t-1}T_{t}' + R_{t}Q_{t}R_{t}' \\\\\r\u0026=\u0026 \\Sigma_{t|t-1}\r\\end{eqnarray}\r$$\r$$ \\begin{eqnarray} E( (Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})'|\\Omega_{t-1}) \u0026amp;=\u0026amp; E( (Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t}-Y_{t|t-1})(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t}-Y_{t|t-1})'|\\Omega_{t-1}) \\\n\u0026amp;=\u0026amp; E(Z_{t}\\alpha_{t}\\alpha_{t}\u0026lsquo;Z_{t}\u0026rsquo; + S_{t}\\epsilon_{t}\\epsilon_{t}\u0026lsquo;S_{t}'|\\Omega_{t-1}) \\\n\u0026amp;=\u0026amp; E(Z_{t}\\alpha_{t}\\alpha_{t}\u0026lsquo;Z_{t}'|\\Omega_{t-1}) + E(S_{t}\\epsilon_{t}\\epsilon_{t}\u0026lsquo;S_{t}'|\\Omega_{t-1}) \\\n\u0026amp;=\u0026amp; Z_{t}E(\\alpha_{t}\\alpha_{t}'|\\Omega_{t-1})Z_{t}\u0026rsquo; + S_{t}E(\\epsilon_{t}\\epsilon_{t}'|\\Omega_{t-1})S_{t}\u0026rsquo; \\\n\u0026amp;=\u0026amp; Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026rsquo; + S_{t}H_{t}S_{t}' \\\n\u0026amp;=\u0026amp; F_{t|t-1} \\end{eqnarray} $$\n\rここで、$t$期の情報集合$\\Omega_{t}$が得られたとします（つまり、観測値$Y_{t}$を入手）。カルマンフィルタでは、$t$期の情報である観測値$Y_{t}$を用いて$a_{t|t-1}$を以下の方程式で更新します。\n$$ E(\\alpha_{t}|\\Omega_{t}) = a_{t|t} = a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}) $$ つまり、観測値と$Y_{t}$の期待値（予測値）の差をあるウェイト$k_{t}$（$k×g$行列）でかけたもので補正をかけるわけです。よって、観測値と予測値が完全に一致していた場合は補正は行われないことになります。ここで重要なのは、ウエイト$k_{t}$をどのように決めるのかです。$k_{t}$は更新後の状態変数の分散$E( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})')= \\Sigma_{t|t}$を最小化するよう決定します。これが、カルマンフィルタが線形最小分散推定量である根拠です。最小化にあたっては以下のベクトル微分が必要になりますので、おさらいをしておきましょう。今回使用するのは以下の事実です。\n$$ \\displaystyle \\frac{\\partial a\u0026rsquo;b}{\\partial b} = \\frac{\\partial b\u0026rsquo;a}{\\partial b} = a \\\n\\displaystyle \\frac{\\partial b\u0026rsquo;Ab}{\\partial b} = 2Ab $$\nここで、$a,b$はベクトル（それぞれ$n×1$ベクトル、$1×n$ベクトル）、$A$は$n×n$の対称行列です。まず、１つ目から証明していきます。$\\displaystyle y = a\u0026rsquo;b = b\u0026rsquo;a = \\sum_{i=1}^{n}a_{i}b_{i}$とします。 このとき、$\\frac{\\partial y}{\\partial b_{i}}=a_{i}$なので、\n$$ \\displaystyle \\frac{\\partial a\u0026rsquo;b}{\\partial b} = \\frac{\\partial b\u0026rsquo;a}{\\partial b} = a $$\n次に２つ目です。$y = b\u0026rsquo;Ab = \\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}b_{i}b_{j}$とします。このとき、\n$$ \\displaystyle \\frac{\\partial y}{\\partial b_{i}} = \\sum_{j=1}^{n}a_{ij}b_{j} + \\sum_{j=1}^{n}a_{ji}b_{j} = 2\\sum_{j=1}^{n}a_{ij}b_{j} = 2a_{i}\u0026lsquo;b $$ よって、\n$$ \\displaystyle \\frac{\\partial y}{\\partial b} = \\left( \\begin{array}{cccc} \\frac{\\partial y}{\\partial b_{1}} \\\n\\vdots \\\n\\frac{\\partial y}{\\partial b_{n}} \\\n\\end{array} \\right) = 2 \\left( \\begin{array}{cccc} \\sum_{j=1}^{n}a_{1j}b_{j} \\\n\\vdots \\\n\\sum_{j=1}^{n}a_{nj}b_{j} \\\n\\end{array} \\right) = 2 \\left( \\begin{array}{cccc} a_{1}\u0026lsquo;b \\\n\\vdots \\\na_{n}\u0026lsquo;b \\\n\\end{array} \\right) = 2Ab $$ さて、準備ができたので、更新後の状態変数の分散$E( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})')$を求めてみましょう。\n\r$$\r\\begin{eqnarray}\rE( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})') \u0026=\u0026 \\Sigma_{t|t} \\\\\r\u0026=\u0026 E\\{ (\\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))(\\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))'\\} \\\\\r\u0026=\u0026 E\\{ ( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )\\} \\\\\r\u0026=\u0026 E\\{ ( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1}) )( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1}) )'\\} \\\\\r\u0026=\u0026 E\\{ ( (I - k_{t}Z_{t})\\alpha_{t} - k_{t}S_{t}\\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})( (I - k_{t}Z_{t})\\alpha_{t} - k_{t}S_{t}\\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})' \\} \\\\\r\u0026=\u0026 E\\{( (I - k_{t}Z_{t})(\\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\\epsilon_{t})( (I - k_{t}Z_{t})(\\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\\epsilon_{t})'\\} \\\\\r\u0026=\u0026 (I - k_{t}Z_{t})\\Sigma_{t|t-1}(I - k_{t}Z_{t})' + k_{t}S_{t}H_{t}S_{t}'k_{t}' \\\\\r\u0026=\u0026 (\\Sigma_{t|t-1} - k_{t}Z_{t}\\Sigma_{t|t-1})(I - k_{t}Z_{t})' + k_{t}S_{t}H_{t}S_{t}'k_{t}' \\\\\r\u0026=\u0026 \\Sigma_{t|t-1} - \\Sigma_{t|t-1}(k_{t}Z_{t})' - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}Z_{t}\\Sigma_{t|t-1}Z_{t}'k_{t}' + k_{t}S_{t}H_{t}S_{t}'k_{t}' \\\\\r\u0026=\u0026 \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}'k_{t}' - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}(Z_{t}\\Sigma_{t|t-1}Z_{t}' + S_{t}H_{t}S_{t}')k_{t}' \\\\\r\\end{eqnarray}\r$$\r\r１回目の式変形で、$a_{t|t}$に上述した更新式を代入し、２回目の式変形で観測方程式と上で計算した$E(Y_{t}|\\Omega_{t-1})$を代入しています。さて、更新後の状態変数の分散$\\Sigma_{t|t}$を$k_{t}$の関数として書き表すことができたので、これを$k_{t}$で微分し、0と置き、$\\Sigma_{t|t}$を最小化する$k_{t}$を求めます。先述した公式で、$a=\\Sigma_{t|t-1}Z_{t}'$、$b=k_{t}'$、$A=(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026rsquo; + S_{t}H_{t}S_{t}')$とすると（$A$は分散共分散行列の和なので対称行列）、\n$$ \\frac{\\partial \\Sigma_{t|t}}{\\partial k_{t}'} = -2(Z_{t}\\Sigma_{t|t-1})\u0026rsquo; + 2(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026rsquo; + S_{t}H_{t}S_{t}')k_{t} = 0 $$\nここから、$k_{t}$を解きなおすと、\n\r$$\r\\begin{eqnarray}\rk_{t} \u0026=\u0026 \\Sigma_{t|t-1}Z_{t}'(Z_{t}\\Sigma_{t|t-1}Z_{t}' + S_{t}H_{t}S_{t}')^{-1} \\\\\r\u0026=\u0026 \\Sigma_{t|t-1}Z_{t}'F_{t|t-1}^{-1}\r\\end{eqnarray}\r$$\r\r突然、$F_{t|t-1}$が出てきました。これは観測変数の予測値の分散$E((Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})'|\\Omega_{t-1})$でした。一方、$\\Sigma_{t|t-1}Z_{t}$は状態変数の予測値の分散を観測変数のスケールに調整したものです（観測空間に写像したもの）。つまり、カルマンゲイン$k_{t}$は状態変数と観測変数の予測値の分散比となっているのです。観測変数にはノイズがあり、観測方程式はいつも誤差０で満たされるわけではありません。また、状態方程式にも誤差項が存在します。状態の遷移も100%モデル通りにはいかないということです。$t$期の観測変数$Y_{t}$が得られたとして、それをどれほど信頼して状態変数を更新するかは観測変数のノイズが状態変数のノイズに比べてどれほど小さいかによります。つまり、相対的に観測方程式が遷移方程式よりも信頼できる場合には状態変数を大きく更新するのです。このように、カルマンフィルタでは、観測方程式と遷移方程式の相対的な信頼度によって、更新の度合いを毎期調整しています。その度合いが分散比であり、カルマンゲインだというわけです。ちなみに欠損値が発生した場合には、観測変数の分散を無限大にし、状態変数の更新を全く行わないという対処を行います。観測変数に信頼がないので当たり前の処置です。この場合は遷移方程式を100%信頼します。これがカルマンフィルタのコアの考え方になります。 更新後の分散を計算しておきます。\n\r$$\r\\begin{eqnarray}\r\\Sigma_{t|t} \u0026=\u0026 \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}'k_{t}' - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}F_{t|t-1}k_{t}' \\\\\r\u0026=\u0026 \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}'k_{t}' - k_{t}Z_{t}\\Sigma_{t|t-1} + (\\Sigma_{t|t-1}Z_{t}'F_{t|t-1}^{-1})F_{t|t-1}k_{t}' \\\\\r\u0026=\u0026 \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}'k_{t}' - k_{t}Z_{t}\\Sigma_{t|t-1} + \\Sigma_{t|t-1}Z_{t}'k_{t}' \\\\\r\u0026=\u0026 \\Sigma_{t|t-1} - k_{t}Z_{t}\\Sigma_{t|t-1} \\\\\r\u0026=\u0026 \\Sigma_{t|t-1} - k_{t}F_{t|t-1}F_{t|t-1}^{-1}Z_{t}\\Sigma_{t|t-1} \\\\\r\u0026=\u0026 \\Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}'\r\\end{eqnarray}\r$$\r\rでは、最終的に導出されたアルゴリズムをまとめたいと思います。\n\r$$\r\\begin{eqnarray}\ra_{t|t-1} \u0026=\u0026 T_{t}a_{t-1|t-1} + c_{t} \\\\\r\\Sigma_{t|t-1} \u0026=\u0026 T_{t}\\Sigma_{t-1|t-1}T_{t}' + R_{t}Q_{t}R_{t}' \\\\\rY_{t|t-1} \u0026=\u0026 Z_{t}a_{t|t-1} + d_{t} \\\\\rF_{t|t-1} \u0026=\u0026 Z_{t}\\Sigma_{t|t-1}Z_{t}' + S_{t}H_{t}S_{t}' \\\\\rk_{t} \u0026=\u0026 \\Sigma_{t|t-1}Z_{t}'F_{t|t-1}^{-1} \\\\\ra_{t|t} \u0026=\u0026 a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}) \\\\\r\\Sigma_{t|t} \u0026=\u0026 \\Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}'\r\\end{eqnarray}\r$$\r\r初期値$a_{0},\\Sigma_{0}$が所与の元で、まず状態変数の予測値$a_{1|0},\\Sigma_{1|0}$を計算します。その結果を用いて、次は観測変数の予測値$Y_{t|t-1},F_{t|t-1}$を計算し、カルマンゲイン$k_{t}$を得ます。$t$期の観測可能なデータを入手したら、更新方程式を用いて$a_{t|t},\\Sigma_{t|t}$を更新します。これをサンプル期間繰り返していくことになります。ちなみに、遷移方程式の誤差項$\\eta_{t}$と定数項$c_{t}$がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致します。つまり、新しいサンプルを入手するたびに`OLS`をやり直す推計方法ということです（今回はその証明は勘弁してください）。\r3. Rで実装する。 以下がRでの実装コードです。\nkalmanfiter \u0026lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){ #------------------------------------------------------------------- # Implemention of Kalman filter # y - observed variable # I - the number of unobserved variable # t - parameter of endogenous variable in state equation # z - parameter of endogenous variable in observable equation # c - constant in state equaion # R - parameter of exogenous variable in state equation # Q - var-cov matrix of exogenous variable in state equation # d - constant in observable equaion # S - parameter of exogenous variable in observable equation # h - var-cov matrix of exogenous variable in observable equation # a_int - initial value of endogenous variable # sig_int - initial value of variance of endogenous variable #------------------------------------------------------------------- library(MASS) # 1.Define Variable if (class(y)!=\u0026#34;matrix\u0026#34;){ y \u0026lt;- as.matrix(y) } N \u0026lt;- NROW(y) # sample size L \u0026lt;- NCOL(y) # the number of observable variable  a_pre \u0026lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable a_fil \u0026lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable sig_pre \u0026lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable sig_fil \u0026lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable y_pre \u0026lt;- array(0,dim = c(L,1,N)) # prediction of observed variable F_pre \u0026lt;- array(0,dim = c(L,L,N)) # prediction of var-cov mat. of observable variable  F_inv \u0026lt;- array(0,dim = c(L,L,N)) # inverse of F_pre k \u0026lt;- array(0,dim = c(I,L,N)) # kalman gain if (any(is.na(a_int))==TRUE){ a_int \u0026lt;- matrix(0,nrow = I,ncol = 1) } if (any(is.na(sig_int))==TRUE){ sig_int \u0026lt;- diag(1,nrow = I,ncol = I) } if (any(is.na(R))==TRUE){ R \u0026lt;- diag(1,nrow = I,ncol = I) } if (any(is.na(Q))==TRUE){ Q \u0026lt;- diag(1,nrow = I,ncol = I) } if (any(is.na(S))==TRUE){ S \u0026lt;- matrix(1,nrow = L,ncol = L) } if (any(is.na(h))==TRUE){ H \u0026lt;- array(0,dim = c(L,L,N)) for(i in 1:N){ diag(H[,,i]) = 1 } }else if (class(h)!=\u0026#34;array\u0026#34;){ H \u0026lt;- array(h,dim = c(NROW(h),NCOL(h),N)) } # fill infinite if observed data is NA for(i in 1:N){ miss \u0026lt;- is.na(y[i,]) diag(H[,,i])[miss] \u0026lt;- 1e+32 } y[is.na(y)] \u0026lt;- 0 # 2.Set Initial Value a_fil[,,1] \u0026lt;- a_int sig_fil[,,1] \u0026lt;- sig_int # 3.Implement Kalman filter for (i in 1:N){ if(class(z)==\u0026#34;array\u0026#34;){ Z \u0026lt;- z[,,i] }else{ Z \u0026lt;- z } a_pre[,,i] \u0026lt;- t%*%a_fil[,,i] + c sig_pre[,,i] \u0026lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R) y_pre[,,i] \u0026lt;- Z%*%a_pre[,,i] + d F_pre[,,i] \u0026lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S) k[,,i] \u0026lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i]) a_fil[,,i+1] \u0026lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i]) sig_fil[,,i+1] \u0026lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i]) } # 4.Aggregate results result \u0026lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z) names(result) \u0026lt;- c(\u0026#34;state prediction\u0026#34;, \u0026#34;state filtered\u0026#34;, \u0026#34;state var prediction\u0026#34;, \u0026#34;state var filtered\u0026#34;, \u0026#34;observable prediction\u0026#34;, \u0026#34;kalman gain\u0026#34;, \u0026#34;parameter of state eq\u0026#34;, \u0026#34;parameter of observable eq\u0026#34;) return(result) } 案外簡単に書けるもんですね。これを使って、Giannone et al (2008)をやり直してみます。データセットは前回記事と変わりません。\n以下、分析用のRコードです。\n#------------------------ # Giannone et. al. 2008  #------------------------ library(MASS) library(xts) # ファクターを計算 f \u0026lt;- 3 z \u0026lt;- scale(dataset1) for (i in 1:nrow(z)){ eval(parse(text = paste(\u0026#34;S_i \u0026lt;- z[i,]%*%t(z[i,])\u0026#34;,sep = \u0026#34;\u0026#34;))) if (i==1){ S \u0026lt;- S_i }else{ S \u0026lt;- S + S_i } } S \u0026lt;- (1/nrow(z))*S gamma \u0026lt;- eigen(S) D \u0026lt;- diag(gamma$values[1:f]) V \u0026lt;- gamma$vectors[,1:f] F_t \u0026lt;- matrix(0,nrow(z),f) for (i in 1:nrow(z)){ eval(parse(text = paste(\u0026#34;F_t[\u0026#34;,i,\u0026#34;,]\u0026lt;- z[\u0026#34;,i,\u0026#34;,]%*%V\u0026#34;,sep = \u0026#34;\u0026#34;))) } lambda_hat \u0026lt;- V psi \u0026lt;- diag(diag(S-V%*%D%*%t(V))) R \u0026lt;- diag(diag(cov(z-z%*%V%*%t(V)))) a \u0026lt;- matrix(0,f,f) b \u0026lt;- matrix(0,f,f) for(t in 2:nrow(z)){ a \u0026lt;- a + F_t[t,]%*%t(F_t[t-1,]) b \u0026lt;- b + F_t[t-1,]%*%t(F_t[t-1,]) } b_inv \u0026lt;- solve(b) A_hat \u0026lt;- a%*%b_inv e \u0026lt;- numeric(f) for (t in 2:nrow(F_t)){ e \u0026lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat } H \u0026lt;- t(e)%*%e Q \u0026lt;- diag(1,f,f) Q[1:f,1:f] \u0026lt;- H p \u0026lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat)) result1 \u0026lt;- kalmanfiter(z,f,A_hat,lambda_hat,c=0,R=NA,Q=Q,d=0,S=NA,h=R,a_int=F_t[1,],sig_int=matrix(p,f,f)) プロットしてみます。\nlibrary(ggplot2) library(tidyverse) ggplot(gather(data.frame(factor1=result1$`state filtered`[1,1,-dim(result1$`state filtered`)[3]],factor2=result1$`state filtered`[2,1,-dim(result1$`state filtered`)[3]],factor3=result1$`state filtered`[3,1,-dim(result1$`state filtered`)[3]],time=as.Date(rownames(dataset1))),key = factor,value = value,-time),aes(x=time,y=value,colour=factor)) + geom_line() giannoneの記事を書いた際は、元論文のMATLABコードを参考にRで書いたのですが、通常のカルマンフィルタとは観測変数の分散共分散行列の逆数の計算方法が違うらしくグラフの形が異なっています。まあでも、概形はほとんど同じですが（なので、ちゃんと動いているはず）。\n4. カルマンスムージング カルマンフィルタの実装は以上で終了なのですが、誤差項の正規性を仮定すれば$T$期までの情報集合$\\Omega_{T}$を用いて、$a_{i|i}, \\Sigma_{i|i}(i = 1:T)$を$a_{i|T}, \\Sigma_{i|T}(i = 1:T)$へ更新することができます。これをカルマンスムージングと呼びます。これを導出してみましょう。その準備として、以下のような$\\alpha_{t|t}$と$\\alpha_{t+1|t}$の混合分布を計算しておきます。\n\r$$\r\\begin{eqnarray}\r(\\alpha_{t|t},\\alpha_{t+1|t}) \u0026=\u0026 N(\r\\left(\r\\begin{array}{cccc}\rE(\\alpha_{t|t}) \\\\\rE(\\alpha_{t+1|t})\r\\end{array}\r\\right),\r\\left(\r\\begin{array}{cccc}\rVar(\\alpha_{t|t}), Cov(\\alpha_{t|t},\\alpha_{t+1|t}) \\\\\rCov(\\alpha_{t+1|t},\\alpha_{t|t}), Var(\\alpha_{t+1|t})\r\\end{array}\r\\right)\r) \\\\\r\u0026=\u0026 N(\r\\left(\r\\begin{array}{cccc}\ra_{t|t} \\\\\ra_{t+1|t}\r\\end{array}\r\\right),\r\\left(\r\\begin{array}{cccc}\r\\Sigma_{t|t}, \\Sigma_{t|t}T_{t}' \\\\\rT_{t}\\Sigma_{t|t}, \\Sigma_{t+1|t} \\end{array}\r\\right)\r)\r\\end{eqnarray}\r$$\r\rここで、$Cov(\\alpha_{t|t},\\alpha_{t+1|t})$は\n\r$$\r\\begin{eqnarray}\rCov(\\alpha_{t+1|t},\\alpha_{t|t}) \u0026=\u0026 Cov(T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}, \\alpha_{t|t}) \\\\\r\u0026=\u0026 T_{t}Cov(\\alpha_{t|t},\\alpha_{t|t}) + Cov(c_{t},\\alpha_{t|t}) + Cov(R_{t}\\eta_{t},\\alpha_{t|t}) \\\\\r\u0026=\u0026 T_{t}Var(\\alpha_{t|t}) = T_{t}\\Sigma_{t|t}\r\\end{eqnarray}\r$$\r\rここで、条件付き多変量正規分布は以下のような分布をしていることを思い出しましょう（ 参考 ）。\n$$ (X_{1},X_{2}) = N( \\left( \\begin{array}{cccc} \\mu_{1} \\\n\\mu_{2} \\end{array} \\right), \\left( \\begin{array}{cccc} \\Sigma_{11}, \\Sigma_{12} \\\n\\Sigma_{21}, \\Sigma_{22} \\end{array} \\right) ) \\\n$$ $$ (X_{1}|X_{2}=x_{2}) = N(\\mu_{1} + \\Sigma_{12}\\Sigma_{22}^{-1}(x_{2}-\\mu_{2}),\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}) $$\nこれを用いて、$(\\alpha_{t|t}|\\alpha_{t+1|t}=a_{t+1})$を計算してみましょう。\n\r$$\r\\begin{eqnarray}\r(\\alpha_{t|t}|\\alpha_{t+1|t}=a_{t+1}) \u0026=\u0026 N(a_{t|t} + \\Sigma_{t|t}T_{t}'\\Sigma_{t+1|t}^{-1}(a_{t+1}-a_{t+1|t}), \\Sigma_{t|t}-\\Sigma_{t|t}T_{t}'\\Sigma_{t+1|t}^{-1}T_{t}\\Sigma_{t|t}) \\\\\r\u0026=\u0026N(a_{t|t} + L_{t}(a_{t+1}-a_{t+1|t}), \\Sigma_{t|t}-L_{t}\\Sigma_{t+1|t}L_{t}')\r\\end{eqnarray}\r$$\r\rただし、$a_{t+1}$の値は観測不可能なので、上式を用いて状態変数を更新することはできません。今、わかるのは$T$期における$a_{t+1|T}$の分布のみです。ということで、$a_{t+1}$を$a_{t+1|T}$で代用し、$\\alpha_{t|T}$の分布を求めてみます。では、計算していきます。$\\alpha_{t|T} = N(E(\\alpha_{t|T}),Var(\\alpha_{t|T}))$ですが、\n\r$$\r\\begin{eqnarray}\rE(\\alpha_{t|T}) \u0026=\u0026 E_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\rVar(\\alpha_{t|T}) \u0026=\u0026 E_{\\alpha_{t+1|T}}(Var(\\alpha_{t|t}|\\alpha_{t+1|t} = \\alpha_{t+1|T})) + Var_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T}))\r\\end{eqnarray}\r$$\r\rというように、$\\alpha_{t+1|T}$も確率変数となるので、繰り返し期待値の法則と繰り返し分散の法則を使用します（こちらを参照）。\n  繰り返し期待値の法則\n$E(x) = E_{Z}(E(X|Y=Z))$\n  繰り返し分散の法則\n$Var(X) = E_{Z}(Var(X|Y=Z))+Var_{Z}(E(X|Y=Z))$\n  よって、\n\r$$\r\\begin{eqnarray}\rE(\\alpha_{t|T}) \u0026=\u0026 E_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\r\u0026=\u0026 a_{t|t} + L_{t}(a_{t+1|T}-a_{t+1|t}) \\\\\rVar(\\alpha_{t|T}) \u0026=\u0026 E_{\\alpha_{t+1|T}}(Var(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) + Var_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\r\u0026=\u0026 \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}' + E( (a_{t|t} + L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))(a_{t|t} + L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))') \\\\\r\u0026=\u0026 \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}' + E( (L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))(L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))') \\\\\r\u0026=\u0026 \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}' + E( (L_{t}\\alpha_{t+1|T} - L_{t}a_{t+1|T})(L_{t}\\alpha_{t+1|T} - L_{t}(a_{t+1|T})') \\\\\r\u0026=\u0026 \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}' + E( L_{t}(\\alpha_{t+1|T} - a_{t+1|T})(\\alpha_{t+1|T} - a_{t+1|T})'L_{t}') \\\\\r\u0026=\u0026 \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}' + L_{t}E( (\\alpha_{t+1|T} - a_{t+1|T})(\\alpha_{t+1|T} - a_{t+1|T})')L_{t}' \\\\\r\u0026=\u0026 \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}' + L_{t}\\Sigma_{t+1|T}L_{t}'\r\\end{eqnarray}\r$$\r\rとなります。カルマンスムージングのアルゴリズムをまとめておきます。\n\r$$\r\\begin{eqnarray}\rL_{t} \u0026=\u0026 \\Sigma_{t|t}T_{t}\\Sigma_{t+1|t}^{-1} \\\\\ra_{t|T} \u0026=\u0026 a_{t|t} + L_{t}(a_{t+1|T} - a_{t+1|t}) \\\\\r\\Sigma_{t|T} \u0026=\u0026 \\Sigma_{t+1|t} + L_{t}(\\Sigma_{t+1|T}-\\Sigma_{t+1|t})L_{t}'\r\\end{eqnarray}\r$$\r\rカルマンスムージングの特徴的な点は後ろ向きに計算をしていく点です。つまり、$T$期から1期に向けて計算を行っていきます。$L_{t}$に関してはそもそもカルマンフィルタを回した時点で計算可能ですが、$\\alpha_{t|T}$は$T$期までのデータが手元にないと計算できません。今、$T$期まで観測可能なデータが入手できたとしましょう。すると、２番目の方程式を用いて、$a_{T-1|T}$を計算します。ちなみに$a_{T|T}$はカルマンフィルタを回した時点ですでに手に入っているので、計算する必要はありません。同時に、３番目の式を用いて$\\Sigma_{T-1|T}$を計算します。そして、$a_{T-1|T},\\Sigma_{T-1|T}$と$L_{T-1}$を用いて$a_{T-2|T},\\Sigma_{T-2|T}$を計算、というように1期に向けて後ろ向きに計算をしていくのです。さきほど、遷移方程式の誤差項$\\eta_{t}$と定数項$c_{t}$がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致すると書きましたが、カルマンスムージングの場合は$T$期までのサンプルでOLSを行った結果と一致します。 Rで実装してみます。\nkalmansmoothing \u0026lt;- function(filter){ #------------------------------------------------------------------- # Implemention of Kalman smoothing # t - parameter of endogenous variable in state equation # z - parameter of endogenous variable in observable equation # a_pre - prediction of state # a_fil - filtered value of state # sig_pre - prediction of var of state # sig_fil - filtered value of state #------------------------------------------------------------------- library(MASS) # 1.Define variable a_pre \u0026lt;- filter$`state prediction` a_fil \u0026lt;- filter$`state filtered` sig_pre \u0026lt;- filter$`state var prediction` sig_fil \u0026lt;- filter$`state var filtered` t \u0026lt;- filter$`parameter of state eq` C \u0026lt;- array(0,dim = dim(sig_pre)) a_sm \u0026lt;- array(0,dim = dim(a_pre)) sig_sm \u0026lt;- array(0,dim = dim(sig_pre)) N \u0026lt;- dim(C)[3] a_sm[,,N] \u0026lt;- a_fil[,,N] sig_sm[,,N] \u0026lt;- sig_fil[,,N] for (i in N:2){ C[,,i-1] \u0026lt;- sig_fil[,,i-1]%*%t(t)%*%ginv(sig_pre[,,i]) a_sm[,,i-1] \u0026lt;- a_fil[,,i-1] + C[,,i-1]%*%(a_sm[,,i]-a_pre[,,i]) sig_sm[,,i-1] \u0026lt;- sig_fil[,,i-1] + C[,,i-1]%*%(sig_sm[,,i]-sig_pre[,,i])%*%t(C[,,i-1]) } result \u0026lt;- list(a_sm,sig_sm,C) names(result) \u0026lt;- c(\u0026#34;state smoothed\u0026#34;, \u0026#34;state var smoothed\u0026#34;, \u0026#34;c\u0026#34;) return(result) } 先ほどのコードの続きでRコードを書いてみます。\nresult2 \u0026lt;- kalmansmoothing(result1) かなりシンプルですね。ちなみにグラフにしましたが、１個目とほぼ変わりませんでした。とりあえず、今日はここまで。\n","date":1549756800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1549756800,"objectID":"d084acea3d1051f530a6486b095c70b4","permalink":"/post/post3/","publishdate":"2019-02-10T00:00:00Z","relpermalink":"/post/post3/","section":"post","summary":"時系列解析には欠かせないカルマンフィルタ。Rでもパッケージが用意されていて非常に便利ですが、ここではいったん理解を優先し、カルマンフィルタの実装を行ってみたいと思います。","tags":["カルマンフィルタ","R"],"title":"カルマンフィルタの実装","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["マクロ経済学"],"content":"　おはこんばんにちは。昨日、Bayesian Vector Autoregressionの記事を書きました。\nその中でハイパーパラメータのチューニングの話が出てきて、なにか効率的にチューニングを行う方法はないかと探していた際にBayesian Optimizationを発見しました。日次GDPでも機械学習の手法を利用しようと思っているので、Bayesian Optimizationはかなり使える手法ではないかと思い、昨日徹夜で理解しました。\nその内容をここで実装しようとは思うのですが、Bayesian Optimizationではガウス回帰（Gaussian Pocess Regression,以下GPR）を使用しており、まずその実装を行おうと持ったのがこのエントリを書いた動機です。Bayesian Optimizationの実装はこのエントリの後にでも書こうかなと思っています。\n1. GPRとは 　GRPとは簡単に言ってしまえば「ベイズ推定を用いた非線形回帰手法の１種」です。モデル自体は線形ですが、カーネルトリックを用いて入力変数を無限個非線形変換したものを説明変数として推定できるところが特徴です（カーネルになにを選択するかによります）。\nGPRが想定しているのは、学習データとして入力データと教師データがそれぞれN個得られており、また入力データに関しては$N+1$個目のデータも得られている状況です。この状況から、$N+1$個目の教師データを予測します。\n教師データにはノイズが含まれており、以下のような確率モデルに従います。\n$$ t_{i} = y_{i} + \\epsilon_{i} $$\nここで、$t_{i}$は$i$番目の観測可能な教師データ（スカラー）、$y_{i}$は観測できない出力データ（スカラー）、$\\epsilon_{i}$は測定誤差で正規分布$N(0,\\beta^{-1})$に従います。$y_{i}$は以下のような確率モデルに従います。\n$$ \\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i}) $$\nここで、$x_{i}$はi番目の入力データベクトル、$\\phi(・)$は非線形関数、$(\\textbf{w}^{T})$は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、$\\phi(x_{i}) = (x_{1,i}, x_{1,i}^{2},\u0026hellip;,x_{1,i}x_{2,i},\u0026hellip;)$を想定しています（$x_{1,i}$は$i$番目の入力データ$x_{i}$の１番目の変数）。教師データの確率モデルから、$i$番目の出力データ$y_{i}$が得られたうえで$t_{i}$が得られる条件付確率は、\n$$ p(t_{i}|y_{i}) = N(t_{i}|y_{i},\\beta^{-1}) $$\nとなります。$\\displaystyle \\textbf{t} = (t_{1},\u0026hellip;,t_{n})^{T}$、$\\displaystyle \\textbf{y} = (y_{1},\u0026hellip;,y_{n})^{T}$とすると、上式を拡張することで\n$$ \\displaystyle p(\\textbf{t}|\\textbf{y}) = N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N}) $$\nと書けます。また、事前分布として$\\textbf{w}$の期待値は0、分散は全て$\\alpha$と仮定します。$\\displaystyle \\textbf{y}$はガウス過程に従うと仮定します。ガウス過程とは、$\\displaystyle \\textbf{y}$の同時分布が多変量ガウス分布に従うもののことです。コードで書くと以下のようになります。\n# Define Kernel function Kernel_Mat \u0026lt;- function(X,sigma,beta){ N \u0026lt;- NROW(X) K \u0026lt;- matrix(0,N,N) for (i in 1:N) { for (k in 1:N) { if(i==k) kdelta = 1 else kdelta = 0 K[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta } } return(K) } N \u0026lt;- 10 # max value of X M \u0026lt;- 1000 # sample size X \u0026lt;- matrix(seq(1,N,length=M),M,1) # create X testK \u0026lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix library(MASS) P \u0026lt;- 6 # num of sample path Y \u0026lt;- matrix(0,M,P) # define Y for(i in 1:P){ Y[,i] \u0026lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y } # Plot matplot(x=X,y=Y,type = \u0026#34;l\u0026#34;,lwd = 2) 　Kernel_Matについては後述しますが、$\\displaystyle \\textbf{y}$の各要素$\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})$の間の共分散行列$K$を入力$x$からカーネル法を用いて計算しています。そして、この$K$と平均0から、多変量正規乱数を6系列生成し、それをプロットしています。\n　これらの系列は共分散行列から計算されるので、各要素の共分散が正に大きくなればなるほど同じ値をとりやすくなるようモデリングされていることになります。また、グラフを見ればわかるように非常になめらかなグラフが生成されており、かつ非常に柔軟な関数を表現できていることがわかります。コードでは計算コストの関係上、入力を0から10に限定して1000個の入力点をサンプルし、作図を行っていますが、原理的には$x$は実数空間で定義されるものであるので、$p(\\textbf{y})$は無限次元の多変量正規分布に従います。 以上のように、$\\displaystyle \\textbf{y}$はガウス過程に従うと仮定するので同時確率$p(\\textbf{y})$は平均0、分散共分散行列が$K$の多変量正規分布$N(\\textbf{y}|0,K)$に従います。ここで、$K$の各要素$K_{i,j}$は、\n\r$$\r\\begin{eqnarray}\rK_{i,j} \u0026=\u0026 cov[y_{i},y_{j}] = cov[\\textbf{w}\\phi(x_{i}),\\textbf{w}\\phi(x_{j})] \\\\\r\u0026=\u0026\\phi(x_{i})\\phi(x_{j})cov[\\textbf{w},\\textbf{w}]=\\phi(x_{i})\\phi(x_{j})\\alpha\r\\end{eqnarray}\r$$\r\rです。ここで、$\\phi(x_{i})\\phi(x_{j})\\alpha$は$\\phi(x_{i})$の**次元が大きくなればなるほど計算量が多く**なります（つまり、非線形変換をかければかけるほど計算が終わらない）。しかし、カーネル関数$k(x,x')$を用いると、計算量は高々入力データ$x_{i},x_{j}$のサンプルサイズの次元になるので、計算がしやすくなります。カーネル関数を用いて$K_{i,j} = k(x_{i},x_{j})$となります。カーネル関数としてはいくつか種類がありますが、以下のガウスカーネルがよく使用されます。\n$$ k(x,x') = a \\exp(-b(x-x')^{2}) $$\n$(\\displaystyle \\textbf{y})$の同時確率が定義できたので、$\\displaystyle \\textbf{t}$の同時確率を求めることができます。\n\r$$\r\\begin{eqnarray}\r\\displaystyle p(\\textbf{t}) \u0026=\u0026 \\int p(\\textbf{t}|\\textbf{y})p(\\textbf{y}) d\\textbf{y} \\\\\r\\displaystyle \u0026=\u0026 \\int N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N})N(\\textbf{y}|0,K)d\\textbf{y} \\\\\r\u0026=\u0026 N(\\textbf{y}|0,\\textbf{C}_{N})\r\\end{eqnarray}\r$$\r\rここで、$\\textbf{C}{N} = K +\\beta^{-1}\\textbf{I}{N}$です。なお、最後の式展開は正規分布の再生性を利用しています（証明は正規分布の積率母関数から容易に導けます）。要は、両者は独立なので共分散は2つの分布の共分散の和となると言っているだけです。個人的には、$p(\\textbf{y})$が先ほど説明したガウス過程の事前分布であり、$p(\\textbf{t}|\\textbf{y})$が尤度関数で、$p(\\textbf{t})$は事後分布をというようなイメージです。事前分布$p(\\textbf{y})$は制約の緩い分布でなめらかであることのみが唯一の制約です。 $N$個の観測可能な教師データ$\\textbf{t}$と$t_{N+1}$の同時確率は、\n$$ p(\\textbf{t},t_{N+1}) = N(\\textbf{t},t_{N+1}|0,\\textbf{C}_{N+1}) $$\nここで、$\\textbf{C}_{N+1}$は、\n\r$$\r\\textbf{C}_{N+1} = \\left(\r\\begin{array}{cccc}\r\\textbf{C}_{N} \u0026 \\textbf{k} \\\\\r\\textbf{k}^{T} \u0026 c \\\\\r\\end{array}\r\\right)\r$$\r\rです。ここで、$\\textbf{k} = (k(x_{1},x_{N+1}),\u0026hellip;,k(x_{N},x_{N+1}))$、$c = k(x_{N+1},x_{N+1})$です。$\\textbf{t}$と$t_{N+1}$の同時分布から条件付分布$p(t_{N+1}|\\textbf{t})$を求めることができます。\n$$ p(t_{N+1}|\\textbf{t}) = N(t_{N+1}|\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{t},c-\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{k}) $$\n条件付分布の計算においては、条件付多変量正規分布の性質を利用しています。上式を見ればわかるように、条件付分布$p(t_{N+1}|\\textbf{t})$は$N+1$個の入力データ、$N$個の教師データ、カーネル関数のパラメータ$a,b$が既知であれば計算可能となっていますので、任意の点を入力データとして与えてやれば、元のData Generating Processを近似することが可能になります。GPRの良いところは上で定義した確率モデル$\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})$を直接推定しなくても予測値が得られるところです。確率モデルには$\\phi(x_{i})$があり、非線形変換により入力データを高次元ベクトルへ変換しています。よって、次元が高くなればなるほど$\\phi(x_{i})\\phi(x_{j})\\alpha$の計算量は大きくなっていきますが、GPRではカーネルトリックを用いているので高々入力データベクトルのサンプルサイズの次元の計算量で事足りることになります。\n2. GPRの実装 　とりあえずここまでをRで実装してみましょう。PRMLのテストデータで実装しているものがあったので、それをベースにいじってみました。\nlibrary(ggplot2) library(grid) # 1.Gaussian Process Regression # PRML\u0026#39;s synthetic data set curve_fitting \u0026lt;- data.frame( x=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000), t=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502)) f \u0026lt;- function(beta, sigma, xmin, xmax, input, train) { kernel \u0026lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function K \u0026lt;- outer(input, input, kernel); # calc gram matrix C_N \u0026lt;- K + diag(length(input))/beta m \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean  m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance x \u0026lt;- seq(xmin,xmax,length=100) output \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)), tx=input,ty=train), aes(x=x1,y=m)) + geom_line() + geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) + geom_point(aes(x=tx,y=ty)) return(output) } grid.newpage() # make a palet pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2 print(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) print(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) print(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1)) print(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) $(\\beta^{-1})$は測定誤差を表しています。$\\beta$が大きい（つまり、測定誤差が小さい）とすでに得られているデータとの誤差が少なくなるように予測値をはじき出すので、over fitting しやすくなります。 上図の左上がそうなっています。左上は$\\beta=400$で、現時点で得られているデータに過度にfitしていることがわかります。逆に$\\beta$が小さいと教師データとの誤差を無視するように予測値をはじき出しますが、汎化性能は向上するかもしれません。右上の図がそれです。$\\beta=4$で、得られているデータ点を平均はほとんど通っていません。$b$は現時点で得られているデータが周りに及ぼす影響の広さを表しています。$b$が小さいと、隣接する点が互いに強く影響を及ぼし合うため、精度は下がるが汎化性能は上がるかもしれません。逆に、$b$が大きいと、個々の点にのみフィットする不自然な結果になります。これは右下の図になります（$b=\\frac{1}{0.03},\\beta=25$）。御覧の通り、$\\beta$が大きいのでoverfitting気味であり、なおかつ$b$も大きいので個々の点のみにfitし、無茶苦茶なグラフになっています。左下のグラフが最もよさそうです。$b=\\frac{1}{0.3},\\beta=2$となっています。試しに、このグラフのx区間を[0,2]へ伸ばしてみましょう。すると、以下のようなグラフがかけます。\ngrid.newpage() # make a palet pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2 print(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1)) print(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) これを見ればわかるように、左下以外のグラフはすぐに95%信頼区間のバンドが広がり、データ点がないところではまったく使い物にならないことがわかります。一方、左下のグラフは1.3~1.4ぐらいまではそこそこのバンドがかけており、我々が直感的に理解する関数とも整合的な点を平均値が通っているように思えます。また、観測可能なデータ点から離れすぎるとパラメータに何を与えようと平均０、分散１の正規分布になることもわかるがわかります。 さて、このようにパラメータの値に応じて、アウトサンプルの予測精度が異なることを示したわけですが、ここで問題となるのはこれらハイパーパラメータをどのようにして推計するかです。これは対数尤度関数$\\ln p(\\textbf{t}|a,b)$を最大にするハイパーパラメータを勾配法により求めます(($\\beta$は少しタイプが異なるようで、発展的な議論では他のチューニング方法をとる模様。まだ、そのレベルにはいけていないのでここではカリブレートすることにします。))。$p(\\textbf{t}) = N(\\textbf{y}|0,\\textbf{C}_{N})$なので、対数尤度関数は\n\r$$\r\\displaystyle \\ln p(\\textbf{t}|a,b,\\beta) = -\\frac{1}{2}\\ln|\\textbf{C}_{N}| - \\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1}\\textbf{k}\r$$\r\rとなります。あとは、これをパラメータで微分し、得られた連立方程式を解くことで最尤推定量が得られます。ではまず導関数を導出してみます。\n$$ \\displaystyle \\frac{\\partial}{\\partial \\theta_{i}} \\ln p(\\textbf{t}|\\theta) = -\\frac{1}{2}Tr(\\textbf{C}_{N}^{-1}\\frac{\\partial \\textbf{C}_{N}}{\\partial \\theta_{i}}) + \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1} \\frac{\\partial\\textbf{C}_{N}}{\\partial\\theta_{i}}\\textbf{C}_{N}^{-1}\\textbf{t} $$\nここで、$\\theta$はパラメータセットで、$\\theta_{i}$は$i$番目のパラメータを表しています。この導関数が理解できない方はこちらの補論にある(C.21)式と(C.22)式をご覧になると良いと思います。今回はガウスカーネルを用いているため、\n$$ \\displaystyle \\frac{\\partial k(x,x')}{\\partial a} = \\exp(-b(x-x')^{2}) \\\n\\displaystyle \\frac{\\partial k(x,x')}{\\partial b} = -a(x-x')^{2}\\exp(-b(x-x')^{2}) $$\nを上式に代入すれば良いだけです。ただ、今回は勾配法により最適なパラメータを求めます。以下、実装のコードです（かなり迷走しています）。\ng \u0026lt;- function(xmin, xmax, input, train){ # initial value beta = 100 b = 1 a = 1 learning_rate = 0.1 itermax \u0026lt;- 1000 if (class(input) == \u0026#34;numeric\u0026#34;){ N \u0026lt;- length(input) } else { N \u0026lt;- NROW(input) } kernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel derivative_a \u0026lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2) derivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2) dloglik_a \u0026lt;- function(C_N,y,x1,x2) { -sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y } dloglik_b \u0026lt;- function(C_N,y,x1,x2) { -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y } # loglikelihood function likelihood \u0026lt;- function(b,a,x,y){ kernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2) K \u0026lt;- outer(x, x, kernel) C_N \u0026lt;- K + diag(N)/beta itermax \u0026lt;- 1000 l \u0026lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y return(l) } K \u0026lt;- outer(input, input, kernel) C_N \u0026lt;- K + diag(N)/beta for (i in 1:itermax){ kernel \u0026lt;- function(x1, x2) a*exp(-b*(x1-x2)^2) derivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2) dloglik_b \u0026lt;- function(C_N,y,x1,x2) { -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y } K \u0026lt;- outer(input, input, kernel) # calc gram matrix C_N \u0026lt;- K + diag(N)/beta l \u0026lt;- 0 if(abs(l-likelihood(b,a,input,train))\u0026lt;0.0001\u0026amp;i\u0026gt;2){ break }else{ a \u0026lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input)) b \u0026lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input)) } l \u0026lt;- likelihood(b,a,input,train) } K \u0026lt;- outer(input, input, kernel) C_N \u0026lt;- K + diag(length(input))/beta m \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) x \u0026lt;- seq(xmin,xmax,length=100) output \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)), tx=input,ty=train), aes(x=x1,y=m)) + geom_line() + geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) + geom_point(aes(x=tx,y=ty)) return(output) } print(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) たしかに、良さそうな感じがします（笑） とりあえず、今日はここまで。\n","date":1543708800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1543708800,"objectID":"fe859c6266641600992fd6a0d6f11947","permalink":"/post/post1/","publishdate":"2018-12-02T00:00:00Z","relpermalink":"/post/post1/","section":"post","summary":"万能すぎて逆に面白くないと話題のガウス回帰を実装してみました。","tags":["R"],"title":"ガウス回帰の実装をやってみた","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"おはこんばんにちは。 前回、統計ダッシュボードからAPI接続で統計データを落とすという記事を投稿しました。 今回はそのデータを、Gianonne et. al. (2008)のマルチファクターモデルにかけ、四半期GDPの予測を行いたいと思います。\n1. Gianonne et. al. (2008)版マルチファクターモデル 元論文\n前回の投稿でも書きましたが、この論文はGiannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標を2つのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しています。 まず、このモデルについてご紹介します。このモデルでは2段階推計を行います。まず主成分分析により経済統計を統計間の相関が0となるファクターへ変換します（参考）。そして、その後の状態空間モデルでの推計で必要になるパラメータをOLS推計し、そのパラメータを使用してカルマンフィルタ＆カルマンスムーザーを回し、ファクターを推計しています。では、具体的な説明に移ります。 統計データを$x_{i,t|v_j}$と定義します。ここで、$i=1,\u0026hellip;,n$は経済統計を表し（つまり$n$が全統計数）、$t=1,\u0026hellip;,T_{iv_j}$は統計$i$のサンプル期間の時点を表しています（つまり、$T_{iv_j}$は統計$i$のその時点での最新データ日付を表す）。また、$v_j$はある時点$j$（2005年など）で得られる情報集合（vintage）を表しています。統計データ$x_{i,t|v_j}$は以下のようにファクター$f_{r,t}$の線形結合で表すことができます（ここで$r$はファクターの数を表す）。\n$$ x_{i,t|v_j} = \\mu_i + \\lambda_{i1}f_{1,t} + \u0026hellip; + \\lambda_{ir}f_{r,t} + \\xi_{i,t|v_j} \\tag{1} $$\n$(\\mu_i)$は定数項、$\\lambda_{ir}$はファクターローディング、$\\xi_{i,t|v_j}$はホワイトノイズの誤差項を表しています。これを行列形式で書くと以下のようになります。\n$$ x_{t|v_j} = \\mu + \\Lambda F_t + \\xi_{t|v_j} = \\mu + \\chi_t + \\xi_{t|v_j} \\tag{2} $$\nここで、$x_{t|v_j} = (x_{1,t|v_j}, \u0026hellip;, x_{n,t|v_j} )^{\\mathrm{T}}$、$\\xi_{t|v_j}=(\\xi_{1,t|v_j}, \u0026hellip;, \\xi_{n,t|v_j})^{\\mathrm{T}}$、$F_t = (f_{1,t}, \u0026hellip;, f_{r,t})^{\\mathrm{T}}$であり、$\\Lambda$は各要素が$ \\lambda_{ij}$の$n\\times r$行列のファクターローディングを表しています。また、$\\chi_t = \\Lambda F_t$です。よって、ファクター$ F_t$を推定するためには、データ$x_{i,t|v_j}$を以下のように基準化したうえで、分散共分散行列を計算し、その固有値問題を解けばよいという事になります。\n$$ \\displaystyle z_{it} = \\frac{1}{\\hat{\\sigma}_i}(x_{it} - \\hat{\\mu}_{it}) \\tag{3} $$\n\rここで、$\\displaystyle \\hat{\\mu}_{it} = 1/T \\sum_{t=1}^T x_{it}$であり、$\\hat{\\sigma}_i = \\sqrt{1/T \\sum_{t=1}^T (x_{it}-\\hat{\\mu_{it}})^2}$です（ここで$T$はサンプル期間）。分散共分散行列$S$を以下のように定義します。\r\r$$ \\displaystyle S = \\frac{1}{T} \\sum_{t=1}^T z_t z_t^{\\mathrm{T}} \\tag{4} $$ 次に、$S$のうち、固有値を大きい順に$r$個取り出し、それを要素にした$ r \\times r$対角行列を$ D$、それに対応する固有ベクトルを$n \\times r$行列にしたものを$ V$と定義します。ファクター$ \\tilde{F}_t$は以下のように推計できます。\n$$ \\tilde{F}_t = V^{\\mathrm{T}} z_t \\tag{5} $$ ファクターローディング$\\Lambda$と誤差項の共分散行列$\\Psi = \\mathop{\\mathbb{E}} [\\xi_t\\xi^{\\mathrm{T}}_t]$は$\\tilde{F}_t$を$z_t$に回帰することで推計します。\n$$ \\displaystyle \\hat{\\Lambda} = \\sum_{t=1}^T z_t \\tilde{F}^{\\mathrm{T}}_t (\\sum_{t=1}^T\\tilde{F}_t\\tilde{F}^{\\mathrm{T}}_t)^{-1} = V \\tag{6} $$\n$$ \\hat{\\Psi} = diag(S - VDV) \\tag{7} $$\n注意して頂きたいのは、ここで推計した$\\tilde{F}_t$は、以下の状態空間モデルでの推計に必要なパラメータを計算するための一時的な推計値であるという事です（２段階推計の１段階目という事）。\n$$ x_{t|v_j} = \\mu + \\Lambda F_t + \\xi_{t|v_j} = \\mu + \\chi_t + \\xi_{t|v_j} \\tag{2} $$\n$$ F_t = AF_{t-1} + u_t \\tag{8} $$ ここで、$u_t$は平均0、分散$H$のホワイトノイズです。再掲している(2)式が観測方程式、(8)式が遷移方程式となっています。推定すべきパラメータは$\\Lambda$、$\\Psi$以外に$A$と$H$があります（$\\mu=0$としています）。$A$は主成分分析により計算した$\\tilde{F}_t$を$VAR(1)$にかけることで推定します。\n$$ \\hat{A} = \\sum_{t=2}^T\\tilde{F}_t\\tilde{F}_{t-1}^{\\mathrm{T}} (\\sum_{t=2}^T\\tilde{F}_{t-1}\\tilde{F}_{t-1}^{\\mathrm{T}})^{-1} \\tag{9} $$ $H$は今推計した$VAR(1)$の誤差項の共分散行列から計算します。これで必要なパラメータの推定が終わりました。次にカルマンフィルタを回します。カルマンフィルタに関してはこちらを参考にしてください。わかりやすいです。これで最終的に$\\hat{F}_{t|v_j}$の推計ができるわけです。 GDPがこれらのファクターで説明可能であり（つまり固有の変動がない）、GDPと月次経済指標がjointly normalであれば以下のような単純なOLS推計でGDPを予測することができます。もちろん月次経済指標の方が早く公表されるので、内生性の問題はないと考えられます。\n\r$$\r\\hat{y}_{3k|v_j} = \\alpha + \\beta^{\\mathrm{T}} \\hat{F}_{3k|v_j} \\tag{10}\r$$\r\r\rここで、$3k$は四半期の最終月を示しています（3月、6月など）$\\hat{y}_{3k|v_j}$は$j$時点で得られる情報集合$v_j$での四半期GDPを表しており、$\\hat{F}_{3k|v_j}$はその時点で推定したファクターを表しています（四半期最終月の値だけを使用している点に注意）。これで推計方法の説明は終わりです。\r\r2. Rで実装する では実装します。前回記事で得られたデータ（dataset）が読み込まれている状態からスタートします。まず、主成分分析でファクターを計算します。なお、前回の記事で3ファクターの累積寄与度が80%を超えたため、今回もファクター数は3にしています。\n#------------------------ # Giannone et. al. 2008  #------------------------ library(xts) library(MASS) library(tidyverse) # 主成分分析でファクターを計算 f \u0026lt;- 3 # ファクター数を定義 a \u0026lt;- which(dataset1$publication == \u0026#34;2012-04-01\u0026#34;) # サンプル開始期間を2012年に設定。 dataset2 \u0026lt;- dataset1[a:nrow(dataset1),] rownames(dataset2) \u0026lt;- dataset2$publication dataset2 \u0026lt;- dataset2[,-2] z \u0026lt;- scale(dataset2) # zは基準化されたサンプルデータ for (i in 1:nrow(z)){ eval(parse(text = paste(\u0026#34;S_i \u0026lt;- z[i,]%*%t(z[i,])\u0026#34;,sep = \u0026#34;\u0026#34;))) if (i==1){ S \u0026lt;- S_i }else{ S \u0026lt;- S + S_i } } S \u0026lt;- (1/nrow(z))*S # 分散共分散行列を計算 (4)式 gamma \u0026lt;- eigen(S) D \u0026lt;- diag(gamma$values[1:f]) V \u0026lt;- gamma$vectors[,1:f] F_t \u0026lt;- matrix(0,nrow(z),f) for (i in 1:nrow(z)){ eval(parse(text = paste(\u0026#34;F_t[\u0026#34;,i,\u0026#34;,]\u0026lt;- z[\u0026#34;,i,\u0026#34;,]%*%V\u0026#34;,sep = \u0026#34;\u0026#34;))) # (5)式を実行 } F_t.xts \u0026lt;- xts(F_t,order.by = as.Date(row.names(z))) plot.zoo(F_t.xts,col = c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;,\u0026#34;green\u0026#34;,\u0026#34;yellow\u0026#34;,\u0026#34;purple\u0026#34;),plot.type = \u0026#34;single\u0026#34;) # 時系列プロット lambda_hat \u0026lt;- V psi \u0026lt;- diag(S-V%*%D%*%t(V)) # (7)式 R \u0026lt;- diag(diag(cov(z-z%*%V%*%t(V)))) 推計したファクター$\\tilde{F}_t$の時系列プロットは以下のようになり、前回princomp関数で計算したファクターと完全一致します（じゃあprincompでいいやんと思われるかもしれませんが実装しないと勉強になりませんので）。\n次に、$VAR(1)$を推計し、パラメータを取り出します。\n# VAR(1)モデルを推計 a \u0026lt;- matrix(0,f,f) b \u0026lt;- matrix(0,f,f) for(t in 2:nrow(z)){ a \u0026lt;- a + F_t[t,]%*%t(F_t[t-1,]) b \u0026lt;- b + F_t[t-1,]%*%t(F_t[t-1,]) } b_inv \u0026lt;- solve(b) A_hat \u0026lt;- a%*%b_inv # (9)式 e \u0026lt;- numeric(f) for (t in 2:nrow(F_t)){ e \u0026lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat } H \u0026lt;- t(e)%*%e Q \u0026lt;- diag(1,f,f) Q[1:f,1:f] \u0026lt;- H $VAR(1)$に関してもvar関数とパラメータの数値が一致することを確認済みです。いよいよカルマンフィルタを実行します。\n# カルマンフィルタを実行 RR \u0026lt;- array(0,dim = c(ncol(z),ncol(z),nrow(z))) # RRは観測値の分散行列（相関はないと仮定） for(i in 1:nrow(z)){ miss \u0026lt;- is.na(z[i,]) R_temp \u0026lt;- diag(R) R_temp[miss] \u0026lt;- 1e+32 # 欠損値の分散は無限大にする RR[,,i] \u0026lt;- diag(R_temp) } zz \u0026lt;- z; zz[is.na(z)] \u0026lt;- 0 # 欠損値（NA）に0を代入（計算結果にはほとんど影響しない）。 a_t \u0026lt;- matrix(0,nrow(zz),f) # a_tは状態変数の予測値 a_tt \u0026lt;- matrix(0,nrow(zz),f) # a_ttは状態変数の更新後の値 a_tt[1,] \u0026lt;- F_t[1,] # 状態変数の初期値には主成分分析で推計したファクターを使用 sigma_t \u0026lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の予測値 sigma_tt \u0026lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の更新値 p \u0026lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat)) sigma_tt[,,1] \u0026lt;- matrix(p,3,3) # 状態変数の分散の初期値はVAR(1)の推計値から計算 y_t \u0026lt;- matrix(0,nrow(zz),ncol(zz)) # y_tは観測値の予測値 K_t \u0026lt;- array(0,dim = c(f,ncol(zz),nrow(zz))) # K_tはカルマンゲイン data.m \u0026lt;- as.matrix(dataset2) # カルマンフィルタを実行 for (t in 2:nrow(zz)){ a_t[t,] \u0026lt;- A_hat%*%a_tt[t-1,] sigma_t[,,t] \u0026lt;- A_hat%*%sigma_tt[,,t-1]%*%t(A_hat) + Q y_t[t,] \u0026lt;- as.vector(V%*%a_t[t,]) S_t \u0026lt;- V%*%sigma_tt[,,t-1]%*%t(V)+RR[,,t] GG \u0026lt;- t(V)%*%diag(1/diag(RR[,,t]))%*%V Sinv \u0026lt;- diag(1/diag(RR[,,t])) - diag(1/diag(RR[,,t]))%*%V%*%ginv(diag(nrow(A_hat))+sigma_t[,,t]%*%GG)%*%sigma_t[,,t]%*%t(V)%*%diag(1/diag(RR[,,t])) K_t[,,t] \u0026lt;- sigma_t[,,t]%*%t(V)%*%Sinv a_tt[t,] \u0026lt;- a_t[t,] + K_t[,,t]%*%(zz[t,]-y_t[t,]) sigma_tt[,,t] \u0026lt;- sigma_t[,,t] - K_t[,,t]%*%V%*%sigma_tt[,,t-1]%*%t(V)%*%t(K_t[,,t]) } F.xts \u0026lt;- xts(a_tt,order.by = as.Date(rownames(data.m))) plot.zoo(F.xts, col = c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;,\u0026#34;green\u0026#34;,\u0026#34;yellow\u0026#34;,\u0026#34;purple\u0026#34;),plot.type = \u0026#34;single\u0026#34;) # 得られた推計値を時系列プロット カルマンフィルタにより推計したファクターの時系列プロットが以下です。遷移方程式がAR(1)だったからかかなり平準化された値となっています。\nでは、この得られたファクターをOLSにかけます。\n# 得られたファクターとGDPをOLSにかける F_q \u0026lt;- as.data.frame(a_tt[seq(3,nrow(a_tt),3),]) # 四半期の終わり月の値だけを引っ張ってくる  colnames(F_q) \u0026lt;- c(\u0026#34;factor1\u0026#34;,\u0026#34;factor2\u0026#34;,\u0026#34;factor3\u0026#34;) colnames(GDP) \u0026lt;- c(\u0026#34;publication\u0026#34;,\u0026#34;GDP\u0026#34;) t \u0026lt;- which(GDP$publication==\u0026#34;2012-04-01\u0026#34;) t2 \u0026lt;- which(GDP$publication==\u0026#34;2015-01-01\u0026#34;) # 2012-2q~2015-1qまでのデータが学習データ、それ以降がテストデータ GDP_q \u0026lt;- GDP[t:nrow(GDP),] dataset.q \u0026lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),]) test \u0026lt;- lm(GDP~factor1 + factor2 + factor3,data=dataset.q) summary(test) ## ## Call:\r## lm(formula = GDP ~ factor1 + factor2 + factor3, data = dataset.q)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1079.4 -779.4 -265.2 905.9 1280.0 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 129440.3 3784.8 34.200 4.74e-09 ***\r## factor1 -776.3 846.0 -0.918 0.389 ## factor2 -699.0 2358.8 -0.296 0.776 ## factor3 552.3 1588.2 0.348 0.738 ## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 1109 on 7 degrees of freedom\r## Multiple R-squared: 0.6667,\tAdjusted R-squared: 0.5239 ## F-statistic: 4.668 on 3 and 7 DF, p-value: 0.04279\rout_of_sample \u0026lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成 test.pred \u0026lt;- predict(test, out_of_sample, interval=\u0026#34;prediction\u0026#34;) pred.GDP.xts \u0026lt;- xts(cbind(test.pred[,1],out_of_sample$GDP),order.by = out_of_sample$publication) plot.zoo(pred.GDP.xts,col = c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;),plot.type = \u0026#34;single\u0026#34;) # 予測値と実績値を時系列プロット OLSの推計結果はfactor1（赤）とfactor2（青）が有意との結果。前回の投稿でも言及したように、factor1（赤）はリスクセンチメントを表していそうなので、係数の符号が負であることは頷ける。ただし、factor2（青）も符号が負なのではなぜなのか…。このファクターは生産年齢人口など経済の潜在能力を表していると思っていたのに。かなり謎。まあとりあえず予測に移りましょう。このモデルを使用したGDPの予測値と実績値の推移はいかのようになりました。直近の精度は悪くない？\nというか、これ完全に単位根の問題を無視してOLSしてしまっているな。ファクターもGDPも完全に単位根を持つけど念のため単位根検定をかけてみます。\nlibrary(tseries) adf.test(F_q$factor1) ## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor1\r## Dickey-Fuller = -2.832, Lag order = 2, p-value = 0.2554\r## alternative hypothesis: stationary\radf.test(F_q$factor2) ## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor2\r## Dickey-Fuller = -2.5433, Lag order = 2, p-value = 0.3654\r## alternative hypothesis: stationary\radf.test(F_q$factor3) ## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor3\r## Dickey-Fuller = -2.5795, Lag order = 2, p-value = 0.3516\r## alternative hypothesis: stationary\radf.test(GDP_q$GDP) ## ## Augmented Dickey-Fuller Test\r## ## data: GDP_q$GDP\r## Dickey-Fuller = -0.25589, Lag order = 3, p-value = 0.9867\r## alternative hypothesis: stationary\rはい。全部単位根もってました…。階差をとったのち、単位根検定を行います。\nGDP_q \u0026lt;- GDP_q %\u0026gt;% mutate(growth.rate=(GDP/lag(GDP)-1)*100) F_q \u0026lt;- F_q %\u0026gt;% mutate(f1.growth.rate=(factor1/lag(factor1)-1)*100, f2.growth.rate=(factor2/lag(factor2)-1)*100, f3.growth.rate=(factor3/lag(factor3)-1)*100) adf.test(GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)]) ## ## Augmented Dickey-Fuller Test\r## ## data: GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)]\r## Dickey-Fuller = -2.5299, Lag order = 3, p-value = 0.368\r## alternative hypothesis: stationary\radf.test(F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)]) ## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)]\r## Dickey-Fuller = -2.5185, Lag order = 2, p-value = 0.3748\r## alternative hypothesis: stationary\radf.test(F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)]) ## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)]\r## Dickey-Fuller = -2.498, Lag order = 2, p-value = 0.3827\r## alternative hypothesis: stationary\radf.test(F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)]) ## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)]\r## Dickey-Fuller = -3.3479, Lag order = 2, p-value = 0.08501\r## alternative hypothesis: stationary\rfactor1だけは5%有意水準で帰無仮説を棄却できない…。困りました。有意水準を10%ということにして、とりあえず階差でOLSしてみます。\ndataset.q \u0026lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),]) colnames(dataset.q) \u0026lt;- c(\u0026#34;publication\u0026#34;,\u0026#34;GDP\u0026#34;,\u0026#34;growth.rate\u0026#34;,\u0026#34;factor1\u0026#34;,\u0026#34;factor2\u0026#34;,\u0026#34;factor3\u0026#34;,\u0026#34;f1.growth.rate\u0026#34;,\u0026#34;f2.growth.rate\u0026#34;,\u0026#34;f3.growth.rate\u0026#34;) test1 \u0026lt;- lm(growth.rate~f1.growth.rate + f2.growth.rate + f3.growth.rate,data=dataset.q) summary(test1) ## ## Call:\r## lm(formula = growth.rate ~ f1.growth.rate + f2.growth.rate + ## f3.growth.rate, data = dataset.q)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.1822 -0.2944 -0.1403 0.3495 1.2200 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|)\r## (Intercept) -0.1900410 0.3956316 -0.480 0.648\r## f1.growth.rate -0.0138647 0.0122615 -1.131 0.301\r## f2.growth.rate -0.0044379 0.0036015 -1.232 0.264\r## f3.growth.rate -0.0007179 0.0006337 -1.133 0.300\r## ## Residual standard error: 0.7832 on 6 degrees of freedom\r## ( 1 個の観測値が欠損のため削除されました )\r## Multiple R-squared: 0.3447,\tAdjusted R-squared: 0.01705 ## F-statistic: 1.052 on 3 and 6 DF, p-value: 0.4357\r推計結果がわるくなりました…。予測値を計算し、実績値とプロットしてみます。\nout_of_sample1 \u0026lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成 test1.pred \u0026lt;- predict(test1, out_of_sample1, interval=\u0026#34;prediction\u0026#34;) pred1.GDP.xts \u0026lt;- xts(cbind(test1.pred[,1],out_of_sample1$growth.rate),order.by = out_of_sample1$publication) plot.zoo(pred1.GDP.xts,col = c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;),plot.type = \u0026#34;single\u0026#34;) # 予測値と実績値を時系列プロット ん～、これはやり直しですね。今日はここまでで勘弁してください…。\n","date":1531699200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1531699200,"objectID":"53a087908899411dab3858677c03ade5","permalink":"/post/post6/","publishdate":"2018-07-16T00:00:00Z","relpermalink":"/post/post6/","section":"post","summary":"前回集めた経済データをGiannone et al (2008)のマルチファクターモデルで推定し、四半期GDPを予測したいと思います。","tags":["R","カルマンフィルタ"],"title":"Gianonne et. al. (2008)のマルチファクターモデルで四半期GDPを予想してみた","type":"post"},{"authors":null,"categories":["マクロ経済学","Webスクレイピング"],"content":"おはこんばんちわ。\n最近、競馬ばっかりやってましたが、そろそろ本業のマクロの方もやらないとなということで今回は日次GDP推計に使用するデータを総務省が公開している統計ダッシュボードから取ってきました。 そもそも、前の記事では四半期GDP速報の精度が低いことをモチベーションに高頻度データを用いてより精度の高い予測値をはじき出すモデルを作れないかというテーマで研究を進めていました。しかし、先行研究を進めていくうちに、どうやら大規模な経済指標を利用することで日次で四半期GDPの予測値を計算することが可能であることが判明しました。しかも、精度も良い(米国ですが)ということで、なんとかこの方向で研究を進めていけないかということになりました。\n1. 先行研究と具体的にやりたいこと 先行研究\nGiannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標をいくつかのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しました。なお、ファクターの計算にはカルマンスムージングを用いています(詳しい推計方法は論文\u0026amp;Technical Appendixを参照)。理論的な定式化は無いのですが、なかなか当たります。そもそも私がこの研究に興味を持ったのは、以下の本を立ち読みした際に参考文献として出てきたからで、いよいよ運用機関などでも使用され始めるのかと思い、やっておこうと思った次第です。\n実践 金融データサイエンス 隠れた構造をあぶり出す6つのアプローチ とりあえずはGiannoneの日本版をやろうかなと思っています。実はこの後に、ファクターモデルとDSGEを組み合わせたモデルがありましてそこまで発展させたいなーなんて思っておりますが。とにかく、ファクターを計算するための経済統計が必要ですので、今回はそれを集めてきたというのがこの記事の趣旨です。\n2. 統計ダッシュボードからのデータの収集 政府や日銀が公表しているデータの一部は統計ダッシュボードから落とすことができます。これは総務省統計局が提供しているもので、これまで利用しにくかった経済統計をより身近に使用してもらおうというのが一応のコンセプトとなっています。似たものに総務省統計局が提供しているestatがありますが、日銀の公表データがなかったり、メールアドレスの登録が必要だったりと非常に使い勝手が悪いです(個人的感想)。ただ、estatにはestatapiというRパッケージがあり、データを整形するのは比較的容易であると言えます。今回、統計ダッシュボードを選択した理由はそうは言っても日銀のデータがないのはダメだろうという理由で、データの整形に関しては関数を組みました。 そもそも統計ダッシュボードは経済統計をグラフなどで見て楽しむ？ものですが、私のような研究をしたい者を対象にAPIを提供してくれています。取得できるデータは大きく分けて6つあります。\nやり方は簡単で、ベースのurlと欲しい統計のIDをGET関数で渡すことによって、データを取得することができます。公式にも以下のように書かれています。\n 基本的な使い方としては、まず①「統計メタ情報（系列）取得」で取得したいデータの[系列コード]を検索し、 その後⑥「統計データ取得」で[系列コード]を検索条件に指定し、その系列の情報を取得します。 （②③④⑤は補助的な情報として独立して取得できるようにしています。データのみ必要な場合は当該機能は不要です。） 具体的な使い方は、以下の「WebAPIの詳細仕様」に記載する[ベースURL]に検索条件となる[パラメータ]を\u0026quot;\u0026amp;\u0026ldquo;で連結し、HTTPリクエスト（GET）を送信することで目的のデータを取得できます。 各パラメータは「パラメータ名=値」のように名称と値を'=\u0026lsquo;で結合し、複数のパラメータを指定する場合は「パラメータ名=値\u0026amp;パラメータ名=値\u0026amp;…」のようにそれぞれのパラメータ指定を\u0026rsquo;\u0026amp;\u0026lsquo;で結合してください。 また、パラメータ値は必ずURLエンコード(文字コードUTF-8)してから結合してください。\n 今回も以下の文献を参考にデータを取ってきたいと思います。\nRによるスクレイピング入門 まず、最初にこのAPIからデータを取得し、得られた結果を分析しやすいように整形する関数を定義したいと思います。\nlibrary(httr) library(estatapi) library(dplyr) library(XML) library(stringr) library(xts) library(GGally) library(ggplot2) library(seasonal) library(dlm) library(vars) library(MASS) # 関数を定義 get_dashboard \u0026lt;- function(ID){ base_url \u0026lt;- \u0026#34;https://dashboard.e-stat.go.jp/api/1.0/JsonStat/getData?\u0026#34; res \u0026lt;- GET( url = base_url, query = list( IndicatorCode=ID ) ) result \u0026lt;- content(res) x \u0026lt;- result$link$item[[1]]$value x \u0026lt;- t(do.call(\u0026#34;data.frame\u0026#34;,x)) date_x \u0026lt;- result$link$item[[1]]$dimension$Time$category$label date_x \u0026lt;- t(do.call(\u0026#34;data.frame\u0026#34;,date_x)) date_x \u0026lt;- str_replace_all(date_x, pattern=\u0026#34;年\u0026#34;, replacement=\u0026#34;/\u0026#34;) date_x \u0026lt;- str_replace_all(date_x, pattern=\u0026#34;月\u0026#34;, replacement=\u0026#34;\u0026#34;) date_x \u0026lt;- as.Date(gsub(\u0026#34;([0-9]+)/([0-9]+)\u0026#34;, \u0026#34;\\\\1/\\\\2/1\u0026#34;, date_x)) date_x \u0026lt;- as.Date(date_x, format = \u0026#34;%m/%d/%Y\u0026#34;) date_x \u0026lt;- as.numeric(date_x) date_x \u0026lt;- as.Date(date_x, origin=\u0026#34;1970-01-01\u0026#34;) #x \u0026lt;- cbind(x,date_x) x \u0026lt;- data.frame(x) x[,1] \u0026lt;- as.character(x[,1])%\u0026gt;%as.numeric(x[,1]) colnames(x) \u0026lt;- c(result$link$item[[1]]$label) x \u0026lt;- x %\u0026gt;% mutate(\u0026#34;publication\u0026#34; = date_x) return(x) } まずベースのurlを定義しています。今回はデータが欲しいので⑥統計データのベースurlを使用します（ 参考）。次にベースurlと統計ID（IndicatorCode）をGET関数で渡し、結果を取得しています。統計IDについてはエクセルファイルで公開されています。得られた結果の中身（リスト形式）をresultに格納し、リストの深層にある原数値データ（value）をxに格納します。原数値データもリスト形式なので、それをdo.callでデータフレームに変換しています。次に、データ日付を取得します。resultの中を深くたどるとTime→category→labelというデータがあり、そこに日付データが保存されているので、それをdate_xに格納し、同じようにデータフレームへ変換します。データの仕様上、日付は「yyyy年mm月」になっていますが、これだとRは日付データとして読み取ってくれないので、str_replace_all等で変換したのち、Date型に変換しています。列名にデータ名（result→link→item[[1]]→label）をつけ、データ日付をxに追加したら完成です。 そのほか、data_connectという関数も定義しています。これはデータ系列によれば、たとえば推計方法の変更などで1980年～2005年の系列と2003年～2018年までの系列の2系列があるようなデータも存在し、この2系列を接続するための関数です。これは単純に接続しているだけなので、説明は省略します。\ndata_connect \u0026lt;- function(x){ a \u0026lt;- min(which(x[,ncol(x)] != \u0026#34;NA\u0026#34;)) b \u0026lt;- x[a,ncol(x)]/x[a,1] c \u0026lt;- x[1:a-1,1]*b return(c) } では、実際にデータを取得していきます。今回取得するデータは月次データとなっています。これは統計dashboardが月次以下のデータがとれないからです。なので、例えば日経平均などは月末の終値を引っ張っています。ただし、GDPは四半期データとなっています。さきほど定義したget_dashboardの使用方法は簡単で、引数に統計ダッシュボードで公開されている統計IDを入力するだけでデータが取れます。今回使用するデータを以下の表にまとめました。\n# データを取得 Nikkei \u0026lt;- get_dashboard(\u0026#34;0702020501000010010\u0026#34;) callrate \u0026lt;- get_dashboard(\u0026#34;0702020300000010010\u0026#34;) TOPIX \u0026lt;- get_dashboard(\u0026#34;0702020590000090010\u0026#34;) kikai \u0026lt;- get_dashboard(\u0026#34;0701030000000010010\u0026#34;) kigyo.bukka \u0026lt;- get_dashboard(\u0026#34;0703040300000090010\u0026#34;) money.stock1 \u0026lt;- get_dashboard(\u0026#34;0702010201000010030\u0026#34;) money.stock2 \u0026lt;- get_dashboard(\u0026#34;0702010202000010030\u0026#34;) money.stock \u0026lt;- dplyr::full_join(money.stock1,money.stock2,by=\u0026#34;publication\u0026#34;) c \u0026lt;- data_connect(money.stock) a \u0026lt;- min(which(money.stock[,ncol(money.stock)] != \u0026#34;NA\u0026#34;)) money.stock[1:a-1,ncol(money.stock)] \u0026lt;- c money.stock \u0026lt;- money.stock[,c(2,3)] cpi \u0026lt;- get_dashboard(\u0026#34;0703010401010090010\u0026#34;) export.price \u0026lt;- get_dashboard(\u0026#34;0703050301000090010\u0026#34;) import.price \u0026lt;- get_dashboard(\u0026#34;0703060301000090010\u0026#34;) import.price$`輸出物価指数（総平均）（円ベース）2015年基準` \u0026lt;- NULL public.expenditure1 \u0026lt;- get_dashboard(\u0026#34;0802020200000010010\u0026#34;) public.expenditure2 \u0026lt;- get_dashboard(\u0026#34;0802020201000010010\u0026#34;) public.expenditure \u0026lt;- dplyr::full_join(public.expenditure1,public.expenditure2,by=\u0026#34;publication\u0026#34;) c \u0026lt;- data_connect(public.expenditure) a \u0026lt;- min(which(public.expenditure[,ncol(public.expenditure)] != \u0026#34;NA\u0026#34;)) public.expenditure[1:a-1,ncol(public.expenditure)] \u0026lt;- c public.expenditure \u0026lt;- public.expenditure[,c(2,3)] export.service \u0026lt;- get_dashboard(\u0026#34;1601010101000010010\u0026#34;) working.population \u0026lt;- get_dashboard(\u0026#34;0201010010000010020\u0026#34;) yukoukyuujinn \u0026lt;- get_dashboard(\u0026#34;0301020001000010010\u0026#34;) hours_worked \u0026lt;- get_dashboard(\u0026#34;0302010000000010000\u0026#34;) nominal.wage \u0026lt;- get_dashboard(\u0026#34;0302020000000010000\u0026#34;) iip \u0026lt;- get_dashboard(\u0026#34;0502070101000090010\u0026#34;) shukka.shisu \u0026lt;- get_dashboard(\u0026#34;0502070102000090010\u0026#34;) zaiko.shisu \u0026lt;- get_dashboard(\u0026#34;0502070103000090010\u0026#34;) sanji.sangyo \u0026lt;- get_dashboard(\u0026#34;0603100100000090010\u0026#34;) retail.sells \u0026lt;- get_dashboard(\u0026#34;0601010201010010000\u0026#34;) GDP1 \u0026lt;- get_dashboard(\u0026#34;0705020101000010000\u0026#34;) GDP2 \u0026lt;- get_dashboard(\u0026#34;0705020301000010000\u0026#34;) GDP \u0026lt;- dplyr::full_join(GDP1,GDP2,by=\u0026#34;publication\u0026#34;) c \u0026lt;- data_connect(GDP) a \u0026lt;- min(which(GDP[,ncol(GDP)] != \u0026#34;NA\u0026#34;)) GDP[1:a-1,ncol(GDP)] \u0026lt;- c GDP \u0026lt;- GDP[,c(2,3)] yen \u0026lt;- get_dashboard(\u0026#34;0702020401000010010\u0026#34;) household.consumption \u0026lt;- get_dashboard(\u0026#34;0704010101000010001\u0026#34;) JGB10y \u0026lt;- get_dashboard(\u0026#34;0702020300000010020\u0026#34;) 今取得したデータは原数値系列のデータが多いので、それらは季節調整をかけます。なぜ季節調整済みのデータを取得しないのかというとそれらのデータは何故か極端にサンプル期間が短くなってしまうからです。ここらへんは使い勝手が悪いです。\n# 季節調整をかける Sys.setenv(X13_PATH = \u0026#34;C:\\\\Program Files\\\\WinX13\\\\x13as\u0026#34;) checkX13() seasoning \u0026lt;- function(data,i,start.y,start.m){ timeseries \u0026lt;- ts(data[,i],frequency = 12,start=c(start.y,start.m)) m \u0026lt;- seas(timeseries) summary(m$data) return(m$series$s11) } k \u0026lt;- seasoning(kikai,1,2005,4) kikai$`機械受注額（船舶・電力を除く民需）` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(kigyo.bukka,1,1960,1) kigyo.bukka$`国内企業物価指数（総平均）2015年基準` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(cpi,1,1970,1) cpi$`消費者物価指数（生鮮食品を除く総合）2015年基準` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(export.price,1,1960,1) export.price$`輸出物価指数（総平均）（円ベース）2015年基準` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(import.price,1,1960,1) import.price$`輸入物価指数（総平均）（円ベース）2015年基準` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(public.expenditure,2,2004,4) public.expenditure$公共工事受注額 \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(export.service,1,1996,1) export.service$`貿易・サービス収支` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(yukoukyuujinn,1,1963,1) yukoukyuujinn$有効求人倍率 \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(hours_worked,1,1990,1) hours_worked$総実労働時間 \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(nominal.wage,1,1990,1) nominal.wage$現金給与総額 \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(iip,1,1978,1) iip$`鉱工業生産指数　2010年基準` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(shukka.shisu,1,1990,1) shukka.shisu$`鉱工業出荷指数　2010年基準` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(zaiko.shisu,1,1990,1) zaiko.shisu$`鉱工業在庫指数　2010年基準` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(sanji.sangyo,1,1988,1) sanji.sangyo$`第３次産業活動指数　2010年基準` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(retail.sells,1,1980,1) retail.sells$`小売業販売額（名目）` \u0026lt;- as.numeric(k) k \u0026lt;- seasoning(household.consumption,1,2010,1) household.consumption$`二人以上の世帯　消費支出（除く住居等）` \u0026lt;- as.numeric(k) GDP.ts \u0026lt;- ts(GDP[,2],frequency = 4,start=c(1980,1)) m \u0026lt;- seas(GDP.ts) GDP$`国内総生産（支出側）（実質）2011年基準` \u0026lt;- m$series$s11 ここでは詳しく季節調整のかけ方は説明しません。x13arimaを使用しています。上述のコードを回す際はx13arimaがインストールされている必要があります。以下の記事を参考にしてください。\n[http://sinhrks.hatenablog.com/entry/2014/11/09/003632]\nでは、データ日付を基準に落としてきたデータを結合し、データセットを作成します。\n# データセットに結合 dataset \u0026lt;- dplyr::full_join(kigyo.bukka,callrate,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,kikai,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,Nikkei,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,money.stock,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,cpi,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,export.price,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,import.price,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,public.expenditure,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,export.service,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,working.population,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,yukoukyuujinn,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,hours_worked,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,nominal.wage,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,iip,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,shukka.shisu,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,zaiko.shisu,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,sanji.sangyo,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,retail.sells,by=\u0026#34;publication\u0026#34;) dataset \u0026lt;- dplyr::full_join(dataset,yen,by=\u0026#34;publication\u0026#34;) colnames(dataset) \u0026lt;- c(\u0026#34;DCGPI\u0026#34;,\u0026#34;publication\u0026#34;,\u0026#34;callrate\u0026#34;,\u0026#34;Machinery_Orders\u0026#34;, \u0026#34;Nikkei225\u0026#34;,\u0026#34;money_stock\u0026#34;,\u0026#34;CPI\u0026#34;,\u0026#34;export_price\u0026#34;, \u0026#34;import_price\u0026#34;,\u0026#34;public_works_order\u0026#34;, \u0026#34;trade_service\u0026#34;,\u0026#34;working_population\u0026#34;, \u0026#34;active_opening_ratio\u0026#34;,\u0026#34;hours_worked\u0026#34;, \u0026#34;wage\u0026#34;,\u0026#34;iip_production\u0026#34;,\u0026#34;iip_shipment\u0026#34;,\u0026#34;iip_inventory\u0026#34;, \u0026#34;ITIA\u0026#34;,\u0026#34;retail_sales\u0026#34;,\u0026#34;yen\u0026#34;) 最後に列名をつけています。datasetはそれぞれのデータの公表開始時期が異なるために大量のNAを含むデータフレームとなっているので、NAを削除するために最もデータの開始時期が遅い機械受注統計に合わせてデータセットを再構築します。\na \u0026lt;- min(which(dataset$Machinery_Orders != \u0026#34;NA\u0026#34;)) dataset1 \u0026lt;- dataset[a:nrow(dataset),] dataset1 \u0026lt;- na.omit(dataset1) rownames(dataset1) \u0026lt;- dataset1$publication dataset1 \u0026lt;- dataset1[,-2] dataset1.xts \u0026lt;- xts(dataset1,order.by = as.Date(rownames(dataset1))) これでとりあえずデータの収集は終わりました。\n3. 得られたデータを主成分分析にかけてみる 本格的な分析はまた今後にしたいのですが、データを集めるだけでは面白くないので、Gianonneらのように主成分分析を行いたいと思います。主成分分析をこれまでに学んだことのない方は以下を参考にしてください。個人的にはわかりやすいと思っています。\n主成分分析(Principal Component Analysis, PCA)～データセットの見える化・可視化といったらまずはこれ！～\nでは主成分分析を実行してみます。Rではprcomp関数を使用することで非常に簡単に主成分分析を行うことができます。\n# 主成分分析を実行 factor.pca \u0026lt;- prcomp(x=dataset1, cor = TRUE) # cor = TRUEでデータの基準化を自動で行ってくれる。 summary(factor.pca) ## Importance of components:\r## PC1 PC2 PC3 PC4 PC5 PC6\r## Standard deviation 1.855e+06 4.268e+05 1.777e+05 1.204e+05 3.967e+04 3733\r## Proportion of Variance 9.374e-01 4.963e-02 8.610e-03 3.950e-03 4.300e-04 0\r## Cumulative Proportion 9.374e-01 9.870e-01 9.956e-01 9.996e-01 1.000e+00 1\r## PC7 PC8 PC9 PC10 PC11 PC12 PC13 PC14 PC15\r## Standard deviation 1347 1063 134.4 4.338 3.66 2.088 1.757 0.8261 0.7207\r## Proportion of Variance 0 0 0.0 0.000 0.00 0.000 0.000 0.0000 0.0000\r## Cumulative Proportion 1 1 1.0 1.000 1.00 1.000 1.000 1.0000 1.0000\r## PC16 PC17 PC18 PC19 PC20 PC21\r## Standard deviation 0.5309 0.3752 0.2632 0.1085 0.01238 0.01038\r## Proportion of Variance 0.0000 0.0000 0.0000 0.0000 0.00000 0.00000\r## Cumulative Proportion 1.0000 1.0000 1.0000 1.0000 1.00000 1.00000\rscreeplot(factor.pca) pc.xts \u0026lt;- xts(factor.pca$x[,1:3], order.by = as.Date(rownames(dataset1))) plot.zoo(pc.xts,col=c(\u0026#34;red\u0026#34;,\u0026#34;blue\u0026#34;,\u0026#34;green\u0026#34;,\u0026#34;purple\u0026#34;,\u0026#34;yellow\u0026#34;),plot.type = \u0026#34;single\u0026#34;) # 主成分を時系列プロット 第3主成分まででデータの約80％が説明できる結果を得たので、第3主成分までのプロットをお見せします。第1主成分（赤）はトレンドを表しており、これから景気の趨勢を読み取れると思います。第2主成分（青）は景気循環項でしょうか。周期的な動きが読み取れます。第3主成分（緑）は2014年に大きく上昇していますが、それ以外は特に目立った動きはありません。謎ですね。とりあえず今日はこれまで。時系列データをトレンド項、循環項、誤差項に分解したような結果でした。次回はGianonne et. al.(2008)の日本版の再現を行いたいと思います。\n","date":1531526400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1531526400,"objectID":"0f37a544ce2716192b081086f7f67b23","permalink":"/post/post7/","publishdate":"2018-07-14T00:00:00Z","relpermalink":"/post/post7/","section":"post","summary":"Rでデータ集めをします。データ分析はデータ集めと前処理が7割を占めるといわれる中、データ集めを自動化すべくウェブスクレイピングを行いました。","tags":["R","API"],"title":"日次GDP推計に使用する経済統計を統計ダッシュボードから集めてみた","type":"post"},{"authors":null,"categories":["競馬","Webスクレイピング"],"content":"みなさん、おはこんばんにちは。\n競馬のレース結果を的中させるモデルを作ろうということで研究をはじめましたが、まずはデータを自分で取ってくるところからやろうとおもいます。どこからデータを取ってくるのかという点が重要になるわけですが、データ先としてはdatascisotistさんがまとめられた非常にわかりやすい記事があります。どこからデータが取れるのかというと大きく分けて二つで、①JRA提供のJRA-VANや電子競馬新聞でおなじみの？JRJDといったデータベース、②netkeiba、yahoo競馬とといった競馬情報サイト、となっています。②の場合は自分でコードを書き、クローリングを行う必要があります。今回は②を選択し、yahoo競馬のデータをクローリングで落としてきたいと思います。Rでクローリングを行うパッケージとしては、rvest, httr, XMLがありますが、今回は1番簡単に使えるrvestを用います。yahoo競馬では以下のように各レース結果が表にまとめられています（5月の日本ダービーの結果）。\nyahoo競馬\n各馬のざっくりとした特徴やレース結果（通過順位等含む）、オッズが掲載されています。とりあえず、このぐらい情報があれば良いのではないかと思います（オッズの情報はもう少し欲しいのですが）。ただ、今後は少しずつ必要になった情報を拡充していこうとも思っています。1986年までのレース結果が格納されており、全データ数は50万件を超えるのではないかと思っています。ただ、単勝オッズが利用できるのは1994年からのようなので今回は1994年から直近までのデータを落としてきます。今回のゴールは、このデータをcsvファイル or SQLに格納することです。\n1. Rvestとは Rvestとは、webスクレイピングパッケージの一種でdplyrでおなじみのHadley Wickhamさんによって作成されたパッケージです。たった数行でwebスクレイピングができる優れものとなっており、操作が非常に簡単であるのが特徴です。今回は以下の本を参考にしました。\nRによるスクレイピング入門 そもそも、htmlも大学1年生にやった程度でほとんど忘れていたのですが、この本はそこも非常にわかりやすく解説されており、非常に実践的な本だと思います。\n2. レース結果をスクレイピングしてみる 実際にyahoo競馬からデータを落としてみたいと思います。コードは以下のようになっています。ご留意頂きたいのはこのコードをそのまま使用してスクレイピングを行うことはご遠慮いただきたいという事です。webスクレイピングは高速でサイトにアクセスするため、サイトへの負荷が大きくなる可能性があります。スクレイピングを行う際は、時間を空けるコーディングするなどその点に留意をして行ってください（最悪訴えられる可能性がありますが、こちらは一切の責任を取りません）。\n# rvestによる競馬データのwebスクレイピング #install.packages(\u0026#34;rvest\u0026#34;) #if (!require(\u0026#34;pacman\u0026#34;)) install.packages(\u0026#34;pacman\u0026#34;) pacman::p_load(qdapRegex) library(rvest) library(stringr) library(dplyr) 使用するパッケージはqdapRegex、rvest、stringr、dplyrです。qdapRegexはカッコ内の文字を取り出すために使用しています。\nkeiba.yahoo \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp/schedule/list/2016/?month=\u0026#34;,k)) race_url \u0026lt;- keiba.yahoo %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) # 全urlを取得 # レース結果のをurlを取得 race_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026#34;result\u0026#34;)==1] # 「result」が含まれるurlを抽出 まず、read_htmlでyahoo競馬のレース結果一覧のhtml構造を引っ張ってきます（リンクは2016年1月の全レース）。ここで、kと出ているのは月を表し、k=1であれば2016年1月のレース結果を引っ張ってくるということです。keiba.yahooを覗いてみると以下のようにそのページ全体のhtml構造が格納されているのが分かります。\nkeiba.yahoo ## {html_document}\r## \u0026lt;html xmlns=\u0026quot;http://www.w3.org/1999/xhtml\u0026quot;\u0026gt;\r## [1] \u0026lt;head\u0026gt;\\n\u0026lt;!---京---\u0026gt;\u0026lt;meta http-equiv=\u0026quot;Content-Type\u0026quot; content=\u0026quot;text/html; cha ...\r## [2] \u0026lt;body\u0026gt;\\n\u0026lt;noscript\u0026gt;\\n \u0026lt;iframe src=\u0026quot;//b.yjtag.jp/iframe?c=QXMP4c3\u0026quot; width=\u0026quot; ...\rrace_urlにはyahoo.keibaのうちの2016年k月にあった全レース結果のリンクを格納しています。html_nodeとはhtml構造のうちどの要素を引っ張るかを指定し、それを引っ張る関数で、簡単に言えばほしいデータの住所を入力する関数であると認識しています（おそらく正しくない）。ここではa要素を引っ張ることにしています。注意すべきことは、html_nodeは欲しい情報をhtml形式で引っ張ることです。なので、テキストデータとしてリンクを保存するためにはhtml_attrを使用する必要があります。html_attrの引数として、リンク属性を表すhrefを渡しています。これでレース結果のurlが取れたと思いきや、実はこれでは他のリンクもとってしまっています。一番わかりやすいのが広告のリンクです。こういったリンクは除外する必要があります。レース結果のurlには\u0026quot;result\u0026quot;が含まれているので、この文字が入っている要素だけを抽出したのが一番最後のコードです。\nfor (i in 1:length(race_url)){ race1 \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,race_url[i])) # レース結果のurlを取得 # レース結果をスクレイピング race_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026#34;) %\u0026gt;% html_table() race_result \u0026lt;- do.call(\u0026#34;data.frame\u0026#34;,race_result) # リストをデータフレームに変更 colnames(race_result) \u0026lt;- c(\u0026#34;order\u0026#34;,\u0026#34;frame_number\u0026#34;,\u0026#34;horse_number\u0026#34;,\u0026#34;horse_name/age\u0026#34;,\u0026#34;time/margin\u0026#34;,\u0026#34;passing_rank/last_3F\u0026#34;,\u0026#34;jockey/weight\u0026#34;,\u0026#34;popularity/odds\u0026#34;,\u0026#34;trainer\u0026#34;) #　列名変更 さて、いよいよレース結果のスクレイピングを行います。さきほど取得したリンク先のhtml構造を一つ一つ取得し、その中で必要なテキスト情報を引っ張るという作業をRに実行させます（なのでループを使う）。race_1にはあるレース結果ページのhtml構造が格納されおり、race_resultにはその結果が入っています。html_nodesの引数に入っているxpathですが、これはXLMフォーマットのドキュメントから効率的に要素を抜き出す言語です。先ほど説明した住所のようなものと思っていただければ良いと思います。その横に書いてある//table[@id = 'raceScore']が住所です。これはwebブラウザから簡単に探すことができます。Firefoxの説明になりますが、ほかのブラウザでも同じような機能があると思います。スクレイプしたい画面でCtrl+Shift+Cを押すと下のような画面が表示されます。\nこのインスペクターの横のマークをクリックすると、カーソルで指した部分のhtml構造（住所）が表示されます。この場合だと、レース結果はtable属性のidがraceScoreの場所に格納されていることが分かります。なので、上のコードではxpath=のところにそれを記述しているのです。そして、レース結果は表（table）形式でドキュメント化されているので、html_tableでごっそりとスクレイプしました。基本的にリスト形式で返されるので、それをデータフレームに変換し、適当に列名をつけています。\n# 通過順位と上り3Fのタイム race_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026#34;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026#34;\\\\d{2}\\\\.\\\\d\u0026#34;))) race_result \u0026lt;- race_result[-6] # タイムと着差 race_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026#34;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026#34;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026#34;))) race_result \u0026lt;- race_result[-5] # 馬名、馬齢、馬体重 race_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;[ァ-ヴー・]+\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;\\\\d{3}\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026#34;))) race_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026#34;\u0026#34;) race_result \u0026lt;- race_result[-4] # ジョッキー race_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026#34;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026#34;))) race_result \u0026lt;- race_result[-4] # オッズと人気 race_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026#34;\\\\(.+\\\\)\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026#34;\\\\d+[^(\\\\d+.\\\\d)]\u0026#34;))) race_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026#34;\u0026#34;) race_result \u0026lt;- race_result[-4] ここまででデータは取得できたわけなのですが、そのデータは綺麗なものにはなっていません。 上のコードでは、その整形作業を行っています。現在、取得したデータは以下のようになっています。\nhead(race_result) {\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"order\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"frame_number\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"horse_number\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"trainer\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"passing_rank\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"last_3F\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"time\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"margin\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_name\"],\"name\":[9],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_age\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight\"],\"name\":[11],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight_change\"],\"name\":[12],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"jockey\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"odds\"],\"name\":[14],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"popularity\"],\"name\":[15],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_date\"],\"name\":[16],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_name\"],\"name\":[17],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"2\",\"3\":\"2\",\"4\":\"河野 通文\",\"5\":\"06-07-06-06\",\"6\":\"35.1\",\"7\":\"3.19.7\",\"8\":\"character(0)\",\"9\":\"センゴクシルバー\",\"10\":\"牡6\",\"11\":\"478\",\"12\":\"+2\",\"13\":\"田中 勝春\",\"14\":\"2.3\",\"15\":\"1\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"1\"},{\"1\":\"2\",\"2\":\"6\",\"3\":\"9\",\"4\":\"武 邦彦\",\"5\":\"06-05-04-04\",\"6\":\"35.7\",\"7\":\"3.19.9\",\"8\":\"1 1/4馬身\",\"9\":\"ジャムシード\",\"10\":\"牡6\",\"11\":\"480\",\"12\":\"0\",\"13\":\"柴田 政人\",\"14\":\"14\",\"15\":\"7\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"2\"},{\"1\":\"3\",\"2\":\"7\",\"3\":\"10\",\"4\":\"白井 寿昭\",\"5\":\"08-07-06-06\",\"6\":\"35.7\",\"7\":\"3.20.1\",\"8\":\"1 1/2馬身\",\"9\":\"ホクセツギンガ\",\"10\":\"牡6\",\"11\":\"500\",\"12\":\"+4\",\"13\":\"小屋敷 昭\",\"14\":\"7.5\",\"15\":\"3\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"3\"},{\"1\":\"4\",\"2\":\"7\",\"3\":\"11\",\"4\":\"矢野 進\",\"5\":\"03-03-03-02\",\"6\":\"36.3\",\"7\":\"3.20.4\",\"8\":\"1 3/4馬身\",\"9\":\"サマーワイン\",\"10\":\"牝5\",\"11\":\"422\",\"12\":\"-4\",\"13\":\"木幡 初広\",\"14\":\"32.6\",\"15\":\"9\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"1\",\"3\":\"1\",\"4\":\"秋山 史郎\",\"5\":\"12-12-12-12\",\"6\":\"35.6\",\"7\":\"3.20.5\",\"8\":\"1/2馬身\",\"9\":\"ダイワジェームス\",\"10\":\"牡6\",\"11\":\"472\",\"12\":\"+6\",\"13\":\"大塚 栄三郎\",\"14\":\"5.9\",\"15\":\"2\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"5\"},{\"1\":\"6\",\"2\":\"5\",\"3\":\"7\",\"4\":\"須貝 彦三\",\"5\":\"11-10-06-06\",\"6\":\"36.1\",\"7\":\"3.20.5\",\"8\":\"ハナ\",\"9\":\"シゲノランボー\",\"10\":\"牡7\",\"11\":\"438\",\"12\":\"-6\",\"13\":\"須貝 尚介\",\"14\":\"8.7\",\"15\":\"4\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rご覧のように、\\nが入っていたり、通過順位と上り3ハロンのタイムが一つのセルに入っていたりとこのままでは分析ができません。不要なものを取り除いたり、データを二つに分割する作業が必要になります。今回の記事ではこの部分について詳しくは説明しません。この部分は正規表現を駆使する必要がありますが、私自身全く詳しくないからです。今回も手探りでやりました。\n# レース情報 race_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026#34;\\\\d+年\\\\d+月\\\\d+日\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026#34;\\\\s\u0026#34;,\u0026#34;\u0026#34;))) # ファイル格納 if (k ==1 \u0026amp;\u0026amp; i == 1){ dataset \u0026lt;- race_result } else { dataset \u0026lt;- rbind(dataset,race_result) }# if文の終わり } # iループの終わり write.csv(race_result,\u0026#34;race_result.csv\u0026#34;) 最後に、レース日時とレース名を抜き出し、データを一時的に格納するコードとcsvファイルに書き出すコードを書いて終了です。完成データセットは以下のような状態になっています。\nhead(dataset) {\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"order\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"frame_number\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"horse_number\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"trainer\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"passing_rank\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"last_3F\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"time\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"margin\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_name\"],\"name\":[9],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_age\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight\"],\"name\":[11],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight_change\"],\"name\":[12],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"jockey\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"odds\"],\"name\":[14],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"popularity\"],\"name\":[15],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_date\"],\"name\":[16],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_name\"],\"name\":[17],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"8\",\"3\":\"11\",\"4\":\"森安 弘昭\",\"5\":\"01-01-01-01\",\"6\":\"35.7\",\"7\":\"2.00.7\",\"8\":\"character(0)\",\"9\":\"ヒダカハヤト\",\"10\":\"牡8\",\"11\":\"488\",\"12\":\"-2\",\"13\":\"大塚 栄三郎\",\"14\":\"29\",\"15\":\"10\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"1\"},{\"1\":\"2\",\"2\":\"5\",\"3\":\"6\",\"4\":\"矢野 進\",\"5\":\"06-06-04-03\",\"6\":\"35.3\",\"7\":\"2.00.8\",\"8\":\"3/4馬身\",\"9\":\"ステージチャンプ\",\"10\":\"牡5\",\"11\":\"462\",\"12\":\"+14\",\"13\":\"岡部 幸雄\",\"14\":\"3.2\",\"15\":\"1\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"2\"},{\"1\":\"3\",\"2\":\"4\",\"3\":\"4\",\"4\":\"新関 力\",\"5\":\"03-03-02-02\",\"6\":\"35.8\",\"7\":\"2.01.1\",\"8\":\"1 3/4馬身\",\"9\":\"マキノトウショウ\",\"10\":\"牡5\",\"11\":\"502\",\"12\":\"+6\",\"13\":\"的場 均\",\"14\":\"6.9\",\"15\":\"4\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"3\"},{\"1\":\"4\",\"2\":\"8\",\"3\":\"12\",\"4\":\"大和田 稔\",\"5\":\"02-02-03-03\",\"6\":\"35.7\",\"7\":\"2.01.1\",\"8\":\"ハナ\",\"9\":\"ペガサス\",\"10\":\"牡5\",\"11\":\"464\",\"12\":\"+4\",\"13\":\"安田 富男\",\"14\":\"5.7\",\"15\":\"2\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"7\",\"3\":\"9\",\"4\":\"田中 和夫\",\"5\":\"07-07-07-05\",\"6\":\"35.8\",\"7\":\"2.01.6\",\"8\":\"3馬身\",\"9\":\"シャマードシンボリ\",\"10\":\"牡7\",\"11\":\"520\",\"12\":\"+4\",\"13\":\"田中 剛\",\"14\":\"29.6\",\"15\":\"12\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"5\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"3\",\"4\":\"中島 敏文\",\"5\":\"09-10-10-09\",\"6\":\"35.5\",\"7\":\"2.01.7\",\"8\":\"3/4馬身\",\"9\":\"モンタミール\",\"10\":\"牡7\",\"11\":\"474\",\"12\":\"+4\",\"13\":\"蓑田 早人\",\"14\":\"10.4\",\"15\":\"6\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r以上です。次回はこのデータセットを使用して、分析を行っていきます。次回までには1994年からのデータを全てスクレイピングしてきます。\n【追記（2018/6/10）】\n上述したスクリプトを用いて、スクレイピングを行ったところエラーが出ました。どうやらレース結果の中には強風などで中止になったものも含まれているらしく、そこでエラーが出る様子（race_resultがcharacter(0)になってしまう）。なので、この部分を修正したスクリプトを以下で公開しておきます。こちらは私の PC環境では正常に作動しています。\n# rvestによる競馬データのwebスクレイピング #install.packages(\u0026#34;rvest\u0026#34;) #if (!require(\u0026#34;pacman\u0026#34;)) install.packages(\u0026#34;pacman\u0026#34;) install.packages(\u0026#34;beepr\u0026#34;) pacman::p_load(qdapRegex) library(rvest) library(stringr) library(dplyr) library(beepr) # pathの設定 setwd(\u0026#34;C:/Users/assiy/Dropbox/競馬統計解析\u0026#34;) for(year in 1994:2018){ # yahoo競馬のレース結果一覧ページの取得 for (k in 1:12){ keiba.yahoo \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp/schedule/list/\u0026#34;, year,\u0026#34;/?month=\u0026#34;,k)) race_url \u0026lt;- keiba.yahoo %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) # 全urlを取得 # レース結果のをurlを取得 race_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026#34;result\u0026#34;)==1] # 「result」が含まれるurlを抽出 for (i in 1:length(race_url)){ Sys.sleep(10) print(str_c(\u0026#34;現在、\u0026#34;, year, \u0026#34;年\u0026#34;, k, \u0026#34;月\u0026#34;, i,\u0026#34;番目のレースの保存中です\u0026#34;)) race1 \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,race_url[i])) # レース結果のurlを取得 # レースが中止でなければ処理を実行 if (identical(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@class = \u0026#39;resultAtt mgnBL fntSS\u0026#39;]\u0026#34;) %\u0026gt;% html_text(),character(0)) == TRUE){ # レース結果をスクレイピング race_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026#34;) %\u0026gt;% html_table() race_result \u0026lt;- do.call(\u0026#34;data.frame\u0026#34;,race_result) # リストをデータフレームに変更 colnames(race_result) \u0026lt;- c(\u0026#34;order\u0026#34;,\u0026#34;frame_number\u0026#34;,\u0026#34;horse_number\u0026#34;,\u0026#34;horse_name/age\u0026#34;,\u0026#34;time/margin\u0026#34;,\u0026#34;passing_rank/last_3F\u0026#34;,\u0026#34;jockey/weight\u0026#34;,\u0026#34;popularity/odds\u0026#34;,\u0026#34;trainer\u0026#34;) #　列名変更 # 通過順位と上り3Fのタイム race_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026#34;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026#34;\\\\d{2}\\\\.\\\\d\u0026#34;))) race_result \u0026lt;- race_result[-6] # タイムと着差 race_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026#34;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026#34;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026#34;))) race_result \u0026lt;- race_result[-5] # 馬名、馬齢、馬体重 race_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;[ァ-ヴー・]+\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;\\\\d{3}\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026#34;))) race_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026#34;\u0026#34;) race_result \u0026lt;- race_result[-4] # ジョッキー race_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026#34;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026#34;))) race_result \u0026lt;- race_result[-4] # オッズと人気 race_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026#34;\\\\(.+\\\\)\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026#34;\\\\d+[^(\\\\d+.\\\\d)]\u0026#34;))) race_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026#34;\u0026#34;) race_result \u0026lt;- race_result[-4] # レース情報 race_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026#34;\\\\d+年\\\\d+月\\\\d+日\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026#34;\\\\s\u0026#34;,\u0026#34;\u0026#34;))) ## ファイル貯めるのかく if (k == 1 \u0026amp;\u0026amp; i == 1 \u0026amp;\u0026amp; year == 1994){ dataset \u0026lt;- race_result } else { dataset \u0026lt;- rbind(dataset,race_result) } # if文2の終わり } # if文1の終わり } # iループの終わり } # kループの終わり beep() } # yearループの終わり write.csv(dataset,\u0026#34;race_result.csv\u0026#34;, row.names = FALSE) これを回すのに16時間かかりました（笑）データ数は想定していたよりは少なく、97939になりました。\n","date":1526688000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1528588800,"objectID":"6b14675d44e331e53c790a5322187755","permalink":"/post/post9/","publishdate":"2018-05-19T00:00:00Z","relpermalink":"/post/post9/","section":"post","summary":"今、競馬×データサイエンスが熱いです。ウマナリティクスなるものがあり、これまでのレース結果からなんらかのモデルを作成し、順位予想や回収率を高める馬券購入方法を考えようとする人が一定数いるようです。今回は競馬をデータ解析するためのデータを取得します。rvestを用いて、ごりごりにクローリングを行いました。","tags":["前処理","R"],"title":"rvestでyahoo競馬にある過去のレース結果をクローリングしてみた","type":"post"},{"authors":null,"categories":[],"content":"どうもはじめまして。 東京にある資産運用会社に勤める新卒1年目の葦原彩人と申します。\n簡単に自己紹介をしたいと思います。 大学院卒の24歳です。 専攻はマクロ経済学で特にDSGEモデル、状態空間モデル、カルマンフィルタ、ベイズ推定、MCMCなんかをやっていました。 指導教官の研究サポートとして自然言語処理をやったこともあります。 もともと学者志望でしたが金銭的な問題で就職することになりました。 今は営業サポートの下働きをしています。\nこのブログは研究への興味関心を捨てることができない私が趣味として研究を進めていく際の備忘録という位置付けになるのだと思います。\n現在進めている研究は以下の２つです。\n 馬券版ファクターモデル 四半期GDP予測モデル  1は、馬券市場が株式市場よりも投機的に魅力のある市場であるという観点から、回収率100%超えを目指す馬券ポートフォリオを構築するモデルを作成できないかというものです。株式にはファーマフレンチのようなファクターモデルがありますが、それを応用して馬券版ファクターモデルなるものを作れないかと思っています。\n2は最近の四半期GDP速報の精度が低いという問題意識から、精度の高い予測モデルを新たに構築できないかというものです。特にこれまでのようなマクロ経済理論に基づいたモデルではなく、機械学習を用い、通常マクロ経済学の研究で使用しないようなデータを織り込んだモデルを作成する方向で研究を進めていきたいと思っています。\nまだ、研究は始めたばかりなのですが、 興味を持ってもらえるように頑張りたいと思います… では、最初はこの辺で。\nよろしくお願いします。\n","date":1526601600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1526601600,"objectID":"122591be03ff19b49a97c0378f58bb18","permalink":"/post/post4/","publishdate":"2018-05-18T00:00:00Z","relpermalink":"/post/post4/","section":"post","summary":"どうもはじめまして。このブログを始めるに当たってまずは自己紹介をしたいと思います。","tags":[],"title":"はじめまして","type":"post"},{"authors":["Ayato Ashihara"],"categories":null,"content":"\rClick the Cite button above to get Bibtex and Cite ME !!\r\r\r","date":1522454400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1522454400,"objectID":"8eb0b83c007669e1fcaec626e70937a6","permalink":"/publication/%E7%B4%80%E8%A6%812019/","publishdate":"2018-03-31T00:00:00Z","relpermalink":"/publication/%E7%B4%80%E8%A6%812019/","section":"publication","summary":"Japan  has  experienced  a  long-lasting  stagnation  since  the  early  1990s.  According  to  the  Reference Dates of Business Cycle, the Cabinet Office of Japan, there are four recession periods between 1987 and 2010,  and  three  of  them  are  considered  to  be  financially-related.  This  implies  that  the  stagnation  wastriggered by financial factors. Nonetheless, many studies using Dynamic Stochastic General Equilibrium models claim that a decline in Total Factor Productivity is the main driver of the stagnation. To resolve this contradiction,  this study estimates the Japanese economy by a New-Keynesian  DSGE  model augmented with  financial  friction  used  in  Christiano,  Motto  and  Rostagno  (2014),  where “risk  shock”  is newly incorporated into the model that refers to uncertainty in the financial market. According to our estimationresults, the estimated risk shock can explain the overall fluctuations of GDP and investment, and thus it isconsidered to be the main driver of the stagnation. We also find that it is highly correlated with the Business condition  Diffusion  Index,  the  Financial  Position  Diffusion  Index  and  the  Lending  Attitude  Index  of  Financial Institutions in Tankan released by the Bank of Japan. Therefore, we conclude that the estimated risk  shock  can  be  interpreted  as  the  firms’   distrust  toward  their  business  conditions,  and  it  delayed  theirinvestment decisions, then causing the prolonged economic contraction.","tags":"","title":"Does Financial Risk Explain Japan’s Great Stagnation?","type":"publication"},{"authors":["Ayato Ashihara"],"categories":null,"content":"\rClick the Cite button above to get Bibtex and Cite ME !!\r\r\r","date":1488844800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1488844800,"objectID":"e1ffa3282e6f7fc6edad66690c0b404c","permalink":"/publication/ael2018/","publishdate":"2017-03-07T00:00:00Z","relpermalink":"/publication/ael2018/","section":"publication","summary":"Using Japanese financial data that provide enough observations under the good and bad regimes of financial conditions, we find that fiscal multipliers are smaller in the bad regime than in the good regime.","tags":[""],"title":"Is fiscal expansion more effective in a financial crisis?","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]