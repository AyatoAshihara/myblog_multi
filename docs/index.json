[{"authors":null,"categories":null,"content":"奈良県出身の院卒4年目の28歳。専攻はマクロ経済（主にDynamic Stochastic General Equilibruim model）、時系列解析やテキストマイニングにも手を出していました。最近はデータラングリングに興味があり、データエンジニアとしてのキャリアを進むため、資格勉強中。現在は某資産運用会社にて運用企画グループに所属。広瀬すずより有村架純よりお姉ちゃんのほうがタイプ。\n","date":1522454400,"expirydate":-62135596800,"kind":"term","lang":"ja","lastmod":1522454400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/ayato-ashihara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ayato-ashihara/","section":"authors","summary":"奈良県出身の院卒4年目の28歳。専攻はマクロ経済（主にDyn","tags":null,"title":"Ayato Ashihara","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"ja","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["キャリア"],"content":"\r\r\r1. 退職しました\r2. 転職理由\r\r入社してエコノミストが求められていないことに気づいた\r転職理由の半分は結婚\r残りの半分は現職でのキャリアパスが想像できなかったから\r\r3. 次の会社・仕事\r4. 今後のブログ更新について\r\r\r1. 退職しました\rおはこんばんにちは。かなり久しぶりの更新になってしまいました。この間、2021年7月7日に結婚し、既婚者になりました。また、2021年4月に同じ部署内のチームを移動し、BPR・業務インフラ整備・金融規制対応などロジ周りの企画に役割が変わりました。そして、2022年2月28日に現職を退社し(最終出社は1月28日)、3月1日から関西の電子部品メーカーで働くことになりました。\n\r2. 転職理由\r入社してエコノミストが求められていないことに気づいた\rそもそも、現職の資産運用会社にはエコノミストを志望して入社しました。金銭的問題から博士課程後期の進学を断念し、大学院時代の研究内容に近い職種に就きたかったためです。バイサイドのエコノミストを志望したのは、単にファンドマネージャー相手の方が高度な分析をしても許されるんじゃないかという短絡的な理由からでした。\nしかし、入社してみると、希望の部署に配属になることは出来ませんでした。最初は機関投資家営業に配属になり、機関投資家向け(年金基金や金融法人)の顧客報告書をひたすら作成していました。社内DBから取得したファンドのパフォーマンスや残高を定められたフォーマットに転記し、ファンドマネージャーにコメントをもらう仕事です。2部署目(3年目)で運用企画に異動になりましたが、こちらでも(社内)資料を作成していました。企画の議論は秘匿性が高く基本的にシニア層のみで行われるため、若手は目的もはっきりとしないまま(ざっくりとした説明はありますが詳細は聞けない)、手を動かすことが求められていました。このように、資料作成要員として3年ほど過ごし、自分の成長として感じられたのは、ExcelやPowerPointに習熟していくことのみでした。次第に「市場価値など全くない人材になってるな」と危機感を覚えましたが、上司からは「まだ若いんだから」と真面目に受け止めてもらえませんでした。\n同時に、エコノミストの言動を参考にしているファンドマネージャーなどほぼ存在しないことも分かりました。エコノミストの付加価値はセミナーでの講演で、その講演でも自分で分析した内容を話すと言うよりはシンクタンクなどのレポートを要約して資料を再構成し、説明するという非生産的な仕事をしているようでした。自分の就職活動時の短絡的な発想を反省しつつ、「やはりマクロ経済学ではお金にならないんだな」ということを再確認しました。\n\r転職理由の半分は結婚\r先述の通り、2021年7月7日に結婚しました。妻とは大学生以来の付き合いであり、7年の交際期間がありました。どちらも地元が関西であり、子供には関西弁をしゃべらせたいという最大の目的から今後を考えて生活拠点を東京から関西に移したいと思うようになりました(他にもお互いの身寄りがいるとか理由はあります)。その時点では、エコノミスト志望でもなかったので、「もはや東京にいる意味もないな」と感じていました。\n2021年4月から配属になった新しい部署では、プロジェクトマネジメントを行う立場になり、ノウハウもない中で業務コンサルのような業務を行っていました。コンサル会社であるケンブリッジの書籍などを読みあさり、トライアンドエラーを繰り返す中で、プロジェクトが前進することに達成感を感じ、現場の方からの信頼も次第に得ることができました。自分たちの考えた企画案が具現化していくことにやりがいを感じ、「この仕事だったら業種も選ばないし、関西でも働けるな」と転職を考え出したのが8月頃でした。\n\r残りの半分は現職でのキャリアパスが想像できなかったから\r生活拠点を関西に移し、プロマネとして働くのは、現職でも遠隔地勤務という勤務形態を利用すれば可能でした。現に、同じ部署にその制度を利用している方がいたし、少なくとも2年はほぼリモートワークだったので、うちの部署なら可能だろうなというイメージも湧いていました。その選択をしなかったのは、現職ではこの職種でキャリアパスを歩んでいく事が想像できなかったからです。\n資産運用会社の花形はファンドマネージャーです。新卒入社の大半がファンドマネージャー志望で入社します。業界内で有名な方も複数存在し、運用者のキャリアパスはそれなりにイメージできると思います1。一方で、運用者以外のキャリアパスとなると、かなり不明瞭で各分野でプロフェッショナルとして仕事をしている方は限定的である印象を受けました。私が所属していた部署は、若手にとっては運用への通過点という位置づけで、この部署でのキャリアパスをイメージ出来る人は上司含めて存在していなかったと思います。部署にノウハウの蓄積がされておらず、シニアの仕事の進め方に疑問を抱くこともしばしばあり、目指したいと思える人物も存在しませんでした2。部署柄、他の部署の方と仕事をする機会も多かったですが、状況は他の部署でも同じようで、運用者志望ではなく社内コンサル業を志望する自分にとっては、周りの支援・理解なく1人前になる想像ができず、「転職するなら早いほうがよいな」と思いました[^3]。\n運用者志望の希望が全員叶うわけではなく、若手の早期退職者に悩む人事は多いと思いますが、一方で運用者以外の受け皿が用意されているとは言い難いと思います。少なくとも、フロントとそれ以外では温度感が全く違います(後者はシニアまでほぼオペレーション担当の印象でした)。運用会社には金融専門職(ファンドマネージャー、エコノミスト、アナリスト)はいても、ビジネスパーソンはいない。この問題は根が深いですね。\n\r\r3. 次の会社・仕事\r上記のような理由から、9月頃に転職活動をはじめました。複数のエージェントサービスを利用し、書類審査を本格的に出し始めたのが10月初旬ごろ、面接を受け出したのが10月半ばでした。そして、11月に内定を頂き、次の会社へ入社を決めました。この会社では、自部署でのキャリアステップの典型例を示してくださり、その中に自分の目指したいものがあったので入社を決めました。\n次の会社は関西の電子部品メーカーです。業務内容は現職で行っている業務とさほど変わらないものと認識しています。ただ、現職よりもデータを起点にした企画を行うということで、BIツールや統計処理なども守備範囲に入ってきそうです。これまでは必要な材料は現場へのヒアリングのみをベースとしており、Excelをこねくり回して出た信頼性がわからない結果をPowerPointスライドにするというやり方だったので、ひとつステップアップかなと思います。また、扱うデータも売上情報や受注情報などこれまでの金融時系列データとは異なってきそうです。ただ、ビジネスへの深い理解が必要という点は変わらないと思うので、その点は早くキャッチアップできるように頑張りたいと思います。\n\r4. 今後のブログ更新について\rこれまでの投稿は金融・経済に関連するものでしたが、今後は可視化・プロジェクトマネジメントなどに内容が移ってきそうです。これまでよりも、気軽に投稿することは増えてくると思うので、更新頻度を上げていきたいと思います！\n\r\rアシスタント→アナリスト→FMやオペレーション担当→投資判断者などでしょうか↩︎\n\r会議のファシリテーションにはかなり悩んでいました。本当であれば、会議が終わった後に細かいフィードバックを頂きたかったし、自分の課題はどこにあるのか指摘して頂きたかったですが、そのようなことが出来る人がいませんでした。「今のやり方のまま仕事を進めてくれたらいいから」しか言われなかった。↩︎\n\r\r\r","date":1643673600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1643673600,"objectID":"ca2b0ce893b5ba4fa305a0b9d2fd4587","permalink":"/post/post26/","publishdate":"2022-02-01T00:00:00Z","relpermalink":"/post/post26/","section":"post","summary":"今後に関するご報告です。","tags":[],"title":"転職します","type":"post"},{"authors":null,"categories":["単発"],"content":"\r\r\r1. ECONDBからのデータ取得\r\rECONDBとは？\rデータ取得方法\r\r2. World Bankからのデータ取得方法\r\r世界銀行から取得できるデータとは？\rデータの検索方法\rデータの取得方法\r\r3. Fama/French Data Libraryからのデータ取得方法\r\rFama/French Data Libraryで取れるデータとは\rデータ取得方法\r\r4. FERDからのデータ取得方法\r\rFREDで取得できるデータとは\rデータ取得方法\r\r5. OECDからのデータ取得方法\r6. Eurostatからのデータ取得方法\r\rEurostatから取得できるデータとは\rデータ取得方法\r\r最後に\r\r\rおはこんばんにちは。最近会社のPCにAnacondaを入れてもらいました。業務で使用することはないのですが、ワークショップで使用するので色々勉強しています。以前、Googleが提供しているEarth Engineから衛星画像を取得して解析した際にPythonを使用しましたが、今回はPythonから様々なデータが取得できるpandas_datareaderを使用したいと思います。pandas_datareaderでは以下のようなデータソースからデータが取得できます。\n\rTiingo\n\rIEX\n\rAlpha Vantage\n\rEnigma\n\rQuandl\n\rSt.Louis FED\n\rKenneth French’s data library\n\rWorld Bank\n\rOECD\n\rEurostat\n\rThrift Saving Plan\n\rNasdaq Trader symbol definitions\n\rStooq\n\rMOEX\n\rNaver Finance\n\r\rなお、このブログではRstuioとblogdownパッケージ、gitを組み合わせてgithub上に記事を投稿しています。ですが、Rstudioとreticulateパッケージのおかげで、pythonを使用した記事もrmdで作成し、htmlとして出力できています。ここでまず、reticulateパッケージを用いてconda仮想環境へ接続する方法を紹介しておきます。\nlibrary(reticulate)\rconda_path \u0026lt;- \u0026quot;C:\\\\Users\\\\hoge\\\\Anaconda3\\\\envs\\\\環境名\u0026quot;\ruse_condaenv(conda_path)\rこれで接続できます。conda_pathには仮想環境へのパスを入力してください。\nimport sys\rsys.version\r## \u0026#39;3.7.6 (default, Jan 8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\u0026#39;\r1. ECONDBからのデータ取得\rpandas_datareaderでは、ECOMDBからマクロ経済関連のデータを取得することができます。\nECONDBとは？\rECONDBは各国の主要マクロ経済データをdashboard形式で提供してくれるWebサイトで、またAPIをサポートしており、PythonやExcelにシームレスにデータを連係してくれます。\n\rデータ取得方法\rpandas_datareaderを用いた使用方法は以下の通りです。\n基本的な使用方法\rpandas_datareaderからデータモジュールをインポートすることから始めます。\nimport pandas_datareader.data as web\rEconDBからデータを取得するには、DataReaderメソッドを呼び出し、以下のようにdata_source引数に'econdb'と適当なqueryを渡せばよいです。\ndf = web.DataReader(query, data_source=\u0026#39;econdb\u0026#39;, **kwargs)\rクエリパラメータの形式は、取得するデータの種類によって異なります。\n\rクエリ指定方法\rデータはいくつかのデータセットに分割されます。データセットには、トピック、頻度、調査方法などの共通の特徴を抽出できるティッカーが付与されています。ユーザーは検索機能を使用してデータセットを探すことができます。UST_MSPDデータセットを例にしてみます。\nページに入ると、いくつかのフィルターがあり、特定のシリーズと特定のタイムフレームに選択を絞り込むことができます。適切なフィルタが設定された状態で、Exportドロップダウンボタンをクリックすると、選択したデータをエクスポートするための多くのオプションとフォーマットが表示されます。その中でも、Export to Pythonは、事前にフォーマットされたパラメータを持つコードの重要な部分を表示します。これをそのまま貼り付けてしまえばデータを取得できます。\nquery = \u0026quot;\u0026amp;\u0026quot;.join([\r\u0026quot;dataset=UST_MSPD\u0026quot;,\r\u0026quot;v=Category\u0026quot;,\r\u0026quot;h=TIME\u0026quot;,\r\u0026quot;from=2018-01-01\u0026quot;,\r\u0026quot;to=2019-12-31\u0026quot;\r])\rdf = web.DataReader(query, \u0026#39;econdb\u0026#39;)\rdf.head()\r## Category Bills ... United States Savings Securities\r## Holder Intragovernmental Holdings ... Totals\r## TIME_PERIOD ... ## 2015-12-01 2928.0 ... 171630\r## 2016-01-01 2642.0 ... 171160\r## 2016-02-01 3584.0 ... 170824\r## 2016-03-01 3582.0 ... 170370\r## 2016-04-01 4176.0 ... 169956\r## ## [5 rows x 51 columns]\r\r実践的な取得コード\rこんなこともできます。\nimport pandas as pd\rfrom matplotlib import pyplot as plt\rimport pandas_datareader.data as web\rfrom datetime import datetime\rimport seaborn as sns\rstart = datetime(1980,1,1)\rend = datetime(2019,12,31)\r# parameters for data from econdb\rcountry = [\u0026#39;US\u0026#39;,\u0026#39;UK\u0026#39;,\u0026#39;JP\u0026#39;,\u0026#39;EU\u0026#39;]\rindicator = [\u0026#39;RGDP\u0026#39;,\u0026#39;CPI\u0026#39;,\u0026#39;URATE\u0026#39;,\u0026#39;CA\u0026#39;,\u0026#39;HOU\u0026#39;,\u0026#39;POP\u0026#39;,\u0026#39;RETA\u0026#39;,\u0026#39;IP\u0026#39;]\r# Parse API from econdb\recon = pd.DataFrame()\rfor cnty in country:\rtemp2 = pd.DataFrame()\rfor idctr in indicator:\rtemp = web.DataReader(\u0026#39;ticker=\u0026#39; + idctr + cnty,\u0026#39;econdb\u0026#39;,start,end)\rtemp.columns = [idctr]\rtemp2 = pd.concat([temp2,temp],join=\u0026#39;outer\u0026#39;,axis=1)\rtemp2 = temp2.assign(kuni=cnty,kijyundate=temp2.index)\recon = pd.concat([econ,temp2],join=\u0026#39;outer\u0026#39;)\recon = econ.reset_index(drop=True)\recon.head()\r# Plot CPI for example\r## RGDP CPI URATE CA HOU POP RETA IP kuni kijyundate\r## 0 6837641.0 78.0 6.3 -10666.0 NaN 226554.0 NaN 53.50 US 1980-01-01\r## 1 NaN 79.0 6.3 NaN NaN 226753.0 NaN 53.51 US 1980-02-01\r## 2 NaN 80.1 6.3 NaN NaN 226955.0 NaN 53.33 US 1980-03-01\r## 3 6696753.0 80.9 6.9 9844.0 NaN 227156.0 NaN 52.23 US 1980-04-01\r## 4 NaN 81.7 7.5 NaN NaN 227387.0 NaN 50.96 US 1980-05-01\rsns.set\r## \u0026lt;function set at 0x000000002EA68558\u0026gt;\rsns.relplot(data=econ,x=\u0026#39;kijyundate\u0026#39;,y=\u0026#39;CPI\u0026#39;,hue=\u0026#39;kuni\u0026#39;,kind=\u0026#39;line\u0026#39;)\r## \u0026lt;seaborn.axisgrid.FacetGrid object at 0x00000000307EFE48\u0026gt;\r## ## C:\\Users\\aashi\\ANACON~1\\envs\\FINANC~1\\lib\\site-packages\\pandas\\plotting\\_matplotlib\\converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\r## ## To register the converters:\r## \u0026gt;\u0026gt;\u0026gt; from pandas.plotting import register_matplotlib_converters\r## \u0026gt;\u0026gt;\u0026gt; register_matplotlib_converters()\r## warnings.warn(msg, FutureWarning)\rplt.show()\r\r\r\r2. World Bankからのデータ取得方法\r世界銀行から取得できるデータとは？\r世界銀行は前身が国際復興開発銀行(IBRD)、国際開発協会(IDA)であることからもわかるように開発系のデータが取得できます。最近ではCOVID-19関連のデータも取得することができます。 pandas_datareaderでは、wb関数を使用することで、World Bank’s World Development Indicatorsと呼ばれる世界銀行の数千ものパネルデータに簡単にアクセスできます。\n\rデータの検索方法\r例えば、北米地域の国々の一人当たりの国内総生産をドルベースで比較したい場合は、search関数を使用します。\nfrom pandas_datareader import wb\rmatches = wb.search(\u0026#39;gdp.*capita.*const\u0026#39;)\rprint(matches.loc[:,[\u0026#39;id\u0026#39;,\u0026#39;name\u0026#39;]])\r## id name\r## 680 6.0.GDPpc_constant GDP per capita, PPP (constant 2011 internation...\r## 9266 NY.GDP.PCAP.KD GDP per capita (constant 2010 US$)\r## 9268 NY.GDP.PCAP.KN GDP per capita (constant LCU)\r## 9270 NY.GDP.PCAP.PP.KD GDP per capita, PPP (constant 2017 internation...\r## 9271 NY.GDP.PCAP.PP.KD.87 GDP per capita, PPP (constant 1987 internation...\rNY.GDP.PCAP.KDがそれに当たることがわかります。2010年のUSドルベースで実質化されているようです。\n\rデータの取得方法\rdownload関数でデータを取得します。\ndat = wb.download(indicator=\u0026#39;NY.GDP.PCAP.KD\u0026#39;, country=[\u0026#39;US\u0026#39;, \u0026#39;CA\u0026#39;, \u0026#39;MX\u0026#39;], start=2010, end=2018)\rprint(dat)\r## NY.GDP.PCAP.KD\r## country year ## Canada 2018 51476.200779\r## 2017 51170.475834\r## 2016 50193.750417\r## 2015 50262.027666\r## 2014 50306.944612\r## 2013 49397.523320\r## 2012 48785.936079\r## 2011 48464.496279\r## 2010 47448.013220\r## Mexico 2018 10403.540397\r## 2017 10301.357885\r## 2016 10205.795753\r## 2015 10037.201490\r## 2014 9839.050191\r## 2013 9693.722969\r## 2012 9690.869065\r## 2011 9477.887185\r## 2010 9271.398233\r## United States 2018 54659.198268\r## 2017 53382.764823\r## 2016 52555.518032\r## 2015 52116.738813\r## 2014 51028.824895\r## 2013 50171.237133\r## 2012 49603.253474\r## 2011 48866.053277\r## 2010 48467.515777\rpandasのdataframe形式でデータを取得できていることが分かります。年と国がindexになっていますね。\n\r\r3. Fama/French Data Libraryからのデータ取得方法\rFama/French Data Libraryで取れるデータとは\r金融関連データになりますが、有名なFama/Frechの3 Factor modelのデータセットがFama/French Data Libraryから取得できます。get_available_datasets関数は、利用可能なすべてのデータセットのリストを返します。\n\rデータ取得方法\rfrom pandas_datareader.famafrench import get_available_datasets\rlen(get_available_datasets())\r## 297\r利用可能なデータセットは297です。 データセットにどんなものがあるか、20個ほどサンプリングしてみます。\nimport random\rprint(random.sample(get_available_datasets(),20))\r## [\u0026#39;6_Portfolios_2x3\u0026#39;, \u0026#39;6_Portfolios_ME_INV_2x3_daily\u0026#39;, \u0026#39;Portfolios_Formed_on_ME\u0026#39;, \u0026#39;F-F_ST_Reversal_Factor\u0026#39;, \u0026#39;Portfolios_Formed_on_VAR\u0026#39;, \u0026#39;Developed_ex_US_6_Portfolios_ME_INV_Daily\u0026#39;, \u0026#39;Developed_ex_US_3_Factors_Daily\u0026#39;, \u0026#39;Developed_25_Portfolios_ME_INV\u0026#39;, \u0026#39;48_Industry_Portfolios_daily\u0026#39;, \u0026#39;Europe_5_Factors\u0026#39;, \u0026#39;Asia_Pacific_ex_Japan_32_Portfolios_ME_BE-ME_INV(TA)_2x4x4\u0026#39;, \u0026#39;Developed_3_Factors\u0026#39;, \u0026#39;Europe_3_Factors\u0026#39;, \u0026#39;Europe_25_Portfolios_ME_INV_Daily\u0026#39;, \u0026#39;6_Portfolios_ME_INV_2x3_Wout_Div\u0026#39;, \u0026#39;6_Portfolios_ME_CFP_2x3_Wout_Div\u0026#39;, \u0026#39;North_America_25_Portfolios_ME_INV\u0026#39;, \u0026#39;Developed_ex_US_Mom_Factor\u0026#39;, \u0026#39;6_Portfolios_ME_OP_2x3_Wout_Div\u0026#39;, \u0026#39;17_Industry_Portfolios_Wout_Div\u0026#39;]\r日本株のポートフォリオも存在します。\nds = web.DataReader(\u0026#39;5_Industry_Portfolios\u0026#39;, \u0026#39;famafrench\u0026#39;)\rprint(ds[\u0026#39;DESCR\u0026#39;])\r## 5 Industry Portfolios\r## ---------------------\r## ## This file was created by CMPT_IND_RETS using the 202009 CRSP database. It contains value- and equal-weighted returns for 5 industry portfolios. The portfolios are constructed at the end of June. The annual returns are from January to December. Missing data are indicated by -99.99 or -999. Copyright 2020 Kenneth R. French\r## ## 0 : Average Value Weighted Returns -- Monthly (59 rows x 5 cols)\r## 1 : Average Equal Weighted Returns -- Monthly (59 rows x 5 cols)\r## 2 : Average Value Weighted Returns -- Annual (5 rows x 5 cols)\r## 3 : Average Equal Weighted Returns -- Annual (5 rows x 5 cols)\r## 4 : Number of Firms in Portfolios (59 rows x 5 cols)\r## 5 : Average Firm Size (59 rows x 5 cols)\r## 6 : Sum of BE / Sum of ME (6 rows x 5 cols)\r## 7 : Value-Weighted Average of BE/ME (6 rows x 5 cols)\r5つ目がポートフォリオに含まれる銘柄数、1つ目がvalue weightedポートフォリオの月次リターンです。\nds[4].head()\r## Cnsmr Manuf HiTec Hlth Other\r## Date ## 2015-11 544 653 736 586 1109\r## 2015-12 542 649 730 583 1099\r## 2016-01 539 638 725 581 1091\r## 2016-02 537 635 718 576 1083\r## 2016-03 536 630 715 576 1074\rds[0].head()\r## Cnsmr Manuf HiTec Hlth Other\r## Date ## 2015-11 0.29 -0.08 0.57 0.72 1.17\r## 2015-12 0.13 -4.66 -2.59 0.38 -2.69\r## 2016-01 -3.30 -3.46 -5.05 -9.40 -8.24\r## 2016-02 0.51 1.39 -0.51 -1.06 -0.08\r## 2016-03 5.81 8.10 7.91 2.92 7.06\r\r\r4. FERDからのデータ取得方法\rFREDで取得できるデータとは\rFREDでは多種多様な経済統計データを取得することができます。サイトへ行くと、以下のように統計毎にページが存在します。この統計名の横についているCPIAUCSLがTickerになっており、これを渡すことで、データを取得することができます。\n\rデータ取得方法\r先ほど見たTickerをDataReader関数に渡し、データソースをfredとすることで、データを取得することができます。\nimport datetime\rstart = datetime.datetime(2010, 1, 1)\rend = datetime.datetime(2013, 1, 27)\rgdp = web.DataReader(\u0026#39;GDP\u0026#39;, \u0026#39;fred\u0026#39;, start, end)\rinflation = web.DataReader([\u0026#39;CPIAUCSL\u0026#39;, \u0026#39;CPILFESL\u0026#39;], \u0026#39;fred\u0026#39;, start, end)\rgdp.head()\r## GDP\r## DATE ## 2010-01-01 14721.350\r## 2010-04-01 14926.098\r## 2010-07-01 15079.917\r## 2010-10-01 15240.843\r## 2011-01-01 15285.828\rinflation.head()\r## CPIAUCSL CPILFESL\r## DATE ## 2010-01-01 217.488 220.633\r## 2010-02-01 217.281 220.731\r## 2010-03-01 217.353 220.783\r## 2010-04-01 217.403 220.822\r## 2010-05-01 217.290 220.962\r\r\r5. OECDからのデータ取得方法\rOECDは以前以下の記事で紹介しましたが、pandas_datareaderでも取得することができます。\nOECD.orgからマクロパネルデータをAPIで取得する\nただ、OECD dataset codeを指定するだけ1なので、pandasdmxよりは自由度が低いです。 あと、前回取得したMEI_ARCHIVEとか指定するとデータが多すぎて、エラーが出ます。OECDデータを取得するときには、国や期間など細かい指定のできるpandasdmxのほうが良いと個人的に思います。\nなお、使用方法はFREDと同様で、データソースにoecdを指定します。\ndf = web.DataReader(\u0026#39;TUD\u0026#39;, \u0026#39;oecd\u0026#39;)\rdf.head()\r## Country Hungary ... Germany ## Source Administrative data ... Survey data ## Series Employees Union members ... Union members Trade union density\r## Year ... ## 2016-01-01 NaN NaN ... NaN NaN\r## 2017-01-01 NaN NaN ... NaN NaN\r## 2018-01-01 NaN NaN ... NaN NaN\r## ## [3 rows x 216 columns]\r\r6. Eurostatからのデータ取得方法\rEurostatから取得できるデータとは\rEurostatは欧州連合の統計局で、主にEU地域のデータを取得することができます。データは以下のように多岐にわたっており、経済金融だけでなく農業や人口動態、輸送、環境等々多種多様なデータを取得することができます。\nIDをどのように取得すればよいのかですが、以下のページにて、取得したいデータを順々に掘り進めていくと黄色で色を付けたようなIDコードが出てきます。これで取得データのIDを特定します。\nただ、eurostatもOECDと同じくsdmxに対応しているため、pandasdmxのほうが使いやすいかもしれません。\n\rデータ取得方法\r一例として、 先ほど見たEmployment and activity by sex and age - annual dataを取得してみます。\ndf = web.DataReader(\u0026#39;lfsi_emp_a\u0026#39;,\u0026#39;eurostat\u0026#39;).unstack()\rdf.head()\r## UNIT AGE SEX INDIC_EM GEO FREQ TIME_PERIOD\r## Percentage of total population From 15 to 24 years Females Active population Austria Annual 2016-01-01 54.6\r## 2017-01-01 53.7\r## 2018-01-01 53.8\r## 2019-01-01 52.5\r## Belgium Annual 2016-01-01 26.2\r## dtype: float64\r\r\r最後に\rpandas_datareaderを使用して、様々なソースから多種多様なデータを取得しました。資産運用会社などで働いている方はbloombergやEIKONからデータを取得できるため、あまり魅力的に感じないかもしれませんが、個人で分析をしている方や定期的にデータを取得したい方は非常によいパッケージだと思います。自分自身、この新しいWebサイトにリニューアルしてから、週次や月次単位で経済分析を上げようかなと思っており、これらを使用して経済の定点観測をしたいなと思っているところです。皆さんも興味あるデータをpandas_datareaderで自動収集してみてください！\n\r\rサイトで統計を選び、export \u0026gt;- SDMX Queryとするとその統計のコードが見れます。↩︎\n\r\r\r","date":1604361600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1604361600,"objectID":"ad0c5e64c8bbcad9f168ce7d59542789","permalink":"/post/post15/","publishdate":"2020-11-03T00:00:00Z","relpermalink":"/post/post15/","section":"post","summary":"pandas_datareaderを使って色々なデータを取得してみました。","tags":["Python","前処理"],"title":"Pythonのpandas_datareaderから色々なデータを取得してみる","type":"post"},{"authors":null,"categories":["マクロ経済学","単発"],"content":"\r\r\r1.OECD.Stat Web API\r2.pandasdmx\r3.実装\r4.別件ですが。。。\r\r\rおはこんばんにちは。マクロ経済データを集める方法はいくつかありますが、各国のデータを集めるとなると一苦労です。ですが、OECDからAPI経由でデータ取得すれば面倒な処理を自動化できます。今日はその方法をご紹介します。\n1.OECD.Stat Web API\rOECD.orgではOECD.Statというサービスを提供しており、OECD加盟国と特定の非加盟国の様々な経済データが提供されています。WEBサイトに行けば手動でcsvデータをダウンロードすることもできますが、定期的にデータを取得し、分析する必要があるならばデータ取得処理を自動化したい衝動に駆られます。OECDはWeb APIを提供しているので、PythonやRさえ使えればこれを実現できます。\n\n以下は、現時点での特定のOECD REST SDMXインターフェースの実装詳細のリストです。\n\r匿名クエリのみがサポートされ、認証はありません。\n\r各レスポンスは1,000,000件のオブザベーションに制限されています。\n\rリクエストURLの最大長は1000文字です。\n\rクロスオリジンリクエストは、CORS ヘッダでサポートされています (CORSについての詳細は こちらを参照)。\n\rエラーは結果には返されませんが、HTTP ステータスコードとメッセージは Web サービスガイドラインに従って設定されます。\n\r存在しないデータセットが要求された場合は、401 Unauthorizedが返されます。\n\rREST クエリの source (または Agency ID) パラメータは必須ですが、「ALL」キーワードはサポートされています。\n\rバージョニングはサポートされていません: 常に最新の実装バージョンが使用されます。\n\rデータの並べ替えはサポートされていません。\n\rlastNObservationsパラメータはサポートされていません。\n\rdimensionAtObservation=AllDimensions が使用されている場合でも、観測は時系列 (またはインポート固有) の順序に従います。\n\r現時点では、参照メタデータの検索はサポートされていません。\n\r\r\r2.pandasdmx\rWeb APIはsdmx-jsonという形式で提供されます。Pythonではこれを使用するための便利なパッケージが存在します。それが**pandasdmx**です。データをダウンロードする方法は以下の通りです。\npandasdmxをimportし、Requestメソッドに引数として’OECD’を渡し、api.Requestオブジェクトを作成する。\r作成したapi.Requestオブジェクトのdataメソッドにクエリ条件を渡し、OECD.orgからsdmx-json形式のデータをダウンロードする。\rダウンロードしたデータをto_pandas()メソッドでpandasデータフレームへ整形する。\r\r\r3.実装\rでは、実際にやってみましょう。取得するのは、「**Revisions Analysis Dataset -- Infra-annual Economic Indicators**」というデータセットです。OECDのデータセットの一つであるMonthly Ecnomic Indicator(MEI)の修正を含む全てのデータにアクセスしているので、主要な経済変数(国内総生産とその支出項目、鉱工業生産と建設生産指数、国際収支、複合主要指標、消費者物価指数、小売取引高、失業率、就業者数、時間当たり賃金、貨マネーサプライ、貿易統計など)について、初出時の速報データから修正が加えられた確報データまで確認することができます。このデータセットでは、1999年2月から毎月の間隔で、過去に主要経済指標データベースで分析可能だったデータのスナップショットが提供されています。つまり、各時点で入手可能なデータに基づく、予測モデルの構築ができるデータセットになっています。最新のデータは有用ですが速報値なので不確実性がつきまといます。バックテストを行う際にはこの状況が再現できず実際の運用よりも良い環境で分析してしまうことが問題になったりします。いわゆるJagged edge問題です。このデータセットでは実運用の状況が再現できるため非常に有用であると思います。今回は以下のデータ項目を取得します。\n\r\r統計概要\r統計ID\r頻度\r\r\r\rGDP\r101\r四半期\r\r鉱工業生産指数\r201\r月次\r\r小売業取引高\r202\r月次\r\rマネーサプライ - 広義流動性\r601\r月次\r\r貿易統計\r702+703\r月次\r\r経常収支\r701\r四半期\r\r就業者数\r502\r月次\r\r失業率\r501\r月次\r\r時間当たり賃金（製造業）\r503\r月次\r\r単位あたり労働コスト\r504\r四半期\r\r建築生産指数\r203\r月次\r\r\r\rまず、関数を定義します。引数はデータベースID、その他ID(国IDや統計ID)、開始地点、終了地点です。\nimport pandasdmx as sdmx\r## C:\\Users\\aashi\\Anaconda3\\lib\\site-packages\\pandasdmx\\remote.py:13: RuntimeWarning: optional dependency requests_cache is not installed; cache options to Session() have no effect\r## RuntimeWarning,\roecd = sdmx.Request(\u0026#39;OECD\u0026#39;)\rdef resp_OECD(dsname,dimensions,start,end):\rdim_args = [\u0026#39;+\u0026#39;.join(d) for d in dimensions]\rdim_str = \u0026#39;.\u0026#39;.join(dim_args)\rresp = oecd.data(resource_id=dsname, key=dim_str + \u0026quot;/all?startTime=\u0026quot; + start + \u0026quot;\u0026amp;endTime=\u0026quot; + end)\rdf = resp.to_pandas().reset_index()\rreturn(df)\rデータを取得する次元を指定します。以下では、①国、②統計項目、③入手時点、④頻度をタプルで指定しています。\ndimensions = ((\u0026#39;USA\u0026#39;,\u0026#39;JPN\u0026#39;,\u0026#39;GBR\u0026#39;,\u0026#39;FRA\u0026#39;,\u0026#39;DEU\u0026#39;,\u0026#39;ITA\u0026#39;,\u0026#39;CAN\u0026#39;,\u0026#39;NLD\u0026#39;,\u0026#39;BEL\u0026#39;,\u0026#39;SWE\u0026#39;,\u0026#39;CHE\u0026#39;),(\u0026#39;201\u0026#39;,\u0026#39;202\u0026#39;,\u0026#39;601\u0026#39;,\u0026#39;702\u0026#39;,\u0026#39;703\u0026#39;,\u0026#39;701\u0026#39;,\u0026#39;502\u0026#39;,\u0026#39;503\u0026#39;,\u0026#39;504\u0026#39;,\u0026#39;203\u0026#39;),(\u0026quot;202001\u0026quot;,\u0026quot;202002\u0026quot;,\u0026quot;202003\u0026quot;,\u0026quot;202004\u0026quot;,\u0026quot;202005\u0026quot;,\u0026quot;202006\u0026quot;,\u0026quot;202007\u0026quot;,\u0026quot;202008\u0026quot;),(\u0026quot;M\u0026quot;,\u0026quot;Q\u0026quot;))\r関数を実行します。\nresult = resp_OECD(\u0026#39;MEI_ARCHIVE\u0026#39;,dimensions,\u0026#39;2019-Q1\u0026#39;,\u0026#39;2020-Q2\u0026#39;)\rresult.count()\r## LOCATION 8266\r## VAR 8266\r## EDI 8266\r## FREQUENCY 8266\r## TIME_PERIOD 8266\r## value 8266\r## dtype: int64\rデータの最初数件を見てみます。\nresult.head()\r## LOCATION VAR EDI FREQUENCY TIME_PERIOD value\r## 0 BEL 201 202001 M 2019-01 112.5\r## 1 BEL 201 202001 M 2019-02 111.8\r## 2 BEL 201 202001 M 2019-03 109.9\r## 3 BEL 201 202001 M 2019-04 113.5\r## 4 BEL 201 202001 M 2019-05 112.1\rデータがTidyな形(Long型)で入っているのがわかります。一番右側のvalueが値として格納されており、その他インデックスは\n\rLOCATION - 国\n\rVAR - 統計項目\n\rEDI - 入手時点(MEI_ARCHIVEの場合)\n\rFREQUENCY - 頻度(月次、四半期等)\n\rTIME_PERIOD - 統計の基準時点\n\r\rとなっています。よって、EDIが異なる行で同じTIME_PERIODが存在します。例えば、上ではベルギー(BEL)の鉱工業生産指数(201)の2020/01時点で利用可能な2019-01~2019-05のデータが表示されています。可視化や回帰も行いやすいLongフォーマットでの提供なので非常にありがたいですね。鉱工業生産指数がアップデートされていく様子を可視化してみました。\nimport seaborn as sns\rimport matplotlib.pyplot as plt\rimport pandas as pd\rresult = result[result[\u0026#39;FREQUENCY\u0026#39;]==\u0026#39;M\u0026#39;]\rresult[\u0026#39;TIME_PERIOD\u0026#39;] = pd.to_datetime(result[\u0026#39;TIME_PERIOD\u0026#39;],format=\u0026#39;%Y-%m\u0026#39;)\rsns.relplot(data=result[lambda df: (df.VAR==\u0026#39;201\u0026#39;) \u0026amp; (pd.to_numeric(df.EDI) \u0026gt; 202004)],x=\u0026#39;TIME_PERIOD\u0026#39;,y=\u0026#39;value\u0026#39;,hue=\u0026#39;LOCATION\u0026#39;,kind=\u0026#39;line\u0026#39;,col=\u0026#39;EDI\u0026#39;)\r## \u0026lt;seaborn.axisgrid.FacetGrid object at 0x00000000316C0188\u0026gt;\rplt.show()\rコロナの経済的な被害が大きくなるにつれて折れ線グラフが落ち込んでいく様子が見て取れる一方、微妙にですが過去値についても速報値→確報値へと修正が行われています。また、国によって統計データの公表にラグがあることも分かります。ベルギーは最も公表が遅いようです。時間があるときに、このデータを使った簡単な予測モデルの分析を追記したいと思います。\n\r4.別件ですが。。。\rPython 3 エンジニア認定データ分析試験に合格しました。合格率70%だけあって、かなり簡単でしたがPythonを基礎から見返すいい機会になりました。今やっている業務ではデータ分析はおろかPythonやRを使う機会すらないので、転職も含めた可能性を考えています。とりあえず、以下の資格を今年度中に取得する予定で、金融にこだわらずにスキルを活かせるポストを探していこうと思います。ダイエットと同じで宣言して自分を追い込まないと。。。\nG検定\rOracle Database Master Silver SQL\rLinuc レベル 1\r基本情報技術者\rAWS 認定ソリューションアーキテクト - アソシエイト\r\r合格状況は都度ブログで報告していきたいと思います。\n\r","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1603065600,"objectID":"3fd599a45600780ad64608f7f529e396","permalink":"/post/post22/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/post/post22/","section":"post","summary":"OECD.orgのAPIを使って、各国のマクロ経済データを取得してみました。","tags":["Python","ガウス過程","前処理","Webスクレイピング","API"],"title":"OECD.orgからマクロパネルデータをAPIで取得する","type":"post"},{"authors":null,"categories":["単発","統計","プログラミング"],"content":"\r\r0. やりたいこと\r今回お見せするのは前述の通り、為替のTickデータを使った前処理(と解析)になります。主眼をRcppを用いた効率化に置いていますので詳しくは踏み入りませんが、やりたいことをざっくりと先に示しておきます。\nやりたいのは、JPY/USDレートの5分刻みリターンからJumpを検知することです。ここでのJumpとはそれまでと比べて為替レートがガクッと上昇(下落)した点です。日中為替レートは小刻みに動きますが、なにかイベントがあると大きく上昇(下落)します。どんなイベントがJumpを引き起こすのかは非常に興味深い点です。これを検証するにはまずJumpを検知する必要があるのです。 参考とするのは以下の論文です。\nSuzanne S. Lee \u0026amp; Per A. Mykland, 2008. “Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,” Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.\nCitationが204もある非常に評価されている論文です。推定方法を掻い摘んで説明します。まず、連続複利リターンを\\(d\\log S(t)\\) for \\(t\u0026gt;0\\)とします。ここで、\\(S(t)\\)は\\(t\\)時点での資産価格です。市場にJumpがない場合、\\(S(t)\\)は以下の確率過程に従うと仮定します。\r\\[\rd\\log S(t) = \\mu(t)dt + \\sigma(t)dW(t) \\tag{1}\r\\]\rここで、\\(W(t)\\)は標準ブラウン運動、\\(\\mu(t)\\)はドリフト項、\\(\\sigma(t)\\)はスポットボラティリティです。また、Jumpがあるとき、\\(S(t)\\)は\r\\[\rd\\log S(t) = \\mu(t)dt + \\sigma(t)dW(t) + Y(t)dJ(t) \\tag{2}\r\\]\rに従うと仮定します。ここで、\\(J(t)\\)は\\(W(t)\\)とは独立したカウント過程です。1\\(Y(t)\\)はジャンプのサイズを表現しており、予測可能な過程であるとします。\n次に、\\(S(t)\\)の対数リターンを考えます。それはつまり\\(\\log S(t_i)/S(t_{i-1})\\)ですが、これは正規分布\\(N(0,\\sigma(t_i))\\)に従います。2ここで、\\(t_{i-1}\\)から\\(t_{i}\\)にJumpがあった際の\\(t_i\\)時点の統計量\\(\\mathcal{L(i)}\\)を以下で定義します。\n\\[\r\\mathcal{L(i)} \\equiv \\frac{|\\log S(t_i)/S(t_{i-1})|}{\\hat{\\sigma}_{t_i}} \\tag{3}\r\\]\r上記は対数リターンの絶対値を単純に標準化したものですが、標準偏差の推定量には以下で定義される”Realized Bipower Variation”を使用しています。\r\\[\r\\hat{\\sigma}_{t_i} = \\frac{1}{K-2}\\sum_{j=i-K+2}^{i-2}|\\log S(t_j)/\\log S(t_{j-1})||\\log S(t_{j-1})/\\log S(t_{j-2})| \\tag{4}\r\\]\r\\(K\\)はWindowに含まれるサンプルサイズの数です。仮に5min刻みリターンを用い、2020/9/10 10:00にJumpが発生した場合、\\(K=270\\)としている場合は前日2020/9/9 11:30から2020/9/11 09:55までのサンプルを用いて計算することになります。やっていることは、リターンの絶対値をかけたものを足し合わせるということですが、これでJumpが生じた次の瞬間(つまり\\(t_{i+1}\\)とか）の推定値がJumpに影響されにくいようです。ちなみに\\(K=270\\)は5min刻みリターンの場合の推奨値と別の文献で紹介されています。\nこうして計算されたJump統計量\\(\\mathcal{L(i)}\\)をどのように統計的検定に用いてJumpを検出するかに話を移しましょう。これは確率変数である\\(\\mathcal{L(i)}\\)の最大値(こちらも確率変数)を考え、その分布から大きく逸脱した値を取った場合(95%点とか)、そのリターンをJumpとします。\r期間\\([t_{i-1},t_{i}]\\)にJumpがないとした場合、この期間の長さ\\(\\Delta=t_{i}-t_{i-1}\\)を\\(0\\)に近づけると、つまり\\(\\Delta\\rightarrow0\\)とすると、標準正規変数の絶対値の最大値は、ガンベル分布に収束します。皆さん大好き極値統計ですね。よって、Jumpは以下の条件が満たされた際に帰無仮説が棄却され、検出することができます。\r\\[\r\\mathcal{L(i)} \u0026gt; G^{-1}(1-\\alpha)S_{n} + C_{n} \\tag{5}\r\\]\rここで、\\(G^{-1}(1-\\alpha)\\)は標準ガンベル分布の\\((1-\\alpha)\\)分位関数です。\\(\\alpha=10%\\)だと2.25になります。また、\r\\[\rS_{n} = \\frac{1}{c(2\\log n)^{0.5}},~ \\\\\rC_{n} = \\frac{(2\\log n)^{0.5}}{c}-\\frac{\\log \\pi+\\log(\\log n)}{2c(2\\log n)^{0.5}}\r\\]\rです(導出はしませんが、1式と2式を使って証明できます)。ここで、\\(c=(2/\\pi)^{0.5}\\)で、\\(n\\)は推定に使用する総サンプルサイズです。 最終的に、\\(Jump_{t_i}\\)は\r\\[\rJump_{t_i} = \\log\\frac{S(t_i)}{S(t_{i-1})}×I(\\mathcal{L(i)} - G^{-1}(1-\\alpha)S_{n} + C_{n})\\tag{6}\r\\]\rで求められることになります。ここで、\\(I(・)\\)は中身が0より大きいと1、それ以外は0を返すIndicator関数です。\n\r1. データの読み込み\rでは、推定方法がわかったのでまずTickデータの読み込みをしましょう。データはQuantDataManagerからcsvを取得し、それを作業ディレクトリに保存しています。\nlibrary(magrittr)\r# Tick dataの読み込み\rstrPath \u0026lt;- r\u0026quot;(C:\\Users\\hogehoge\\JPYUSD_Tick_2011.csv)\u0026quot;\rJPYUSD \u0026lt;- readr::read_csv(strPath)\r関係ないんですが、最近Rを4.0.2へ上げました。4.0以上ではPythonでできた文字列のEscapeができるとうことで今までのストレスが解消されてかなりうれしいです。\rデータは以下のような感じで、日付の他にBid値、Ask値と取引量が格納されています。なお、ここでは2011年のTickを使用しています。東日本大震災の時のドル円を対象とするためです。\nsummary(JPYUSD)\r## DateTime Bid Ask Volume ## Min. :2011-01-03 07:00:00 Min. :75.57 Min. :75.58 Min. : 1.00 ## 1st Qu.:2011-03-30 15:09:23 1st Qu.:77.43 1st Qu.:77.44 1st Qu.: 2.00 ## Median :2011-06-15 14:00:09 Median :80.40 Median :80.42 Median : 2.00 ## Mean :2011-06-22 05:43:11 Mean :79.91 Mean :79.92 Mean : 2.55 ## 3rd Qu.:2011-09-09 13:54:51 3rd Qu.:81.93 3rd Qu.:81.94 3rd Qu.: 3.00 ## Max. :2011-12-30 06:59:59 Max. :85.52 Max. :85.54 Max. :90.00\rちなみに、DateTimeはUTC基準で日本時間だと2011/1/3 07:00:00から2011-12-30 06:59::59(米国時間2011-12-30 16:59:59)までを含んでいます。サンプルサイズは約1200万件です。\nNROW(JPYUSD)\r## [1] 11946621\r\r2. 前処理\rでは次にBidとAskから仲値を計算し、後でリターンを算出するために対数を取っておきます。\n# AskとBidの仲値を計算し、対数化(対数リターン算出用)\rJPYUSD \u0026lt;- JPYUSD %\u0026gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %\u0026gt;% dplyr::mutate(logMid = log(Mid))\r現状不規則に並んでいる取引データを5min刻みのリターンに整形します。やり方は、\r1. 1年間を5min毎に刻んだPOSIXctベクトルを作る。\r2. 1.を引数として渡すと、その5minのWindowのうち、最初と最後のサンプルから対数リターンを順々に計算する関数を作成する。\r3. 実行。\rという計画です。まず、1.のベクトルを作成します。\n# 5min刻みでのリターンを算出するためのPOSIXベクトルを作成(288×日数)\rstart \u0026lt;- as.POSIXct(\u0026quot;2011-01-02 22:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)\rend \u0026lt;- as.POSIXct(\u0026quot;2011-12-31 21:55:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)\rfrom \u0026lt;- seq(from=start,to=end,by=5*60)\rでは、2.に移ろうということなんですが、データが1200万件もあるとRでpurrr::mapとかapply属を使用したとしても、関数呼び出しに時間がかかって結構非効率だったりします。。。sapplyでやってみましたがなかなか処理が完了せず、強制終了しました。こういうときには、Rccpが便利です。Rはグラフや統計処理のための非常に便利な関数が多数ありますが、ユーザーで定義した関数の呼び出しを含む、大量の繰り返し処理を苦手とします(スクリプト言語なのでコンパイル言語よりはという意味です)。なので、繰り返し処理の部分だけ、C++で書いてしまって、それをRcppをつかってRの関数としてコンパイルし、実行。結果の集計や可視化、執筆はRで行うというフローが非常に効率的です。\rまた、RccpはRに似た違和感の少ない記述方法でC++を記述するのを助けてくれます。詳しいことは以下を見れば問題ないと思います。かなりまとまっていて控えめに言って神です。\nみんなのRcpp\nでは、2.にあたるコードを書いていきます。コーディングに当たってはネット上の記事を参考にしました。C++はRよりも歴史があるし、使用者も多いので知りたい情報はすぐ見つけられます。\n#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;algorithm\u0026gt;\rusing namespace Rcpp;\r//[[Rcpp::plugins(cpp11)]]\r// [[Rcpp::export]]\rDataFrame Rolling_r_cpp(\rDataFrame input, //（計測時刻time, 計測値data）のデータフレーム\rnewDatetimeVector from, //計算するタイミングの始点ベクトル\rdouble time_window = 5*60) //計算するwindow幅（秒）\r{ // 計測時刻と計測値をベクトルとして取り出す\rnewDatetimeVector time = input[\u0026quot;DateTime\u0026quot;]; // 今回は time は昇順にソートされているのが前提です。\rNumericVector data = input[\u0026quot;logMid\u0026quot;];\r// 計算するタイミングの終点ベクトル\rnewDatetimeVector to = from + time_window;\r// 計算する数\rR_xlen_t N = from.length();\r// 格納するベクトル\rNumericVector value(N);\r// ベクトル要素の位置をあらわすオブジェクト\rnewDatetimeVector::iterator begin = time.begin();\rnewDatetimeVector::iterator end = time.end();\rnewDatetimeVector::iterator p1 = begin;\rnewDatetimeVector::iterator p2 = begin;\r// window i についてループ\rfor(R_xlen_t i = 0; i \u0026lt; N; ++i){\r// Rcout \u0026lt;\u0026lt; \u0026quot;i=\u0026quot; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;;\rdouble f = from[i]; //windowの始点の時刻\rdouble t = f + time_window; //windowの終点の時刻\r// windowの終点が最初の計測時刻以前の時はNA、または\r// windowの始点が最後の計測時刻のより後の時はNA\rif(t \u0026lt;= *begin || f \u0026gt; *(end-1)){ value[i] = NA_REAL;\rcontinue;//次のループへ\r}\r// ベクトル time の位置 p1 以降の要素xから\r// 時刻がwindowの始点f「以降」である「最初の要素」の位置を p1 とする\rp1 = std::find_if(p1, end, [\u0026amp;f](double x){return f\u0026lt;=x;});\r// p1 = std::lower_bound(p1, end, f); //上と同義\r// ベクトル time の位置 p1 以降の要素xから\r// 時刻がwindowの終点t「より前」である「最後の要素」の位置を p2 とする\r// （下では、時刻がwindowの終点t「以降」である「最初の要素」の１つ前の位置、にすることで実現している’）\rp2 = std::find_if(p1, end, [\u0026amp;t](double x){return t\u0026lt;=x;}) - 1 ;\r// p2 = std::lower_bound(p1, end, t) - 1 ;//上と同義\r// 要素の位置p1,p2を、要素番号i1, i2に変換する\rR_xlen_t i1 = p1 - begin;\rR_xlen_t i2 = p2 - begin; // 要素番号の確認\r// C++は要素番号が0から始まるのでRに合わせるために1を足している\r// Rcout \u0026lt;\u0026lt; \u0026quot;i1 = \u0026quot; \u0026lt;\u0026lt; i1+1 \u0026lt;\u0026lt; \u0026quot; i2 = \u0026quot; \u0026lt;\u0026lt; i2+1 \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;;\r// 該当する範囲のデータについて計算する\rif(i1\u0026gt;i2) {\rvalue[i] = NA_REAL; // window内にデータがない場合\r} else { value[i] = data[i2] - data[i1];\r}\r// ↑を変更することで様々なwindow関数を作成できる\r}\r// 計算した時間と、値をデータフレームとして出力する\rDataFrame out =\rDataFrame::create(\rNamed(\u0026quot;from\u0026quot;, from),\rNamed(\u0026quot;r\u0026quot;, value*100));\rreturn out;\r}\rRcpp::sourceCppでコンパイルしたら、以下のようにRの関数として実行します。\nsystem.time(results \u0026lt;- Rolling_r_cpp(JPYUSD,from))\r## ユーザ システム 経過 ## 0.06 0.00 0.07\rはい。1200万件のデータの処理に1秒かかりません。便利ー。\nsummary(results)\r## from r ## Min. :2011-01-02 22:00:00 Min. :-1.823 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 ## Median :2011-07-03 09:57:30 Median : 0.000 ## Mean :2011-07-03 09:57:30 Mean : 0.000 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 ## Max. :2011-12-31 21:55:00 Max. : 2.880 ## NA\u0026#39;s :29977\r問題なく、リターンが計算されています。では、Realized Bipower Variationの計算に移りましょう。5min刻みの場合はWindowの長さは270が推奨でしたが、そこも引数として柔軟を持たせた作りにします。また、NAの処理についても丁寧に行います。\n#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;cmath\u0026gt;\rusing namespace Rcpp;\r//[[Rcpp::plugins(cpp11)]]\r// [[Rcpp::export]]\rfloat rbv_cpp(\rNumericVector x, // rbvを計算するリターンベクトル\rbool na_rm = true) // xにNAが含まれている場合、取り除いて計算するか\r{\r// 計算回数を取得\rR_xlen_t N = x.length();\r// 計算結果を入れる変数を定義\rfloat out = 0;\r// xの欠損有無を確認\rLogicalVector lg_NA = is_na(x);\r// xにNAが存在した場合、そのNAを除いて計算するかどうか\rif(any(lg_NA).is_true() and na_rm==FALSE){\rout = NA_REAL; // NAを計算結果として出力\r} else {\r// NAを除く場合\rif (any(lg_NA).is_true() and na_rm==TRUE){\rx[is_na(x)==TRUE] = 0.00; // NAに0を埋め、実質的に計算から除外する\r}\r// rbvの分子(総和)を計算\rfor(R_xlen_t i = 1; i \u0026lt; N; ++i){\rout = out + std::abs(x[i])*std::abs(x[i-1]);\r}\r// 平均値を計算し、ルートをとる\rlong denomi; //分母\rif(N-sum(lg_NA)-2\u0026gt;0){\rdenomi = N-sum(lg_NA)-2;\r} else {\rdenomi = 1;\r}\rout = out/denomi;\rout = std::sqrt(out);\r}\rreturn out;\r}\r// [[Rcpp::export]]\rDataFrame Rolling_rbv_cpp(\rDataFrame input, //（計測時刻time, 計測値data）のデータフレーム\rint K = 270, // 計算するRolling Window幅\rbool na_pad = false, // Window幅が足りないときにNAを返すか\rbool na_remove = false // Window幅の中にNAが存在した場合、除いて計算を行うか\r){\r// リターンベクトルとサンプル数を取り出す\rNumericVector data = input[\u0026quot;r\u0026quot;];\rR_xlen_t T = data.length();\r// 計算結果を格納するベクトルを準備\rNumericVector value(T);\r// Windows幅毎にRBVを計算し、格納する\rif(na_pad==TRUE){\rvalue[0] = NA_REAL; // NAを返す\rvalue[1] = NA_REAL; // NAを返す\rvalue[2] = NA_REAL; // NAを返す\r} else {\rvalue[0] = 0; // 0を返す\rvalue[1] = 0; // 0を返す\rvalue[2] = 0; // NAを返す\r}\rfor(R_xlen_t t = 3; t \u0026lt; T; ++t){\r// Windows幅が足りるかどうかで処理を分岐\rif (t-K\u0026gt;=0){\rvalue[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // 通常計算を実行\r} else if(na_pad==FALSE) {\rvalue[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Kに満たない不完全なWidnows幅で計算を実行\r} else {\rvalue[t] = NA_REAL; // NAを返す\r}\r}\r// 計算した時間と値をデータフレームとして出力する\rDataFrame out =\rDataFrame::create(\rNamed(\u0026quot;from\u0026quot;, input[\u0026quot;from\u0026quot;]),\rNamed(\u0026quot;r\u0026quot;, data),\rNamed(\u0026quot;rbv\u0026quot;,value));\rreturn out;\r}\rでは、これもコンパイルし、Rで実行します。\nsystem.time(results \u0026lt;- results %\u0026gt;% Rolling_rbv_cpp(na_remove = FALSE))\r## ユーザ システム 経過 ## 1.00 0.38 1.45\rこちらも一瞬ですね。\n\r3. Jump統計量の計算\rでは、次に今計算したリターンと標準偏差から統計量\\(\\mathcal{L}_{t_i}\\)を計算しましょう。\n# 対数リターンの絶対値を標準化=Jump統計量\rresults \u0026lt;- results %\u0026gt;% dplyr::mutate(J=ifelse(rbv\u0026gt;0,abs(r)/rbv,NA))\r今こんな感じです。\nsummary(results)\r## from r rbv J ## Min. :2011-01-02 22:00:00 Min. :-1.823 Min. :0.00 Min. : 0.00 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 1st Qu.:0.02 1st Qu.: 0.28 ## Median :2011-07-03 09:57:30 Median : 0.000 Median :0.02 Median : 0.64 ## Mean :2011-07-03 09:57:30 Mean : 0.000 Mean :0.03 Mean : 0.93 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 3rd Qu.:0.03 3rd Qu.: 1.23 ## Max. :2011-12-31 21:55:00 Max. : 2.880 Max. :0.16 Max. :58.60 ## NA\u0026#39;s :29977 NA\u0026#39;s :44367 NA\u0026#39;s :44423\rでは、Jump検定に移りましょう。まず、必要な関数を定義しておきます。\n# Jump検定を計算するための定数\u0026amp;関数を準備\rc \u0026lt;- (2/pi)^0.5\rCn \u0026lt;- function(n){\rreturn((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5))\r}\rSn \u0026lt;- function(n){\r1/(c*(2*log(n))^0.5)\r}\rでは検定を行います。棄却されたサンプルは1、それ以外は0を返します。\n# Jump検定(10%)を実行(返り値はlogical)\rN \u0026lt;- NROW(results$J)\rresults \u0026lt;- results %\u0026gt;% dplyr::mutate(Jump = J \u0026gt; 2.25*Sn(N) + Cn(N))\rsummary(results)\r## from r rbv J ## Min. :2011-01-02 22:00:00 Min. :-1.823 Min. :0.00 Min. : 0.00 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 1st Qu.:0.02 1st Qu.: 0.28 ## Median :2011-07-03 09:57:30 Median : 0.000 Median :0.02 Median : 0.64 ## Mean :2011-07-03 09:57:30 Mean : 0.000 Mean :0.03 Mean : 0.93 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 3rd Qu.:0.03 3rd Qu.: 1.23 ## Max. :2011-12-31 21:55:00 Max. : 2.880 Max. :0.16 Max. :58.60 ## NA\u0026#39;s :29977 NA\u0026#39;s :44367 NA\u0026#39;s :44423 ## Jump ## Mode :logical ## FALSE:59864 ## TRUE :257 ## NA\u0026#39;s :44423 ## ## ## \r\r4. ggplot2を用いた可視化\r数値が計算できましたので可視化しましょう。2011/03/11の日中のJPY/USDの5min刻み対数リターンの推移とJumpを重ねてPlotします。ちなみに横軸は日本時間に修正しています。\n# 2011/03/11の東日本大震災発生時のJumpについてPlot\rresults %\u0026gt;% dplyr::filter(from \u0026gt;= as.POSIXct(\u0026quot;2011-03-11 00:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;),from \u0026lt; as.POSIXct(\u0026quot;2011-03-12 00:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x=from,y=r)) +\rggplot2::geom_path(linetype=3) +\rggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=\u0026quot;red\u0026quot;)) +\rggplot2::scale_x_datetime(date_breaks = \u0026quot;2 hours\u0026quot;, labels = scales::date_format(format=\u0026quot;%H:%M\u0026quot;,tz=\u0026quot;Asia/Tokyo\u0026quot;)) +\rggplot2::ggtitle(\u0026quot;JPY/USD Jumps within Tohoku earthquake on 2011-3-11\u0026quot;)\r## Warning: Removed 36 row(s) containing missing values (geom_path).\r## Warning: Removed 36 row(s) containing missing values (geom_path).\rここまで執筆するのに結構時間使っていて、今23:37なんで深い考察は控えますが、震災が発生したのが14:46:18ですから市場は震災直後即座に円安に反応したことが分かります。その後なぜか円高方向へ進み19:00にはピークになっています。安全資産の円とか言われますが、この時ばかりは不確実性の高まりからして安全じゃないだろと思いますが。。。\n\r5. まとめ\rRcppを使ったR分析の効率化について紹介しました。C++は愚直にコードを書いてもRより格段に処理が早いのでコーディングミスしにくい印象です。学術的な実装をやるときは内容が複雑になるのでこれはありがたいです。また、コンパイルエラーが起こってもRStudioを使っていればどこでコンパイルエラーが起こっているか手がかりをくれますのでその点でもストレスはないのでお勧めです。\n\r\r非負、整数、非減少の値を持つ確率過程のこと。↩︎\n\r平均はドリフト項の形状により必ずしも0にはなりませんが、今ドリフト項は十分小さい値を想定しているのでこの書き方にさせてください。論文ではより厳密に定義しています。↩︎\n\r\r\r","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1599696000,"objectID":"a2572500921f2507d6076f96260ed768","permalink":"/post/post21/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/post/post21/","section":"post","summary":"おはこんばんにちは。為替のTickデータを使った解析を行っているんですが、サンプルサイズが1年間のデータで1200万件にも及びます。メモリには乗るんですが、一つ一つに対して少し複雑な処理を行おうとすると処理にかなり時間がかかったりして非常に非効率です。今回はRcppパッケージを用いて、C++で書いた関数をR上の関数としてコンパイルし処理速度を高めるという方法を紹介したいと思います。","tags":["R","C++","前処理","金融"],"title":"Rcppでデータハンドリングを高速に行う(Tickデータの処理を事例に)","type":"post"},{"authors":null,"categories":["競馬"],"content":"\r\r\r1. Pre-trainedモデルのダウンロード\r2. 背景削除処理の実行\r3. CNNを用いた分析\r4. Shap値を用いた結果解釈\r5.まとめ\r\r\rおはこんばんにちは。前回、競走馬の馬体写真からCNNを用いて順位を予想するモデルを構築しました。結果は芳しくなく、特にshap値を用いた要因分析を行うと馬体よりも背景の厩舎に反応している様子が見えたりと分析の精緻化が必要となりました。今回はPytorchのPre-trainedモデルを用いて馬体写真から背景を切り出し、馬体のみとなった写真で再分析を行いたいと思います。\n1. Pre-trainedモデルのダウンロード\rコードはこちらのものを参考にしています。まず、パッケージをインストールします。\nimport numpy as np\rimport cv2\rimport matplotlib.pyplot as plt\rimport torch\rimport torchvision\rfrom torchvision import transforms\rimport glob\rfrom PIL import Image\rimport PIL\rimport os\r学習済みモデルのインストールを行います。\n#学習済みモデルをインストール\rdevice = torch.device(\u0026quot;cuda:0\u0026quot; if torch.cuda.is_available() else \u0026quot;cpu\u0026quot;)\rmodel = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)\rmodel = model.to(device)\rmodel.eval()\rどうやら全てのPre-trainedモデルは、同じ方法で正規化された形状\\(（N, 3, H, W）\\)の3チャンネルRGB画像のミニバッチを想定しているようです。ここで\\(N\\)は画像の数、\\(H\\)と\\(W\\)は少なくとも224ピクセルであることが想定されています。画像は、[0, 1]の範囲にスケーリングされ、その後、平均値＝[0.485, 0.456, 0.406]と標準値＝[0.229, 0.224, 0.225]を使用して正規化される必要があります。ということで、前処理を行う関数を定義します。\n#前処理\rpreprocess = transforms.Compose([\rtransforms.ToTensor(),\rtransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\r])\r\r2. 背景削除処理の実行\rでは、前回記事のseleniumを用いたコードで収集した画像を読み込み、1枚1枚背景削除処理を行っていきます。\n#フォルダを指定\rfolders = os.listdir(r\u0026quot;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026quot;)\r#それぞれのフォルダから画像を読み込み、Image関数を使用してRGB値ベクトル(numpy array)へ変換\rfor i, folder in enumerate(folders):\rfiles = glob.glob(\u0026quot;C:/Users/aashi/umanalytics/photo/image/\u0026quot; + folder + \u0026quot;/*.jpg\u0026quot;)\rindex = i\rfor k, file in enumerate(files):\rimg_array = np.fromfile(file, dtype=np.uint8)\rimg = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\rh,w,_ = img.shape\rinput_tensor = preprocess(img)\rinput_batch = input_tensor.unsqueeze(0).to(device)\rwith torch.no_grad():\routput = model(input_batch)[\u0026#39;out\u0026#39;][0]\routput_predictions = output.argmax(0)\rmask_array = output_predictions.byte().cpu().numpy()\rImage.fromarray(mask_array*255).save(r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\\mask.jpg\u0026#39;)\rmask = cv2.imread(r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\\mask.jpg\u0026#39;)\rbg = np.full_like(img,255)\rimg = cv2.multiply(img.astype(float), mask.astype(float)/255)\rbg = cv2.multiply(bg.astype(float), 1.0 - mask.astype(float)/255)\routImage = cv2.add(img, bg)\rImage.fromarray(outImage.astype(np.uint8)).convert(\u0026#39;L\u0026#39;).save(file)\r行っている処理はPre-trainedモデルで以下のようなmask画像を出力し、実際の画像のnumpy配列とmask画像を統合して、背景削除画像を生成しています。出力例は以下のような感じです。\nplt.gray()\rplt.figure(figsize=(20,20))\rplt.subplot(1,3,1)\rplt.imshow(img)\rplt.subplot(1,3,2)\rplt.imshow(mask)\rplt.subplot(1,3,3)\rplt.imshow(outImage)\rplt.show()\rplt.close()\rフォルダはこんな感じです。うまく処理できているものもあれば調教師の方が映ってしまっているのもありますね。物体を識別して、馬だけをmaskする方法もあるとは思いますがこのモデルでは物体のラベリングまではできないのでこのまま進みます。\nフォルダ\n\r\r3. CNNを用いた分析\rここからは前回記事と同じ内容です。結果のみ掲載します。\n## Test accuracy: 0.0\r## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000004AC69EC8\u0026gt;\r散々な結果になりました。\rまったく識別できていません。馬体写真には順位を予測するような特徴量はないんでしょうか。それともG1の出走馬ではバラツキがなく、識別不可能なのでしょうか。いずれいにせよ、ちょっと厳しそうです。\n\r4. Shap値を用いた結果解釈\r前回同様、どのように失敗したのかshap値を使って検証してみましょう。この画像を例として使います。\nplt.imshow(X_test[4])\rplt.show()\rplt.close()\rimport shap\rbackground = X_resampled[np.random.choice(X_resampled.shape[0],100,replace=False)]\re = shap.GradientExplainer(model,background)\rshap_values = e.shap_values(X_test[[4]])\rshap.image_plot(shap_values[1],X_test[[4]])\r前足から顔にかけてを評価しているようです。意外に臀部を評価している様子はありません。\r各層において画像のどの側面を捉えているかを可視化してみたいと思います。\nfrom keras import models\rlayer_outputs = [layer.output for layer in model.layers[:8]]\rlayer_names = []\rfor layer in model.layers[:8]:\rlayer_names.append(layer.name)\rimages_per_row = 16\ractivation_model = models.Model(inputs=model.input, outputs=layer_outputs)\ractivations = activation_model.predict(X_train[[0]])\rfor layer_name, layer_activation in zip(layer_names, activations):\rn_features = layer_activation.shape[-1]\rsize = layer_activation.shape[1]\rn_cols = n_features // images_per_row\rdisplay_grid = np.zeros((size * n_cols, images_per_row * size))\rfor col in range(n_cols):\rfor row in range(images_per_row):\rchannel_image = layer_activation[0,\r:, :,\rcol * images_per_row + row]\rchannel_image -= channel_image.mean()\rchannel_image /= channel_image.std()\rchannel_image *= 64\rchannel_image += 128\rchannel_image = np.clip(channel_image, 0, 255).astype(\u0026#39;uint8\u0026#39;)\rdisplay_grid[col * size : (col + 1) * size,\rrow * size : (row + 1) * size] = channel_image\rscale = 1. / size\rplt.figure(figsize=(scale * display_grid.shape[1],\rscale * display_grid.shape[0]))\rplt.title(layer_name)\rplt.grid(False)\rplt.imshow(display_grid, cmap=\u0026#39;viridis\u0026#39;)\rplt.show()\rplt.close()\rこっちはやっぱり分からないですね。\n\r5.まとめ\r厩舎背景を削除し、再実行してみましたが結果変わらずでした。PyTorchを使ったり、背景削除を行ういい経験にはなりましたが結果は伴わずということで馬体写真はいったんここでストップです。\n\r","date":1597190400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1597190400,"objectID":"5c27c7fa877e5e758b556cda7d09ec5b","permalink":"/post/post20/","publishdate":"2020-08-12T00:00:00Z","relpermalink":"/post/post20/","section":"post","summary":"前回の馬体モデルでは厩舎の背景が邪魔だったのでトリミングしました。","tags":["Python","機械学習","前処理"],"title":"PytorchのPre-trainedモデルで馬体写真の背景を自動トリミングする","type":"post"},{"authors":null,"categories":["単発","仕事関連","統計"],"content":"\r\r\r1. 今回のテーマ「バックテスト」とは？\r2. バックテストはオーバーフィットする\r3. シャープレシオが従う分布とは\r4. the minimum backtest lengthを導出してみる\r4. 終わりに\r\r\r1. 今回のテーマ「バックテスト」とは？\rバックテストは、アルゴリズムによる投資戦略のヒストリカルシミュレーションです。バックテストは、立案した投資戦略がある期間にわたって実行されていた場合に発生したであろう利益と損失をアルゴリズムを用いて計算します。その際、シャープレシオやインフォメーションレシオなどの投資戦略のパフォーマンスを評価する一般的な統計量が使用されています。投資家は通常、これらのバックテストの統計量を調査し、最高のパフォーマンスを発揮する投資(運用)戦略に資産配分を決定するため、資産運用会社は良好なパフォーマンスを血のにじむような回数のバックテストを試行錯誤し、資料を作ってプレゼンしたりするわけです。\n3倍3分法のバックテスト\n\r投資家の立場に立つなら、バックテストされた投資戦略のパフォーマンスについては、インサンプル(IS)とアウトオブサンプル(OOS)を区別することが重要です。ISのパフォーマンスは、投資戦略の設計に使用したサンプル（機械学習の文献では「学習期間」や「訓練セット」と呼ばれる物です）でシミュレートしたものです。一方、OOSパフォーマンスは、投資戦略の設計に使用されなかったサンプル（別名「テストセット」）でシミュレーションされたものです。バックテストは、そのパフォーマンスを持ってその投資戦略の有効性を占う物ですので、ISのパフォーマンスがOOSのパフォーマンスと一致している場合に再現性が担保され、現実的であるということができます。ただ、アウトサンプルの結果はこれからの結果であるので、バックテストを受け取った時点でそのバックテストが信頼に足るものか判断することは難しいです。hold-out法などで、以下のように学習データとテストデータを分け、OOSでのテストを行っているものもありますが、OOSの結果をフィードバックして戦略の改善ができる以上、純粋なアウトサンプルとは呼べません。\nですので、ファンドマネージャーから良い結果のバックテストを受け取った場合、そのシミュレーションがどれだけ現実的であるかをなんとかして評価することが非常に重要となります。また、ファンドマネージャーも自身のバックテスト結果が持つ不確実性を理解しておくことが重要です。今回はバックテストのシミュレーションの現実性をどのようにして評価するのか、再現性のあるバックテストを行うためには何に注意すれば良いのかを調べてみたいと思います。\n\r2. バックテストはオーバーフィットする\rBailey, Borwein, López de Prado and Zhu(2015)は、どのような金融時系列でも、バックテストのシミュレーションをオーバーフィット(過学習)させることが(比較的)簡単にできると主張しています。ここで、オーバーフィットとは、機械学習の概念であり，モデルが一般的な構造よりも特定の観察データ(ISデータ)にフォーカスしてしまう状況を表します。\nBailey et. al.(2015)では、この主張の一例として株式戦略のバックテスト結果が芳しくない状況が挙げられています。バックテストではその名の通り過去データを使用しているので、具体的に損失が発生している銘柄を特定することが可能で、その銘柄の推奨を削除するためにいくつかのパラメータを追加し、取引システムを設計することで、パフォーマンスを向上させることができるというわけです（「データ・スヌーピング」として知られているテクニック）。数回シミュレーションを繰り返えせば、特定のサンプルに存在するが、母集団の中では稀であるかもしれない特徴から利益を得る「最適なパラメータ」を導くことができます。\n機械学習の文献では、オーバーフィッティングの問題を対処するための膨大な研究の蓄積があります。ですが、Bailey et. al.(2015)は、機械学習の文脈で提案されている手法は一般的に複数の投資問題には適用できないと主張します。その理由は以下4点のようです。\n機械学習でオーバーフィッティングを防ぐ手法は、予測の説明力や質を評価するために、その事象が定義される領域において明示的な点推定と信頼区間を必要としますが、このような明確な予測を行う投資戦略はほとんどないため。\n\r例えば、「E-mini S\u0026amp;P500は、金曜日の終値で1標準偏差5ポイントで1,600前後になると予測されています」とはあまり言われず、むしろ「買い」または「強い買い」といった定性的な推奨が提供されることが一般的です。しかも、この予想は予測の有効期限も明示されず、なにか予期せぬ事象が発生した際に変更がなされます。一方、定量予測では金曜日の終値と明記されています。\r\r仮に特定の投資戦略が予測式に依存していたとしても、投資戦略の他の構成要素がオーバーフィットされている可能性がある。\n\r言い換えれば、単に予測式を調整する以外にも、投資戦略をオーバーフィットさせる方法はたくさんあるということです。\r\r回帰のオーバーフィットの方法はパラメトリックであり、金融の場合観察不可能なデータに関する多くの仮定を含むため。\n\rいくつかの手法は試行回数をコントロールしていないため。\n\r\rBailey et. al.(2015)では、バックテストのパフォーマンスが比較的低い投資戦略を特定するためには、比較的少ない試行回数が必要であることを示しています。ここでの試行回数とは試行錯誤の回数だと思ってください。また、試行回数に応じて必要とされるバックテストの期間であるthe minimum backtest length（MinBTL）を計算しています。この論文では、パフォーマンスを評価するために常にシャープレシオが使用されていますが、他のパフォーマンス指標にも応用できるそうです。その内容を見てみましょう。\n\r3. シャープレシオが従う分布とは\rMinBTLを導出するために、まずシャープレシオの(漸近)分布を導出します。そもそも、投資戦略の設計は、通常、特定のパターンが金融変数の将来値を予測するのに役立つかもしれないという事前知識または信念から始まります。例えば、さまざまな満期の債券の間にリードラグ効果を認識している場合は、イールドカーブが上昇した場合に均衡値への回帰に賭ける戦略を設計することができます。このモデルは、cointegration equation、ベクトル誤差補正モデル、確率微分方程式のシステムなどの形をとることが考えられます。\nこのようなモデル構成（または試行）の数は膨大であり、ファンドマネージャーは当然、戦略のパフォーマンスを最大化するものを選択したいと考え、そのためにヒストリカルシミュレーション（バックテスト）を行います(前述)。バックテストでは、最適なサンプルサイズ、シグナルの更新頻度、リスクサイジング、ストップロス、最大保有期間などなどを他の変数との兼ね合いの中で評価します。\nこの論文中でパフォーマンス評価の尺度として使用されるシャープレシオは、過去のリターンのサンプルに基づいて、戦略のパフォーマンスを評価する統計量で、BMに対する平均超過リターン/標準偏差(リスク)として定義されます。通常には、「リスク1標準偏差に対するリターン」と解釈され、資産クラスにもよりますが1を上回っていると非常に良い戦略であると見なせます。以下では、ある戦略の超過リターン\\(r_t\\)がi.i.d.の確率変数であり、正規分布に従うと仮定します。つまり、\\(r_t\\)の分布は\\(r_s(t\\neq s)\\)と独立であることを仮定しています。あまり現実的な仮定ではありませんが。。。\n\\[\rr_t \\sim \\mathcal{N}(\\mu,\\sigma^2)\r\\]\rここで、\\(\\mathcal{N}\\)は平均\\(\\mu\\)、分散\\(\\sigma^2\\)の正規分布を表しています。今、時点t~t-q+1の超過リターン\\(r_{t}(q)\\)を\n\\[\rr_{t}(q) \\equiv r_{t} + r_{t-1} + ... + r_{t-q+1}\r\\]\rと定義すると(複利部分を無視してます)、年率化されたシャープレシオは\n\\[\r\\begin{eqnarray}\rSR(q) \u0026amp;=\u0026amp; \\frac{E[r_{t}(q)]}{\\sqrt{Var(r_{t}(q))}}\\\\\r\u0026amp;=\u0026amp; \\frac{q\\mu}{\\sqrt{q}\\sigma}\\\\\r\u0026amp;=\u0026amp; \\frac{\\mu}{\\sigma}\\sqrt{q}\r\\end{eqnarray}\r\\]\rと表すことができます。ここで、\\(q\\)は年毎のリターンの数(頻度)です。例えば、日次リターンの場合\\(q=365\\)となります(閏年を除く)。\r\\(\\mu\\)と\\(\\sigma\\)は一般に未知ですので、\\(SR\\)の真値を知ることはできません。なので、\\(R_t\\)を標本リターン、リスクフリーレート\\(R^f\\)(定数)とすると、標本平均\\(\\hat{\\mu}=1/T\\sum_{t=1}^T R_{t}-R^f\\)と標本標準偏差\\(\\hat{\\sigma}=\\sqrt{1/T\\sum_{t=1}^{T}(R_{t}-\\hat{\\mu})}\\)を用いてシャープレシオの推定値を計算することになります(\\(T\\)はバックテストを行うサンプルサイズ)。\n\\[\r\\hat{SR}(q) = \\frac{\\hat{\\mu}}{\\hat{\\sigma}}\\sqrt{q}\r\\]\r必然的な結果として、\\(SR\\)の計算はかなりの推定誤差が伴う可能性が高くなります。では、本節の本題、\\(\\hat{SR}\\)の漸近分布を導出してみましょう。まず、\\(\\hat{\\mu}\\)と\\(\\hat{\\sigma}^2\\)の漸近分布はi.i.d.と\\(\\mu, \\sigma\\)が有限な値をとることから中心極限定理を適用することにより、\n\\[\r\\sqrt{T}\\hat{\\mu}\\sim^{a}\\mathcal{N}(\\mu,\\sigma^2), \\\\\r\\sqrt{T}\\hat{\\sigma}^2\\sim^a\\mathcal{N}(\\sigma^2,2\\sigma^4)\r\\]\rとなります。シャープレシオはこの\\(\\hat{\\mu}\\)と\\(\\hat{\\sigma}^2\\)から計算される確率変数であるので、この関数を\\(g(\\hat{{\\boldsymbol \\theta}})\\)と表しましょう。ここで、\\(\\hat{{\\boldsymbol \\theta}}=(\\hat{\\mu},\\hat{\\sigma}^2)\u0026#39;\\)です。今、i.i.d.であるので\\(\\hat{{\\boldsymbol \\theta}}\\)は互いに独立となり、上記の議論から漸近同時分布は\n\\[\r\\sqrt{T}\\hat{{\\boldsymbol \\theta}} \\sim^a \\mathcal{N}({\\boldsymbol \\theta},{\\boldsymbol V_{\\boldsymbol \\theta}})\r\\]\rと書けます。ここで、\\({\\boldsymbol V_{\\boldsymbol \\theta}}\\)は\n\\[\r{\\boldsymbol V_{\\boldsymbol \\theta}} = \\left( \\begin{array}{cccc}\r\\sigma^2 \u0026amp; 0\\\\\r0 \u0026amp; 2\\sigma^4\\\\\r\\end{array}\r\\right)\r\\]\rです。シャープレシオの推定値は今\\(g(\\hat{{\\boldsymbol \\theta}})\\)と\\(\\hat{{\\boldsymbol \\theta}}\\)だけの関数になっていますのでデルタ法より、\n\\[\r\\hat{SR} = g(\\hat{{\\boldsymbol \\theta}}) \\sim^a \\mathcal{N}(g({\\boldsymbol \\theta}),\\boldsymbol V_g)\r\\]\rと漸近的に正規分布に従います。ここで、\\(\\boldsymbol V_g\\)は\n\\[\r\\boldsymbol V_g=\\frac{\\partial g}{\\partial{\\boldsymbol \\theta}}{\\boldsymbol V_{\\boldsymbol \\theta}}\\frac{\\partial g}{\\partial{\\boldsymbol \\theta}\u0026#39;}\r\\]\rです。\\(g({\\boldsymbol \\theta})=\\mu/\\sigma\\)なので、\n\\[\r\\frac{\\partial g}{\\partial{\\boldsymbol \\theta}\u0026#39;} = \\left[ \\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\\\\r\\frac{\\partial g}{\\partial \\sigma^2}\\\\\r\\end{array}\r\\right]\r= \\left[ \\begin{array}{cccc}\r\\frac{1}{\\sigma}\\\\\r-\\frac{\\mu}{2\\sigma^3}\\\\\r\\end{array}\r\\right]\r\\]\rよって、\n\\[\r\\begin{eqnarray}\r\\boldsymbol V_g \u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}, \\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right)\r\\left( \\begin{array}{cccc}\r\\sigma^2 \u0026amp; 0\\\\\r0 \u0026amp; 2\\sigma^4\\\\\r\\end{array}\r\\right)\r\\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\\\\r\\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right) \\\\\r\u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\sigma^2, \\frac{\\partial g}{\\partial \\sigma}2\\sigma^4\\\\\r\\end{array}\r\\right)\r\\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\\\\r\\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right) \\\\\r\u0026amp;=\u0026amp; (\\frac{\\partial g}{\\partial \\mu})^2\\sigma^2 + (\\frac{\\partial g}{\\partial \\sigma})^2\\sigma^4 \\\\\r\u0026amp;=\u0026amp; 1 + \\frac{\\mu^2}{2\\sigma^2} \\\\\r\u0026amp;=\u0026amp; 1 + \\frac{1}{2}SR^2\r\\end{eqnarray}\r\\]\rと導出することができます。シャープレシオの絶対値が大きくなるほど指数的に分散が大きくなる傾向があるので良いパフォーマンスを見た時には注意が必要かもしれません。年率化されたシャープレシオの推定値\\(\\hat{SR}(q)\\)が従う分布はここから\n\\[\r\\hat{SR}(q)\\sim^a \\mathcal{N}(\\sqrt{q}SR,\\frac{V(q)}{T}) \\\\\rV(q) = q{\\boldsymbol V}_g = q(1 + \\frac{1}{2}SR^2)\r\\]\rとなります。今、\\(y\\)をバックテストを行う年数とすると\\(T=yq\\)と書け、これを用いて上式を以下のように書き換えることができます(日次リターンで3年計測の場合、サンプルサイズ\\(T\\)は\\(T=3×365=1095\\))。\n\\[\r\\hat{SR}(q)\\sim^a \\mathcal{N}(\\sqrt{q}SR,\\frac{1+\\frac{1}{2}SR^2}{y}) \\tag{1}\r\\]\r頻度\\(q\\)はシャープレシオの平均には影響しますが分散には影響を及ぼしません。これでシャープレシオの推定値の漸近分布を導出することができました。さて、これを使ってなにをしたかったのかということですが、私たちは今バックテストの信頼性について考えていたのでした。つまり、FMが新商品を開発するために頭をひねって考え出した\\(N\\)個の投資戦略案のバックテストをした際に、それらのシャープレシオの真値がどれも0であるにも関わらず、非常に高い(良い)値が出る確率はいかほどなのかということです。Bailey et. al.(2015)では以下のように記述されていました。\nHow high is the expected maximum Sharpe ratio IS among a set of strategy configurations where the true Sharpe ratio is zero?\nまた、期待最大シャープレシオの値を小さくするためには、いったいどれほどの期間バックテストをすべきなのかも知りたいわけです。\n\r4. the minimum backtest lengthを導出してみる\r今考えている状況は、\\(\\mu=0\\)で\\(y\\)を簡単化のために1年とすると(1)式より\\(\\hat{SR}(q)\\)は標準正規分布\\(\\mathcal{N}(0,1)\\)に従います。さて、今から私たちは\\(\\hat{SR}_n(n=1,2,...N)\\)の最大値\\(\\max[\\hat{SR}]_N\\)の期待値について考えていくのですが、勘の良い人ならお気づきの通り、議論は極値統計の文脈に入っていくことになります。\\(\\hat{SR}_n\\sim\\mathcal{N}(0,1)\\)はi.i.d.なので、その最大統計量の極値分布はFisher-Tippett-Gnedenko定理よりガンベル分布になります(証明追えてないです、ごめんなさい)。\n\\[\r\\lim_{N\\rightarrow\\infty}prob[\\frac{\\max[\\hat{SR}]_N-\\alpha}{\\beta}\\leq x] = G(x) = e^{-e^{-x}}\r\\]\rここで、\\(\\alpha=Z(x)^{-1}[1-1/N], \\beta=Z(x)^{-1}[1-1/Ne^{-1}]-\\alpha\\)で、\\(Z(x)\\)は標準正規分布の累積分布関数を表しています。ガンベル分布のモーメント母関数\\(M_x(t)\\)は\n\\[\r\\begin{eqnarray}\rM_x(t) \u0026amp;=\u0026amp; E[e^{tx}] = \\int_{-\\infty}^\\infty e^{tx}e^{-x}e^{-e^{-x}}dx \\\\\r\\end{eqnarray}\r\\]\rと書け、\\(x=-\\log(y)\\)と変数変換すると\\(dx/dy=-1/y=-(e^{-x})^{-1}\\)なので、\n\\[\r\\begin{eqnarray}\rM_x(t) \u0026amp;=\u0026amp; \\int_{\\infty}^0-e^{-t\\log(y)}e^{-y}dy \\\\\r\u0026amp;=\u0026amp; \\int_{0}^\\infty y^{-t}e^{-y}dy \\\\\r\u0026amp;=\u0026amp; \\Gamma(1-t)\r\\end{eqnarray}\r\\]\rとなります。\\(\\Gamma(x)\\)はガンマ関数です。ここから、標準化された最大統計量の期待値(平均)は\n\\[\r\\begin{eqnarray}\r\\lim_{N\\rightarrow\\infty} E[\\frac{\\max[\\hat{SR}]_N-\\alpha}{\\beta}] \u0026amp;=\u0026amp; M_x\u0026#39;(t)|_{t=0} \\\\\r\u0026amp;=\u0026amp; (-1)\\Gamma\u0026#39;(1) \\\\\r\u0026amp;=\u0026amp; (-1)(-\\gamma) = \\gamma\r\\end{eqnarray}\r\\]\rとなります。ここで、\\(\\gamma\\approx0.5772156649...\\)はEuler-Mascheroni定数です。よって、\\(N\\)が大きいとき、i.i.d.の標準正規分布の最大統計量の期待値は\n\\[\rE[\\max[\\hat{SR}]] \\approx \\alpha + \\gamma\\beta = (1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}] \\tag{2}\r\\]\rと近似できます(\\(N\u0026gt;1\\))。これがBailey et. al.(2015)のProposition 1.になります。\\(E[\\max[\\hat{SR}]]\\)を戦略数(試行錯誤数)\\(N\\)の関数としてプロットしたのが以下になります。\nlibrary(ggplot2)\rExMaxSR = function(N){\rgamma_ct = -digamma(1)\rZ = qnorm(0.99)\rreturn((1-gamma_ct)*Z*(1-1/N) + gamma_ct*Z*(1-1/N*exp(1)^{-1}))\r}\rN = list(0:100)\rresult = purrr::map(N,ExMaxSR)\rggplot2::ggplot(data.frame(ExpMaxSR = unlist(result),N = unlist(N)),aes(x=N,y=ExpMaxSR)) +\rgeom_line(size=1) + ylim(0,3)\r小さい\\(N\\)に対して急激に\\(\\max[\\hat{SR}]\\)の期待値が上昇していることがわかると思います。\\(N=10\\)の時、\\(\\max[\\hat{SR}]=1.54\\)となっており、全ての戦略のシャープレシオの真値が0にも拘わらず、少なくとも1つは見かけ上かなり良いパフォーマンスの戦略が見つかることが期待されます。金融ではhold-out法でのバックテストはしばしば使用されるかと思いますが、この方法は試行(錯誤)回数を考慮に入れていないため、\\(N\\)が大きいときには信頼に足る結果を返してくれないわけです。バックテストの結果を向上させるため、闇雲にあれやこれやとシミュレーションを行うことは非常に危険だと思いませんか？最終的にプレゼン資料に上がってくるのは\\(N\\)個の戦略のうち、最もパフォーマンスが良いもののみですから、今回の例のように10個戦略を考えただけでもどれかはシャープレシオが1.87付近に分布しているわけです。試行錯誤数なんてもちろん資料には記載しませんから、非常にミスリーディングなわけです。こういった資料を評価する際にはまず偽陽性を疑ってかかった方がいいかもしれません。\nでは、どうすれば良いのかという話ですが、Bailey et. al.(2015)では、Minimum Backtest Lengthを計算しています。要は試行(錯誤)数\\(N\\)を増やすにつれて、バックテストの年数\\(y\\)も伸ばしていけよと戒めているわけです。\\(N\\)とMinimum Backtest Lengthの関係性を示していきましょう。先ほどと同じく\\(\\mu=0\\)を仮定しますが、\\(y\\neq 1\\)であるケースを考えます。年率化シャープレシオの最大統計量の期待値は(2)式より、\n\\[\rE[\\max[\\hat{SR}(q)]_N] \\approx y^{-1/2}((1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}])\r\\]\rとなります。これを\\(y\\)に対して解いてやることでMinBTLが求まります。\n\\[\rMinBTL \\approx (\\frac{(1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}]}{\\bar{E[\\max[\\hat{SR}(q)]_N]}})^2\r\\]\rここで、\\(\\bar{E[\\max[\\hat{SR}(q)]_N]}\\)は\\(E[\\max[\\hat{SR}(q)]_N]\\)の上限値で、シャープレシオの真値が0である\\(N\\)戦略でシャープレシオの最大統計量が取りうる値を抑えます。その際に、必要なバックテスト年数\\(y\\)がMinBTLとして導出されるのです。\\(\\bar{E[\\max[\\hat{SR}(q)]_N]}=1\\)として、MinBTLを\\(N\\)の関数としてプロットしたものが以下です。\nMinBTL \u0026lt;- function(N,MaxSR){\rreturn((ExMaxSR(N)/MaxSR)^2)\r}\rN = list(1:100)\rresult = purrr::map2(N,1,MinBTL)\rggplot2::ggplot(data.frame(MinBTL = unlist(result),N = unlist(N)),aes(x=N,y=MinBTL)) +\rgeom_line(size=1) + ylim(0,6)\rsimSR \u0026lt;- function(T1){\rr = rnorm(T1)\rreturn(mean(r)/sd(r))\r}\r仮にバックテスト年数が3年以内しかできない場合は試行(錯誤)回数\\(N\\)はほぼ1回に抑えないといけないことになります。3年以内の場合は一発で当ててねという厳しめの制約です。注意しないといけないのは、MinBTLの範囲内でバックテストを行っていたとしてもオーバーフィットすることは考えられるということです。つまり、MinBTLは必要条件であって十分条件でないというわけです。\n\r4. 終わりに\rLópez de Prado(2018)では、オーバーフィッティングを防ぐ汎用的な手段として以下が挙げられています。\nDevelop models for entire asset classes or investment universes, rather than for specific securities. Investors diversify, hence they do not make mistake X only on security Y. If you find mistake X only on security Y, no matter how apparently profitable, it is likely a false discovery.\r(拙訳：特定の有価証券ではなく、アセットクラス全体またはユニバース全体のモデルを開発すること。投資家はリスクを分散させているので、彼らはある証券Yだけに対してミスXをすることはありません。あなたが証券YだけにミスXを見つけた場合は、それがどんなに明らかに有益であっても、誤発見である可能性が高い。)\n\rApply bagging as a means to both prevent overfitting and reduce the variance of the forecasting error. If bagging deteriorates the performance of a strategy, it was likely overfit to a small number of observations or outliers.\r(拙訳：オーバーフィットを防ぎ、予測誤差の分散を減らすための手段として、バギングを適用すること。バギングが戦略のパフォーマンスを悪化させる場合、それは少数の観測値または外れ値にオーバーフィットした可能性が高い。)\n\rDo not backtest until all your research is complete.\r(拙訳：すべてのリサーチが完了するまでバックテストをしないこと。)\n\rRecord every backtest conducted on a dataset so that the probability of backtest overfitting may be estimated on the final selected result (see Bailey, Borwein, López de Prado and Zhu(2017)), and the Sharpe ratio may be properly deflated by the number of trials carried out (Bailey and López de Prado(2014.b)).\r(拙訳：研究者が最終的に選択したバックテスト結果がオーバーフィットしている確率を推定できるように、単一の(同じ)データセットで実施されたバックテストをすべて記録すること（Bailey, Borwein, López de Prado and Zhu [2017]）、また、実施された試行数によってシャープレシオを適切にデフレーションできるようにすること（Bailey and López de Prado [2014]）。)\n\rSimulate scenarios rather than history. A standard backtest is a historical simulation, which can be easily overfit. History is just the random path that was realized, and it could have been entirely different. Your strategy should be profitable under a wide range of scenarios, not just the anecdotal historical path. It is harder to overfit the outcome of thousands of “what if” scenarios.\r(拙訳：ヒストリカルではなくシナリオをシミュレーションすること。標準的なバックテストはヒストリカルシミュレーションであり、オーバーフィットしやすい。歴史(これまでの実績)はランダムなパスの実現値に過ぎず、全く違ったものになっていた可能性があります。あなたの戦略は、逸話的なヒストリカルパスではなく、様々なシナリオの下で利益を得ることができるものであるべきです。何千もの「もしも」のシナリオ結果をオーバーフィットさせるのは(ヒストリカルシミュレーションで過学習するよりも)より難しいことです。)\n\rDo not research under the influence of a backtest. If the backtest fails to identify a profitable strategy,\rstart from scratch. Resist the temptation of reusing those results.\r(拙訳：バックテストのフィードバックを受けてリサーチしないこと。バックテストが有益な戦略を見つけ出すことに失敗した場合は、ゼロからリサーチを再始動してください。それらの結果を再利用する誘惑に抗ってください。)\n\r\r3と6は本日の論文と関係のある文脈だと思います。この分野は他にも研究の蓄積があるので、業務でバックテストを行うという人は運用手法の勉強もいいですが、そもそものお作法としてバックテストの正しい運用方法について学ぶことをお勧めします。\nさて、いつもとは違う観点で、少しメタ的なトピックに取り組んでみました。自分自身仕事柄バックテスト結果などを見ることも多いですし、このブログでもしばしばhold-out法でのバックテストをしています。得られた結果の不確実性を理解して、評価できるよう今後もこのトピックの研究を追っていきたいと思います。\n\r","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1594166400,"objectID":"ffd693614e217f006fe4ab18e5859efb","permalink":"/post/post19/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/post/post19/","section":"post","summary":"金融であればクオンツの方は新規運用戦略の立案をする際に、バックテストを行ってパフォーマンスの確認をすることがあると思います。今回は、バックテストのオーバーフィッティングがアウトオブサンプル・パフォーマンスに及ぼす影響について調べたので備忘録をかねてまとめてみました。","tags":["機械学習","金融","時系列解析"],"title":"そのバックテスト本当に再現性ありますか？","type":"post"},{"authors":null,"categories":["競馬"],"content":"\r\r\r1. データ収集のためのクローリング\r\rデータをどこから取得するか\rseleniumでクローリングを実行する\r\r2. Kerasを用いてCNNを学習させる\r\rKerasとは？\rCNNとは？\rコーディング\r不均衡データ調整のためのアンダーサンプリング\r\r3. Shap値を用いた結果解釈\r4. 最後に\r\r\rおはこんばんにちは。今回は競馬予想についての記事を書きたいと思います。前回、LightGBMを用いてyahoo競馬から取得したレース結果データ(テーブルデータ)を用いて、競馬順位予想モデルを作成しました。前回は構造データを用いましたが、このご時世ですからこんな分析は誰にでもできるわけです。時代は非構造データ、というわけで今回は馬体画像から特徴量を抽出し、順位予想を行う畳み込みニューラルネットワーク(Convolutional Neural Network, CNN)を作成してみました。画像解析はEarth Engineを用いた衛星画像の解析に続いて2回目、深層学習はこのブログでは初めてと言うことになります。なお、Pythonを使用しています。\n1. データ収集のためのクローリング\rまず、馬体画像をネットから収集することから始めます。1番良いのはレース当日のパドックの写真を使用することでしょう。ただ、パドックの写真をまとまった形で掲載してくれているサイトは調べた限りは存在しませんでした。もしかしたら、Youtubeに競馬ファンの方がパドック動画を上げていらっしゃるかも知れませんので、それを画像に切り抜いて使う or 動画としてCNN→RNNのEncoder-Decoderモデルに適用すると面白いかもしれません。しかし、そこまでの能力は今の自分にはありません。\nデータをどこから取得するか\rそこで、今回はデイリーのWebサイトからデータを取得しています。ここには直近1年間?のG1レースに出馬する競走馬のレース前の馬体写真が掲載されています。実際のレース場へ行けない馬券師さんたちはこの写真を見て馬の状態を分析していると思われます。\nなお、このサイトには出馬全頭の馬体写真が掲載されているわけではありません。また、G1の限られたレースのみですので、そもそも全ての馬が仕上がっている可能性もあり、差がつかないことも十分予想されます。ただ、手っ取り早くやってみることを優先し、今回はこのデータを使用することにしたいと思います。\n\rseleniumでクローリングを実行する\rクローリングにはseleniumを使用します。今回はCNNがメインなのでWebクローリングについては説明しません。使用したコードは以下です。\n【注意】以下のコードを使用される場合は自己責任でお願いします。\nfrom selenium import webdriver\rfrom selenium.webdriver.chrome.options import Options\rfrom selenium.webdriver.support.select import Select\rfrom selenium.webdriver.common.by import By\rfrom selenium.webdriver.common.keys import Keys\rfrom selenium.webdriver.common.alert import Alert\rfrom selenium.webdriver.support.ui import WebDriverWait\rfrom selenium.webdriver.support import expected_conditions as EC\rfrom selenium.common.exceptions import TimeoutException\rfrom selenium.webdriver.common.action_chains import ActionChains\rfrom time import sleep\rfrom urllib import request\rimport random\r# seleniumのオプション設定（おまじない）\roptions = Options()\roptions.add_argument(\u0026#39;--disable-gpu\u0026#39;);\roptions.add_argument(\u0026#39;--disable-extensions\u0026#39;);\roptions.add_argument(\u0026#39;--proxy-server=\u0026quot;direct://\u0026quot;\u0026#39;);\roptions.add_argument(\u0026#39;--proxy-bypass-list=*\u0026#39;);\roptions.add_argument(\u0026#39;--start-maximized\u0026#39;);\r# driver指定\rDRIVER_PATH = r\u0026#39;C:/Users/aashi/Desktop/chromedriver_win32/chromedriver.exe\u0026#39;\rdriver = webdriver.Chrome(executable_path=DRIVER_PATH, chrome_options=options)\r# urlを渡し、サイトへアクセス\rurl = \u0026#39;https://www.daily.co.jp/horse/horsecheck/photo/\u0026#39;\rdriver.get(url)\rdriver.implicitly_wait(15) # オブジェクトのロード待ちの最大時間でこれを越えるとエラー\rsleep(5) # webページの遷移を行うので1秒sleep\r# 各レース毎に画像データ保存\rselector0 = \u0026quot;body \u0026gt; div \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; div \u0026gt; section \u0026gt; a\u0026quot;\relements = driver.find_elements_by_css_selector(selector0)\rfor i in range(0,len(elements)):\relements = driver.find_elements_by_css_selector(selector0)\relement = elements[i]\relement.click()\rsleep(5) # webページの遷移を行うので5秒sleep\rtarget = driver.find_element_by_link_text(\u0026#39;Ｇ１馬体診断写真集のTOP\u0026#39;)\ractions = ActionChains(driver)\ractions.move_to_element(target)\ractions.perform()\rsleep(5) # webページの遷移を行うので5秒sleep\rselector = \u0026quot;body \u0026gt; div.wrapper.horse.is-fixedHeader.is-fixedAnimation \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; article \u0026gt; div.photoDetail-wrapper \u0026gt; section \u0026gt; div \u0026gt; figure\u0026quot;\rfigures = driver.find_elements_by_css_selector(selector)\rdownload_dir = r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026#39;\rselector = \u0026quot;body \u0026gt; div \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; article \u0026gt; div.photoDetail-wrapper \u0026gt; section \u0026gt; h1\u0026quot;\rrace_name = driver.find_element_by_css_selector(selector).text\rfor figure in figures:\rimg_name = figure.find_element_by_tag_name(\u0026#39;figcaption\u0026#39;).text\rhorse_src = figure.find_element_by_tag_name(\u0026#39;img\u0026#39;).get_attribute(\u0026quot;src\u0026quot;) save_name = download_dir + \u0026#39;/\u0026#39; + race_name + \u0026#39;_\u0026#39; + img_name + \u0026#39;.jpg\u0026#39;\rrequest.urlretrieve(horse_src,save_name)\rdriver.back()\r保存した画像を実際のレース結果と突合し、手作業で上位3位以内グループとそれ以外のグループに分けました。以下のような感じで画像が保存されています。\n保存された馬体画像\n\rこれで元データの収集が完了しました。\n\r\r2. Kerasを用いてCNNを学習させる\rKerasとは？\rさて、次にKerasを使ってCNNを学習させましょう。まず、KerasとはTensorflowやTheano上で動くNeural Networkライブラリの1つです。Kerasは比較的短いコードでモデルを組むことができ、また学習アルゴリズムが多いことが特徴のようです。\n\rCNNとは？\rCNNは画像解析を行う際によく使用される(Deep) Neural Networkの1種で、その名の通りConvolution(畳み込み)を追加した物となっています。畳み込みとは以下のような処理のことを言います。\n畳み込み層の処理\n\rここのインプットとは画像データのことです。画像解析では画像を数値として認識し、解析を行います。コンピュータ上の画像はRGB値という、赤(Red)、緑(Green)、青(Blue)の3色の0~255までの数値の強弱で表現されています。赤255、緑0、青0といった形で3層のベクトルになっており、この場合完全な赤が表現されます。上図の場合、a,b,cなどが各ピクセルのRGB値のいずれかを表していると考えることができます。畳み込みはこのRGB値をカーネルと呼ばれる行列との内積をとることで画像の特徴量を計算します。畳み込み層の意味は以下の動画がわかりやすいです。\n\rカーネルを上手くその画像の特徴的な部分を取得できるように学習することで、画像の識別が可能になります。畳み込み層はCNNの最重要部分だと思います。\nCNNの全体像\n\r上図のようにCNNは畳み込み意外にももちろん入力層や出力層など通常のNeural Networkと同じ層も持っています。なお、MaxPooling層について知りたい人は以下の動画を参照されてください。\n\r深層学習の学習方法については最急下降法(勾配法)がオーソドックスなものとして知られていますが、Adamなど色々な拡張アルゴリズムが提案されています。基本的には、Adamはmomentumを使用することが多いでしょうか。\n\rコーディング\rでは、実際にコーディングしていきます。\nfrom keras.utils import np_utils\rfrom keras.models import Sequential\rfrom keras.layers.convolutional import MaxPooling2D\rfrom keras.layers import Activation, Conv2D, Flatten, Dense,Dropout\rfrom sklearn.model_selection import train_test_split\rfrom keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop, Nadam\rfrom PIL import Image\rimport numpy as np\rimport glob\rimport matplotlib.pyplot as plt\rimport time\rimport os\rまず最初に収集してきた画像データを数値データに変換し学習データを作成します。\rディレクトリ構造は以下のようになっており、上位画像とその他画像が別ディレクトリに保存されています。各ディレクトリから画像を読み込む際に、上位画像には1、その他には0というカテゴリ変数を与えます。\n馬体写真\n\r上位\rその他\r\r#フォルダを指定\rfolders = os.listdir(r\u0026quot;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026quot;)\r#画総数を指定(今回は50×50×3)。\rimage_size = 300\rdense_size = len(folders)\rX = []\rY = []\r#それぞれのフォルダから画像を読み込み、Image関数を使用してRGB値ベクトル(numpy array)へ変換\rfor i, folder in enumerate(folders):\rfiles = glob.glob(\u0026quot;C:/Users/aashi/umanalytics/photo/image/\u0026quot; + folder + \u0026quot;/*.jpg\u0026quot;)\rindex = i\rfor k, file in enumerate(files):\rimage = Image.open(file)\rimage = image.convert(\u0026quot;L\u0026quot;).convert(\u0026quot;RGB\u0026quot;)\rimage = image.resize((image_size, image_size)) #画素数を落としている\rdata = np.asarray(image)\rX.append(data)\rY.append(index)\rX = np.array(X)\rY = np.array(Y)\rX = X.astype(\u0026#39;float32\u0026#39;)\rX = X / 255.0 # 0~1へ変換\rX.shape\rY = np_utils.to_categorical(Y, dense_size)\r#訓練データとテストデータへ変換\rX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)\r訓練データとテストデータの分割ができました。今考えているのは「上位」と「その他」の2値分類となっていますが、「上位」を3位以内と定義したので不均衡なデータとなっています(その他データが上位データの5倍くらい)。こういった場合、そのままのデータで学習をするとサンプルサイズが多い方のラベル(この場合「その他」)を予測しやすくなり、バイアスのあるモデルとなります。よって、学習データは2クラスそれぞれが同じサンプルサイズとなるよう調整してやる必要があります。\nindex_zero = np.random.choice(np.array(np.where(y_train[:,1]==0))[0,],np.count_nonzero(y_train[:,1]==1),replace=False)\rindex_one = np.array(np.where(y_train[:,1]==1))[0]\ry_resampled = y_train[np.hstack((index_one,index_zero))]\rX_resampled = X_train[np.hstack((index_one,index_zero))]\r学習データにはこのy_resampledとX_resampledを使用します。次に、CNNを構築していきます。Kerasでは、sequential modelを指定し、addメソッドで層を追加して行くことでモデルを定義します。\nmodel = Sequential()\rmodel.add(Conv2D(32, (3, 3), padding=\u0026#39;same\u0026#39;,input_shape=X_train.shape[1:]))\rmodel.add(Activation(\u0026#39;relu\u0026#39;))\rmodel.add(Conv2D(32, (3, 3)))\rmodel.add(Activation(\u0026#39;relu\u0026#39;))\rmodel.add(MaxPooling2D(pool_size=(2, 2)))\rmodel.add(Dropout(0.25))\rmodel.add(Conv2D(64, (3, 3), padding=\u0026#39;same\u0026#39;))\rmodel.add(Activation(\u0026#39;relu\u0026#39;))\rmodel.add(Conv2D(64, (3, 3)))\rmodel.add(Activation(\u0026#39;relu\u0026#39;))\rmodel.add(MaxPooling2D(pool_size=(2, 2)))\rmodel.add(Dropout(0.25))\rmodel.add(Flatten())\rmodel.add(Dense(512))\rmodel.add(Activation(\u0026#39;relu\u0026#39;))\rmodel.add(Dropout(0.5))\rmodel.add(Dense(dense_size))\rmodel.add(Activation(\u0026#39;softmax\u0026#39;))\rmodel.summary()\r## Model: \u0026quot;sequential\u0026quot;\r## _________________________________________________________________\r## Layer (type) Output Shape Param # ## =================================================================\r## conv2d (Conv2D) (None, 300, 300, 32) 896 ## _________________________________________________________________\r## activation (Activation) (None, 300, 300, 32) 0 ## _________________________________________________________________\r## conv2d_1 (Conv2D) (None, 298, 298, 32) 9248 ## _________________________________________________________________\r## activation_1 (Activation) (None, 298, 298, 32) 0 ## _________________________________________________________________\r## max_pooling2d (MaxPooling2D) (None, 149, 149, 32) 0 ## _________________________________________________________________\r## dropout (Dropout) (None, 149, 149, 32) 0 ## _________________________________________________________________\r## conv2d_2 (Conv2D) (None, 149, 149, 64) 18496 ## _________________________________________________________________\r## activation_2 (Activation) (None, 149, 149, 64) 0 ## _________________________________________________________________\r## conv2d_3 (Conv2D) (None, 147, 147, 64) 36928 ## _________________________________________________________________\r## activation_3 (Activation) (None, 147, 147, 64) 0 ## _________________________________________________________________\r## max_pooling2d_1 (MaxPooling2 (None, 73, 73, 64) 0 ## _________________________________________________________________\r## dropout_1 (Dropout) (None, 73, 73, 64) 0 ## _________________________________________________________________\r## flatten (Flatten) (None, 341056) 0 ## _________________________________________________________________\r## dense (Dense) (None, 512) 174621184 ## _________________________________________________________________\r## activation_4 (Activation) (None, 512) 0 ## _________________________________________________________________\r## dropout_2 (Dropout) (None, 512) 0 ## _________________________________________________________________\r## dense_1 (Dense) (None, 2) 1026 ## _________________________________________________________________\r## activation_5 (Activation) (None, 2) 0 ## =================================================================\r## Total params: 174,687,778\r## Trainable params: 174,687,778\r## Non-trainable params: 0\r## _________________________________________________________________\rでは、学習パートに入ります。アルゴリズムにはAdadeltaを使用します。よくわかってないんですけどね。。。\noptimizers =\u0026quot;Adadelta\u0026quot;\rresults = {}\repochs = 50\rmodel.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=optimizers, metrics=[\u0026#39;accuracy\u0026#39;])\rresults = model.fit(X_resampled, y_resampled, validation_split=0.2, epochs=epochs)\r\r不均衡データ調整のためのアンダーサンプリング\rここから、Testデータで2値分類を行うのですが、学習データをアンダーサンプリングしているので、予測確率を計算する際にアンダーサンプリングを行ったサンプル選択バイアスが生じてしまいます。論文はこちら。\rよって、補正が必要になるのですがこの部分の定式化をここでしておきたいと思います。現在行っている2値分類問題を説明千数\\(X\\)から2値を取る目的変数\\(Y\\)を予測する問題と表現することにします。データセット\\((X,Y)\\)は正例が負例よりもかなり少なく、負例のサンプルサイズを正例に合わせたデータセットを\\((X_s,Y_s)\\)とします。ここで、\\((X,Y)\\)のサンプル組が\\((X_s,Y_s)\\)にも含まれる場合に1を取り、含まれない場合に0をとるカテゴリ変数\\(s\\)を定義します。\rデータセット\\((X,Y)\\)を用いて構築したモデルに説明変数\\(x\\)を与えた時、正例と予測する条件付き確率は\\(P(y=1|x)\\)で表すことができます。一方、\\((X_s,Y_s)\\)を用いて構築したモデルで正例を予測する条件付き確率はベイズの定理とカテゴリ変数\\(s\\)を用いて、\n\\[\rP(y=1|x,s=1) = \\frac{P(s=1|y=1)P(y=1|x)}{P(s=1|y=1)P(y=1|x) + P(s=1|y=0)P(y=0|x)}\r\\]\rと書けます。\\((X_s,Y_s)\\)は負例のサンプルサイズを正例に合わせているため、\\(P(s=1,y=1)=1\\)であるので上式は\n\\[\rP(y=1|x,s=1) = \\frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)P(y=0|x)}\r= \\frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)(1-P(y=1|x))}\r\\]\rと書き換えることができます。\\(P(s=1|y=0)\\neq0\\)であることは\\((X_s,Y_s)\\)の定義より自明です(0だと正例しかない不均衡データになる)。よって、\\(P(y=0,x)\\neq0\\)である限り、アンダーサンプリングのモデルが正例とはじき出す確率は元のデータセットが出す確率に対して正のバイアスがあることがわかります。求めたいのはバイアスのない\\(P(y=1|x)\\)なので\\(P=P(y=1|x),P_s=P(y|x,s=1),\\beta=P(s=1,y=0)\\)とすると、\n\\[\rP = \\frac{\\beta P_s}{\\beta P_s-P_s+1}\r\\]\rとかけ、この関係式を用いてバイアスを補正することができます。\r今確認したことを関数として定義しましょう。\ndef calibration(y_proba, beta):\rreturn y_proba / (y_proba + (1 - y_proba) / beta)\rsampling_rate = sum(y_train[:,1]) / sum(1-y_train[:,1])\ry_proba_calib = calibration(model.predict(X_test), sampling_rate)\ry_pred = np_utils.to_categorical(np.argmax(y_proba_calib,axis=1), dense_size)\rfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\rscore = accuracy_score(y_test, y_pred)\rprint(\u0026#39;Test accuracy:\u0026#39;, score)\r## Test accuracy: 0.288135593220339\rまったく良くない結果です。ConfusionMatrixを出してみたところどうやらうまくいっていないことがわかりました。\nConfusionMatrixDisplay(confusion_matrix(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))).plot()\r## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000004939E748\u0026gt;\rplt.show()\rplt.close()\r不均衡データのバイアス修正はしたんですが、それでもなお負値を予測しやすいモデルとなっています。これでは使えないですね。\n\r\r3. Shap値を用いた結果解釈\r今学習したモデルのshap値を考え、結果の解釈をしたいと思います。shap値については時間があれば、説明を追記したいと思います。簡単に言えば、CNNが画像のどの部分に特徴を捉え、馬が上位に入るかを予想したかを可視化で捉えることができます。この馬の解析をすることにします。\nplt.imshow(X_test[0])\rplt.show()\rplt.close()\rimport shap\rbackground = X_train[np.random.choice(X_train.shape[0],100,replace=False)]\re = shap.GradientExplainer(model,background)\rshap_values = e.shap_values(X_test[[0]])\rshap.image_plot(shap_values[1],X_test[[0]])\r非常に微妙ですが、足や臀部などを評価しているようにみえます。ですが、背景に反応しているようにも見えるので馬体のみ取り出すトリミングをやる必要がありますね。これは物体検知のモデルを構築する必要がありそうです。また、今度の機会に考えます。\r各層において画像のどの側面を捉えているかを可視化してみたいと思います。\nfrom keras import models\rlayer_outputs = [layer.output for layer in model.layers[:8]]\rlayer_names = []\rfor layer in model.layers[:8]:\rlayer_names.append(layer.name)\rimages_per_row = 16\ractivation_model = models.Model(inputs=model.input, outputs=layer_outputs)\ractivations = activation_model.predict(X_train[[0]])\rfor layer_name, layer_activation in zip(layer_names, activations):\rn_features = layer_activation.shape[-1]\rsize = layer_activation.shape[1]\rn_cols = n_features // images_per_row\rdisplay_grid = np.zeros((size * n_cols, images_per_row * size))\rfor col in range(n_cols):\rfor row in range(images_per_row):\rchannel_image = layer_activation[0,\r:, :,\rcol * images_per_row + row]\rchannel_image -= channel_image.mean()\rchannel_image /= channel_image.std()\rchannel_image *= 64\rchannel_image += 128\rchannel_image = np.clip(channel_image, 0, 255).astype(\u0026#39;uint8\u0026#39;)\rdisplay_grid[col * size : (col + 1) * size,\rrow * size : (row + 1) * size] = channel_image\rscale = 1. / size\rplt.figure(figsize=(scale * display_grid.shape[1],\rscale * display_grid.shape[0]))\rplt.title(layer_name)\rplt.grid(False)\rplt.imshow(display_grid, cmap=\u0026#39;viridis\u0026#39;)\rplt.show()\rplt.close()\r自分が未熟なこともあり、解釈が難しいですね。\n\r4. 最後に\r正直まったく上手くいっていません。やはり馬体から順位予測をするのは難しいのでしょうか。ほかの変数と掛け合わせると結果が変わったりするのでしょうか。今のままだとよい特徴量を抽出することができていないように思います。\rYoutubeからパドック動画を取得して、Encoder-Decoderモデルで解析するところまでやらないとうまくいかないんですかね。自分の実力が十分上がれば是非やってみたいと思います(いつになることやら)。それまでには、PCのスペックを上げないといけません。定額給付金を使うかな。。。\n\r","date":1593907200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1593907200,"objectID":"2b56fac2972c6864e2f0b41d2ce414de","permalink":"/post/post18/","publishdate":"2020-07-05T00:00:00Z","relpermalink":"/post/post18/","section":"post","summary":"馬体写真という新しい観点から競馬予測モデルを作りました。","tags":["Python","Webスクレイピング","機械学習"],"title":"CNNを使って馬体写真から順位予想してみた","type":"post"},{"authors":null,"categories":["単発","Tips"],"content":"\r\r\r1. Mecab(RMeCab)とは？\r\r固有名詞に弱いデフォルトのMeCab\r新語に強いNEologd辞書\r\r2. インストール手順\r\rA. Windows Subsystem for Linux(WSL)のインストール\rB. Ubuntu Linuxのインストール\rC. Ubuntu LinuxにMeCabをインストール\rD. Ubuntu for LinuxにNEologd辞書をインストール\rE. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー\rF. Windows内で辞書ファイルのコンパイル(SHIFT-JIS)を行う。\r\r3. まとめ\r\r\r皆さんおはこんばんにちは。\r最近在宅勤務で運動不足ですが、平日休日問わず研究活動をしています。\r学術誌投稿を目指したBlog記事には書けない内容なので、更新はストップしてしまっていますがちゃんと活動はしています。\n今回はその研究の中で利用しているMeCabにまつわるTipsのご紹介です。MeCabというと形態素解析ソフトということはこのブログを読まれている方はお分かりかと思いますが、その辞書にNEologd辞書を使用できるようにしてみたというのが内容です。MeCabってなに？という方もいらっしゃるかもしれませんので、かなり簡単にご紹介します。\n1. Mecab(RMeCab)とは？\rMeCabは日本語テキストマイニングで使用される形態素解析ソフトです。英語などの言語とは異なり、日本語は単語毎にスペースを置かないため、テキストマイニングを行う際、そのままでは文章を単語単位に区切って集計するといったことができません。例えば、「これはペンです。」という文章はそのままでは「これはペンです。」という1つの単語として認識されてしまいます。ですが、単語の頻度分析を行う際などは、「これ/は/ペン/です/。」という風に文章を単語レベルにまで分割し、「ペン」という特徴量を取得したいわけです。それができるのがRMeCabで、この処理は形態素解析と呼ばれます。1そして、RMeCabとはこのMeCabのラッパーになります。RからMeCabを使用するにはRMeCabを使用する必要があります。使用方法は簡単で、RMeCabC関数に文章を渡すだけです。\nlibrary(magrittr)\rlibrary(RMeCab)\rRMeCabC(\u0026quot;これはペンです。\u0026quot;) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 助動詞 記号 ## \u0026quot;これ\u0026quot; \u0026quot;は\u0026quot; \u0026quot;ペン\u0026quot; \u0026quot;です\u0026quot; \u0026quot;。\u0026quot;\rRMeCabC関数は結果をリストで返してくるので、unlistでcharacterに変換しています。\n固有名詞に弱いデフォルトのMeCab\r便利なように思えるのですが、デフォルトのMeCabは固有名詞に弱いという弱点があります。例えば、「欅坂46が赤いきつねを食べている。」という文章を形態素解析してみましょう。\nRMeCabC(\u0026quot;欅坂46が赤いきつねを食べている。\u0026quot;) %\u0026gt;% unlist()\r## 名詞 名詞 名詞 助詞 形容詞 名詞 助詞 動詞 ## \u0026quot;欅\u0026quot; \u0026quot;坂\u0026quot; \u0026quot;46\u0026quot; \u0026quot;が\u0026quot; \u0026quot;赤い\u0026quot; \u0026quot;きつね\u0026quot; \u0026quot;を\u0026quot; \u0026quot;食べ\u0026quot; ## 助詞 動詞 記号 ## \u0026quot;て\u0026quot; \u0026quot;いる\u0026quot; \u0026quot;。\u0026quot;\r予想していた結果は「欅坂46/が/赤いきつね/を/食べ/て/いる/。」でしょう。ただ、固有名詞を上手く形態素解析することができていないため、不必要なところで分割がなされており、「赤い/きつね/を/食べ」という部分については「赤い（動物の）きつね」を食べているかのような解析結果になっています。赤いきつねの「きつね」と動物の「きつね」を同等に扱ってしまうので、問題があります。また、経済においても以下のように日経平均株価が分割され、新聞社の「日経」と株価の「日経」が、大統領の「トランプ」とカードの「トランプ」が区別できないといったことが想定されます。\nRMeCabC(\u0026quot;今週の日経平均株価は、上値抵抗線を突破して上昇！\u0026quot;) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 名詞 名詞 助詞 記号 名詞 名詞 名詞 助詞 ## \u0026quot;今週\u0026quot; \u0026quot;の\u0026quot; \u0026quot;日経\u0026quot; \u0026quot;平均\u0026quot; \u0026quot;株価\u0026quot; \u0026quot;は\u0026quot; \u0026quot;、\u0026quot; \u0026quot;上値\u0026quot; \u0026quot;抵抗\u0026quot; \u0026quot;線\u0026quot; \u0026quot;を\u0026quot; ## 名詞 動詞 助詞 名詞 記号 ## \u0026quot;突破\u0026quot; \u0026quot;し\u0026quot; \u0026quot;て\u0026quot; \u0026quot;上昇\u0026quot; \u0026quot;！\u0026quot;\rRMeCabC(\u0026quot;トランプ政権による経済活動再開の指針発表が評価される\u0026quot;) %\u0026gt;% unlist()\r## 名詞 名詞 助詞 名詞 名詞 名詞 助詞 ## \u0026quot;トランプ\u0026quot; \u0026quot;政権\u0026quot; \u0026quot;による\u0026quot; \u0026quot;経済\u0026quot; \u0026quot;活動\u0026quot; \u0026quot;再開\u0026quot; \u0026quot;の\u0026quot; ## 名詞 名詞 助詞 名詞 動詞 動詞 ## \u0026quot;指針\u0026quot; \u0026quot;発表\u0026quot; \u0026quot;が\u0026quot; \u0026quot;評価\u0026quot; \u0026quot;さ\u0026quot; \u0026quot;れる\u0026quot;\rこれはデフォルトでMeCabが使用している辞書に原因があります。IPA辞書(ipadic)と呼ばれる物で、奈良先端科学技術大学院大学が公開している茶筌と呼ばれる形態素解析ソフト用に作られました。こちらを見ると辞書の更新は2007年でストップしており、新語や流行語がアップデートされていないために上記の固有名詞が上手く形態素解析できないことがわかります。\n\r新語に強いNEologd辞書\r最近MeCabでよく使用されている辞書にNEologd辞書というものがあります。NEologd辞書とは、Web上から得た新語に対応しており、頻繁に更新されるMeCab用のシステム辞書です。2Twitterのアカウントには、\n\r特色は語彙の多さと更新頻度、新語の採録の速さ、読み仮名の正確さ、表記揺れへの対応。おもな解析対象はWeb上のニュース記事や流行した出来事。\rおもな用途は文書分類、文書ベクトル作成、単語埋め込みベクトル作成、読み仮名付与。\n\rと記載されており、上述したIPA辞書の弱点を補完する辞書となっています。今回の記事はその辞書のインストール方法についてですが、インストールした辞書の威力を先にお見せしておきます。実行するには、RMeCab関数に辞書ファイル(.dicファイル)のパスを渡してやれば良いです。\ndic_directory \u0026lt;- \u0026quot;C:\\\\hogehoge\\\\mecab-user-dict-seed.yyyymmdd.dic\u0026quot;\rRMeCabC(\u0026quot;欅坂46が赤いきつねを食べている。\u0026quot;,dic=dic_directory) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 助詞 動詞 助詞 ## \u0026quot;欅坂46\u0026quot; \u0026quot;が\u0026quot; \u0026quot;赤いきつね\u0026quot; \u0026quot;を\u0026quot; \u0026quot;食べ\u0026quot; \u0026quot;て\u0026quot; ## 動詞 記号 ## \u0026quot;いる\u0026quot; \u0026quot;。\u0026quot;\rRMeCabC(\u0026quot;今週の日経平均株価は、上値抵抗線を突破して上昇！\u0026quot;,dic=dic_directory) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 助詞 記号 ## \u0026quot;今週\u0026quot; \u0026quot;の\u0026quot; \u0026quot;日経平均株価\u0026quot; \u0026quot;は\u0026quot; \u0026quot;、\u0026quot; ## 名詞 名詞 名詞 助詞 名詞 ## \u0026quot;上値\u0026quot; \u0026quot;抵抗\u0026quot; \u0026quot;線\u0026quot; \u0026quot;を\u0026quot; \u0026quot;突破\u0026quot; ## 動詞 助詞 名詞 記号 ## \u0026quot;し\u0026quot; \u0026quot;て\u0026quot; \u0026quot;上昇\u0026quot; \u0026quot;！\u0026quot;\rRMeCabC(\u0026quot;トランプ政権による経済活動再開の指針発表が評価される\u0026quot;,dic=dic_directory) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 名詞 助詞 ## \u0026quot;トランプ政権\u0026quot; \u0026quot;による\u0026quot; \u0026quot;経済活動\u0026quot; \u0026quot;再開\u0026quot; \u0026quot;の\u0026quot; ## 名詞 名詞 助詞 名詞 動詞 ## \u0026quot;指針\u0026quot; \u0026quot;発表\u0026quot; \u0026quot;が\u0026quot; \u0026quot;評価\u0026quot; \u0026quot;さ\u0026quot; ## 動詞 ## \u0026quot;れる\u0026quot;\r上値抵抗線は1語カウントしてほしいところではありますが、それ以外は上手く形態素解析できていそうです。\rこのNEologd辞書ですが、Windowsのインストールが想定されていません。つまり、Windowsユーザーは直接インストールすることができないのです。Windowsユーザーは少々ややこしい手順を踏まなければなりません。今回はそのややこしい手順を解説する記事です。Linuxでインストールを行い、それをWindows環境にコピー、その後辞書ファイルを作成します。\n\r\r2. インストール手順\rA. Windows Subsystem for Linux(WSL)のインストール\rWindows Power Shellを管理者権限で開き、\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\rを実行する。WSLをインストールすることができます。\npowerShellでの実行の様子\n\r\rB. Ubuntu Linuxのインストール\rMicrosoft Storeよりubuntuをダウンロードする。\nubuntuの画面\n\rインストールが完了したらubuntuを起動し、初期設定を完了させる(ID、パスワード)。\n\rC. Ubuntu LinuxにMeCabをインストール\rubuntuのコマンドプロンプトで、\nsudo apt-get update\rsudo apt install mecab\rsudo apt install libmecab-dev\rsudo apt install mecab-ipadic-utf8\rを入力し、MeCabをインストール。\nubuntuでmecabをインストール\n\r\rD. Ubuntu for LinuxにNEologd辞書をインストール\rubuntsuのコマンドプロンプトで\nsudo apt install make\rgit clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\rcd mecab-ipadic-neologd\rsudo bin/install-mecab-ipadic-neologd -n -a\rを実行。NEologd辞書ファイルが(/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/)にインストールできます。\n\rE. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー\rubuntuのコマンドプロンプトで\nexplorer.exe .\rを入力。エクスプローラーが立ち上がるので、/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/(ディレクトリ)をWindowsの任意のディレクトリにコピーする。完了したらUbuntuは閉じる。\nエクスプローラーでubuntu内部ディレクトリを確認する図\n\r\rF. Windows内で辞書ファイルのコンパイル(SHIFT-JIS)を行う。\rコピーしたディレクトリ内の以下のcsvを辞書ファイル(.dic)にコンパイルします。(yyyymmddは辞書の最終更新日なので変更してください)\n-mecab-ipadic-neologd-buildmecab-ipadic-2.7.0-20070801-neologd-yyyymmdd-mecab-user-dict-seed.yyyymmdd.csv\rその際、元ファイルはエンコーディングがUTF-8となっているので、SHIFT-JISへ変換することに注意です。これをしないと、RMeCabを実行したときに結果が文字化けします。コンパイルにはMeCabのmecab-dict-indexというバイナリファイルを使用します。自分は以下のディレクトリに存在しました。\nC:\r-Program Files (x86)\r-MeCab\r-bin\r-mecab-dict-index.exe\nコマンドプロンプトを立ち上げ、\nC:\\Program Files (x86)\\MeCab\\bin\\mecab-dict-index.exe -d .../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmdd(ファイルの保存場所) -u NEologd.yyyymmdd.dic -f utf-8 -t shift-jis mecab-ipadic-neologd\\buildmecab\\ipadic-2.7.0-20070801-neologd-yyyymmdd\\mecab-user-dict-seed.yyyymmdd.csv \rを適宜変更の上、入力。.../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmddに辞書がコンパイルされ、NEologd.yyyymmdd.dicができます。\n\r\r3. まとめ\r以上で、NEologd辞書が使用できるようになります。非常に強力なツールなので使用してみてください。なお、辞書がアップデートされた際は同じ手続きを行う必要があります。\n\rUbuntu for Linuxを使用しなくてもダウンロードできそうな方法ありました。\nhttps://qiita.com/zincjp/items/c61c441426b9482b5a48\nただ、自分は実行していないので実際にできるかはわかりません。辞書が更新されたら、やってみたいと思います。\n\r\rMeCab内部の仕組みについてはこちらの記事が参考になります。↩︎\n\r2020/4/19時点の最近版は2020/3/15更新の辞書です。↩︎\n\r\r\r","date":1587254400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1587254400,"objectID":"9a9dac6ab7ada9192e2cf9879cae5535","permalink":"/post/post17/","publishdate":"2020-04-19T00:00:00Z","relpermalink":"/post/post17/","section":"post","summary":"新語に強いNEologd辞書をMecabにインストールしてみました。","tags":["R","テキストマイニング"],"title":"WindowsにNEologd辞書をインストールして、RMeCabを実行する方法","type":"post"},{"authors":null,"categories":["競馬"],"content":"\r\r\r1.データインポート\r2. 予測モデルの作成\r3. shapでの結果解釈\r4. 最後に\r\r\rおはこんばんにちは。かなり久しぶりではありますが、Pythonの勉強をかねて以前yahoo.keibaで収集した競馬のレース結果データから、レース結果を予想するモデルを作成したいと思います。\n1.データインポート\rまず、前回sqliteに保存したレース結果データをpandasデータフレームへ保存します。\nconn = sqlite3.connect(r\u0026#39;C:\\hogehoge\\horse_data.db\u0026#39;)\rsql = r\u0026#39;SELECT * FROM race_result\u0026#39;\rdf = pd.read_sql(con=conn,sql=sql)\rデータの中身を確認してみましょう。列は以下のようになっています。orderが着順となっています。\ndf.columns\r## Index([\u0026#39;order\u0026#39;, \u0026#39;frame_number\u0026#39;, \u0026#39;horse_number\u0026#39;, \u0026#39;trainer\u0026#39;, \u0026#39;passing_rank\u0026#39;,\r## \u0026#39;last_3F\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;margin\u0026#39;, \u0026#39;horse_name\u0026#39;, \u0026#39;horse_age\u0026#39;, \u0026#39;horse_sex\u0026#39;,\r## \u0026#39;horse_weight\u0026#39;, \u0026#39;horse_weight_change\u0026#39;, \u0026#39;brinker\u0026#39;, \u0026#39;jockey\u0026#39;,\r## \u0026#39;jockey_weight\u0026#39;, \u0026#39;jockey_weight_change\u0026#39;, \u0026#39;odds\u0026#39;, \u0026#39;popularity\u0026#39;,\r## \u0026#39;race_date\u0026#39;, \u0026#39;race_course\u0026#39;, \u0026#39;race_name\u0026#39;, \u0026#39;race_distance\u0026#39;, \u0026#39;type\u0026#39;,\r## \u0026#39;race_turn\u0026#39;, \u0026#39;race_condition\u0026#39;, \u0026#39;race_weather\u0026#39;, \u0026#39;colour\u0026#39;, \u0026#39;owner\u0026#39;,\r## \u0026#39;farm\u0026#39;, \u0026#39;locality\u0026#39;, \u0026#39;horse_birthday\u0026#39;, \u0026#39;father\u0026#39;, \u0026#39;mother\u0026#39;, \u0026#39;prize\u0026#39;,\r## \u0026#39;http\u0026#39;],\r## dtype=\u0026#39;object\u0026#39;)\rorderの中身を確認してみると、括弧（）がついている物が多く、また取消や中止、失格などが存在するため、文字型に認識されていることがわかります。ちなみに括弧（）内の順位は入線順位というやつで、他馬の走行を妨害したりして順位が降着させられたことを意味します（http://www.jra.go.jp/judge/）。\ndf.loc[:,\u0026#39;order\u0026#39;].unique()\r## array([\u0026#39;1\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;14\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;13\u0026#39;,\r## \u0026#39;4\u0026#39;, \u0026#39;16\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;取消\u0026#39;, \u0026#39;中止\u0026#39;, \u0026#39;除外\u0026#39;, \u0026#39;17\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;4(3)\u0026#39;, \u0026#39;2(1)\u0026#39;,\r## \u0026#39;3(2)\u0026#39;, \u0026#39;6(4)\u0026#39;, \u0026#39;失格\u0026#39;, \u0026#39;9(8)\u0026#39;, \u0026#39;16(6)\u0026#39;, \u0026#39;12(12)\u0026#39;, \u0026#39;13(9)\u0026#39;, \u0026#39;6(3)\u0026#39;,\r## \u0026#39;10(7)\u0026#39;, \u0026#39;6(5)\u0026#39;, \u0026#39;9(3)\u0026#39;, \u0026#39;11(8)\u0026#39;, \u0026#39;13(2)\u0026#39;, \u0026#39;12(9)\u0026#39;, \u0026#39;14(7)\u0026#39;,\r## \u0026#39;10(1)\u0026#39;, \u0026#39;16(8)\u0026#39;, \u0026#39;14(6)\u0026#39;, \u0026#39;10(3)\u0026#39;, \u0026#39;12(1)\u0026#39;, \u0026#39;13(6)\u0026#39;, \u0026#39;7(1)\u0026#39;,\r## \u0026#39;12(6)\u0026#39;, \u0026#39;6(2)\u0026#39;, \u0026#39;11(2)\u0026#39;, \u0026#39;15(6)\u0026#39;, \u0026#39;13(10)\u0026#39;, \u0026#39;14(4)\u0026#39;, \u0026#39;7(5)\u0026#39;,\r## \u0026#39;17(4)\u0026#39;, \u0026#39;9(7)\u0026#39;, \u0026#39;16(14)\u0026#39;, \u0026#39;12(11)\u0026#39;, \u0026#39;14(2)\u0026#39;, \u0026#39;8(2)\u0026#39;, \u0026#39;9(5)\u0026#39;,\r## \u0026#39;11(5)\u0026#39;, \u0026#39;12(7)\u0026#39;, \u0026#39;11(1)\u0026#39;, \u0026#39;12(8)\u0026#39;, \u0026#39;7(4)\u0026#39;, \u0026#39;5(4)\u0026#39;, \u0026#39;13(12)\u0026#39;,\r## \u0026#39;14(3)\u0026#39;, \u0026#39;10(2)\u0026#39;, \u0026#39;11(10)\u0026#39;, \u0026#39;18(3)\u0026#39;, \u0026#39;10(4)\u0026#39;, \u0026#39;15(8)\u0026#39;, \u0026#39;8(3)\u0026#39;,\r## \u0026#39;5(1)\u0026#39;, \u0026#39;10(5)\u0026#39;, \u0026#39;7(3)\u0026#39;, \u0026#39;5(2)\u0026#39;, \u0026#39;9(1)\u0026#39;, \u0026#39;13(3)\u0026#39;, \u0026#39;16(11)\u0026#39;,\r## \u0026#39;11(3)\u0026#39;, \u0026#39;18(15)\u0026#39;, \u0026#39;11(6)\u0026#39;, \u0026#39;10(6)\u0026#39;, \u0026#39;14(12)\u0026#39;, \u0026#39;12(5)\u0026#39;, \u0026#39;15(14)\u0026#39;,\r## \u0026#39;17(8)\u0026#39;, \u0026#39;18(6)\u0026#39;, \u0026#39;4(2)\u0026#39;, \u0026#39;18(10)\u0026#39;, \u0026#39;16(7)\u0026#39;, \u0026#39;13(1)\u0026#39;, \u0026#39;16(10)\u0026#39;,\r## \u0026#39;15(7)\u0026#39;, \u0026#39;9(4)\u0026#39;, \u0026#39;15(5)\u0026#39;, \u0026#39;12(3)\u0026#39;, \u0026#39;8(7)\u0026#39;, \u0026#39;15(2)\u0026#39;, \u0026#39;12(10)\u0026#39;,\r## \u0026#39;14(9)\u0026#39;, \u0026#39;3(1)\u0026#39;, \u0026#39;6(1)\u0026#39;, \u0026#39;14(5)\u0026#39;, \u0026#39;15(4)\u0026#39;, \u0026#39;11(4)\u0026#39;, \u0026#39;12(4)\u0026#39;,\r## \u0026#39;16(4)\u0026#39;, \u0026#39;9(2)\u0026#39;, \u0026#39;13(5)\u0026#39;, \u0026#39;12(2)\u0026#39;, \u0026#39;15(1)\u0026#39;, \u0026#39;4(1)\u0026#39;, \u0026#39;14(13)\u0026#39;,\r## \u0026#39;14(1)\u0026#39;, \u0026#39;13(7)\u0026#39;, \u0026#39;5(3)\u0026#39;, \u0026#39;8(6)\u0026#39;, \u0026#39;15(13)\u0026#39;, \u0026#39;7(2)\u0026#39;, \u0026#39;15(11)\u0026#39;,\r## \u0026#39;10(9)\u0026#39;, \u0026#39;11(9)\u0026#39;, \u0026#39;8(4)\u0026#39;, \u0026#39;15(3)\u0026#39;, \u0026#39;13(4)\u0026#39;, \u0026#39;16(12)\u0026#39;, \u0026#39;16(5)\u0026#39;,\r## \u0026#39;18(11)\u0026#39;, \u0026#39;10(8)\u0026#39;, \u0026#39;18(8)\u0026#39;, \u0026#39;14(8)\u0026#39;, \u0026#39;16(9)\u0026#39;, \u0026#39;8(5)\u0026#39;, \u0026#39;8(1)\u0026#39;,\r## \u0026#39;14(11)\u0026#39;, \u0026#39;9(6)\u0026#39;, \u0026#39;16(13)\u0026#39;, \u0026#39;16(15)\u0026#39;, \u0026#39;11(11)\u0026#39;, \u0026#39;15(10)\u0026#39;, \u0026#39;7(6)\u0026#39;],\r## dtype=object)\rまずここを修正しましょう。括弧を除去してint型に型変更し、入線順位は新たな列arriving orderとして追加します。\ndf[\u0026#39;arriving order\u0026#39;] = df[df.order.str.contains(r\u0026#39;\\d*\\(\\d*\\)\u0026#39;,regex=True)][\u0026#39;order\u0026#39;].replace(r\u0026#39;\\d+\\(\u0026#39;,r\u0026#39;\u0026#39;,regex=True).replace(r\u0026#39;\\)\u0026#39;,r\u0026#39;\u0026#39;,regex=True).astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;arriving order\u0026#39;].unique()\r## array([nan, 3., 1., 2., 4., 8., 6., 12., 9., 7., 5., 10., 14.,\r## 11., 15., 13.])\rdf[\u0026#39;order\u0026#39;] = df[\u0026#39;order\u0026#39;].replace(r\u0026#39;\\(\\d+\\)\u0026#39;,r\u0026#39;\u0026#39;,regex=True)\rdf = df[lambda df: ~df.order.str.contains(r\u0026#39;(取消|中止|除外|失格)\u0026#39;,regex=True)]\r## C:\\Users\\aashi\\Anaconda3\\envs\\umanalytics\\lib\\site-packages\\pandas\\core\\strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\r## return func(self, *args, **kwargs)\rdf[\u0026#39;order\u0026#39;] = df[\u0026#39;order\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;order\u0026#39;].unique()\r## array([ 1., 7., 2., 8., 5., 15., 6., 12., 11., 14., 3., 13., 4.,\r## 16., 9., 10., 17., 18.])\rきれいなfloat型に処理することができました。では、次にラスト3Fのタイムの前処理に移ります。前走のラスト3Fのタイムを予測に使用します。\nimport numpy as np\rdf[\u0026#39;last_3F\u0026#39;] = df[\u0026#39;last_3F\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False).astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;last_3F\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;last_3F\u0026#39;].shift(-1)\r前走のレースと順位、追加順位もデータセットへ含めましょう。\ndf[\u0026#39;prerace\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;race_name\u0026#39;].shift(-1)\rdf[\u0026#39;preorder\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;order\u0026#39;].shift(-1)\rdf[\u0026#39;prepassing\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;passing_rank\u0026#39;].shift(-1)\r出走時点で獲得している累積賞金額も追加します。\ndf[\u0026#39;preprize\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;prize\u0026#39;].shift(-1)\rdf[\u0026#39;preprize\u0026#39;] = df[\u0026#39;preprize\u0026#39;].fillna(0)\rdf[\u0026#39;margin\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;margin\u0026#39;].shift(-1)\rその他、欠損値やデータ型の修正、カテゴリデータのラベルエンコーディングです。\ndf[\u0026#39;horse_weight\u0026#39;] = df[\u0026#39;horse_weight\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;margin\u0026#39;] = df[\u0026#39;margin\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False)\rdf[\u0026#39;horse_age\u0026#39;] = df[\u0026#39;horse_age\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;horse_weight_change\u0026#39;] = df[\u0026#39;horse_weight_change\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;jockey_weight\u0026#39;] = df[\u0026#39;jockey_weight\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;race_distance\u0026#39;] = df[\u0026#39;race_distance\u0026#39;].replace(r\u0026#39;m\u0026#39;,r\u0026#39;\u0026#39;,regex=True).astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;race_turn\u0026#39;] = df[\u0026#39;race_turn\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False)\rdf.loc[df[\u0026#39;order\u0026#39;]!=1,\u0026#39;order\u0026#39;] = 0\rdf[\u0026#39;race_turn\u0026#39;] = df[\u0026#39;race_turn\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;colour\u0026#39;] = df[\u0026#39;colour\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;prepassing\u0026#39;] = df[\u0026#39;prepassing\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;prerace\u0026#39;] = df[\u0026#39;prerace\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;father\u0026#39;] = df[\u0026#39;father\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;mother\u0026#39;] = df[\u0026#39;mother\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rfrom sklearn import preprocessing\rcat_list = [\u0026#39;trainer\u0026#39;, \u0026#39;horse_name\u0026#39;, \u0026#39;horse_sex\u0026#39;, \u0026#39;brinker\u0026#39;, \u0026#39;jockey\u0026#39;, \u0026#39;race_course\u0026#39;, \u0026#39;race_name\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;race_turn\u0026#39;, \u0026#39;race_condition\u0026#39;, \u0026#39;race_weather\u0026#39;, \u0026#39;colour\u0026#39;, \u0026#39;father\u0026#39;, \u0026#39;mother\u0026#39;, \u0026#39;prerace\u0026#39;, \u0026#39;prepassing\u0026#39;]\rfor column in cat_list:\rtarget_column = df[column]\rle = preprocessing.LabelEncoder()\rle.fit(target_column)\rlabel_encoded_column = le.transform(target_column)\rdf[column] = pd.Series(label_encoded_column).astype(\u0026#39;category\u0026#39;)\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\rimport pandas_profiling as pdq\rprofile = pdq.ProfileReport(df)\rprofile\r\r2. 予測モデルの作成\rではLightGBMで予測モデルを作ってみます。optunaのLightGBMを使用して、ハイパーパラメータチューニングを行い、学習したモデルを用いて計算したテストデータの予測値と実績値のconfusion matrixならびに正解率を算出します。\nimport optuna.integration.lightgbm as lgb\rfrom sklearn.model_selection import train_test_split\ry = df[\u0026#39;order\u0026#39;]\rx = df.drop([\u0026#39;order\u0026#39;,\u0026#39;passing_rank\u0026#39;,\u0026#39;time\u0026#39;,\u0026#39;odds\u0026#39;,\u0026#39;popularity\u0026#39;,\u0026#39;owner\u0026#39;,\u0026#39;farm\u0026#39;,\u0026#39;locality\u0026#39;,\u0026#39;horse_birthday\u0026#39;,\u0026#39;http\u0026#39;,\u0026#39;prize\u0026#39;,\u0026#39;race_date\u0026#39;,\u0026#39;margin\u0026#39;],axis=1)\rX_train, X_test, y_train, y_test = train_test_split(x, y)\rX_train, x_val, y_train, y_val = train_test_split(X_train, y_train)\rlgb_train = lgb.Dataset(X_train, y_train)\rlgb_eval = lgb.Dataset(x_val, y_val)\rlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\rlgbm_params = {\r\u0026#39;objective\u0026#39;: \u0026#39;binary\u0026#39;,\r\u0026#39;boost_from_average\u0026#39;: False\r}\rbest_params, history = {}, []\rmodel = lgb.train(lgbm_params, lgb_train, categorical_feature = cat_list,valid_sets = lgb_eval, num_boost_round=100,early_stopping_rounds=20,best_params=best_params,tuning_history=history, verbose_eval=False)\rbest_params\rdef calibration(y_proba, beta):\rreturn y_proba / (y_proba + (1 - y_proba) / beta)\rsampling_rate = y_train.sum() / len(y_train)\ry_proba = model.predict(X_test, num_iteration=model.best_iteration)\ry_proba_calib = calibration(y_proba, sampling_rate)\ry_pred = np.vectorize(lambda x: 1 if x \u0026gt; 0.49 else 0)(y_proba_calib)\r可視化パートです。\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\rimport matplotlib.pyplot as plt\rimport seaborn as sns\r# AUC (Area Under the Curve) を計算する\rfpr, tpr, thresholds = roc_curve(y_test, y_pred)\rauc = auc(fpr, tpr)\r# ROC曲線をプロット\rplt.plot(fpr, tpr, label=\u0026#39;ROC curve (area = %.2f)\u0026#39;%auc)\rplt.legend()\rplt.title(\u0026#39;ROC curve\u0026#39;)\rplt.xlabel(\u0026#39;False Positive Rate\u0026#39;)\rplt.ylabel(\u0026#39;True Positive Rate\u0026#39;)\rplt.grid(True)\rplt.show()\rplt.close()\r# Confusion Matrixを生成\rConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()\r## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000004F873388\u0026gt;\rplt.show()\rplt.close()\raccuracy_score(y_test, y_pred)\r## 0.9300814465677578\rprecision_score(y_test, y_pred)\r## 0.9426605504587156\raccuracy_score（予測精度）が90%を超え、precision_Score（適合率、陽=1着と予想したデータの正解率）もいい感じです。\nrecall_score(y_test, y_pred)\r## 0.01211460236986382\rf1_score(y_test, y_pred)\r## 0.02392177405273267\r一方、recall_score(再現性、陽=1着のサンプルのうち実際に正解した割合)が低く偽陰性が高いことが確認できます。その結果、F1値も低くなっていますね。競馬予測モデルの場合、偽陰性が高いことは偽陽性が高いことよりはましなのですが、回収率を上げるためには偽陰性を下げることを頑張らなければいけません。これは今後の課題ですね。次節ではshapley値を使って要因分解をしたいと思います。。\n\r3. shapでの結果解釈\rimport shap\rshap.initjs()\r## \u0026lt;IPython.core.display.HTML object\u0026gt;\rexplainer = shap.TreeExplainer(model)\r## Setting feature_perturbation = \u0026quot;tree_path_dependent\u0026quot; because no background data was given.\rshap_values = explainer.shap_values(X_test)\r## LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\rまず、各特徴量の重要度を見ることにします。summary_plotメソッドを使用します。\nshap.summary_plot(shap_values, X_test)\r横軸は各特徴量の平均的な重要度を表しています(shap値の絶対値)。preprize(前走までの賞金獲得金額)やhorse_age、preorder(前走の着順)などが予測に重要であることが分かります。特にpreprizeの重要度は1着の予測、1着以外の予測どちらに対しても大きいです。horse_ageも同様です。ただ、これでは重要というだけで定性的な評価はできません。例えば、preprizeが大きい→1位になる確率が上昇といった関係が確認できれば、それは重要な情報になり得ます。次にそれを確認します。summary_plotメソッドを使用します。\nshap.summary_plot(shap_values[1], X_test)\r上図も各特徴量の重要度を表しています(今回は絶対値ではありません)。今回はそれぞれの特徴量の重要度がバイオリンプロットによって表されており、かつ特徴量の値の大きさで色分けがされています。例えば、preprizeだと横軸が0以上の部分でのみ赤色の分布が発生しており、ここからpreprizeの特徴量が大きい、つまり前走までの獲得賞金額が多いと平均的に1着の確率が上がるという当たり前の解釈をすることができます。\r他にも、horse_age,preorder,last_3Fは特徴量が小さくなるほど1着になる確率があがることも読み取れます。horse_weight, jokey_weightは大きくなるほど1着になる確率が上がるようです。一方、その他は特に定性的な関係を読み取ることはできません。\n次に、特徴量と確率の関係をより詳しく確認してみましょう。先ほど、preprizeは特徴量が大きくなるほど1着になる確率が上昇するということがわかりました。ただ、その確率の上昇は1次関数的に増加するのか、指数的に増大するのか、それとも\\(\\log x\\)のように逓減していくのか、わかりません。dependence_plotを使用してそれを確認してみましょう。\nshap.dependence_plot(ind=\u0026quot;preprize\u0026quot;, shap_values=shap_values[1], features=X_test)\r上図は学習したLightGBMをpreprizeの関数として見たときの概形をplotしたものです。先に確認したとおり、やはり特徴量が大きくなるにつれ、1着になる確率が上昇していきます。ただ、その上昇は徐々に逓減していき、2000万円を超えるところでほぼ頭打ちとなります。また、上図ではhorse_ageでの色分けを行っており、preprizeとの関係性も確認できるようになっています。やはり、直感と同じく、preprizeが高い馬の中でもhorse_ageが若い馬の1着確率が高くなることが見て取れます。\npreorderのdependence_plotも確認してみましょう。\nshap.dependence_plot(ind=\u0026quot;preorder\u0026quot;, shap_values=shap_values[1], features=X_test)\rやはり、前走の着順が上位になるほど1着確率が高まることがここからも分かります。また、その確率は6着以上とそれ以外で水準感が変わることも分かります。last_3Fのタイムとの関係性も確認していますが、こちらはあまり関連性はなさそうです。\n\r4. 最後に\rLightGBMを使用し、競馬の予測モデルを作成してみました。さすがLightGBMといった感じで、予測精度は高かったです。また、shap値を使用した重要特徴量の検出も上手くいきました。これによって、LightGBMの気持ちを理解し、より良い特徴量の発見を進めていくことでモデリングの精度を高めていこうと思います。\n\r","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1582934400,"objectID":"eb4dfbeb63c342ef9c2eb0c104bfc7e1","permalink":"/post/post16/","publishdate":"2020-02-29T00:00:00Z","relpermalink":"/post/post16/","section":"post","summary":"Kagglerが大好きなLightGBMで競馬の順位予想してみました。","tags":["Python","前処理","機械学習"],"title":"LightGBMを使用して競馬結果を予想してみる","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r1. データ収集\r2. 月次解析パート\r3. 日次解析パート\r\r\rおはこんばんにちは。とある理由で10年物長期金利のフィッティングを行いたいと思いました。というわけで、USデータを用いて解析していきます。まず、データを収集しましょう。quantmodパッケージを用いて、FREDからデータを落とします。getsymbols(キー,from=開始日,src=\"FRED\", auto.assign=TRUE)で簡単にできちゃいます。ちなみにキーはFREDのHPで確認できます。\n1. データ収集\rlibrary(quantmod)\r# data name collected\rsymbols.name \u0026lt;- c(\u0026quot;10-Year Treasury Constant Maturity Rate\u0026quot;,\u0026quot;Effective Federal Funds Rate\u0026quot;,\u0026quot;\rConsumer Price Index for All Urban Consumers: All Items\u0026quot;,\u0026quot;Civilian Unemployment Rate\u0026quot;,\u0026quot;3-Month Treasury Bill: Secondary Market Rate\u0026quot;,\u0026quot;Industrial Production Index\u0026quot;,\u0026quot;\r10-Year Breakeven Inflation Rate\u0026quot;,\u0026quot;Trade Weighted U.S. Dollar Index: Broad, Goods\u0026quot;,\u0026quot;\rSmoothed U.S. Recession Probabilities\u0026quot;,\u0026quot;Moody\u0026#39;s Seasoned Baa Corporate Bond Yield\u0026quot;,\u0026quot;5-Year, 5-Year Forward Inflation Expectation Rate\u0026quot;,\u0026quot;Personal Consumption Expenditures\u0026quot;)\r# Collect economic data\rsymbols \u0026lt;- c(\u0026quot;GS10\u0026quot;,\u0026quot;FEDFUNDS\u0026quot;,\u0026quot;CPIAUCSL\u0026quot;,\u0026quot;UNRATE\u0026quot;,\u0026quot;TB3MS\u0026quot;,\u0026quot;INDPRO\u0026quot;,\u0026quot;T10YIEM\u0026quot;,\u0026quot;TWEXBMTH\u0026quot;,\u0026quot;RECPROUSM156N\u0026quot;,\u0026quot;BAA\u0026quot;,\u0026quot;T5YIFRM\u0026quot;,\u0026quot;PCE\u0026quot;)\rgetSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, src = \u0026quot;FRED\u0026quot;, auto.assign = TRUE)\r## [1] \u0026quot;GS10\u0026quot; \u0026quot;FEDFUNDS\u0026quot; \u0026quot;CPIAUCSL\u0026quot; \u0026quot;UNRATE\u0026quot; ## [5] \u0026quot;TB3MS\u0026quot; \u0026quot;INDPRO\u0026quot; \u0026quot;T10YIEM\u0026quot; \u0026quot;TWEXBMTH\u0026quot; ## [9] \u0026quot;RECPROUSM156N\u0026quot; \u0026quot;BAA\u0026quot; \u0026quot;T5YIFRM\u0026quot; \u0026quot;PCE\u0026quot;\rmacro_indicator \u0026lt;- merge(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE)\rrm(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE,USEPUINDXD)\r\r2. 月次解析パート\rデータは\rこちら\rから参照できます。では、推計用のデータセットを作成していきます。被説明変数は10-Year Treasury Constant Maturity Rate(GS10)です。説明変数は以下の通りです。\n\r\r説明変数名\rキー\r代理変数\r\r\r\rFederal Funds Rate\rFEDFUNDS\r短期金利\r\rConsumer Price Index\rCPIAUCSL\r物価\r\rUnemployment Rate\rUNRATE\r雇用関連\r\r3-Month Treasury Bill\rTB3MS\r短期金利\r\rIndustrial Production Index\rINDPRO\r景気\r\rBreakeven Inflation Rate\rT10YIEM\r物価\r\rTrade Weighted Dollar Index\rTWEXBMTH\r為替\r\rRecession Probabilities\rRECPROUSM156N\r景気\r\rMoody’s Seasoned Baa Corporate Bond Yield\rBAA\rリスクプレミアム\r\rInflation Expectation Rate\rT5YIFRM\r物価\r\rPersonal Consumption Expenditures\rPCE\r景気\r\rEconomic Policy Uncertainty Index\rUSEPUINDXD\r政治\r\r\r\rかなり適当な変数選択ではあるんですが、マクロモデリング的に長期金利ってどうやってモデル化するかというとちゃんとやってない場合が多いです。DSGEでは効率市場仮説に従って10年先までの短期金利のパスをリンクしたものと長期金利が等しくなると定式化するのが院生時代のモデリングでした（マクロファイナンスの界隈ではちゃんとやってそう）。そういうわけで、短期金利を説明変数に加えています。そして、短期金利に影響を与えるであろう物価にかかる指標も3つ追加しました。加えて、景気との相関が強いことはよく知られているので景気に関するデータも追加しました。これらはそもそもマクロモデルでは短期金利は以下のようなテイラールールに従うとモデリングすることが一般的であることが背景にあります。\n\\[\rr_t = \\rho r_{t-1} + \\alpha \\pi_{t} + \\beta y_{t}\r\\]\rここで、\\(r_t\\)は政策金利（短期金利）、\\(\\pi_t\\)はインフレ率、\\(y_t\\)はoutputです。\\(\\rho, \\alpha, \\beta\\)はdeep parameterと呼ばれるもので、それぞれ慣性、インフレ率への金利の感応度、outputに対する感応度を表しています。\\(\\rho=0,\\beta=0\\)の時、\\(\\alpha\u0026gt;=1\\)でなければ合理的期待均衡解が得られないことは「テイラーの原理」として有名です。\rその他、Corporate bondとの裁定関係も存在しそうなMoody's Seasoned Baa Corporate Bond Yieldも説明変数に追加しています。また、欲を言えばVIX指数と財政に関する指標を追加したいところです。財政に関する指数はQuateryかAnnualyなので今回のようなmonthlyの推計には使用することができません。この部分は最もネックなところです。なにか考え付いたら再推計します。\nでは、推計に入ります。今回は説明変数が多いのでlasso回帰を行い、有効な変数を絞り込みたいと思います。また、比較のためにOLSもやります。説明変数は被説明変数の1期前の値を使用します。おそらく、1期前でもデータの公表時期によっては翌月の推計に間に合わない可能性もありますが、とりあえずこれでやってみます。\n# make dataset\rtraindata \u0026lt;- na.omit(merge(macro_indicator[\u0026quot;2003-01-01::2015-12-31\u0026quot;][,1],stats::lag(macro_indicator[\u0026quot;2003-01-01::2015-12-31\u0026quot;][,-1],1)))\rtestdata \u0026lt;- na.omit(merge(macro_indicator[\u0026quot;2016-01-01::\u0026quot;][,1],stats::lag(macro_indicator[\u0026quot;2016-01-01::\u0026quot;][,-1],1)))\r# fitting OLS\rtrial1 \u0026lt;- lm(GS10~.,data = traindata)\rsummary(trial1)\r## ## Call:\r## lm(formula = GS10 ~ ., data = traindata)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.76208 -0.21234 0.00187 0.21595 0.70493 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 14.3578405 4.3524691 3.299 0.001226 ** ## FEDFUNDS -0.2011132 0.1438774 -1.398 0.164335 ## CPIAUCSL -0.0702011 0.0207761 -3.379 0.000938 ***\r## UNRATE -0.2093502 0.0796052 -2.630 0.009477 ** ## TB3MS 0.2970160 0.1413796 2.101 0.037410 * ## INDPRO -0.0645376 0.0260343 -2.479 0.014339 * ## T10YIEM 1.1484487 0.1769925 6.489 1.32e-09 ***\r## TWEXBMTH -0.0317345 0.0118155 -2.686 0.008091 ** ## RECPROUSM156N -0.0099083 0.0021021 -4.713 5.72e-06 ***\r## BAA 0.7793520 0.0868628 8.972 1.49e-15 ***\r## T5YIFRM -0.4551318 0.1897695 -2.398 0.017759 * ## PCE 0.0009087 0.0002475 3.672 0.000339 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.2981 on 143 degrees of freedom\r## Multiple R-squared: 0.9203, Adjusted R-squared: 0.9142 ## F-statistic: 150.1 on 11 and 143 DF, p-value: \u0026lt; 2.2e-16\r自由度修正済み決定係数高めですね。2015/12/31までのモデルを使って、アウトサンプルのデータ(2016/01/01~)を予測し、平均二乗誤差を計算します。\nest.OLS.Y \u0026lt;- predict(trial1,testdata[,-1])\rY \u0026lt;- as.matrix(testdata[,1])\rmse.OLS \u0026lt;- sum((Y - est.OLS.Y)^2) / length(Y)\rmse.OLS\r## [1] 0.1431734\r次にlasso回帰です。Cross Validationを行い、\\(\\lambda\\)を決めるglmnetパッケージのcv.glmnet関数を使用します。\n# fitting lasso regression\rlibrary(glmnet)\rtrial2 \u0026lt;- cv.glmnet(as.matrix(traindata[,-1]),as.matrix(traindata[,1]),family=\u0026quot;gaussian\u0026quot;,alpha=1)\rplot(trial2)\rtrial2$lambda.min\r## [1] 0.000436523\rcoef(trial2,s=trial2$lambda.min)\r## 12 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## 1\r## (Intercept) 12.0180676188\r## FEDFUNDS -0.1041665345\r## CPIAUCSL -0.0574470880\r## UNRATE -0.1919723880\r## TB3MS 0.2110222475\r## INDPRO -0.0610115260\r## T10YIEM 1.1688912397\r## TWEXBMTH -0.0242324285\r## RECPROUSM156N -0.0095154487\r## BAA 0.7600115062\r## T5YIFRM -0.4575241038\r## PCE 0.0007486169\rUnemployment Rate、3-Month Treasury Bill、Breakeven Inflation Rate、Moody's Seasoned Baa Corporate Bond Yield、Inflation Expectation Rateの回帰係数が大きくなるという結果ですね。失業率以外は想定内の結果です。ただ、今回の結果を見る限り景気との相関は低そうです（逆向きにしか効かない？）。MSEを計算します。\nest.lasso.Y \u0026lt;- predict(trial2, newx = as.matrix(testdata[,-1]), s = trial2$lambda.min, type = \u0026#39;response\u0026#39;)\rmse.lasso \u0026lt;- sum((Y - est.lasso.Y)^2) / length(Y)\rmse.lasso\r## [1] 0.1318541\rlasso回帰のほうが良い結果になりました。lasso回帰で計算した予測値と実績値を時系列プロットしてみます。\nlibrary(tidyverse)\rggplot(gather(data.frame(actual=Y[,1],lasso_prediction=est.lasso.Y[,1],OLS_prediction=est.OLS.Y,date=as.POSIXct(rownames(Y))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +\rgeom_line(size=1.5) +\rscale_x_datetime(breaks = \u0026quot;6 month\u0026quot;,date_labels = \u0026quot;%Y-%m\u0026quot;) +\rscale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,3.5))\r方向感はいい感じです。一方で、2016年1月からや2018年12月以降の急激な金利低下は予測できていません。この部分については何か変数を考えるorローリング推計を実施する、のいずれかをやってみないと精度が上がらなそうです。\n\r3. 日次解析パート\r月次での解析に加えて日次での解析もやりたいとおもいます。日次データであればデータの公表は市場が閉まり次第の場合が多いので、いわゆるjagged edgeの問題が起こりにくいと思います。まずは日次データの収集から始めます。\n# data name collected\rsymbols.name \u0026lt;- c(\u0026quot;10-Year Treasury Constant Maturity Rate\u0026quot;,\u0026quot;Effective Federal Funds Rate\u0026quot;,\u0026quot;\r6-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar\u0026quot;,\u0026quot;NASDAQ Composite Index\u0026quot;,\u0026quot;3-Month Treasury Bill: Secondary Market Rate\u0026quot;,\u0026quot;Economic Policy Uncertainty Index for United States\u0026quot;,\u0026quot;\r10-Year Breakeven Inflation Rate\u0026quot;,\u0026quot;Trade Weighted U.S. Dollar Index: Broad, Goods\u0026quot;,\u0026quot;Moody\u0026#39;s Seasoned Baa Corporate Bond Yield\u0026quot;,\u0026quot;5-Year, 5-Year Forward Inflation Expectation Rate\u0026quot;)\r# Collect economic data\rsymbols \u0026lt;- c(\u0026quot;DGS10\u0026quot;,\u0026quot;DFF\u0026quot;,\u0026quot;USD6MTD156N\u0026quot;,\u0026quot;NASDAQCOM\u0026quot;,\u0026quot;DTB3\u0026quot;,\u0026quot;USEPUINDXD\u0026quot;,\u0026quot;T10YIE\u0026quot;,\u0026quot;DTWEXB\u0026quot;,\u0026quot;DBAA\u0026quot;,\u0026quot;T5YIFR\u0026quot;)\rgetSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, src = \u0026quot;FRED\u0026quot;, auto.assign = TRUE)\r## [1] \u0026quot;DGS10\u0026quot; \u0026quot;DFF\u0026quot; \u0026quot;USD6MTD156N\u0026quot; \u0026quot;NASDAQCOM\u0026quot; \u0026quot;DTB3\u0026quot; ## [6] \u0026quot;USEPUINDXD\u0026quot; \u0026quot;T10YIE\u0026quot; \u0026quot;DTWEXB\u0026quot; \u0026quot;DBAA\u0026quot; \u0026quot;T5YIFR\u0026quot;\rNASDAQCOM.r \u0026lt;- ROC(na.omit(NASDAQCOM))\rmacro_indicator.d \u0026lt;- merge(DGS10,DFF,USD6MTD156N,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)\rrm(DGS10,DFF,USD6MTD156N,NASDAQCOM,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)\r次にデータセットを構築します。学習用と訓練用にデータを分けます。実際の予測プロセスを考え、2営業日前のデータを説明変数に用いています。\n# make dataset\rtraindata.d \u0026lt;- na.omit(merge(macro_indicator.d[\u0026quot;1980-01-01::2010-12-31\u0026quot;][,1],stats::lag(macro_indicator.d[\u0026quot;1980-01-01::2010-12-31\u0026quot;][,-1],2)))\rtestdata.d \u0026lt;- na.omit(merge(macro_indicator.d[\u0026quot;2010-01-01::\u0026quot;][,1],stats::lag(macro_indicator.d[\u0026quot;2010-01-01::\u0026quot;][,-1],2)))\r# fitting OLS\rtrial1.d \u0026lt;- lm(DGS10~.,data = traindata.d)\rsummary(trial1.d)\r## ## Call:\r## lm(formula = DGS10 ~ ., data = traindata.d)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.82445 -0.12285 0.00469 0.14332 0.73789 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -3.5051208 0.1690503 -20.734 \u0026lt; 2e-16 ***\r## DFF 0.0818269 0.0239371 3.418 0.000653 ***\r## USD6MTD156N -0.0135771 0.0233948 -0.580 0.561799 ## NASDAQCOM -0.3880217 0.4367334 -0.888 0.374483 ## DTB3 0.1227984 0.0280283 4.381 1.29e-05 ***\r## USEPUINDXD -0.0006611 0.0001086 -6.087 1.58e-09 ***\r## T10YIE 0.6980971 0.0355734 19.624 \u0026lt; 2e-16 ***\r## DTWEXB 0.0270128 0.0012781 21.135 \u0026lt; 2e-16 ***\r## DBAA 0.2988122 0.0182590 16.365 \u0026lt; 2e-16 ***\r## T5YIFR 0.3374944 0.0381111 8.856 \u0026lt; 2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.2173 on 1114 degrees of freedom\r## Multiple R-squared: 0.8901, Adjusted R-squared: 0.8892 ## F-statistic: 1002 on 9 and 1114 DF, p-value: \u0026lt; 2.2e-16\r依然決定係数は高めです。\nest.OLS.Y.d \u0026lt;- predict(trial1.d,testdata.d[,-1])\rY.d \u0026lt;- as.matrix(testdata.d[,1])\rmse.OLS.d \u0026lt;- sum((Y.d - est.OLS.Y.d)^2) / length(Y.d)\rmse.OLS.d\r## [1] 0.8003042\r次にlasso回帰です。CVで\\(\\lambda\\)を決定。\n# fitting lasso regression\rtrial2.d \u0026lt;- cv.glmnet(as.matrix(traindata.d[,-1]),as.matrix(traindata.d[,1]),family=\u0026quot;gaussian\u0026quot;,alpha=1)\rplot(trial2.d)\rtrial2.d$lambda.min\r## [1] 0.001472377\rcoef(trial2.d,s=trial2.d$lambda.min)\r## 10 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## 1\r## (Intercept) -3.4186530904\r## DFF 0.0707022021\r## USD6MTD156N . ## NASDAQCOM -0.2675513858\r## DTB3 0.1204092358\r## USEPUINDXD -0.0006506183\r## T10YIE 0.6915446819\r## DTWEXB 0.0270389569\r## DBAA 0.2861031504\r## T5YIFR 0.3376304446\rliborの係数値が0になりました。MSEはOLSの方が高い結果に。\nest.lasso.Y.d \u0026lt;- predict(trial2.d, newx = as.matrix(testdata.d[,-1]), s = trial2.d$lambda.min, type = \u0026#39;response\u0026#39;)\rmse.lasso.d \u0026lt;- sum((Y.d - est.lasso.Y.d)^2) / length(Y.d)\rmse.lasso.d\r## [1] 0.8378427\r予測値をプロットします。\nggplot(gather(data.frame(actual=Y.d[,1],lasso_prediction=est.lasso.Y.d[,1],OLS_prediction=est.OLS.Y.d,date=as.POSIXct(rownames(Y.d))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +\rgeom_line(size=1.5) +\rscale_x_datetime(breaks = \u0026quot;2 year\u0026quot;,date_labels = \u0026quot;%Y-%m\u0026quot;) +\rscale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,5))\r月次と同じく、OLSとlassoで予測値に差はほとんどありません。なかなかいい感じに変動を捉えることができていますが、2011年のUnited States federal government credit-rating downgradesによる金利低下や2013年の景気回復に伴う金利上昇は捉えることができていません。日次の景気指標はPOSデータくらいしかないんですが、強いて言うなら最近使用した夜間光の衛星画像データなんかは使えるかもしれません。時間があればやってみます。とりあえず、いったんこれでこの記事は終わりたいと思います。ここまで読み進めていただき、ありがとうございました。\n\r","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1563235200,"objectID":"4bae285f397707d2b9754abbeb8ebce3","permalink":"/post/post14/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/post/post14/","section":"post","summary":"単なる思い付きです。","tags":["R","時系列解析","金融"],"title":"10年物長期金利をフィッティングしてみる","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r1. Earth Engineを使うための事前準備\r2. Python APIを用いた衛星画像データの取得\r\rImage…\rImageCollection…\rFeatureCollection…\r\r\r\r皆さんおはこんばんにちわ。前回、GPLVMモデルを用いたGDP予測モデルを構築しました。ただ、ナウキャスティングというからにはオルタナティブデータを用いた解析を行いたいところではあります。ふと、以下の記事を見つけました。\n焦点：ナウキャストのＧＤＰ推計、世界初の衛星画像利用　利用拡大も\nこちらは東京大学の渡辺努先生が人工衛星画像を用いてGDP予測モデルを開発したというものです。記事には\n\r米国の海洋大気庁が運営する気象衛星「スオミＮＰＰ」が日本上空を通過する毎日午前１時３０分時点の画像を購入し、縦、横７２０メートル四方のマス目ごとの明るさを計測する。同じ明るさでも、農地、商業用地、工業用地など土地の用途によって経済活動の大きさが異なるため、国土地理院の土地利用調査を参照。土地の用途と、明るさが示す経済活動の相関を弾き出し、この結果を考慮した上で、明るさから経済活動の大きさを試算する。\r（中略）衛星画像のように誰もが入手可能な公表データであれば、政府、民間の区別なく分析が可能であるため、渡辺氏はこれを「統計の民主化」と呼び、世界的な潮流になると予想している。\n\rと書かれており、衛星写真を用いた分析に興味を惹かれました。 衛星写真って誰でも利用可能か？というところですが、GoogleがEarth Engineというサービスを提供していることがわかりました。\nhttps://earthengine.google.com/\n\r（拙訳）Google Earth Engineは、数ペタバイトの衛星画像群と地理空間データセットを惑星規模の解析機能と組み合わせ、科学者、研究者、開発者が変化を検出し、傾向を射影し、地球の変容を定量化することを可能にします。\n\r研究・教育・非営利目的ならば、なんと無料で衛星写真データを解析することができます。具体的に何ができるのかは以下の動画を見てください。\n\r今回はそんなEath Engineのpython APIを用いて衛星画像データを取得し、解析していきたいと思います。\n1. Earth Engineを使うための事前準備\rEarth Engineを使用するためには、Google Accountを使って申請を行う必要があります。先ほどの画像の右上の「Sign Up」からできます。申請を行って、Gmailに以下のようなメールが来るととりあえずEarth Engineは使用できるようになります。\nとりあえずというのはWEB上のEarth Engine コードエディタは使用できるということです。コードエディタというのは以下のようなもので、ブラウザ上でデータを取得したり、解析をしたり、解析結果をMAPに投影したりすることができる便利ツールです。Earth Engineの本体はむしろこいつで、APIは副次的なものと考えています。\n真ん中のコードエディタにコードを打っていきますが、言語はjavascriptです(APIはpythonとjavascript両方あるんですけどね)。解析結果をMAPに投影したり、reference（左）を参照したり、Consoleに吐き出したデータを確認することができるのでかなり便利です。が、データを落とした後で高度な解析を行いたい場合はpythonを使ったほうが慣れているので今回はAPIを使用しています。\r話が脱線しました。さて、Earth Engineの承認を得たら、pipでearthengine-apiをインストールしておきます。そして、コマンドプロンプト上で、earthengine authenticateと打ちます。そうすると、勝手にブラウザが立ち上がり、以下のようにpython apiのauthenticationを行う画面がでますので「次へ」を押下します。\n次に以下のような画面にいきますので、そのまま承認します。これでauthenticationの完成です。pythonからAPIが使えます。\n\r2. Python APIを用いた衛星画像データの取得\rPython APIを使用する準備ができました。ここからは衛星画像データを取得していきます。以下にあるようにEarth Engineにはたくさんのデータセットが存在します。\nhttps://developers.google.com/earth-engine/datasets/\n今回はVIIRS Stray Light Corrected Nighttime Day/Night Band Composites Version 1というデータセットを使用します。このデータセットは世界中の夜間光の光量を月次単位で平均し、提供するものです。サンプル期間は2014-01~現在です。\nEarth Engineにはいくつかの固有なデータ型が存在します。覚えておくべきものは以下の3つです。\nImage…\rある１時点におけるrasterデータです。imageオブジェクトはいくつかのbandで構成されています。このbandはデータによって異なりますが、おおよそのデータはbandそれぞれがRGB値を表していたりします。Earth Engineを使用する上で最も基本的なデータです。\n\rImageCollection…\rImageオブジェクトを時系列に並べたオブジェクトです。今回は時系列解析をするのでこのデータを使用します。\n\rFeatureCollection…\rGeoJSON Featureです。地理情報を表すGeometryオブジェクトやそのデータのプロパティ（国名等）が格納されています。今回は日本の位置情報を取得する際に使用しています。\nではコーディングしていきます。まず、日本の地理情報のFeatureCollectionオブジェクトを取得します。地理情報はFusion Tablesに格納されていますので、IDで引っ張りCountryがJapanのものを抽出します。ee.FeatureCollection()の引数にIDを入力すれば簡単に取得できます。\nimport ee\rfrom dateutil.parser import parse\ree.Initialize()\r# get Japan geometory as FeatureCollection from fusion table\rjapan = ee.FeatureCollection(\u0026#39;ft:1tdSwUL7MVpOauSgRzqVTOwdfy17KDbw-1d9omPw\u0026#39;).filter(ee.Filter.eq(\u0026#39;Country\u0026#39;, \u0026#39;Japan\u0026#39;))\r次に夜間光の衛星画像を取得してみます。こちらもee.ImageCollection()にデータセットのIDを渡すと取得できます。なお、ここではbandを月次の平均光量であるavg_radに抽出しています。\n# get night-light data from earth engine from 2014-01-01 to 2019-01-01\rdataset = ee.ImageCollection(\u0026#39;NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\u0026#39;).filter(ee.Filter.date(\u0026#39;2014-01-01\u0026#39;,\u0026#39;2019-01-01\u0026#39;)).select(\u0026#39;avg_rad\u0026#39;)\r取得した衛星画像を日本周辺に切り出し、画像ファイルとして出力してみましょう。画像ファイルの出力はimageオブジェクトで可能です（そうでないと画像がたくさん出てきてしまいますからね。。。）。今取得したのはImageCollectionオブジェクトですからImageオブジェクトへ圧縮してやる必要があります（上がImageCollectionオブジェクト、下が圧縮されたImageオブジェクト）。\nここでは、ImageCollectionオブジェクトの中にあるのImageオブジェクトの平均値をとってサンプル期間の平均的な画像を出力してみたいと思います。ImageCollection.mean()でできます。また、.visualize({min:0.5})でピクセル値が0.5以上でフィルターをかけています。こうしないと雲と思われるものやゴミ？みたいなものがついてしまいます。次に、ここまで加工した画像データをダウンロードするurlを.getDownloadURLメソッドで取得しています。その際、regionで切り出す範囲をポリゴン値で指定し、scaleでデータの解像度を指定しています（scaleが小さすぎると処理が重すぎるらしくエラーが出て処理できません）。\ndataset.mean().visualize(min=0.5).getDownloadURL(dict(name=\u0026#39;thumbnail\u0026#39;,region=[[[120.3345348936478, 46.853488838010854],[119.8071911436478, 24.598157870729043],[148.6353161436478, 24.75788466523463],[149.3384411436478, 46.61252884462868]]],scale=5000))\r取得した画像が以下です。\nやはり、東京を中心とした関東圏、大阪を中心とした関西圏、愛知、福岡、北海道（札幌周辺）の光量が多く、経済活動が活発であることがわかります。また、陸内よりも沿岸部で光量が多い地域があることがわかります。これは経済活動とは直接関係しない現象のような気もします。今回は分析対象外ですが、北緯38度を境に北側が真っ暗になるのが印象的です。これは言うまでもなく北朝鮮と韓国の境界線ですから、両国の経済活動水準の差が視覚的にコントラストされているのでしょう。今回使用したデータセットは2014年からのものですが、他のデータセットでは1990年代からのデータが取得できるものもあります（その代わり最近のデータは取れませんが）。それらを用いて朝鮮半島や中国の経済発展を観察するのも面白いかもしれません。\nさて、画像は取得できましたがこのままでは解析ができません。ここからは夜間光をピクセル値にマッピングしたデータを取得し、数値的な解析を試みます。ただ、先ほどとはデータ取得の手続きが少し変わります。というのも、今度は日本各地で各ピクセル単位ごとにさまざまな値をとる夜間光を集約し、1つの代用値にしなければならないからです。ピクセルごとの数値を手に入れたところで解析するには手に余ってしまいますからね。イメージは以下のような感じです（Earth Engineサイトから引用）。\n先ほど取得した夜間光のImageCollectionのある1時点の衛星画像が左です。その中に日本というRegionが存在し、それをee.Reducerによって定量的に集約（aggregate）します。Earth Engine APIには.reduceRegions()メソッドが用意されていますのでそれを用いればいいです。引数は、reducer=集約方法（ここでは合計値）、collection=集約をかけるregion（FeatureCollectionオブジェクト）、scale=解像度、です。以下では、ImageCollection（dataset）の中にある1番目のImageオブジェクトに.reduceRegions()メソッドをかけています。\n# initialize output box\rtime0 = dataset.first().get(\u0026#39;system:time_start\u0026#39;);\rfirst = dataset.first().reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(\u0026#39;time_start\u0026#39;, time0)\r我々は時系列データが欲しいわけですから、ImageCollection内にあるImageそれぞれに対して同じ処理を行う必要があります。Earth Engineにはiterateという便利な関数があり、引数に処理したい関数を渡せばfor文いらずでこの処理を行ってくれます。ここではImageオブジェクトにreduceRegionsメソッドを処理したComputed Objectを以前に処理したものとmergeするmyfuncという関数を定義し、それをiterateに渡しています。最後に、先ほどと同じく生成したデータをgetDownloadURLメソッドを用いてurlを取得しています（ファイル形式はcsv）。\n# define reduceRegions function for iteration\rdef myfunc(image,first):\radded = image.reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(\u0026#39;time_start\u0026#39;, image.get(\u0026#39;system:time_start\u0026#39;))\rreturn ee.FeatureCollection(first).merge(added)\r# implement iteration\rnightjp = dataset.filter(ee.Filter.date(\u0026#39;2014-02-01\u0026#39;,\u0026#39;2019-01-01\u0026#39;)).iterate(myfunc,first)\r# get url to download\ree.FeatureCollection(nightjp).getDownloadURL(filetype=\u0026#39;csv\u0026#39;,selectors=ee.FeatureCollection(nightjp).first().propertyNames().getInfo())\rCSVファイルのurlが取得できました。この時系列をプロットして今日は終わりにしたいと思います。\rデータを読み込むとこんな感じです。\nimport pandas as pd\rimport matplotlib.pyplot as plt\rimport os\ros.environ[\u0026#39;QT_QPA_PLATFORM_PLUGIN_PATH\u0026#39;] = \u0026#39;C:/Users/aashi/Anaconda3/Library/plugins/platforms\u0026#39;\rplt.style.use(\u0026#39;ggplot\u0026#39;)\rnightjp_csv.head()\r## system:index sum Country Unnamed: 3 Unnamed: 4\r## 0 2014/1/1 881512.4572 Japan NaN NaN\r## 1 2014/2/1 827345.3551 Japan NaN NaN\r## 2 2014/3/1 729110.4619 Japan NaN NaN\r## 3 2014/4/1 612665.8866 Japan NaN NaN\r## 4 2014/5/1 661434.5027 Japan NaN NaN\rplt.plot(pd.to_datetime(nightjp_csv[\u0026#39;system:index\u0026#39;]),nightjp_csv[\u0026#39;sum\u0026#39;])\rかなり季節性がありますね。冬場は日照時間が少ないこともあって光量が増えているみたいです。それにしても急激な増え方ですが。次回はこのデータと景況感の代理変数となる経済統計を元に統計解析を行いたいと思います。おたのしみに。\n\r\r","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1563235200,"objectID":"59f787335065eeaf96823fbe7b34c9e7","permalink":"/post/post12/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/post/post12/","section":"post","summary":"オルタナティブデータを用いた解析やってみました。","tags":["Python","前処理","Earth Engine"],"title":"Google Earth Engine APIで衛星画像データを取得し、景況感をナウキャスティングしてみる","type":"post"},{"authors":null,"categories":["競馬"],"content":"2回目になりますが、またrvestで過去のレース結果を落としてみたいと思います。過去の記事を見てないという人は先にそちらをご覧になられることをお勧めします。\n今回データを取り直そうと思ったのは、競馬の分析をした際により多くの項目を説明変数に加えて、分析をしたいと思ったからです。なので、今回は前回のRスクリプトに追記を行う形でプログラムを作成しました。新たに追加したデータ項目は以下の14個です。\n芝かダートか\r右回りか左回りか\rレースコンディション（良や稍重など）\r天候\r馬の毛色（栗毛、鹿毛など）\r馬主\r生産者\r産地\r生年月日\r父馬\r母馬\rそのレースまでの獲得賞金（2003年から入手可能）\rジョッキーの体重\rジョッキーの体重の増減\r\r実はまだデータ収集は終わっていなくて、Rのプログラムがずっと実行中になっています（3日くらい回しています）。しかし、プログラム自体はきっちり回っているのでスクリプトの紹介をしていこうと思います。もしかしたら追記で結果を書くかもしれません。\n1. スクリプトの中身\rまずはパッケージの呼び出しです。\n# rvestによる競馬データのwebスクレイピング\r#install.packages(\u0026quot;rvest\u0026quot;)\r#if (!require(\u0026quot;pacman\u0026quot;)) install.packages(\u0026quot;pacman\u0026quot;)\r#install.packages(\u0026quot;beepr\u0026quot;)\r#install.packages(\u0026quot;RSQLite\u0026quot;)\rpacman::p_load(qdapRegex)\rlibrary(rvest)\rlibrary(stringr)\rlibrary(dplyr)\rlibrary(beepr)\rlibrary(RSQLite)\rかなりwarnningが出るのでそれを禁止し、SQLiteに接続しています\n# warnning禁止\roptions(warn=-1)\r# SQLiteへの接続\rcon = dbConnect(SQLite(), \u0026quot;horse_data.db\u0026quot;, synchronous=\u0026quot;off\u0026quot;)\r1994年からしかオッズが取れないので、1994年から直近までのデータを取得します。yahoo競馬では月ごとにレースがまとめられているので、それを変数として使用しながらデータをとっていきます。基本的には、該当年、該当月のレース結果一覧へアクセスし、そのページ上の各日の個々の競馬場ごとのタイムテーブルへのリンクを取得します。個々の競馬場でレースはだいたい12ほどあるので、そのリンクを取得し、各レースのレース結果ページにアクセスします。そして、レース結果を取得していきます。まず、各日の個々の競馬場ごとのタイムテーブルへのリンクの取得方法です。\nfor(year in 1994:2019){\rstart.time \u0026lt;- Sys.time() # 計算時間を図る\r# yahoo競馬のレース結果一覧ページの取得\rfor (k in 1:12){ # kは月を表す\rtryCatch(\r{\rkeiba.yahoo \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp/schedule/list/\u0026quot;, year,\u0026quot;/?month=\u0026quot;,k)) # 該当年、該当月のレース結果一覧にアクセス\rSys.sleep(2)\rrace_lists \u0026lt;- keiba.yahoo %\u0026gt;%\rhtml_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) # 全urlを取得\r# 競馬場ごとの各日のレースリストを取得\rrace_lists \u0026lt;- race_lists[str_detect(race_lists, pattern=\u0026quot;race/list/\\\\d+/\u0026quot;)==1] # 「result」が含まれるurlを抽出\r}\r, error = function(e){signal \u0026lt;- 1}\r)\rここでは、取得したリンクのurlにresultという文字が含まれているものだけを抽出しています。要はそれが各競馬場のレーステーブルへのリンクとなります。ここからは取得した競馬場のレーステーブルのリンクを用いて、そのページにアクセスし、全12レースそれぞれのレース結果が掲載されているページのリンクを取得していきます。\n for (j in 1:length(race_lists)){ # jは当該年月にあったレーステーブルへのリンクを表す\rtryCatch(\r{\rrace_list \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,race_lists[j]))\rrace_url \u0026lt;- race_list %\u0026gt;% html_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) # 全urlを取得\r# レース結果のurlを取得\rrace_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026quot;result\u0026quot;)==1] # 「result」が含まれるurlを抽出\r}\r, error = function(e){signal \u0026lt;- 1}\r)\r各レース結果へのリンクが取得できたので、ここからはいよいよレース結果の取得とその整形パートに入ります。かなり長ったらしく複雑なコードになってしまいました。レース結果は以下のようなテーブル属性に格納されているので、まずそれを単純に引っ張ってきます。\n for (i in 1:length(race_url)){ # iは当該年月当該競馬場で開催されたレースを表す\rprint(str_c(\u0026quot;現在、\u0026quot;, year, \u0026quot;年\u0026quot;, k, \u0026quot;月\u0026quot;,j, \u0026quot;グループ、\u0026quot;, i,\u0026quot;番目のレースの保存中です\u0026quot;))\rtryCatch(\r{\rrace1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,race_url[i])) # レース結果のurlを取得\rsignal \u0026lt;- 0\rSys.sleep(2)\r}\r, error = function(e){signal \u0026lt;- 1}\r)\r# レースが中止orこれまでの過程でエラーでなければ処理を実行\rif (identical(race1 %\u0026gt;%\rhtml_nodes(xpath = \u0026quot;//div[@class = \u0026#39;resultAtt mgnBL fntSS\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text(),character(0)) == TRUE \u0026amp;\u0026amp; signal == 0){\r# レース結果をスクレイピング\rrace_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_table()\rrace_result \u0026lt;- do.call(\u0026quot;data.frame\u0026quot;,race_result) # リストをデータフレームに変更\rcolnames(race_result) \u0026lt;- c(\u0026quot;order\u0026quot;,\u0026quot;frame_number\u0026quot;,\u0026quot;horse_number\u0026quot;,\u0026quot;horse_name/age\u0026quot;,\u0026quot;time/margin\u0026quot;,\u0026quot;passing_rank/last_3F\u0026quot;,\u0026quot;jockey/weight\u0026quot;,\u0026quot;popularity/odds\u0026quot;,\u0026quot;trainer\u0026quot;) #　列名変更\rtableをただ取得しただけでは以下のように、一つのセルに複数の情報が入っていたりと分析には使えないデータとなっています。なので、これを成型する必要が出てきます。\n # 通過順位と上り3Fのタイム\rrace_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- race_result[-6]\r# タイムと着差\rrace_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026quot;)))\rrace_result$margin[race_result$order==1] \u0026lt;- \u0026quot;トップ\u0026quot;\rrace_result$margin[race_result$margin==\u0026quot;character(0)\u0026quot;] \u0026lt;- \u0026quot;大差\u0026quot;\rrace_result$margin[race_result$order==0] \u0026lt;- NA\rrace_result \u0026lt;- race_result[-5]\r# 馬名、馬齢、馬体重\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;[ァ-ヴー・]+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026quot;)))\rrace_result$horse_sex \u0026lt;- str_extract(race_result$horse_age, pattern = \u0026quot;牡|牝|せん\u0026quot;)\rrace_result$horse_age \u0026lt;- str_extract(race_result$horse_age, pattern = \u0026quot;\\\\d\u0026quot;)\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\d{3}\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026quot;)))\rrace_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- dplyr::mutate(race_result,brinker=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;B\u0026quot;)))\rrace_result$brinker[race_result$brinker!=\u0026quot;B\u0026quot;] \u0026lt;- \u0026quot;N\u0026quot;\rrace_result \u0026lt;- race_result[-4]\r# ジョッキー\rrace_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026quot;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,jockey_weight=as.character(str_extract_all(race_result$`jockey/weight`,\u0026quot;\\\\d{2}\u0026quot;)))\rrace_result$jockey_weight_change \u0026lt;- 0\rrace_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026quot;☆\u0026quot;)==1] \u0026lt;- 1\rrace_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026quot;△\u0026quot;)==1] \u0026lt;- 2\rrace_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026quot;△\u0026quot;)==1] \u0026lt;- 3\rrace_result \u0026lt;- race_result[-4]\r# オッズと人気\rrace_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\(.+\\\\)\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\d+[^(\\\\d+.\\\\d)]\u0026quot;)))\rrace_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\r次に、今取得したtable以外の情報も取り込むことにします。具体的には、レース名や天候、馬場状態、日付、競馬場などです。これらの情報はレース結果ページの上部に掲載されています。\n # レース情報\rrace_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_distance \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//p[@id = \u0026#39;raceTitMeta\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026quot;\\\\d+年\\\\d+月\\\\d+日\u0026quot;)))\rrace_result$race_date \u0026lt;- str_replace_all(race_result$race_date,\u0026quot;年\u0026quot;,\u0026quot;/\u0026quot;)\rrace_result$race_date \u0026lt;- str_replace_all(race_result$race_date,\u0026quot;月\u0026quot;,\u0026quot;/\u0026quot;)\rrace_result$race_date \u0026lt;- as.Date(race_result$race_date)\rrace_course \u0026lt;- as.character(str_extract_all(race_date,pattern = \u0026quot;札幌|函館|福島|新潟|東京|中山|中京|京都|阪神|小倉\u0026quot;))\rrace_result$race_course \u0026lt;- race_course\rrace_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,race_distance=as.character(str_extract_all(race_distance,\u0026quot;\\\\d+m\u0026quot;)))\rrace_type=as.character(str_extract_all(race_distance,pattern = \u0026quot;芝|ダート\u0026quot;))\rrace_result$type \u0026lt;- race_type\rrace_turn \u0026lt;- as.character(str_extract_all(race_distance,pattern = \u0026quot;右|左\u0026quot;))\rrace_result$race_turn \u0026lt;- race_turn\rif(length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg ryou\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_condition \u0026lt;- \u0026quot;良\u0026quot;\r} else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg yayaomo\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_condition \u0026lt;- \u0026quot;稍重\u0026quot;\r} else if (length(race1 %\u0026gt;%\rhtml_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg omo\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_condition \u0026lt;- \u0026quot;重\u0026quot;\r} else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg furyou\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_condition \u0026lt;- \u0026quot;不良\u0026quot;\r} else race_result$race_condition \u0026lt;- \u0026quot;NA\u0026quot;\rif (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg hare\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_weather \u0026lt;- \u0026quot;晴れ\u0026quot;\r} else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg ame\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_weather \u0026lt;- \u0026quot;曇り\u0026quot;\r} else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg kumori\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_weather \u0026lt;- \u0026quot;雨\u0026quot;\r} else race_result$race_weather \u0026lt;- \u0026quot;その他\u0026quot;\r次は各馬の情報です。 実はさきほど取得したtableの馬名はリンクになっており、そのリンクをたどると各馬の情報が取得できます（毛色や生年月日など）。\n horse_url \u0026lt;- race1 %\u0026gt;% html_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) horse_url \u0026lt;- horse_url[str_detect(horse_url, pattern=\u0026quot;directory/horse\u0026quot;)==1] # 馬情報のリンクだけ抽出する\rfor (l in 1:length(horse_url)){\rtryCatch(\r{\rhorse1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,horse_url[l]))\rSys.sleep(0.5)\rhorse_name \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;dirTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026quot;) %\u0026gt;% html_text()\rhorse \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;dirTitName\u0026#39;]/ul\u0026quot;) %\u0026gt;% html_text()\rrace_result$colour[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;毛色：.+\u0026quot;)) race_result$owner[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;馬主：.+\u0026quot;))\rrace_result$farm[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;生産者：.+\u0026quot;))\rrace_result$locality[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;産地：.+\u0026quot;))\rrace_result$horse_birthday[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;\\\\d+年\\\\d+月\\\\d+日\u0026quot;))\rrace_result$father[race_result$horse_name==horse_name] \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026quot;//td[@class = \u0026#39;bloodM\u0026#39;][@rowspan = \u0026#39;4\u0026#39;]\u0026quot;) %\u0026gt;% html_text()\rrace_result$mother[race_result$horse_name==horse_name] \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026quot;//td[@class = \u0026#39;bloodF\u0026#39;][@rowspan = \u0026#39;4\u0026#39;]\u0026quot;) %\u0026gt;% html_text()\r}\r, error = function(e){\rrace_result$colour[race_result$horse_name==horse_name] \u0026lt;- NA race_result$owner[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$farm[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$locality[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$horse_birthday[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$father[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$mother[race_result$horse_name==horse_name] \u0026lt;- NA\r}\r)\r}\rrace_result$colour \u0026lt;- str_replace_all(race_result$colour,\u0026quot;毛色：\u0026quot;,\u0026quot;\u0026quot;)\rrace_result$owner \u0026lt;- str_replace_all(race_result$owner,\u0026quot;馬主：\u0026quot;,\u0026quot;\u0026quot;)\rrace_result$farm \u0026lt;- str_replace_all(race_result$farm,\u0026quot;生産者：\u0026quot;,\u0026quot;\u0026quot;)\rrace_result$locality \u0026lt;- str_replace_all(race_result$locality,\u0026quot;産地：\u0026quot;,\u0026quot;\u0026quot;)\r#race_result$horse_birthday \u0026lt;- str_replace_all(race_result$horse_birthday,\u0026quot;年\u0026quot;,\u0026quot;/\u0026quot;)\r#race_result$horse_birthday \u0026lt;- str_replace_all(race_result$horse_birthday,\u0026quot;月\u0026quot;,\u0026quot;/\u0026quot;)\r#race_result$horse_birthday \u0026lt;- as.Date(race_result$horse_birthday)\rrace_result \u0026lt;- dplyr::arrange(race_result,horse_number) # 馬番順に並べる\r次にそのレースまでに獲得した賞金額を落としに行きます。これはレース結果のページの出馬表と書かれたリンクをたどるとアクセスできます。ここに賞金があるのでそれを取得します。\n yosou_url \u0026lt;- race1 %\u0026gt;% html_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) yosou_url \u0026lt;- yosou_url[str_detect(yosou_url, pattern=\u0026quot;denma\u0026quot;)==1]\rif (length(yosou_url)==1){\ryosou1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,yosou_url)) Sys.sleep(2)\ryosou \u0026lt;- yosou1 %\u0026gt;% html_nodes(xpath = \u0026quot;//td[@class = \u0026#39;txC\u0026#39;]\u0026quot;) %\u0026gt;% as.character()\rprize \u0026lt;- yosou[grepl(\u0026quot;万\u0026quot;,yosou)==TRUE] %\u0026gt;% str_extract_all(\u0026quot;\\\\d+万\u0026quot;)\rprize \u0026lt;- t(do.call(\u0026quot;data.frame\u0026quot;,prize)) %\u0026gt;% as.character()\rrace_result$prize \u0026lt;- prize\rrace_result$prize \u0026lt;- str_replace_all(race_result$prize,\u0026quot;万\u0026quot;,\u0026quot;\u0026quot;) %\u0026gt;% as.numeric()\r} else race_result$prize \u0026lt;- NA\r取得した各レース結果を格納するdatasetというデータフレームを作成し、データを格納していきます。1年ごとにそれをSQLite\rへ保存していきます。\n ## ファイル貯めるのかく\rif (k == 1 \u0026amp;\u0026amp; i == 1 \u0026amp;\u0026amp; j == 1){\rdataset \u0026lt;- race_result\r} else {\rdataset \u0026lt;- rbind(dataset,race_result)\r} # if文2の終わり\r}else\r{\rprint(\u0026quot;保存できませんでした\u0026quot;) }# if文1の終わり\r} # iループの終わり\r} # jループ終わり\r} # kループの終わり\rbeep(3)\rwrite.csv(dataset,\u0026quot;race_result2.csv\u0026quot;, row.names = FALSE)\rif (year == 1994){\rdbWriteTable(con, \u0026quot;race_result\u0026quot;, dataset)\r} else {\rdbWriteTable(con, \u0026quot;temp\u0026quot;, dataset)\rdbSendQuery(con, \u0026quot;INSERT INTO race_result select * from temp\u0026quot;)\rdbSendQuery(con, \u0026quot;DROP TABLE temp\u0026quot;)\r} # ifの終わり\r} # yearループの終わり\rend.time \u0026lt;- Sys.time()\rprint(str_c(\u0026quot;処理時間は\u0026quot;,end.time-start.time,\u0026quot;です。\u0026quot;))\rbeep(5)\roptions(warn = 1)\rdbDisconnect(con)\r以上です。取れたデータは以下のようになりました。\nhead(race_result)\r## order frame_number horse_number trainer passing_rank last_3F time\r## 1 10 1 1 田中 剛 09-09 39.0 1.14.3\r## 2 16 1 2 天間 昭一 11-11 40.3 1.15.7\r## 3 15 2 3 田中 清隆 14-14 39.4 1.15.1\r## 4 9 2 4 中舘 英二 08-08 39.1 1.14.3\r## 5 12 3 5 根本 康広 11-11 39.0 1.14.4\r## 6 4 3 6 杉浦 宏昭 04-04 38.4 1.13.2\r## margin horse_name horse_age horse_sex horse_weight\r## 1 アタマ サトノジョニー 3 牡 512\r## 2 3 1/2馬身 ツギノイッテ 3 牡 464\r## 3 3馬身 ギュウホ 3 牡 444\r## 4 2 1/2馬身 セイウンメラビリア 3 牝 466\r## 5 クビ サバイバルトリック 3 牝 450\r## 6 アタマ ステイホット 3 牝 474\r## horse_weight_change brinker jockey jockey_weight jockey_weight_change\r## 1 +30 N 松岡 正海 56 0\r## 2 +8 N 西田 雄一郎 56 0\r## 3 +8 N 杉原 誠人 56 0\r## 4 +10 N 村田 一誠 54 0\r## 5 -2 N 野中 悠太郎 51 0\r## 6 -2 N 大野 拓弥 54 0\r## odds popularity race_date race_course race_name race_distance type\r## 1 40.3 9 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 2 340.9 16 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 3 283.1 14 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 4 299.7 15 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 5 26.7 8 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 6 2.4 1 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## race_turn race_condition race_weather colour owner\r## 1 右 良 晴れ 栗毛 株式会社 サトミホースカンパニー\r## 2 右 良 晴れ 黒鹿毛 西村 新一郎\r## 3 右 良 晴れ 鹿毛 有限会社 ミルファーム\r## 4 右 良 晴れ 青鹿毛 西山 茂行\r## 5 右 良 晴れ 黒鹿毛 福田 光博\r## 6 右 良 晴れ 栗毛 小林 善一\r## farm locality horse_birthday father\r## 1 千代田牧場 新ひだか町 2016年1月29日 オルフェーヴル\r## 2 織笠 時男 青森県 2016年4月17日 スクワートルスクワート\r## 3 神垣 道弘 新ひだか町 2016年4月19日 ジャングルポケット\r## 4 石郷岡 雅樹 新冠町 2016年4月21日 キンシャサノキセキ\r## 5 原田牧場 日高町 2016年4月30日 リーチザクラウン\r## 6 社台ファーム 千歳市 2016年3月13日 キャプテントゥーレ\r## mother prize\r## 1 スパークルジュエル 0\r## 2 エプソムアイリス 0\r## 3 デライトシーン 0\r## 4 ドリームシップ 0\r## 5 フリーダムガール 180\r## 6 ステイアライヴ 455\r\r","date":1562976000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1562976000,"objectID":"d5c2d4bb023ae8c645620087122f1f62","permalink":"/post/post11/","publishdate":"2019-07-13T00:00:00Z","relpermalink":"/post/post11/","section":"post","summary":"前回のデータ収集では足りないデータがあったので再度スクレイピングしてみました。","tags":["R","Webスクレイピング","前処理","SQL"],"title":"rvestでyahoo競馬にある過去のレース結果をスクレイピングしてみた（2回目）","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r\r1. GPLVMとは\r2. 最もPrimitiveなGP-LVM\r3. Rでの実装\r\r\rおはこんばんにちは。\rずいぶん前にGianonne et al (2008)のマルチファクターモデルで四半期GDPの予想を行いました。\r結果としては、ある程度は予測精度が出ていたものの彼らの論文ほどは満足のいくものではありませんでした。原因としてはクロスセクショナルなデータ不足が大きいと思われ、現在収集方法についてもEXCELを用いて改修中です。しかし一方で、マルチファクターモデルの改善も考えたいと思っています。前回は月次経済統計を主成分分析（実際にはカルマンフィルタ）を用いて次元削減を行い、主成分得点を説明変数としてGDPに回帰しました。今回はこの主成分分析のド発展版であるGaussian Process Latent Variable Model(GPLVM)を用いてファクターを計算し、それをGDPに回帰したいと思います。\n1. GPLVMとは\rGPLVMとは、Gaussian Process Modelの一種です。以前、Gaussian Process Regressionの記事を書きました。\n最も基本的なGaussian Process Modelは上の記事のようなモデルで、非説明変数\\(Y=(y_{1},y_{2},...,y_{n})\\)と説明変数\\(X=(\\textbf{x}_{1},\\textbf{x}_{2},...,\\textbf{x}_{n})\\)があり、以下のような関係式で表される際にそのモデルを直接推定することなしに新たな説明変数\\(X\\)の入力に対し、非説明変数\\(Y\\)の予測値をはじき出すというものでした。\n\\[\r\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(\\textbf{x}_{i})\r\\]\nここで、\\(\\textbf{x}_{i}\\)は\\(i\\)番目の説明変数ベクトル、\\(\\phi(・)\\)は非線形関数、 \\(\\textbf{w}^{T}\\)は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、\\(\\phi(\\textbf{x}_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\\)を想定しています（\\(x_{1,i}\\)は\\(i\\)番目の入力データ\\(\\textbf{x}_{i}\\)の１番目の変数）。詳しくは過去記事を参照してください。\n今回やるGPLVMは説明変数ベクトルが観測できない潜在変数（Latent Variable）であるところが特徴です。以下のスライドが非常にわかりやすいですが、GP-LVMは確率的主成分分析（PPCA）の非線形版という位置付けになっています。\n資料\nでは具体的な説明に移ります。GPLVMは主成分分析の発展版ですので、主に次元削減のために行われることを想定しています。つまり、データセットがあったとして、サンプルサイズ\\(n\\)よりも変数の次元\\(p\\)が大きいような場合を想定しています。\n\r2. 最もPrimitiveなGP-LVM\r先述したようにGPLVMはPPCAの非線形版です。なので、GPLVMを説明するスタートはPPCAになります。観測可能な\\(D\\)次元データセットを\\(\\{\\textbf{y}_{n}\\}_{n=1}^{N}\\)とします。そして、潜在変数を\\(\\textbf{x}_{n}\\)とおきます。今、データセットと潜在変数の間には以下のような関係があるとします。\n\\[\r\\textbf{y}_{n} = \\textbf{W}\\textbf{x}_{n} + \\epsilon_{n}\r\\]\nここで、\\(\\textbf{W}\\)はウェイト行列、\\(\\epsilon_{n}\\)はかく乱項で\\(N(0,\\beta^{-1}\\textbf{I})\\)に従います（被説明変数が多次元になることに注意）。また、\\(\\textbf{x}_{n}\\)は\\(N(0,\\textbf{I})\\)に従います。このとき、\\(\\textbf{y}_{n}\\)の尤度を\\(\\textbf{x}_{n}\\)を周辺化することで表現すると、\n\\[\r\\begin{eqnarray*}\r\\displaystyle p(\\textbf{y}_{n}|\\textbf{W},\\beta) \u0026amp;=\u0026amp; \\int p(\\textbf{y}_{n}|\\textbf{x}_{n},\\textbf{W},\\beta)N(0,\\textbf{I})d\\textbf{x}_{n} \\\\\r\\displaystyle \u0026amp;=\u0026amp; \\int N(\\textbf{W}\\textbf{x}_{n},\\beta^{-1}\\textbf{I})N(0,\\textbf{I})d\\textbf{x}_{n} \\\\\r\u0026amp;=\u0026amp; N(0,\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}) \\\\\r\\displaystyle \u0026amp;=\u0026amp; \\frac{1}{(2\\pi)^{DN/2}|\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}|^{N/2}}\\exp(\\frac{1}{2}\\textbf{tr}( (\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{YY}^{T}))\r\\end{eqnarray*}\r\\]\nとなります。ここで、\\(p(\\textbf{y}_{n}|\\textbf{x}_{n},\\textbf{W},\\beta)=N(\\textbf{W}\\textbf{x}_{n},\\beta^{-1}\\textbf{I})\\)です。平均と分散は以下から求めました。\n\\[\r\\begin{eqnarray*}\rE(\\textbf{y}_{n}|\\textbf{W},\\beta) \u0026amp;=\u0026amp; E(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n}) \\\\\r\u0026amp;=\u0026amp; E(\\textbf{W}\\textbf{x}_{n}) + E(\\epsilon_{n}) \\\\\r\u0026amp;=\u0026amp; \\textbf{W}E(\\textbf{x}_{n}) + E(\\epsilon_{n}) = 0\r\\end{eqnarray*}\r\\]\n\\[\r\\begin{eqnarray*}\rE[(\\textbf{y}_{n}|\\textbf{W},\\beta)(\\textbf{y}_{n}|\\textbf{W},\\beta)^{T}] \u0026amp;=\u0026amp; E[ (\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n} - 0)(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n} - 0)^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ (\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n})(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n})^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ \\textbf{W}\\textbf{x}_{n}(\\textbf{W}\\textbf{x}_{n})^{T} + \\textbf{W}\\textbf{x}_{n}\\epsilon_{n}^{T} + \\epsilon_{n}\\textbf{W}\\textbf{x}_{n}^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ \\textbf{W}\\textbf{x}_{n}(\\textbf{W}\\textbf{x}_{n})^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ \\textbf{W}\\textbf{x}_{n}\\textbf{x}_{n}^{T}\\textbf{W}^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ \\textbf{W}\\textbf{x}_{n}\\textbf{x}_{n}^{T}\\textbf{W}^{T}] + E[\\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026amp;=\u0026amp; \\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}\r\\end{eqnarray*}\r\\]\n\\(\\textbf{W}\\)を求めるためには\\(\\textbf{y}_{n}\\)がi.i.d.と仮定し、以下のようなデータセット全体の尤度を最大化すれば良いことになります。\n\\[\r\\displaystyle p(\\textbf{Y}|\\textbf{W},\\beta) = \\prod_{n=1}^{N}p(\\textbf{y}_{n}|\\textbf{W},\\beta)\r\\]\nここで、\\(\\textbf{Y}\\)は\\(N×D\\)の計画行列です。このように、PPCAでは\\(\\textbf{x}_{n}\\)を周辺化し、\\(\\textbf{W}\\)を最適化します。逆に、Lawrence(2004)では\\(\\textbf{W}\\)を周辺化し、\\(\\textbf{x}_{n}\\)します（理由は後述）。\\(\\textbf{W}\\)を周辺化するために、\\(\\textbf{W}\\)に事前分布を与えましょう。\n\\[\r\\displaystyle p(\\textbf{W}) = \\prod_{i=1}^{D}N(\\textbf{w}_{i}|0,\\alpha^{-1}\\textbf{I})\r\\]\nここで、\\(\\textbf{w}_{i}\\)はウェイト行列\\(\\textbf{W}\\)の\\(i\\)番目の列です。では、\\(\\textbf{W}\\)を周辺化して\\(\\textbf{Y}\\)の尤度関数を導出してみます。やり方はさっきとほぼ同じなので省略します。\n\\[\r\\displaystyle p(\\textbf{Y}|\\textbf{X},\\beta) = \\frac{1}{(2\\pi)^{DN/2}|K|^{D/2}}\\exp(\\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T}))\r\\]\nここで、\\(\\textbf{K}=\\alpha^2\\textbf{X}\\textbf{X}^{T} + \\beta^{-1}\\textbf{I}\\)は\\(p(\\textbf{Y}|\\textbf{X},\\beta)\\)の分散共分散行列で、\\(\\textbf{X}=(\\textbf{x}_{1},\\textbf{x}_{2},...,\\textbf{x}_{N})^{T}\\)は入力ベクトルです。対数尤度は\n\\[\r\\displaystyle L = - \\frac{DN}{2}\\ln{2\\pi} - \\frac{1}{2}\\ln{|\\textbf{K}|} - \\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T})\r\\]\n周辺化のおかげでウェイト\\(\\textbf{W}\\)が消えたのでこれを\\(X\\)で微分してみましょう。\n\\[\r\\displaystyle\\frac{\\partial L}{ \\partial \\textbf{X}} = \\alpha^2 \\textbf{K}^{-1}\\textbf{Y}\\textbf{Y}^{T}\\textbf{K}^{-1}\\textbf{X} - \\alpha^2 D\\textbf{K}^{-1}\\textbf{X}\r\\]\nここから、\n\\[\r\\displaystyle \\frac{1}{D}\\textbf{Y}\\textbf{Y}^{T}\\textbf{K}^{-1}\\textbf{X} = \\textbf{X}\r\\]\nここで、特異値分解を用いると\n\\[\r\\textbf{X} = \\textbf{ULV}^{T}\r\\]\nとなります。\\(\\textbf{U} = (\\textbf{u}_{1},\\textbf{u}_{2},...,\\textbf{u}_{q})\\)は\\(N×q\\)直交行列、\\(\\textbf{L} = diag(l_{1},l_{2},..., l_{q})\\)は\\(q×q\\)の特異値を対角成分に並べた行列、\\(\\textbf{V}\\)は\\(q×q\\)直交行列です。これを先ほどの式に代入すると、\n\\[\r\\begin{eqnarray*}\r\\textbf{K}^{-1}\\textbf{X} \u0026amp;=\u0026amp; (\\alpha^2\\textbf{X}\\textbf{X}^{T} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{X} \\\\\r\u0026amp;=\u0026amp; \\textbf{X}(\\alpha^2\\textbf{X}^{T}\\textbf{X} + \\beta^{-1}\\textbf{I})^{-1} \\\\\r\u0026amp;=\u0026amp; \\textbf{ULV}^{T}(\\alpha^2\\textbf{VLU}^{T}\\textbf{ULV}^{T} + \\beta^{-1}\\textbf{I})^{-1} \\\\\r\u0026amp;=\u0026amp; \\textbf{ULV}^{T}\\textbf{V}(\\alpha^2\\textbf{LU}^{T}\\textbf{UL} + \\beta^{-1}\\textbf{I}^{-1})\\textbf{V}^{T} \\\\\r\u0026amp;=\u0026amp; \\textbf{UL}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{V}^{T}\r\\end{eqnarray*}\r\\]\nなので、\n\\[\r\\begin{eqnarray*}\r\\displaystyle \\frac{1}{D}\\textbf{Y}\\textbf{Y}^{T}\\textbf{UL}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1})\\textbf{V}^{T} \u0026amp;=\u0026amp; \\textbf{ULV}^{T}\\\\\r\\displaystyle \\textbf{Y}\\textbf{Y}^{T}\\textbf{UL} \u0026amp;=\u0026amp; D\\textbf{U}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{L} \\\\\r\\end{eqnarray*}\r\\]\nとなります。\\(l_{j}\\)が0でなければ、\\(\\textbf{Y}\\textbf{Y}^{T}\\textbf{u}_{j} = D(\\alpha^2 l_{j}^{2} + \\beta^{-1})\\textbf{u}_{j}\\)となり、\\(\\textbf{U}\\)のそれぞれの列は\\(\\textbf{Y}\\textbf{Y}^{T}\\)の固有ベクトルであり、対応する固有値\\(\\lambda_{j}\\)は\\(D(\\alpha^2 l_{j}^{2} + \\beta^{-1})\\)となります。つまり、未知であった\\(X=ULV\\)が実は\\(\\textbf{Y}\\textbf{Y}^{T}\\)の固有値問題から求めることが出来るというわけです。\\(l_{j}\\)は上式を利用して、\n\\[\r\\displaystyle l_{j} = (\\frac{\\lambda_{j}}{D\\alpha^2} - \\frac{1}{\\beta\\alpha^2})^{1/2}\r\\]\nと\\(\\textbf{Y}\\textbf{Y}^{T}\\)の固有値\\(\\lambda_{j}\\)とパラメータから求められることがわかります。よって、\\(X=ULV\\)は\n\\[\r\\textbf{X} = \\textbf{U}_{q}\\textbf{L}\\textbf{V}^{T}\r\\]\nとなります。ここで、\\(\\textbf{U}_{q}\\)は\\(\\textbf{Y}\\textbf{Y}^{T}\\)の固有ベクトルを\\(q\\)個取り出したものです。\\(\\beta\\)が限りなく大きければ（=観測誤差が限りなく小さければ）通常のPCAと一致します。\n以上がPPCAです。GPLVMはPPCAで確率モデルとして想定していた以下のモデルを拡張します。\n\\[\r\\textbf{y}_{n} = \\textbf{W}^{T}\\textbf{x}_{n} + \\epsilon_{n}\r\\]\n具体的には、通常のガウス過程と同様、\n\\[\r\\displaystyle \\textbf{y}_{n} = \\textbf{W}^{T}\\phi(\\textbf{x}_{n})+ \\epsilon_{n}\r\\]\nという風に基底関数\\(\\phi(\\textbf{x}_{n})\\)をかませて拡張します。\\(\\phi(・)\\)は平均\\(\\textbf{0}\\)、分散共分散行列\\(\\textbf{K}_{\\textbf{x}}\\)のガウス過程と仮定します。分散共分散行列\\(\\textbf{K}_{\\textbf{x}}\\)は\n\\[\r\\textbf{K}_{\\textbf{x}} = \\alpha^2\\phi(\\textbf{x})\\phi(\\textbf{x})^T\r\\]\nであり、入力ベクトル\\(\\textbf{X}\\)を\\(\\phi(\\textbf{・})\\)で非線形変換した特徴量\\(\\phi(\\textbf{x})\\)が近いほど、出力値\\(\\textbf{Y}\\)も近くなりやすいという性質があることになります。GPLVMではこの性質を逆に利用しています。つまり、出力値\\(Y_i\\)と\\(Y_j\\)が近い→\\(\\phi(\\textbf{x}_i)\\)と\\(\\phi(\\textbf{x}_j)\\)が近い（内積が大きい）→\\(\\textbf{K}_{x,ij}\\)が大きい→観測不可能なデータ\\(X_{i}\\)と\\(X_{j}\\)は近い値（or同じようなパターン）をとる。\rこの議論からもわかるように、\\(\\textbf{K}_{\\textbf{x}}\\)は入力ベクトル\\(\\textbf{X}\\)それぞれの距離を表したものになります。分散共分散行列の計算には入力ベクトル\\(\\textbf{X}\\)を基底関数\\(\\phi(\\textbf{・})\\)で非線形変換した後、内積を求めるといったことをする必要はなく、カーネル関数を計算するのみでOKです。今回は王道中の王道RBFカーネルを使用していますので、これを例説明します。\nRBFカーネル（スカラーに対する）\r\\[\r\\theta_{1}\\exp(-\\frac{1}{\\theta_{2}}(x-x^T)^2)\r\\]\nこのRBFカーネルは以下の基底関数と対応しています。\n\\[\r\\phi(x)_h = \\tau\\exp(-\\frac{1}{r}(x-h)^2)\r\\]\n例えば、この基底関数で入力\\(x\\)を変換したものを\\(2H^2+1\\)個並べた関数を\n\\[\r\\phi(x) = (\\phi(x)_{-H^2}, ..., \\phi(x)_{0},...,\\phi(x)_{H^2})\r\\]\n入力\\(x\\)の特徴量だとすると\\(x\u0026#39;\\)との共分散\\(K_{x}(x,x\u0026#39;)\\)は内積の和なので\n\\[\rK_{x}(x,x\u0026#39;) = \\sum_{h=-H^2}^{H^2}\\phi_{h}(x)\\phi_{h}(x\u0026#39;)\r\\]\nとなります。ここで、\\(H \\to \\infty\\)とし、グリッドを極限まで細かくしてみます。\n\\[\r\\begin{eqnarray*}\rK_{x}(x,x\u0026#39;) \u0026amp;=\u0026amp; \\lim_{H \\to \\infty}\\sum_{h=-H^2}^{H^2}\\phi_{h}(x)\\phi_{h}(x\u0026#39;) \\\\\r\u0026amp;\\to\u0026amp;\\int_{-\\infty}^{\\infty}\\tau\\exp(-\\frac{1}{r}(x-h)^2)\\tau\\exp(-\\frac{1}{r}(x\u0026#39;-h)^2)dh \\\\\r\u0026amp;=\u0026amp; \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{r}\\{(x-h)^2+(x\u0026#39;-h)^2\\})dh \\\\\r\u0026amp;=\u0026amp; \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{r}\\{2(h-\\frac{x+x\u0026#39;}{2})^2+\\frac{1}{2}(x-x\u0026#39;)^2\\})dh \\\\\r\\end{eqnarray*}\r\\]\nとなります。\\(h\\)に関係のない部分を積分の外に出します。\n\\[\r\\begin{eqnarray*}\r\u0026amp;=\u0026amp; \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{2}{r}(h-\\frac{x+x\u0026#39;}{2})^2)dh\\exp(-\\frac{1}{2r}(x-x\u0026#39;)^2) \\\\\r\\end{eqnarray*}\r\\]\n残った積分を見ると、正規分布の正規化定数と等しいことがわかります。\n\\[\r\\begin{eqnarray*}\r\\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{2\\sigma}(h-\\frac{x+x\u0026#39;}{2})^2)dh \u0026amp;=\u0026amp; \\int_{-\\infty}^{\\infty}\\exp(-\\frac{2}{r}(h-\\frac{x+x\u0026#39;}{2})^2)dh\\\\\r\\sigma \u0026amp;=\u0026amp; \\frac{r}{4}\r\\end{eqnarray*}\r\\]\nとなるので、ガウス積分の公式を用いて\n\\[\r\\begin{eqnarray*}\r\u0026amp;=\u0026amp; \\tau^2 \\sqrt{\\frac{\\pi r}{2}}\\exp(-\\frac{1}{2r}(x-x\u0026#39;)^2)　\\\\\r\u0026amp;=\u0026amp; \\theta_{1}\\exp(-\\frac{1}{\\theta_{2}}(x-x’)^2)\r\\end{eqnarray*}\r\\]\nとなり、RBFカーネルと等しくなることがわかります。よって、RBFカーネルで計算した共分散は上述した基底関数で入力\\(x\\)を無限次元へ拡張した特徴量ベクトルの内積から計算した共分散と同値になることがわかります。つまり、入力\\(x\\)と\\(x\u0026#39;\\)のスカラーの計算のみで\\(K_{x}(x,x\u0026#39;)\\)ができてしまうという夢のような計算効率化が可能になるわけです。無限次元特徴量ベクトルの回帰問題なんて普通計算できませんからね。。。カーネル関数は偉大です。\r前の記事にも載せましたが、RBFカーネルで分散共分散行列を計算したガウス過程のサンプルパスは以下通りです（\\(\\theta_1=1,\\theta_2=0.5\\)）。\n# Define Kernel function\rKernel_Mat \u0026lt;- function(X,sigma,beta){\rN \u0026lt;- NROW(X)\rK \u0026lt;- matrix(0,N,N)\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta\r}\r}\rreturn(K)\r}\rN \u0026lt;- 10 # max value of X\rM \u0026lt;- 1000 # sample size\rX \u0026lt;- matrix(seq(1,N,length=M),M,1) # create X\rtestK \u0026lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix\rlibrary(MASS)\rP \u0026lt;- 6 # num of sample path\rY \u0026lt;- matrix(0,M,P) # define Y\rfor(i in 1:P){\rY[,i] \u0026lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y\r}\r# Plot\rmatplot(x=X,y=Y,type = \u0026quot;l\u0026quot;,lwd = 2)\r非常に滑らかな関数となっていることがわかります。RBFのほかにもカーネル関数は存在します。カーネル関数を変えると基底関数が変わりますから、サンプルパスは大きく変わることになります。\nGPLVMの推定方法に話を進めましょう。PPCAの時と同じく、以下の尤度関数を最大化する観測不能な入力\\(x\\)を推定値とします。\n\\[\r\\displaystyle L = - \\frac{DN}{2}\\ln{2\\pi} - \\frac{1}{2}\\ln{|\\textbf{K}|} - \\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T})\r\\]\nただ、PPCAとは異なり、その値は解析的に求めることができません。尤度関数の導関数は今や複雑な関数であり、展開することができないからです。よって、共役勾配法を用いて数値的に計算するのが主流なようです（自分は準ニュートン法で実装）。解析的、数値的のどちらにせよ導関数を求めておくことは必要なので、導関数を求めてみます。\n\\[\r\\frac{\\partial L}{\\partial \\textbf{K}_x} = \\frac{1}{2}(\\textbf{K}_x^{-1}\\textbf{Y}\\textbf{Y}^T\\textbf{K}_x^{-1}-D\\textbf{K}_x^{-1})\r\\]\nなので、チェーンルールから\n\\[\r\\frac{\\partial L}{\\partial \\textbf{x}} = \\frac{\\partial L}{\\partial \\textbf{K}_x}\\frac{\\partial \\textbf{K}_x}{\\partial \\textbf{x}}\r\\]\nなので、カーネル関数を決め、\\(\\textbf{x}\\)に初期値を与えてやれば勾配法によって尤度\\(L\\)が最大となる点を探索することができます。今回使用するRBFカーネルで\\(\\frac{\\partial \\textbf{K}_x}{\\partial x_{nj}}\\)を計算してみます。\n\\[\r\\begin{eqnarray*}\r\\frac{\\partial \\textbf{K}_x(\\textbf{x}_n,\\textbf{x}_n\u0026#39;)}{x_{nj}}\u0026amp;=\u0026amp;\\frac{\\partial\\theta_{1}\\exp(-\\frac{|\\textbf{x}_n-\\textbf{x}_n\u0026#39;|^2}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026amp;=\u0026amp; \\frac{\\partial\\theta_{1}\\exp(-\\frac{(\\textbf{x}_n-\\textbf{x}_n\u0026#39;)^T(\\textbf{x}_n-\\textbf{x}_n\u0026#39;)}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026amp;=\u0026amp; \\frac{\\partial\\theta_{1}\\exp(-\\frac{-(\\textbf{x}_n^T\\textbf{x}_n-2\\textbf{x}_n\u0026#39;^T\\textbf{x}_n+\\textbf{x}_n\u0026#39;^T\\textbf{x}_n\u0026#39;)}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026amp;=\u0026amp; -2\\textbf{K}_x(\\textbf{x}_n\\textbf{x}_n\u0026#39;)\\frac{(x_{nj}-x_{n\u0026#39;j})}{\\theta_2}\r\\end{eqnarray*}\r\\]\n\\(j\\)番目の潜在変数の\\(n\\)番目のサンプルそれぞれに導関数を計算し、それを分散共分散行列と同じ行列に整理したものと\\(\\frac{\\partial L}{\\partial \\textbf{K}_x}\\)との要素ごとの積を足し合わせたものが勾配となります。\n\r3. Rでの実装\rGPLVMをRで実装します。使用するデータは以前giannoneの記事で使用したものと同じものです。\nESTIMATE_GPLVM \u0026lt;- function(Y,P,sigma){\r# 1. Set initial value\rY \u0026lt;- as.matrix(Y)\reigenvector \u0026lt;- eigen(cov(Y))$vectors\rX \u0026lt;- Y%*%eigenvector[,1:P] # initial value\rN \u0026lt;- NROW(Y) # Sample Size\rD \u0026lt;- NCOL(Y) # Dimention of dataset\rX0 \u0026lt;- c(as.vector(X))\rsigma \u0026lt;- var(matrix(Y,dim(Y)[1]*dim(Y)[2],1))\r# 2. Define log likelihood function\rloglik \u0026lt;- function(X0,Y,N,P,D,beta,sigma){\rX \u0026lt;- matrix(X0,N,P)\rK \u0026lt;- matrix(0,N,N)\rscale \u0026lt;- diag(sqrt(3/((apply(X, 2, max) -apply(X, 2, min))^2)))\rX \u0026lt;- X%*%scale\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- sigma*exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)\r}\r}\rL \u0026lt;- - D*N/2*log(2*pi) - D/2*log(det(K)) - 1/2*sum(diag(ginv(K)%*%Y%*%t(Y))) #loglikelihood\rreturn(L)\r}\r# 3. Define derivatives of log likelihood function\rdloglik \u0026lt;- function(X0,P,D,N,Y,beta,sigma){\rX \u0026lt;- matrix(X0,N,P)\rK \u0026lt;- matrix(0,N,N)\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)\r}\r}\rinvK \u0026lt;- ginv(K)\rdLdK \u0026lt;- invK%*%Y%*%t(Y)%*%invK - D*invK\rdLdx \u0026lt;- matrix(0,N,P)\rfor (j in 1:P){\rfor(i in 1:N){\rdKdx \u0026lt;- matrix(0,N,N)\rfor (k in 1:N){\rdKdx[i,k] \u0026lt;- dKdx[k,i] \u0026lt;- -exp(-(t(X[i,]-X[k,])%*%(X[i,]-X[k,]))*0.5)*((X[i,j]-X[k,j])*0.5)\r}\rdLdx[i,j] \u0026lt;- sum(dLdK*dKdx)\r}\r}\rreturn(dLdx)\r}\r# 4. Optimization\rres \u0026lt;- optim(X0, loglik, dloglik, Y = Y, N=N, P=P, D=D, beta = exp(2), sigma = sigma,\rmethod = \u0026quot;BFGS\u0026quot;, control = list(fnscale = -1,trace=1000,maxit=10000))\routput \u0026lt;- matrix(res$par,N,P)\rresult \u0026lt;- list(output,res,P)\rnames(result) \u0026lt;- c(\u0026quot;output\u0026quot;,\u0026quot;res\u0026quot;,\u0026quot;P\u0026quot;)\rreturn(result)\r}\rGPLVM_SELECT \u0026lt;- function(Y){\rD \u0026lt;- NCOL(Y)\rlibrary(stringr)\rfor (i in 1:D){\rif (i == 1){\rresult \u0026lt;- ESTIMATE_GPLVM(Y,i)\rP \u0026lt;- 2\rprint(str_c(\u0026quot;STEP\u0026quot;, i, \u0026quot; loglikelihood \u0026quot;, as.numeric(result$res$value)))\r}else{\rtemp \u0026lt;- ESTIMATE_GPLVM(Y,i)\rprint(str_c(\u0026quot;STEP\u0026quot;, i, \u0026quot; loglikelihood \u0026quot;, as.numeric(temp$res$value)))\rif (result$res$value \u0026lt; temp$res$value){\rresult \u0026lt;- temp\rP \u0026lt;- i\r}\r}\r}\rprint(str_c(\u0026quot;The optimal number of X is \u0026quot;, P))\rprint(str_c(\u0026quot;loglikelihood \u0026quot;, as.numeric(result$res$value)))\rreturn(result)\r}\rresult \u0026lt;- ESTIMATE_GPLVM(scale(Y),5)\r## initial value 613.864608 ## final value 609.080329 ## converged\rlibrary(tidyverse)\rggplot(gather(as.data.frame(result$output),key = name,value = value),\raes(x=rep(dataset1$publication,5),y=value,colour=name)) + geom_line(size=1) +\rxlab(\u0026quot;Date\u0026quot;) +\rggtitle(\u0026quot;5 economic factors\u0026quot;)\rlibrary(xts)\rX.xts \u0026lt;- xts(result$output,order.by = dataset1$publication)\rX.q.xts \u0026lt;- apply.quarterly(X.xts,mean)\rX.3m.xts \u0026lt;- X.xts[endpoints(X.xts,on=\u0026quot;quarters\u0026quot;),]\rif (months(index(X.q.xts)[NROW(X.q.xts)]) %in% c(\u0026quot;3月\u0026quot;,\u0026quot;6月\u0026quot;,\u0026quot;9月\u0026quot;,\u0026quot;12月\u0026quot;)){\r} else X.q.xts \u0026lt;- X.q.xts[-NROW(X.q.xts),]\rif (months(index(X.3m.xts)[NROW(X.3m.xts)]) %in% c(\u0026quot;3月\u0026quot;,\u0026quot;6月\u0026quot;,\u0026quot;9月\u0026quot;,\u0026quot;12月\u0026quot;)){\r} else X.3m.xts \u0026lt;- X.3m.xts[-NROW(X.3m.xts),]\rcolnames(X.xts) \u0026lt;- c(\u0026quot;factor1\u0026quot;,\u0026quot;factor2\u0026quot;,\u0026quot;factor3\u0026quot;,\u0026quot;factor4\u0026quot;,\u0026quot;factor5\u0026quot;) GDP$publication \u0026lt;- GDP$publication + months(2)\rGDP.q \u0026lt;- GDP[GDP$publication\u0026gt;=index(X.q.xts)[1] \u0026amp; GDP$publication\u0026lt;=index(X.q.xts)[NROW(X.q.xts)],]\rrg \u0026lt;- lm(scale(GDP.q$GDP)~X.q.xts[-54])\rrg2 \u0026lt;- lm(scale(GDP.q$GDP)~X.3m.xts[-54])\rsummary(rg)\r## ## Call:\r## lm(formula = scale(GDP.q$GDP) ~ X.q.xts[-54])\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.31774 -0.06927 -0.03224 0.06690 0.31062 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.0445000 0.0317777 1.400 0.177 ## X.q.xts[-54]X.1 -0.3181108 0.0106335 -29.916 \u0026lt; 2e-16 ***\r## X.q.xts[-54]X.2 0.0004621 0.0175913 0.026 0.979 ## X.q.xts[-54]X.3 0.1737612 0.0249186 6.973 9.09e-07 ***\r## X.q.xts[-54]X.4 -0.0060035 0.0376324 -0.160 0.875 ## X.q.xts[-54]X.5 0.0646009 0.0430678 1.500 0.149 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.1617 on 20 degrees of freedom\r## Multiple R-squared: 0.9791, Adjusted R-squared: 0.9739 ## F-statistic: 187.3 on 5 and 20 DF, p-value: 4.402e-16\rsummary(rg2)\r## ## Call:\r## lm(formula = scale(GDP.q$GDP) ~ X.3m.xts[-54])\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.35613 -0.11430 0.00258 0.15171 0.36506 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.003798 0.041789 -0.091 0.928 ## X.3m.xts[-54]1 -0.312411 0.014008 -22.303 1.34e-15 ***\r## X.3m.xts[-54]2 0.022873 0.025672 0.891 0.384 ## X.3m.xts[-54]3 0.152350 0.031080 4.902 8.62e-05 ***\r## X.3m.xts[-54]4 -0.019237 0.047809 -0.402 0.692 ## X.3m.xts[-54]5 -0.032419 0.055662 -0.582 0.567 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.21 on 20 degrees of freedom\r## Multiple R-squared: 0.9647, Adjusted R-squared: 0.9559 ## F-statistic: 109.3 on 5 and 20 DF, p-value: 8.102e-14\r決定係数が大幅に改善しました。\r3ヶ月の平均をとったファクターの方がパフォーマンスが良さそうなので、こちらで実際のGDPと予測値のプロットを行ってみます。\nggplot(gather(data.frame(fit=rg$fitted.values,actual=scale(GDP.q$GDP),Date=GDP.q$publication),key,value,-Date),aes(y=value,x=Date,colour=key)) +\rgeom_line(size=1) +\rggtitle(\u0026quot;fit v.s. actual GDP\u0026quot;)\rいかがでしょうか。個人的にはかなりフィッティングできている印象があります（もはや経済理論など不要なのでしょうか）。ただ、最も新しい値を除いては未来の値が情報量として加味された上で推計されていることになりますから、フェアではありません。正しく予測能力を検証するためには四半期ごとに逐次的に回帰を行う必要があります。\nというわけで、アウトサンプルの予測力がどれほどあるのかをテストしてみたいと思います。まず、2005年4月から2007年3月までの月次統計データでファクターを計算し、データ頻度を四半期に集約します。そして、2007年1Qのデータを除いて、GDPに回帰します。回帰したモデルの係数を用いて、2007年1Qのファクターデータで同時点のGDPの予測値を計算し、それを実績値と比較します。次は2005年4月から2007年6月までのデータを用いて･･･という感じでアウトサンプルの予測を行ってみます。\nlibrary(lubridate)\rtest_df \u0026lt;- data.frame()\rfor (i in as.list(seq(as.Date(\u0026quot;2015-04-01\u0026quot;),as.Date(\u0026quot;2019-03-01\u0026quot;),by=\u0026quot;quarter\u0026quot;))){\rday(i) \u0026lt;- days_in_month(i)\rtraindata \u0026lt;- dataset1[dataset1$publication\u0026lt;=i,]\rX_train \u0026lt;- ESTIMATE_GPLVM(scale(traindata[,-2]),5)\rX_train.xts \u0026lt;- xts(X_train$output,order.by = traindata$publication)\rX_train.q.xts \u0026lt;- apply.quarterly(X_train.xts,mean)\rif (months(index(X_train.q.xts)[NROW(X_train.q.xts)]) %in% c(\u0026quot;3月\u0026quot;,\u0026quot;6月\u0026quot;,\u0026quot;9月\u0026quot;,\u0026quot;12月\u0026quot;)){\r} else X_train.q.xts \u0026lt;- X_train.q.xts[-NROW(X_train.q.xts),]\rcolnames(X_train.q.xts) \u0026lt;- c(\u0026quot;factor1\u0026quot;,\u0026quot;factor2\u0026quot;,\u0026quot;factor3\u0026quot;,\u0026quot;factor4\u0026quot;,\u0026quot;factor5\u0026quot;) GDP_train.q \u0026lt;- scale(GDP[GDP$publication\u0026gt;=index(X_train.q.xts)[1] \u0026amp; GDP$publication\u0026lt;=index(X_train.q.xts)[NROW(X_train.q.xts)],2])\rrg_train \u0026lt;- lm(GDP_train.q[-NROW(GDP_train.q)]~.,data=X_train.q.xts[-NROW(X_train.q.xts)])\rsummary(rg_train)\rtest_df \u0026lt;- rbind(test_df,data.frame(predict(rg_train,X_train.q.xts[NROW(X_train.q.xts)],interval = \u0026quot;prediction\u0026quot;,level=0.90),GDP=GDP_train.q[NROW(GDP_train.q)]))\r}\r計算できました。グラフにしてみましょう。先ほどのグラフよりは精度が悪くなりました。特にリーマンの後は予測値の信頼区間（90%）が大きく拡大しており、不確実性が増大していることもわかります。2010年以降に関しては実績値は信頼区間にほど入っており、予測モデルとしての性能はまあまあなのかなと思います。ただ、リーマンのような金融危機もズバッと当てるところにロマンがあると思うので元データの改善を図りたいと思います。この記事はいったんここで終了です。\nggplot(gather(data.frame(test_df,Date=as.Date(rownames(test_df))),,,-c(lwr,upr,Date)),aes(y=value,x=Date,colour=key)) +\rgeom_ribbon(aes(ymax=upr,ymin=lwr,fill=\u0026quot;band\u0026quot;),alpha=0.1,linetype=\u0026quot;blank\u0026quot;) +\rgeom_line(size=1) +\rggtitle(\u0026quot;out-sample test\u0026quot;)\r\r","date":1558828800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1558828800,"objectID":"453e55777bb25992fabfa629782065a2","permalink":"/post/post8/","publishdate":"2019-05-26T00:00:00Z","relpermalink":"/post/post8/","section":"post","summary":"最近私が注目しているのがガウス過程。今回はそのガウス過程の中でも、主成分分析のようにデータセットの次元削減のために使用されるGaussian Process Latent Variable Model（GPLVM）の紹介をします。","tags":["R","ガウス過程","GPLVM"],"title":"GPLVMでマルチファクターモデルを構築してみた","type":"post"},{"authors":null,"categories":["仕事関連"],"content":"\r\r\r\r1. 最小分散ポートフォリオ\r2. 分散共分散行列をどのように求めるか\r\rA. Constant conditional correlation (CCC) model\rB. Dynamic Conditional Correlation (DCC) model\rC. Dynamic Equicorrelation (DECO) model\r\r3. テスト用データの収集\r\r\rおはこんばんにちは。勤め先で、アセットアロケーションに関するワークショップに参加したので、この分野は完全なる専門外ですがシミュレーションをしてみたいと思います。今回は、最小分散ポートフォリオ(minimum variance portfolio)を基本ポートフォリオとしたうえで、その分散共分散行列（予測値）をどのように推計するのかという点について先行研究を参考にエクササイズしていきたいと思います。先行研究は以下の論文です（オペレーションリサーチのジャーナルでした）。\nAsset Allocation with Correlation: A Composite Trade-Off\n1. 最小分散ポートフォリオ\r最小分散ポートフォリオの詳しい説明はここでは割愛しますが、要は各資産（内株、外株、内債、外債、オルタナ）のリターンの平均と分散を計算し、それらを縦軸平均値、横軸分散の二次平面にプロットしたうえで、投資可能範囲を計算し、その集合の中で最も分散が小さくなるポートフォリオの事らしいです（下図参照）。\nminimum variance portfolio\n\r先行研究のCarroll et. al. (2017)では、\n\rthis paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.\n\rと記載されており、現状でも期待リターンの推計は難しく、それを必要としない最小分散ポートフォリオは有益で実践的な手法であるといえます。最小分散ポートフォリオの目的関数は、その名の通り「分散を最小化すること」です。今、各資産のリターンを集めたベクトルを[tex:r]、各資産の保有ウェイトを\\(\\theta\\)、ポートフォリオリターンを\\(R_{p}\\)で表すことにすると、ポートフォリオ全体の分散\\(var(R_{p})\\)は以下のように記述できます。\n\\[\rvar(R_{p}) = var(r^{T}\\theta) = E( (r^{T}\\theta)(r^{T}\\theta)^{T}) = \\theta^{T}\\Sigma\\theta\r\\]\nここで\\(Sigma\\)は\\(r\\)の分散共分散行列です。よって、最小化問題は以下になります。\n\\[\r\\min_{\\theta}(\\theta^{T}\\Sigma\\theta) \\\\\rs.t 1^{T}\\theta = 1\r\\]\nここでは、フルインベストメントを制約条件に加えています。ラグランジュ未定乗数法を用いてこの問題を解いてみましょう。ラグランジュ関数\\(L\\)は以下のようになります。\n\\[\rL = \\theta^{T}\\Sigma\\theta + \\lambda(1^{T}\\theta - 1)\r\\]\n1階の条件は、\n\\[\r\\displaystyle\\frac{\\partial L}{\\partial \\theta} = 2\\Sigma\\theta + 1\\lambda = 0 \\\\\r\\displaystyle \\frac{\\partial L}{\\partial \\lambda} = 1^{T} \\theta = 1\r\\]\n1本目の式を\\(\\theta\\)について解くと、\n\\[\r\\theta = \\Sigma^{-1}1\\lambda^{*}\r\\]\nとなります。ここで、\\(\\lambda^{*}=-1/2\\lambda\\)です。これを2本目の式に代入し、\\(\\lambda^{*}\\)について解きます。\n\\[\r1^{T}\\Sigma1\\lambda^{*} = 1 \\\\\r\\displaystyle \\lambda^{*} = \\frac{1}{1^{T}\\Sigma^{-1}1}\r\\]\n\\(\\theta = \\Sigma^{-1}1\\lambda^{*}\\)だったので、\\(\\lambda^{*}\\)を消去すると、\n\\[\r\\displaystyle \\theta_{gmv} = \\frac{\\Sigma^{-1}1}{1^{T}\\Sigma^{-1}1}\r\\]\nとなり、最適なウェイトを求めることができました。とりあえず、これをRで実装しておきます。\ngmv \u0026lt;- function(r_dat,r_cov){\rlibrary(MASS)\ri \u0026lt;- matrix(1,NCOL(r_dat),1)\rr_weight \u0026lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i)\rwr_dat \u0026lt;- r_dat*as.numeric(r_weight)\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;) return(result)\r}\rnlgmv \u0026lt;- function(r_dat,r_cov){\rqp.out \u0026lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))),\rbvec=c(1,rep(0,NCOL(r_dat))),meq=1)\rr_weight \u0026lt;- qp.out$solution\rwr_dat \u0026lt;- r_dat*r_weight\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;)\rreturn(result)\r}\r入力は各資産のリターンと分散共分散行列になっています。出力はウェイト、リターン、リスクです。nlgmvは最小分散ポートフォリオの空売り制約バージョンです。解析的な解は得られないので、数値的に買いを求めています。\n\r2. 分散共分散行列をどのように求めるか\r最小分散ポートフォリオの計算式は求めることができました。次は、その入力である分散共分散行列をどうやって求めるのかについて分析したいと思います。一番原始的な方法はその時点以前に利用可能なリターンデータを標本として分散共分散行列を求め、その値を固定して最小分散ポートフォリオを求めるというヒストリカルなアプローチかと思います（つまりウェイトも固定）。ただ、これはあくまで過去の平均値を将来の予想値に使用するため、いろいろ問題が出てくるかと思います。専門外の私が思いつくものとしては、前日ある資産Aのリターンが大きく下落したという場面で明日もこの資産の分散は大きくなることが予想されるにも関わらず、平均値を使用するため昨日の効果が薄められてしまうことでしょうか。それに、ウェイトを最初から変更しないというのも時間がたつにつれ、最適点から離れていく気がします。ただ、ではどう推計するのかついてはこの分野でも試行錯誤が行われているようです。Carroll et. al. (2017)でも、\n\rThe estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.\n\rと述べられていました。この論文では以下のようなモデルを用いて推計が行われています。いずれも、分散共分散行列を時変としているところに特徴があります。\nA. Constant conditional correlation (CCC) model\r元論文はこちら。\nまず、分散共分散行列と相関行列の関係性から、\\(\\Sigma_{t} = D_{t}R_{t}D_{t}\\)となります。ここで、\\(R_{t}\\)は相関行列、\\(D_{t}\\)は\\(diag(\\sigma_{1,t},...,\\sigma_{N,t})\\)で各資産\\(tt\\)期の標準偏差\\(\\sigma_{i,t}\\)を対角成分に並べた行列です。ここから、\\(D_{t}\\)と\\(R_{t}\\)を分けて推計していきます。まず、\\(D_{t}\\)ですが、こちらは以下のような多変量GARCHモデル(1,1)で推計します。\n\\[\rr_{t} = \\mu + u_{t} \\\\\ru_{t} = \\sigma_{t}\\epsilon \\\\\r\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}u_{t-1}^{2} + \\alpha_{2}\\sigma_{t-1}^{2} \\\\\r\\epsilon_{t} = NID(0,1) \\\\\rE(u_{t}|u_{t-1}) = 0\r\\]\nここで、\\(\\mu\\)はリターンの標本平均です。\\(\\alpha_{i}\\)は推定すべきパラメータ。\\(D_{t}\\)をGARCHで推計しているので、リターンの分布が正規分布より裾野の厚い分布に従い、またリターンの変化は一定ではなく前日の分散に依存する関係をモデル化しているといえるのではないでしょうか。とりあえずこれで\\(D_{t}\\)の推計はできたということにします。次に\\(R_{t}\\)の推計ですが、このモデルではリターンを標本として求めるヒストリカルなアプローチを取ります。つまり、\\(R_{t}\\)は定数です。よって、リターン変動の大きさは時間によって変化するが、各資産の相対的な関係性は不変であるという仮定を置いていることになります。\n\rB. Dynamic Conditional Correlation (DCC) model\r元論文はこちら。\nこちらのモデルでは、\\(D_{t}\\)を求めるところまでは①と同じですが、[\\(R_{t}\\)の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、\\(tex:t\\)期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列\\(R_{t}\\)は、\n\\[\rR_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2}\r\\]\nです。ここで、\\(Q_{t}\\)は\\(t\\)期での条件付分散共分散行列で以下のように定式化されます。\n\\[\rQ_{t} = \\bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\\epsilon_{i,t-1}\\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1}\r\\]\nここで、\\(\\bar{Q}\\)はヒストリカルな方法で計算した分散共分散行列であり、\\(a,b\\)はパラメータです。この方法では、先ほどとは異なり、リターン変動の大きさが時間によって変化するだけでなく、各資産の相対的な関係性も通時的に変化していくという仮定を置いていることになります。金融危機時には全資産のリターンが下落し、各資産の相関が正になる事象も観測されていることから、この定式化は魅力的であるということができるのではないでしょうか。\n\rC. Dynamic Equicorrelation (DECO) model\r元論文はこちら。\nこの論文はまだきっちり読めていないのですが、相関行列\\(R_{t}\\)の定義から\n\\[\rR_{t} = (1-\\rho_{t})I_{N} + \\rho_{t}1\r\\]\nとなるようです。ここで、\\(\\rho_{t}\\)はスカラーでequicorrelationの程度を表す係数です。equicorrelationとは平均的なペアワイズ相関の事であると理解しています。つまりは欠損値がなければ普通の相関と変わりないんじゃないかと。ただ、資産が増えればそのような問題にも対処する必要があるのでその点ではよい推定量のようです。\\(\\rho_{t}\\)は以下のように求めることができます。\n\\[\r\\displaystyle \\rho_{t} = \\frac{1}{N(N-1)}(\\iota^{T}R_{t}^{DCC}\\iota - N) = \\frac{2}{N(N-1)}\\sum_{i\u0026gt;j}\\frac{q_{ij,t}}{\\sqrt{q_{ii,t} q_{jj,t}}}\r\\]\nここで、\\(\\iota\\)はN×1ベクトルで要素は全て1です。また、\\(q_{ij,t}\\)は\\(Q_{t}\\)のi,j要素です。\nさて、分散共分散行列のモデル化ができたところで、ここまでをRで実装しておきます。\ncarroll \u0026lt;- function(r_dat,FLG){\rlibrary(rmgarch)\rif(FLG == \u0026quot;benchmark\u0026quot;){\rH \u0026lt;- cov(r_dat)\r}else{\r#1. define variables\rN \u0026lt;- NCOL(r_dat) # the number of assets\r#2. estimate covariance matrix\rbasic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = \u0026#39;sGARCH\u0026#39;), distribution.model = \u0026#39;norm\u0026#39;)\rmulti_garch = multispec(replicate(N, basic_garch))\rdcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = \u0026quot;mvnorm\u0026quot;,model = \u0026quot;DCC\u0026quot;)\rfit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE))\rforecast_dcc_garch \u0026lt;- dccforecast(fit_dcc_garch)\rif (FLG == \u0026quot;CCC\u0026quot;){\r#Constant conditional correlation (CCC) model\rD \u0026lt;- sigma(forecast_dcc_garch)\rR_ccc \u0026lt;- cor(r_dat)\rH \u0026lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1])\rcolnames(H) \u0026lt;- colnames(r_dat)\rrownames(H) \u0026lt;- colnames(r_dat)\r}\relse{\r#Dynamic Conditional Correlation (DCC) model\rH \u0026lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1])\rif (FLG == \u0026quot;DECO\u0026quot;){\r#Dynamic Equicorrelation (DECO) model\rone \u0026lt;- matrix(1,N,N)\riota \u0026lt;- rep(1,N)\rQ_dcc \u0026lt;- rcor(forecast_dcc_garch,type=\u0026quot;Q\u0026quot;)[[1]][,,1]\rrho \u0026lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N))\rD \u0026lt;- sigma(forecast_dcc_garch)\rR_deco \u0026lt;- (1-rho)*diag(1,N,N) + rho*one\rH \u0026lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1])\rcolnames(H) \u0026lt;- colnames(r_dat)\rrownames(H) \u0026lt;- colnames(r_dat)\r}\r}\r}\rreturn(H)\r}\r本来であれば、パッケージを使用するべきではないのですが、今日はエクササイズなので推計結果だけを追い求めたいと思います。GARCHについては再来週ぐらいに記事を書く予定です。\rこれで準備ができました。この関数にリターンデータを入れて、分散共分散行列を計算し、それを用いて最小分散ポートフォリオを計算することができるようになりました。\n\r\r3. テスト用データの収集\rデータは以下の記事を参考にしました。\n(Introduction to Asset Allocation)[https://www.r-bloggers.com/introduction-to-asset-allocation/]\n使用したのは、以下のインデックスに連動するETF(iShares)の基準価額データです。\n\rS\u0026amp;P500\rNASDAQ100\rMSCI Emerging Markets\rRussell 2000\rMSCI EAFE\rUS 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index)\rU.S. Real Estate(the Dow Jones US Real Estate Index)\rgold bullion market\r\rまず、データ集めです。\nlibrary(quantmod)\r#**************************\r# ★8 ASSETS SIMULATION\r# SPY - S\u0026amp;P 500 # QQQ - Nasdaq 100\r# EEM - Emerging Markets\r# IWM - Russell 2000\r# EFA - EAFE\r# TLT - 20 Year Treasury\r# IYR - U.S. Real Estate\r# GLD - Gold\r#**************************\r# load historical prices from Yahoo Finance\rsymbol.names = c(\u0026quot;S\u0026amp;P 500\u0026quot;,\u0026quot;Nasdaq 100\u0026quot;,\u0026quot;Emerging Markets\u0026quot;,\u0026quot;Russell 2000\u0026quot;,\u0026quot;EAFE\u0026quot;,\u0026quot;20 Year Treasury\u0026quot;,\u0026quot;U.S. Real Estate\u0026quot;,\u0026quot;Gold\u0026quot;)\rsymbols = c(\u0026quot;SPY\u0026quot;,\u0026quot;QQQ\u0026quot;,\u0026quot;EEM\u0026quot;,\u0026quot;IWM\u0026quot;,\u0026quot;EFA\u0026quot;,\u0026quot;TLT\u0026quot;,\u0026quot;IYR\u0026quot;,\u0026quot;GLD\u0026quot;)\rgetSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, auto.assign = TRUE)\r#gn dates for all symbols \u0026amp; convert to monthly\rhist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)\rmonth.ends = endpoints(hist.prices, \u0026#39;day\u0026#39;)\rhist.prices = Cl(hist.prices)[month.ends, ]\rcolnames(hist.prices) = symbols\r# remove any missing data\rhist.prices = na.omit(hist.prices[\u0026#39;1995::\u0026#39;])\r# compute simple returns\rhist.returns = na.omit( ROC(hist.prices, type = \u0026#39;discrete\u0026#39;) )\r# compute historical returns, risk, and correlation\ria = list()\ria$expected.return = apply(hist.returns, 2, mean, na.rm = T)\ria$risk = apply(hist.returns, 2, sd, na.rm = T)\ria$correlation = cor(hist.returns, use = \u0026#39;complete.obs\u0026#39;, method = \u0026#39;pearson\u0026#39;)\ria$symbols = symbols\ria$symbol.names = symbol.names\ria$n = length(symbols)\ria$hist.returns = hist.returns\r# convert to annual, year = 12 months\rannual.factor = 12\ria$expected.return = annual.factor * ia$expected.return\ria$risk = sqrt(annual.factor) * ia$risk\rrm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)\rリターンをプロットするとこんな感じです。\nPerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = \u0026quot;パフォーマンスサマリー\u0026quot;)\r次に、バックテストのコーディングを行います。一気にコードを公開します。\n# BACK TEST\rbacktest \u0026lt;- function(r_dat,FLG,start_date,span,learning_term,port){\r#-----------------------------------------\r# BACKTEST\r# r_dat - return data(xts object) # FLG - flag(CCC,DCC,DECO)\r# start_date - start date for backtest\r# span - rebalance frequency\r# learning_term - learning term (days)\r# port - method of portfolio optimization\r#-----------------------------------------\rlibrary(stringi)\rinitial_dat \u0026lt;- r_dat[stri_c(as.Date(start_date)-learning_term,\u0026quot;::\u0026quot;,as.Date(start_date))]\rfor (i in NROW(initial_dat):NROW(r_dat)) {\rif (i == NROW(initial_dat)){\rH \u0026lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG)\rif (port == \u0026quot;nlgmv\u0026quot;){\rresult \u0026lt;- nlgmv(initial_dat,H)\r}else if (port == \u0026quot;risk parity\u0026quot;){\rresult \u0026lt;- risk_parity(initial_dat,H)\r}\rweight \u0026lt;- t(result$weight)\rcolnames(weight) \u0026lt;- colnames(initial_dat)\rp_return \u0026lt;- initial_dat[NROW(initial_dat),]*result$weight\r} else {\rif (i %in% endpoints(r_dat,span)){\rH \u0026lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG)\rif (port == \u0026quot;nlgmv\u0026quot;){\rresult \u0026lt;- nlgmv(test_dat,H)\r}else if (port == \u0026quot;risk parity\u0026quot;){\rresult \u0026lt;- risk_parity(test_dat,H)\r}\r}\rweight \u0026lt;- rbind(weight,t(result$weight))\rp_return \u0026lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight)\r}\rif (i != NROW(r_dat)){\rterm \u0026lt;- stri_c(index(r_dat[i+1,])-learning_term,\u0026quot;::\u0026quot;,index(r_dat[i+1,])) test_dat \u0026lt;- r_dat[term]\r}\r}\rp_return$portfolio \u0026lt;- xts(apply(p_return,1,sum),order.by = index(p_return))\rweight.xts \u0026lt;- xts(weight,order.by = index(p_return))\rresult \u0026lt;- list(p_return,weight.xts)\rnames(result) \u0026lt;- c(\u0026quot;return\u0026quot;,\u0026quot;weight\u0026quot;)\rreturn(result)\r}\rCCC \u0026lt;- backtest(hist.returns,\u0026quot;CCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDCC \u0026lt;- backtest(hist.returns,\u0026quot;DCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDECO \u0026lt;- backtest(hist.returns,\u0026quot;DECO\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rbenchmark \u0026lt;- backtest(hist.returns,\u0026quot;benchmark\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rresult \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)\rcolnames(result) \u0026lt;- c(\u0026quot;CCC\u0026quot;,\u0026quot;DCC\u0026quot;,\u0026quot;DECO\u0026quot;,\u0026quot;benchmark\u0026quot;)\r計算結果をグラフにしてみます。\nPerformanceAnalytics::charts.PerformanceSummary(result,main = \u0026quot;BACKTEST\u0026quot;)\r空売り制約を課したので、上で定義した最小分散ポートフォリオは使用していません。どうやらこれだけで解析的に解くのは難しいらしく、数値的に解くことにしています。リバランス期間を週次にしたので、自前のPCでは計算に時間がかかりましたが、結果が計算できました。\nリーマン以降はどうやらベンチマークである等ウェイトポートフォリオよりをアウトパフォームしているようです。特に、DECOはいい感じです。そもそもDECOとDCCはほぼ変わらないパフォーマンスであると思っていたのですが、どうやら自分の理解が足らないらしく、論文の読み返す必要があるようです。Equicorrelationの意味をもう一度考えてみたいと思います。それぞれの組入比率の推移は以下のようになりました。\n# plot allocation weighting\rd_allocation \u0026lt;- function(ggweight,title){\r#install.packages(\u0026quot;tidyverse\u0026quot;)\rlibrary(tidyverse)\rggweight \u0026lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method)\rggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) +\rgeom_area(colour=\u0026quot;black\u0026quot;,size=.1) +\rscale_y_continuous(limits = c(0,1)) +\rlabs(title=title) + facet_grid(method~.)\r}\rgmv_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)))\r# plot allocation weighting\rd_allocation(gmv_weight,\u0026quot;GMV Asset Allocation\u0026quot;)\rリーマンの際にTLT、つまり米国債への比率を増やしているようです。CCCとDCCはそれ以外の部分でも米国債への比率が高く、よく挙げられる最小分散ポートフォリオの問題点がここでも発生しているようです。一方、DECOがやはり個性的な組入比率の推移をしており、ここらを考えてももう一度論文を読み返してみる必要がありそうです。\n追記（2019/3/3）\rこれまでは、最小分散ポートフォリオで分析をしていましたが、リスクパリティの結果も見たいなと言うことで、そのコードも書いてみました。\nrisk_parity \u0026lt;- function(r_dat,r_cov){\rfn \u0026lt;- function(weight, r_cov) {\rN \u0026lt;- NROW(r_cov)\rrisks \u0026lt;- weight * (r_cov %*% weight)\rg \u0026lt;- rep(risks, times = N) - rep(risks, each = N)\rreturn(sum(g^2))\r}\rdfn \u0026lt;- function(weight,r_cov){\rout \u0026lt;- weight\rfor (i in 0:length(weight)) {\rup \u0026lt;- dn \u0026lt;- weight\rup[i] \u0026lt;- up[i]+.0001\rdn[i] \u0026lt;- dn[i]-.0001\rout[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002\r}\rreturn(out)\r}\rstd \u0026lt;- sqrt(diag(r_cov)) x0 \u0026lt;- 1/std/sum(1/std)\rres \u0026lt;- nloptr::nloptr(x0=x0,\reval_f=fn,\reval_grad_f=dfn,\reval_g_eq=function(weight,r_cov) { sum(weight) - 1 },\reval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) },\rlb=rep(0,length(std)),ub=rep(1,length(std)),\ropts = list(\u0026quot;algorithm\u0026quot;=\u0026quot;NLOPT_LD_SLSQP\u0026quot;,\u0026quot;print_level\u0026quot; = 0,\u0026quot;xtol_rel\u0026quot;=1.0e-8,\u0026quot;maxeval\u0026quot; = 1000),\rr_cov = r_cov)\rr_weight \u0026lt;- res$solution\rnames(r_weight) \u0026lt;- colnames(r_cov)\rwr_dat \u0026lt;- r_dat*r_weight\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;)\rreturn(result)\r}\rCCC \u0026lt;- backtest(hist.returns,\u0026quot;CCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDCC \u0026lt;- backtest(hist.returns,\u0026quot;DCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDECO \u0026lt;- backtest(hist.returns,\u0026quot;DECO\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rbenchmark \u0026lt;- backtest(hist.returns,\u0026quot;benchmark\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rresult \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)\rcolnames(result) \u0026lt;- c(\u0026quot;CCC\u0026quot;,\u0026quot;DCC\u0026quot;,\u0026quot;DECO\u0026quot;,\u0026quot;benchmark\u0026quot;)\r結果はこんな感じ。\nPerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026quot;BACKTEST COMPARISON\u0026quot;)\rlibrary(plotly)\r# plot allocation weighting\rriskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026quot;benchmark\u0026quot;,Date=index(benchmark$weight)))\rPerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026quot;BACKTEST COMPARISON\u0026quot;)\rriskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026quot;benchmark\u0026quot;,Date=index(benchmark$weight)))\r# plot allocation weighting\rd_allocation(riskparity_weight, \u0026quot;Risk Parity Asset Allocation\u0026quot;)\rどの手法もbenchmarkをアウトパーフォームできているという好ましい結果になりました。\rやはり、分散共分散行列の推計がうまくいっているようです。また、DECOのパフォーマンスがよいのは、相関行列に各資産ペアの相関係数の平均値を用いているため、他の手法よりもリスク資産の組み入れが多くなったからだと思われます。ウェイトは以下の通りです。\nとりあえず、今日はここまで。\n\r","date":1550361600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1551571200,"objectID":"bc248846d8caf05f6b02c4f3564abd29","permalink":"/post/post2/","publishdate":"2019-02-17T00:00:00Z","relpermalink":"/post/post2/","section":"post","summary":"社内ワークショップがあり、休日は国会図書館に籠り、先行研究を漁った結果、微妙なアセットアロケーションモデルを構築できました。","tags":["R"],"title":"Asset Allocation ModelをRで組んでみた。","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r\r1. カルマンフィルタとは？\r2. カルマンフィルタアルゴリズムの導出\r3. Rで実装する。\r4. カルマンスムージング\r\r\rおはこんばんにちは。かなり久しぶりの投稿となってしまいました。決して研究をさぼっていたのではなく、BVARのコーディングに手こずっていました。あと少しで完成します。さて、今回はBVARやこの前のGiannnone et a (2008)のような分析でも大活躍のカルマンフィルタを実装してしまいたいと思います。このブログではパッケージソフトに頼らず、基本的に自分で一から実装を行い、研究することをポリシーとしていますので、これから頻繁に使用するであろうカルマンフィルタを関数として実装してしまうことは非常に有益であると考えます。今回はRで実装をしましたので、そのご報告をします。\n1. カルマンフィルタとは？\rまず、カルマンフィルタに関する簡単な説明を行います。非常にわかりやすい記事があるので、こちらを読んでいただいたほうがより分かりやすいかと思います。\nカルマンフィルタとは、状態空間モデルを解くアルゴリズムの一種です。状態空間モデルとは、手元の観測可能な変数から観測できない変数を推定するモデルであり、以下のような形をしています。\n\\[\rY_{t} = Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} \\\\\r\\alpha_{t} = T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}\r\\]\nここで、\\(Y_{t}\\)は\\(g×1\\)ベクトルの観測可能な変数(観測変数)、\\(Z_{t}\\)は\\(g×k\\)係数行列、\\(\\alpha_{t}\\)は\\(k×1\\)ベクトルの観測不可能な変数(状態変数)、\\(T_{t}\\)は\\(k×k\\)係数行列です。また、\\(\\epsilon_{t}\\)は観測変数の誤差項、\\(\\eta_{t}\\)は状態変数の誤差項です。これらの誤差項はそれぞれ\\(N(0,H_{t})\\), \\(N(0,Q_{t})\\)に従います（\\(H_{t},Q_{t}\\)は分散共分散行列）。\\(d_{t}\\), \\(c_{t}\\)は定数項です。1本目の式は観測方程式、2本目の式は遷移方程式と呼ばれます。\r状態空間モデルを使用する例として、しばしば池の魚の数を推定する問題が使用されます。今、池の中の魚の全数が知りたいとして、その推定を考えます。観測時点毎に池の中の魚をすべて捕まえてその数を調べるのは現実的に困難なので、一定期間釣りをして釣れた魚をサンプルに全数を推定することを考えます。ここで、釣れた魚は観測変数、池にいる魚の全数は状態変数と考えることができます。今、経験的に釣れた魚の数と全数の間に以下のような関係があるとします。\n\\[\rY_{t} = 0.01\\alpha_{t} + 5 + \\epsilon_{t}\r\\]\rこれが観測方程式になります。また、魚の全数は過去の値からそれほど急速には変化しないと考えられるため、以下のようなランダムウォークに従うと仮定します。\n\\[\r\\alpha_{t} = \\alpha_{t-1} + 500 + \\eta_{t}\r\\]\rこれが遷移方程式になります。あとは、これをカルマンフィルタアルゴリズムを用いて計算すれば、観測できない魚の全数を推定することができます。\rこのように状態空間モデルは非常に便利なモデルであり、また応用範囲も広いです。例えば、販売額から潜在顧客数を推定したり、クレジットスプレッドやトービンのQ等経済モデル上の概念として存在する変数を推定する、BVARのようにVARや回帰式の時変パラメータ推定などにも使用されます。\n\r2. カルマンフィルタアルゴリズムの導出\rさて、非常に便利な状態空間モデルの説明はこれくらいにして、カルマンフィルタの説明に移りたいと思います。カルマンフィルタは状態空間モデルを解くアルゴリズムの一種であると先述しました。つまり、他にも状態空間モデルを解くアルゴリズムは存在します。カルマンフィルタアルゴリズムは一般に誤差項の正規性の仮定を必要としないフィルタリングアルゴリズムであり、観測方程式と遷移方程式の線形性の仮定さえあれば、線形最小分散推定量となります。カルマンフィルタアルゴリズムの導出にはいくつかの方法がありますが、今回はこの線形最小分散推定量としての導出を行います。まず、以下の３つの仮定を置きます。\n初期値\\(\\alpha_{0}\\)は正規分布\\(N(a_{0},\\Sigma_{0})\\)に従う確率ベクトルである(\\(a_{t}\\)は\\(\\alpha_{t}\\)の推定値)。\r誤差項\\(\\epsilon_{t}\\),\\(\\eta_{s}\\)は全ての\\(t\\),\\(s\\)で互いに独立で、初期値ベクトル\\(\\alpha_{0}\\)と無相関である（\\(E(\\epsilon_{t}\\eta_{s})=0\\), \\(E(\\epsilon_{t}\\alpha_{0})=0\\), \\(E(\\eta_{t}\\alpha_{0})=0\\)）。\r2より、\\(E(\\epsilon_{t}\\alpha_{t}\u0026#39;)=0\\)、\\(E(\\eta_{t}\\alpha_{t-1}\u0026#39;)=0\\)。\r\rまず、\\(t-1\\)期の情報集合\\(\\Omega_{t-1}\\)が既知の状態での\\(\\alpha_{t}\\)と\\(Y_{t}\\)の期待値（予測値）を求めてみましょう。上述した状態空間モデルと誤差項の期待値がどちらもゼロである事実を用いると、以下のように計算することができます。\n\\[\rE(\\alpha_{t}|\\Omega_{t-1}) = a_{t|t-1} = T_{t}a_{t-1|t-1} + c_{t}\rE(Y_{t}|\\Omega_{t-1}) = Y_{t|t-1} = Z_{t}a_{t|t-1} + d_{t}\r\\]\nここで、次に、これらの分散を求めます。\n\\[\r\\begin{eqnarray}\rE( (\\alpha_{t}-a_{t|t-1})(\\alpha_{t}-a_{t|t-1})\u0026#39;|\\Omega_{t-1}) \u0026amp;=\u0026amp; E( (T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}-a_{t|t-1})(T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}-a_{t|t-1})\u0026#39;|\\Omega_{t-1}) \\\\ \u0026amp;=\u0026amp; E(T_{t}\\alpha_{t-1}\\alpha_{t-1}\u0026#39;T_{t}\u0026#39; + R_{t}\\eta_{t}\\eta_{t}\u0026#39;R_{t}\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; E(T_{t}\\alpha_{t-1}\\alpha_{t-1}\u0026#39;T_{t}\u0026#39;|\\Omega_{t-1}) + E(R_{t}\\eta_{t}\\eta_{t}\u0026#39;R_{t}\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; T_{t}E(\\alpha_{t-1}\\alpha_{t-1}\u0026#39;|\\Omega_{t-1})T_{t}\u0026#39; + R_{t}E(\\eta_{t}\\eta_{t}\u0026#39;|\\Omega_{t-1})R_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; T_{t}\\Sigma_{t-1|t-1}T_{t}\u0026#39; + R_{t}Q_{t}R_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1}\r\\end{eqnarray}\r\\]\n\\[\r\\begin{eqnarray}\rE( (Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})\u0026#39;|\\Omega_{t-1}) \u0026amp;=\u0026amp; E( (Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t}-Y_{t|t-1})(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t}-Y_{t|t-1})\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; E(Z_{t}\\alpha_{t}\\alpha_{t}\u0026#39;Z_{t}\u0026#39; + S_{t}\\epsilon_{t}\\epsilon_{t}\u0026#39;S_{t}\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; E(Z_{t}\\alpha_{t}\\alpha_{t}\u0026#39;Z_{t}\u0026#39;|\\Omega_{t-1}) + E(S_{t}\\epsilon_{t}\\epsilon_{t}\u0026#39;S_{t}\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; Z_{t}E(\\alpha_{t}\\alpha_{t}\u0026#39;|\\Omega_{t-1})Z_{t}\u0026#39; + S_{t}E(\\epsilon_{t}\\epsilon_{t}\u0026#39;|\\Omega_{t-1})S_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; F_{t|t-1}\r\\end{eqnarray}\r\\]\nここで、\\(t\\)期の情報集合\\(\\Omega_{t}\\)が得られたとします（つまり、観測値\\(Y_{t}\\)を入手）。カルマンフィルタでは、\\(t\\)期の情報である観測値\\(Y_{t}\\)を用いて\\(a_{t|t-1}\\)を以下の方程式で更新します。\n\\[\rE(\\alpha_{t}|\\Omega_{t}) = a_{t|t} = a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1})\r\\]\rつまり、観測値と\\(Y_{t}\\)の期待値（予測値）の差をあるウェイト\\(k_{t}\\)（\\(k×g\\)行列）でかけたもので補正をかけるわけです。よって、観測値と予測値が完全に一致していた場合は補正は行われないことになります。ここで重要なのは、ウエイト\\(k_{t}\\)をどのように決めるのかです。\\(k_{t}\\)は更新後の状態変数の分散\\(E( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})\u0026#39;)= \\Sigma_{t|t}\\)を最小化するよう決定します。これが、カルマンフィルタが線形最小分散推定量である根拠です。最小化にあたっては以下のベクトル微分が必要になりますので、おさらいをしておきましょう。今回使用するのは以下の事実です。\n\\[\r\\displaystyle \\frac{\\partial a\u0026#39;b}{\\partial b} = \\frac{\\partial b\u0026#39;a}{\\partial b} = a \\\\\r\\displaystyle \\frac{\\partial b\u0026#39;Ab}{\\partial b} = 2Ab\r\\]\nここで、\\(a,b\\)はベクトル（それぞれ\\(n×1\\)ベクトル、\\(1×n\\)ベクトル）、\\(A\\)は\\(n×n\\)の対称行列です。まず、１つ目から証明していきます。\\(\\displaystyle y = a\u0026#39;b = b\u0026#39;a = \\sum_{i=1}^{n}a_{i}b_{i}\\)とします。\rこのとき、\\(\\frac{\\partial y}{\\partial b_{i}}=a_{i}\\)なので、\n\\[\r\\displaystyle \\frac{\\partial a\u0026#39;b}{\\partial b} = \\frac{\\partial b\u0026#39;a}{\\partial b} = a\r\\]\n次に２つ目です。\\(y = b\u0026#39;Ab = \\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}b_{i}b_{j}\\)とします。このとき、\n\\[\r\\displaystyle \\frac{\\partial y}{\\partial b_{i}} = \\sum_{j=1}^{n}a_{ij}b_{j} + \\sum_{j=1}^{n}a_{ji}b_{j} = 2\\sum_{j=1}^{n}a_{ij}b_{j} = 2a_{i}\u0026#39;b\r\\]\rよって、\n\\[\r\\displaystyle \\frac{\\partial y}{\\partial b} =\r\\left(\r\\begin{array}{cccc}\r\\frac{\\partial y}{\\partial b_{1}} \\\\\r\\vdots \\\\\r\\frac{\\partial y}{\\partial b_{n}} \\\\\r\\end{array}\r\\right) = 2\r\\left(\r\\begin{array}{cccc}\r\\sum_{j=1}^{n}a_{1j}b_{j} \\\\\r\\vdots \\\\\r\\sum_{j=1}^{n}a_{nj}b_{j} \\\\\r\\end{array}\r\\right) = 2\r\\left(\r\\begin{array}{cccc}\ra_{1}\u0026#39;b \\\\\r\\vdots \\\\\ra_{n}\u0026#39;b \\\\\r\\end{array}\r\\right)\r= 2Ab\r\\]\rさて、準備ができたので、更新後の状態変数の分散\\(E( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})\u0026#39;)\\)を求めてみましょう。\n\\[\r\\begin{eqnarray}\rE( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})\u0026#39;) \u0026amp;=\u0026amp; \\Sigma_{t|t} \\\\\r\u0026amp;=\u0026amp; E\\{ (\\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))(\\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))\u0026#39;\\} \\\\\r\u0026amp;=\u0026amp; E\\{ ( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )\\} \\\\\r\u0026amp;=\u0026amp; E\\{ ( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1}) )( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1}) )\u0026#39;\\} \\\\\r\u0026amp;=\u0026amp; E\\{ ( (I - k_{t}Z_{t})\\alpha_{t} - k_{t}S_{t}\\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})( (I - k_{t}Z_{t})\\alpha_{t} - k_{t}S_{t}\\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})\u0026#39; \\} \\\\\r\u0026amp;=\u0026amp; E\\{( (I - k_{t}Z_{t})(\\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\\epsilon_{t})( (I - k_{t}Z_{t})(\\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\\epsilon_{t})\u0026#39;\\} \\\\\r\u0026amp;=\u0026amp; (I - k_{t}Z_{t})\\Sigma_{t|t-1}(I - k_{t}Z_{t})\u0026#39; + k_{t}S_{t}H_{t}S_{t}\u0026#39;k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; (\\Sigma_{t|t-1} - k_{t}Z_{t}\\Sigma_{t|t-1})(I - k_{t}Z_{t})\u0026#39; + k_{t}S_{t}H_{t}S_{t}\u0026#39;k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}(k_{t}Z_{t})\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; + k_{t}S_{t}H_{t}S_{t}\u0026#39;k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39;)k_{t}\u0026#39; \\\\\r\\end{eqnarray}\r\\]\n１回目の式変形で、\\(a_{t|t}\\)に上述した更新式を代入し、２回目の式変形で観測方程式と上で計算した\\(E(Y_{t}|\\Omega_{t-1})\\)を代入しています。さて、更新後の状態変数の分散\\(\\Sigma_{t|t}\\)を\\(k_{t}\\)の関数として書き表すことができたので、これを\\(k_{t}\\)で微分し、0と置き、\\(\\Sigma_{t|t}\\)を最小化する\\(k_{t}\\)を求めます。先述した公式で、\\(a=\\Sigma_{t|t-1}Z_{t}\u0026#39;\\)、\\(b=k_{t}\u0026#39;\\)、\\(A=(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39;)\\)とすると（\\(A\\)は分散共分散行列の和なので対称行列）、\n\\[\r\\frac{\\partial \\Sigma_{t|t}}{\\partial k_{t}\u0026#39;} = -2(Z_{t}\\Sigma_{t|t-1})\u0026#39; + 2(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39;)k_{t} = 0\r\\]\nここから、\\(k_{t}\\)を解きなおすと、\n\\[\r\\begin{eqnarray}\rk_{t} \u0026amp;=\u0026amp; \\Sigma_{t|t-1}Z_{t}\u0026#39;(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39;)^{-1} \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1}Z_{t}\u0026#39;F_{t|t-1}^{-1}\r\\end{eqnarray}\r\\]\n突然、\\(F_{t|t-1}\\)が出てきました。これは観測変数の予測値の分散\\(E((Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})\u0026#39;|\\Omega_{t-1})\\)でした。一方、\\(\\Sigma_{t|t-1}Z_{t}\\)は状態変数の予測値の分散を観測変数のスケールに調整したものです（観測空間に写像したもの）。つまり、カルマンゲイン\\(k_{t}\\)は状態変数と観測変数の予測値の分散比となっているのです。観測変数にはノイズがあり、観測方程式はいつも誤差０で満たされるわけではありません。また、状態方程式にも誤差項が存在します。状態の遷移も100%モデル通りにはいかないということです。\\(t\\)期の観測変数\\(Y_{t}\\)が得られたとして、それをどれほど信頼して状態変数を更新するかは観測変数のノイズが状態変数のノイズに比べてどれほど小さいかによります。つまり、相対的に観測方程式が遷移方程式よりも信頼できる場合には状態変数を大きく更新するのです。このように、カルマンフィルタでは、観測方程式と遷移方程式の相対的な信頼度によって、更新の度合いを毎期調整しています。その度合いが分散比であり、カルマンゲインだというわけです。ちなみに欠損値が発生した場合には、観測変数の分散を無限大にし、状態変数の更新を全く行わないという対処を行います。観測変数に信頼がないので当たり前の処置です。この場合は遷移方程式を100%信頼します。これがカルマンフィルタのコアの考え方になります。\r更新後の分散を計算しておきます。\n\\[\r\\begin{eqnarray}\r\\Sigma_{t|t} \u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}F_{t|t-1}k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + (\\Sigma_{t|t-1}Z_{t}\u0026#39;F_{t|t-1}^{-1})F_{t|t-1}k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - k_{t}Z_{t}\\Sigma_{t|t-1} \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - k_{t}F_{t|t-1}F_{t|t-1}^{-1}Z_{t}\\Sigma_{t|t-1} \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}\u0026#39;\r\\end{eqnarray}\r\\]\nでは、最終的に導出されたアルゴリズムをまとめたいと思います。\n\\[\r\\begin{eqnarray}\ra_{t|t-1} \u0026amp;=\u0026amp; T_{t}a_{t-1|t-1} + c_{t} \\\\\r\\Sigma_{t|t-1} \u0026amp;=\u0026amp; T_{t}\\Sigma_{t-1|t-1}T_{t}\u0026#39; + R_{t}Q_{t}R_{t}\u0026#39; \\\\\rY_{t|t-1} \u0026amp;=\u0026amp; Z_{t}a_{t|t-1} + d_{t} \\\\\rF_{t|t-1} \u0026amp;=\u0026amp; Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39; \\\\\rk_{t} \u0026amp;=\u0026amp; \\Sigma_{t|t-1}Z_{t}\u0026#39;F_{t|t-1}^{-1} \\\\\ra_{t|t} \u0026amp;=\u0026amp; a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}) \\\\\r\\Sigma_{t|t} \u0026amp;=\u0026amp; \\Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}\u0026#39;\r\\end{eqnarray}\r\\]\r初期値\\(a_{0},\\Sigma_{0}\\)が所与の元で、まず状態変数の予測値\\(a_{1|0},\\Sigma_{1|0}\\)を計算します。その結果を用いて、次は観測変数の予測値\\(Y_{t|t-1},F_{t|t-1}\\)を計算し、カルマンゲイン\\(k_{t}\\)を得ます。\\(t\\)期の観測可能なデータを入手したら、更新方程式を用いて\\(a_{t|t},\\Sigma_{t|t}\\)を更新します。これをサンプル期間繰り返していくことになります。ちなみに、遷移方程式の誤差項\\(\\eta_{t}\\)と定数項\\(c_{t}\\)がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致します。つまり、新しいサンプルを入手するたびにOLSをやり直す推計方法ということです（今回はその証明は勘弁してください）。\n\r3. Rで実装する。\r以下がRでの実装コードです。\nkalmanfiter \u0026lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){\r#-------------------------------------------------------------------\r# Implemention of Kalman filter\r# y - observed variable\r# I - the number of unobserved variable\r# t - parameter of endogenous variable in state equation\r# z - parameter of endogenous variable in observable equation\r# c - constant in state equaion\r# R - parameter of exogenous variable in state equation\r# Q - var-cov matrix of exogenous variable in state equation\r# d - constant in observable equaion\r# S - parameter of exogenous variable in observable equation\r# h - var-cov matrix of exogenous variable in observable equation\r# a_int - initial value of endogenous variable\r# sig_int - initial value of variance of endogenous variable\r#-------------------------------------------------------------------\rlibrary(MASS)\r# 1.Define Variable\rif (class(y)!=\u0026quot;matrix\u0026quot;){\ry \u0026lt;- as.matrix(y)\r}\rN \u0026lt;- NROW(y) # sample size\rL \u0026lt;- NCOL(y) # the number of observable variable a_pre \u0026lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable\ra_fil \u0026lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable\rsig_pre \u0026lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable\rsig_fil \u0026lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable\ry_pre \u0026lt;- array(0,dim = c(L,1,N)) # prediction of observed variable\rF_pre \u0026lt;- array(0,dim = c(L,L,N)) # prediction of var-cov mat. of observable variable F_inv \u0026lt;- array(0,dim = c(L,L,N)) # inverse of F_pre\rk \u0026lt;- array(0,dim = c(I,L,N)) # kalman gain\rif (any(is.na(a_int))==TRUE){\ra_int \u0026lt;- matrix(0,nrow = I,ncol = 1)\r}\rif (any(is.na(sig_int))==TRUE){\rsig_int \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(R))==TRUE){\rR \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(Q))==TRUE){\rQ \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(S))==TRUE){\rS \u0026lt;- matrix(1,nrow = L,ncol = L)\r}\rif (any(is.na(h))==TRUE){\rH \u0026lt;- array(0,dim = c(L,L,N))\rfor(i in 1:N){\rdiag(H[,,i]) = 1\r}\r}else if (class(h)!=\u0026quot;array\u0026quot;){\rH \u0026lt;- array(h,dim = c(NROW(h),NCOL(h),N))\r}\r# fill infinite if observed data is NA\rfor(i in 1:N){\rmiss \u0026lt;- is.na(y[i,])\rdiag(H[,,i])[miss] \u0026lt;- 1e+32\r}\ry[is.na(y)] \u0026lt;- 0\r# 2.Set Initial Value\ra_fil[,,1] \u0026lt;- a_int\rsig_fil[,,1] \u0026lt;- sig_int\r# 3.Implement Kalman filter\rfor (i in 1:N){\rif(class(z)==\u0026quot;array\u0026quot;){\rZ \u0026lt;- z[,,i]\r}else{\rZ \u0026lt;- z\r}\ra_pre[,,i] \u0026lt;- t%*%a_fil[,,i] + c\rsig_pre[,,i] \u0026lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)\ry_pre[,,i] \u0026lt;- Z%*%a_pre[,,i] + d\rF_pre[,,i] \u0026lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)\rk[,,i] \u0026lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])\ra_fil[,,i+1] \u0026lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])\rsig_fil[,,i+1] \u0026lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])\r}\r# 4.Aggregate results\rresult \u0026lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)\rnames(result) \u0026lt;- c(\u0026quot;state prediction\u0026quot;, \u0026quot;state filtered\u0026quot;, \u0026quot;state var prediction\u0026quot;, \u0026quot;state var filtered\u0026quot;, \u0026quot;observable prediction\u0026quot;, \u0026quot;kalman gain\u0026quot;,\r\u0026quot;parameter of state eq\u0026quot;, \u0026quot;parameter of observable eq\u0026quot;)\rreturn(result)\r}\r案外簡単に書けるもんですね。これを使って、Giannone et al (2008)をやり直してみます。データセットは前回記事と変わりません。\n以下、分析用のRコードです。\n#------------------------\r# Giannone et. al. 2008 #------------------------\rlibrary(MASS)\rlibrary(xts)\r# ファクターを計算\rf \u0026lt;- 3\rz \u0026lt;- scale(dataset1)\rfor (i in 1:nrow(z)){\reval(parse(text = paste(\u0026quot;S_i \u0026lt;- z[i,]%*%t(z[i,])\u0026quot;,sep = \u0026quot;\u0026quot;)))\rif (i==1){\rS \u0026lt;- S_i\r}else{\rS \u0026lt;- S + S_i\r}\r}\rS \u0026lt;- (1/nrow(z))*S\rgamma \u0026lt;- eigen(S)\rD \u0026lt;- diag(gamma$values[1:f])\rV \u0026lt;- gamma$vectors[,1:f]\rF_t \u0026lt;- matrix(0,nrow(z),f)\rfor (i in 1:nrow(z)){\reval(parse(text = paste(\u0026quot;F_t[\u0026quot;,i,\u0026quot;,]\u0026lt;- z[\u0026quot;,i,\u0026quot;,]%*%V\u0026quot;,sep = \u0026quot;\u0026quot;)))\r}\rlambda_hat \u0026lt;- V\rpsi \u0026lt;- diag(diag(S-V%*%D%*%t(V)))\rR \u0026lt;- diag(diag(cov(z-z%*%V%*%t(V))))\ra \u0026lt;- matrix(0,f,f)\rb \u0026lt;- matrix(0,f,f)\rfor(t in 2:nrow(z)){\ra \u0026lt;- a + F_t[t,]%*%t(F_t[t-1,])\rb \u0026lt;- b + F_t[t-1,]%*%t(F_t[t-1,])\r}\rb_inv \u0026lt;- solve(b)\rA_hat \u0026lt;- a%*%b_inv\re \u0026lt;- numeric(f)\rfor (t in 2:nrow(F_t)){\re \u0026lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat\r}\rH \u0026lt;- t(e)%*%e\rQ \u0026lt;- diag(1,f,f)\rQ[1:f,1:f] \u0026lt;- H\rp \u0026lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))\rresult1 \u0026lt;- kalmanfiter(z,f,A_hat,lambda_hat,c=0,R=NA,Q=Q,d=0,S=NA,h=R,a_int=F_t[1,],sig_int=matrix(p,f,f))\rプロットしてみます。\nlibrary(ggplot2)\rlibrary(tidyverse)\rggplot(gather(data.frame(factor1=result1$`state filtered`[1,1,-dim(result1$`state filtered`)[3]],factor2=result1$`state filtered`[2,1,-dim(result1$`state filtered`)[3]],factor3=result1$`state filtered`[3,1,-dim(result1$`state filtered`)[3]],time=as.Date(rownames(dataset1))),key = factor,value = value,-time),aes(x=time,y=value,colour=factor)) + geom_line()\rgiannoneの記事を書いた際は、元論文のMATLABコードを参考にRで書いたのですが、通常のカルマンフィルタとは観測変数の分散共分散行列の逆数の計算方法が違うらしくグラフの形が異なっています。まあでも、概形はほとんど同じですが（なので、ちゃんと動いているはず）。\n\r4. カルマンスムージング\rカルマンフィルタの実装は以上で終了なのですが、誤差項の正規性を仮定すれば\\(T\\)期までの情報集合\\(\\Omega_{T}\\)を用いて、\\(a_{i|i}, \\Sigma_{i|i}(i = 1:T)\\)を\\(a_{i|T}, \\Sigma_{i|T}(i = 1:T)\\)へ更新することができます。これをカルマンスムージングと呼びます。これを導出してみましょう。その準備として、以下のような\\(\\alpha_{t|t}\\)と\\(\\alpha_{t+1|t}\\)の混合分布を計算しておきます。\n\\[\r\\begin{eqnarray}\r(\\alpha_{t|t},\\alpha_{t+1|t}) \u0026amp;=\u0026amp; N(\r\\left(\r\\begin{array}{cccc}\rE(\\alpha_{t|t}) \\\\\rE(\\alpha_{t+1|t})\r\\end{array}\r\\right),\r\\left(\r\\begin{array}{cccc}\rVar(\\alpha_{t|t}), Cov(\\alpha_{t|t},\\alpha_{t+1|t}) \\\\\rCov(\\alpha_{t+1|t},\\alpha_{t|t}), Var(\\alpha_{t+1|t})\r\\end{array}\r\\right)\r) \\\\\r\u0026amp;=\u0026amp; N(\r\\left(\r\\begin{array}{cccc}\ra_{t|t} \\\\\ra_{t+1|t}\r\\end{array}\r\\right),\r\\left(\r\\begin{array}{cccc}\r\\Sigma_{t|t}, \\Sigma_{t|t}T_{t}\u0026#39; \\\\\rT_{t}\\Sigma_{t|t}, \\Sigma_{t+1|t} \\end{array}\r\\right)\r)\r\\end{eqnarray}\r\\]\rここで、\\(Cov(\\alpha_{t|t},\\alpha_{t+1|t})\\)は\n\\[\r\\begin{eqnarray}\rCov(\\alpha_{t+1|t},\\alpha_{t|t}) \u0026amp;=\u0026amp; Cov(T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}, \\alpha_{t|t}) \\\\\r\u0026amp;=\u0026amp; T_{t}Cov(\\alpha_{t|t},\\alpha_{t|t}) + Cov(c_{t},\\alpha_{t|t}) + Cov(R_{t}\\eta_{t},\\alpha_{t|t}) \\\\\r\u0026amp;=\u0026amp; T_{t}Var(\\alpha_{t|t}) = T_{t}\\Sigma_{t|t}\r\\end{eqnarray}\r\\]\nここで、条件付き多変量正規分布は以下のような分布をしていることを思い出しましょう（\r参考\r）。\n\\[\r(X_{1},X_{2}) = N(\r\\left(\r\\begin{array}{cccc}\r\\mu_{1} \\\\\r\\mu_{2}\r\\end{array}\r\\right),\r\\left(\r\\begin{array}{cccc}\r\\Sigma_{11}, \\Sigma_{12} \\\\\r\\Sigma_{21}, \\Sigma_{22}\r\\end{array}\r\\right)\r) \\\\\r\\]\r\\[\r(X_{1}|X_{2}=x_{2}) = N(\\mu_{1} + \\Sigma_{12}\\Sigma_{22}^{-1}(x_{2}-\\mu_{2}),\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21})\r\\]\nこれを用いて、\\((\\alpha_{t|t}|\\alpha_{t+1|t}=a_{t+1})\\)を計算してみましょう。\n\\[\r\\begin{eqnarray}\r(\\alpha_{t|t}|\\alpha_{t+1|t}=a_{t+1}) \u0026amp;=\u0026amp; N(a_{t|t} + \\Sigma_{t|t}T_{t}\u0026#39;\\Sigma_{t+1|t}^{-1}(a_{t+1}-a_{t+1|t}), \\Sigma_{t|t}-\\Sigma_{t|t}T_{t}\u0026#39;\\Sigma_{t+1|t}^{-1}T_{t}\\Sigma_{t|t}) \\\\\r\u0026amp;=\u0026amp;N(a_{t|t} + L_{t}(a_{t+1}-a_{t+1|t}), \\Sigma_{t|t}-L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39;)\r\\end{eqnarray}\r\\]\nただし、\\(a_{t+1}\\)の値は観測不可能なので、上式を用いて状態変数を更新することはできません。今、わかるのは\\(T\\)期における\\(a_{t+1|T}\\)の分布のみです。ということで、\\(a_{t+1}\\)を\\(a_{t+1|T}\\)で代用し、\\(\\alpha_{t|T}\\)の分布を求めてみます。では、計算していきます。\\(\\alpha_{t|T} = N(E(\\alpha_{t|T}),Var(\\alpha_{t|T}))\\)ですが、\n\\[\r\\begin{eqnarray}\rE(\\alpha_{t|T}) \u0026amp;=\u0026amp; E_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\rVar(\\alpha_{t|T}) \u0026amp;=\u0026amp; E_{\\alpha_{t+1|T}}(Var(\\alpha_{t|t}|\\alpha_{t+1|t} = \\alpha_{t+1|T})) + Var_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T}))\r\\end{eqnarray}\r\\]\nというように、\\(\\alpha_{t+1|T}\\)も確率変数となるので、繰り返し期待値の法則と繰り返し分散の法則を使用します（こちらを参照）。\n*繰り返し期待値の法則\r\\(E(x) = E_{Z}(E(X|Y=Z))\\)\n*繰り返し分散の法則\r\\(Var(X) = E_{Z}(Var(X|Y=Z))+Var_{Z}(E(X|Y=Z))\\)\nよって、\n\\[\r\\begin{eqnarray}\rE(\\alpha_{t|T}) \u0026amp;=\u0026amp; E_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\r\u0026amp;=\u0026amp; a_{t|t} + L_{t}(a_{t+1|T}-a_{t+1|t}) \\\\\rVar(\\alpha_{t|T}) \u0026amp;=\u0026amp; E_{\\alpha_{t+1|T}}(Var(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) + Var_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + E( (a_{t|t} + L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))(a_{t|t} + L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))\u0026#39;) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + E( (L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))(L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))\u0026#39;) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + E( (L_{t}\\alpha_{t+1|T} - L_{t}a_{t+1|T})(L_{t}\\alpha_{t+1|T} - L_{t}(a_{t+1|T})\u0026#39;) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + E( L_{t}(\\alpha_{t+1|T} - a_{t+1|T})(\\alpha_{t+1|T} - a_{t+1|T})\u0026#39;L_{t}\u0026#39;) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + L_{t}E( (\\alpha_{t+1|T} - a_{t+1|T})(\\alpha_{t+1|T} - a_{t+1|T})\u0026#39;)L_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + L_{t}\\Sigma_{t+1|T}L_{t}\u0026#39;\r\\end{eqnarray}\r\\]\nとなります。カルマンスムージングのアルゴリズムをまとめておきます。\n\\[\r\\begin{eqnarray}\rL_{t} \u0026amp;=\u0026amp; \\Sigma_{t|t}T_{t}\\Sigma_{t+1|t}^{-1} \\\\\ra_{t|T} \u0026amp;=\u0026amp; a_{t|t} + L_{t}(a_{t+1|T} - a_{t+1|t}) \\\\\r\\Sigma_{t|T} \u0026amp;=\u0026amp; \\Sigma_{t+1|t} + L_{t}(\\Sigma_{t+1|T}-\\Sigma_{t+1|t})L_{t}\u0026#39;\r\\end{eqnarray}\r\\]\rカルマンスムージングの特徴的な点は後ろ向きに計算をしていく点です。つまり、\\(T\\)期から1期に向けて計算を行っていきます。\\(L_{t}\\)に関してはそもそもカルマンフィルタを回した時点で計算可能ですが、\\(\\alpha_{t|T}\\)は\\(T\\)期までのデータが手元にないと計算できません。今、\\(T\\)期まで観測可能なデータが入手できたとしましょう。すると、２番目の方程式を用いて、\\(a_{T-1|T}\\)を計算します。ちなみに\\(a_{T|T}\\)はカルマンフィルタを回した時点ですでに手に入っているので、計算する必要はありません。同時に、３番目の式を用いて\\(\\Sigma_{T-1|T}\\)を計算します。そして、\\(a_{T-1|T},\\Sigma_{T-1|T}\\)と\\(L_{T-1}\\)を用いて\\(a_{T-2|T},\\Sigma_{T-2|T}\\)を計算、というように1期に向けて後ろ向きに計算をしていくのです。さきほど、遷移方程式の誤差項\\(\\eta_{t}\\)と定数項\\(c_{t}\\)がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致すると書きましたが、カルマンスムージングの場合は\\(T\\)期までのサンプルでOLSを行った結果と一致します。\rRで実装してみます。\nkalmansmoothing \u0026lt;- function(filter){\r#-------------------------------------------------------------------\r# Implemention of Kalman smoothing\r# t - parameter of endogenous variable in state equation\r# z - parameter of endogenous variable in observable equation\r# a_pre - prediction of state\r# a_fil - filtered value of state\r# sig_pre - prediction of var of state\r# sig_fil - filtered value of state\r#-------------------------------------------------------------------\rlibrary(MASS)\r# 1.Define variable\ra_pre \u0026lt;- filter$`state prediction`\ra_fil \u0026lt;- filter$`state filtered`\rsig_pre \u0026lt;- filter$`state var prediction`\rsig_fil \u0026lt;- filter$`state var filtered`\rt \u0026lt;- filter$`parameter of state eq`\rC \u0026lt;- array(0,dim = dim(sig_pre))\ra_sm \u0026lt;- array(0,dim = dim(a_pre))\rsig_sm \u0026lt;- array(0,dim = dim(sig_pre))\rN \u0026lt;- dim(C)[3]\ra_sm[,,N] \u0026lt;- a_fil[,,N]\rsig_sm[,,N] \u0026lt;- sig_fil[,,N]\rfor (i in N:2){\rC[,,i-1] \u0026lt;- sig_fil[,,i-1]%*%t(t)%*%ginv(sig_pre[,,i])\ra_sm[,,i-1] \u0026lt;- a_fil[,,i-1] + C[,,i-1]%*%(a_sm[,,i]-a_pre[,,i])\rsig_sm[,,i-1] \u0026lt;- sig_fil[,,i-1] + C[,,i-1]%*%(sig_sm[,,i]-sig_pre[,,i])%*%t(C[,,i-1])\r}\rresult \u0026lt;- list(a_sm,sig_sm,C)\rnames(result) \u0026lt;- c(\u0026quot;state smoothed\u0026quot;, \u0026quot;state var smoothed\u0026quot;, \u0026quot;c\u0026quot;)\rreturn(result)\r}\r先ほどのコードの続きでRコードを書いてみます。\nresult2 \u0026lt;- kalmansmoothing(result1)\rかなりシンプルですね。ちなみにグラフにしましたが、１個目とほぼ変わりませんでした。とりあえず、今日はここまで。\n\r","date":1549756800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1549756800,"objectID":"74e0e5783c70722e767e0eb0cf2bbb08","permalink":"/post/post3/","publishdate":"2019-02-10T00:00:00Z","relpermalink":"/post/post3/","section":"post","summary":"時系列解析には欠かせないカルマンフィルタ。Rでもパッケージが用意されていて非常に便利ですが、ここではいったん理解を優先し、カルマンフィルタの実装を行ってみたいと思います。","tags":["カルマンフィルタ","R"],"title":"カルマンフィルタの実装","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r\r1. GPRとは\r2. GPRの実装\r\r\r　おはこんばんにちは。昨日、Bayesian Vector Autoregressionの記事を書きました。\nその中でハイパーパラメータのチューニングの話が出てきて、なにか効率的にチューニングを行う方法はないかと探していた際にBayesian Optimizationを発見しました。日次GDPでも機械学習の手法を利用しようと思っているので、Bayesian Optimizationはかなり使える手法ではないかと思い、昨日徹夜で理解しました。\nその内容をここで実装しようとは思うのですが、Bayesian Optimizationではガウス回帰（Gaussian Pocess Regression,以下GPR）を使用しており、まずその実装を行おうと持ったのがこのエントリを書いた動機です。Bayesian Optimizationの実装はこのエントリの後にでも書こうかなと思っています。\n1. GPRとは\r　GRPとは簡単に言ってしまえば「ベイズ推定を用いた非線形回帰手法の１種」です。モデル自体は線形ですが、カーネルトリックを用いて入力変数を無限個非線形変換したものを説明変数として推定できるところが特徴です（カーネルになにを選択するかによります）。\nGPRが想定しているのは、学習データとして入力データと教師データがそれぞれN個得られており、また入力データに関しては\\(N+1\\)個目のデータも得られている状況です。この状況から、\\(N+1\\)個目の教師データを予測します。\n教師データにはノイズが含まれており、以下のような確率モデルに従います。\n\\[\rt_{i} = y_{i} + \\epsilon_{i}\r\\]\nここで、\\(t_{i}\\)は\\(i\\)番目の観測可能な教師データ（スカラー）、\\(y_{i}\\)は観測できない出力データ（スカラー）、\\(\\epsilon_{i}\\)は測定誤差で正規分布\\(N(0,\\beta^{-1})\\)に従います。\\(y_{i}\\)は以下のような確率モデルに従います。\n\\[\r\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\r\\]\nここで、\\(x_{i}\\)はi番目の入力データベクトル、\\(\\phi(・)\\)は非線形関数、 \\(\\textbf{w}^{T}\\)は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、\\(\\phi(x_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\\)を想定しています（\\(x_{1,i}\\)は\\(i\\)番目の入力データ\\(x_{i}\\)の１番目の変数）。教師データの確率モデルから、\\(i\\)番目の出力データ\\(y_{i}\\)が得られたうえで\\(t_{i}\\)が得られる条件付確率は、\n\\[\rp(t_{i}|y_{i}) = N(t_{i}|y_{i},\\beta^{-1})\r\\]\nとなります。\\(\\displaystyle \\textbf{t} = (t_{1},...,t_{n})^{T}\\)、\\(\\displaystyle \\textbf{y} = (y_{1},...,y_{n})^{T}\\)とすると、上式を拡張することで\n\\[\r\\displaystyle p(\\textbf{t}|\\textbf{y}) = N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N})\r\\]\nと書けます。また、事前分布として\\(\\textbf{w}\\)の期待値は0、分散は全て\\(\\alpha\\)と仮定します。\\(\\displaystyle \\textbf{y}\\)はガウス過程に従うと仮定します。ガウス過程とは、\\(\\displaystyle \\textbf{y}\\)の同時分布が多変量ガウス分布に従うもののことです。コードで書くと以下のようになります。\n# Define Kernel function\rKernel_Mat \u0026lt;- function(X,sigma,beta){\rN \u0026lt;- NROW(X)\rK \u0026lt;- matrix(0,N,N)\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta\r}\r}\rreturn(K)\r}\rN \u0026lt;- 10 # max value of X\rM \u0026lt;- 1000 # sample size\rX \u0026lt;- matrix(seq(1,N,length=M),M,1) # create X\rtestK \u0026lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix\rlibrary(MASS)\rP \u0026lt;- 6 # num of sample path\rY \u0026lt;- matrix(0,M,P) # define Y\rfor(i in 1:P){\rY[,i] \u0026lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y\r}\r# Plot\rmatplot(x=X,y=Y,type = \u0026quot;l\u0026quot;,lwd = 2)\r　Kernel_Matについては後述しますが、\\(\\displaystyle \\textbf{y}\\)の各要素\\(\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\\)の間の共分散行列\\(K\\)を入力\\(x\\)からカーネル法を用いて計算しています。そして、この\\(K\\)と平均0から、多変量正規乱数を6系列生成し、それをプロットしています。\n　これらの系列は共分散行列から計算されるので、各要素の共分散が正に大きくなればなるほど同じ値をとりやすくなるようモデリングされていることになります。また、グラフを見ればわかるように非常になめらかなグラフが生成されており、かつ非常に柔軟な関数を表現できていることがわかります。コードでは計算コストの関係上、入力を0から10に限定して1000個の入力点をサンプルし、作図を行っていますが、原理的には\\(x\\)は実数空間で定義されるものであるので、\\(p(\\textbf{y})\\)は無限次元の多変量正規分布に従います。\r以上のように、\\(\\displaystyle \\textbf{y}\\)はガウス過程に従うと仮定するので同時確率\\(p(\\textbf{y})\\)は平均0、分散共分散行列が\\(K\\)の多変量正規分布\\(N(\\textbf{y}|0,K)\\)に従います。ここで、\\(K\\)の各要素\\(K_{i,j}\\)は、\n\\[\r\\begin{eqnarray}\rK_{i,j} \u0026amp;=\u0026amp; cov[y_{i},y_{j}] = cov[\\textbf{w}\\phi(x_{i}),\\textbf{w}\\phi(x_{j})] \\\\\r\u0026amp;=\u0026amp;\\phi(x_{i})\\phi(x_{j})cov[\\textbf{w},\\textbf{w}]=\\phi(x_{i})\\phi(x_{j})\\alpha\r\\end{eqnarray}\r\\]\nです。ここで、\\(\\phi(x_{i})\\phi(x_{j})\\alpha\\)は\\(\\phi(x_{i})\\)の次元が大きくなればなるほど計算量が多くなります（つまり、非線形変換をかければかけるほど計算が終わらない）。しかし、カーネル関数\\(k(x,x\u0026#39;)\\)を用いると、計算量は高々入力データ\\(x_{i},x_{j}\\)のサンプルサイズの次元になるので、計算がしやすくなります。カーネル関数を用いて\\(K_{i,j} = k(x_{i},x_{j})\\)となります。カーネル関数としてはいくつか種類がありますが、以下のガウスカーネルがよく使用されます。\n\\[\rk(x,x\u0026#39;) = a \\exp(-b(x-x\u0026#39;)^{2})\r\\]\n\\(\\displaystyle \\textbf{y}\\)の同時確率が定義できたので、\\(\\displaystyle \\textbf{t}\\)の同時確率を求めることができます。\n\\[\r\\begin{eqnarray}\r\\displaystyle p(\\textbf{t}) \u0026amp;=\u0026amp; \\int p(\\textbf{t}|\\textbf{y})p(\\textbf{y}) d\\textbf{y} \\\\\r\\displaystyle \u0026amp;=\u0026amp; \\int N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N})N(\\textbf{y}|0,K)d\\textbf{y} \\\\\r\u0026amp;=\u0026amp; N(\\textbf{y}|0,\\textbf{C}_{N})\r\\end{eqnarray}\r\\]\nここで、\\(\\textbf{C}_{N} = K + \\beta^{-1}\\textbf{I}_{N}\\)です。なお、最後の式展開は正規分布の再生性を利用しています（証明は正規分布の積率母関数から容易に導けます）。要は、両者は独立なので共分散は2つの分布の共分散の和となると言っているだけです。個人的には、\\(p(\\textbf{y})\\)が先ほど説明したガウス過程の事前分布であり、\\(p(\\textbf{t}|\\textbf{y})\\)が尤度関数で、\\(p(\\textbf{t})\\)は事後分布をというようなイメージです。事前分布\\(p(\\textbf{y})\\)は制約の緩い分布でなめらかであることのみが唯一の制約です。\r\\(N\\)個の観測可能な教師データ\\(\\textbf{t}\\)と\\(t_{N+1}\\)の同時確率は、\n\\[\rp(\\textbf{t},t_{N+1}) = N(\\textbf{t},t_{N+1}|0,\\textbf{C}_{N+1})\r\\]\nここで、\\(\\textbf{C}_{N+1}\\)は、\n\\[\r\\textbf{C}_{N+1} = \\left(\r\\begin{array}{cccc}\r\\textbf{C}_{N} \u0026amp; \\textbf{k} \\\\\r\\textbf{k}^{T} \u0026amp; c \\\\\r\\end{array}\r\\right)\r\\]\nです。ここで、\\(\\textbf{k} = (k(x_{1},x_{N+1}),...,k(x_{N},x_{N+1}))\\)、\\(c = k(x_{N+1},x_{N+1})\\)です。\\(\\textbf{t}\\)と\\(t_{N+1}\\)の同時分布から条件付分布\\(p(t_{N+1}|\\textbf{t})\\)を求めることができます。\n\\[\rp(t_{N+1}|\\textbf{t}) = N(t_{N+1}|\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{t},c-\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{k})\r\\]\n条件付分布の計算においては、条件付多変量正規分布の性質を利用しています。上式を見ればわかるように、条件付分布\\(p(t_{N+1}|\\textbf{t})\\)は\\(N+1\\)個の入力データ、\\(N\\)個の教師データ、カーネル関数のパラメータ\\(a,b\\)が既知であれば計算可能となっていますので、任意の点を入力データとして与えてやれば、元のData Generating Processを近似することが可能になります。GPRの良いところは上で定義した確率モデル\\(\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\\)を直接推定しなくても予測値が得られるところです。確率モデルには\\(\\phi(x_{i})\\)があり、非線形変換により入力データを高次元ベクトルへ変換しています。よって、次元が高くなればなるほど\\(\\phi(x_{i})\\phi(x_{j})\\alpha\\)の計算量は大きくなっていきますが、GPRではカーネルトリックを用いているので高々入力データベクトルのサンプルサイズの次元の計算量で事足りることになります。\n\r2. GPRの実装\r　とりあえずここまでをRで実装してみましょう。PRMLのテストデータで実装しているものがあったので、それをベースにいじってみました。\nlibrary(ggplot2)\rlibrary(grid)\r# 1.Gaussian Process Regression\r# PRML\u0026#39;s synthetic data set\rcurve_fitting \u0026lt;- data.frame(\rx=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000),\rt=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502))\rf \u0026lt;- function(beta, sigma, xmin, xmax, input, train) {\rkernel \u0026lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function\rK \u0026lt;- outer(input, input, kernel); # calc gram matrix\rC_N \u0026lt;- K + diag(length(input))/beta\rm \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance\rx \u0026lt;- seq(xmin,xmax,length=100)\routput \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),\rtx=input,ty=train),\raes(x=x1,y=m)) + geom_line() +\rgeom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +\rgeom_point(aes(x=tx,y=ty))\rreturn(output)\r}\rgrid.newpage() # make a palet\rpushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2\rprint(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))\rprint(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2))\rprint(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))\rprint(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) \r\\(\\beta^{-1}\\)は測定誤差を表しています。\\(\\beta\\)が大きい（つまり、測定誤差が小さい）とすでに得られているデータとの誤差が少なくなるように予測値をはじき出すので、over fitting しやすくなります。上図の左上がそうなっています。左上は\\(\\beta=400\\)で、現時点で得られているデータに過度にfitしていることがわかります。逆に\\(\\beta\\)が小さいと教師データとの誤差を無視するように予測値をはじき出しますが、汎化性能は向上するかもしれません。右上の図がそれです。\\(\\beta=4\\)で、得られているデータ点を平均はほとんど通っていません。\\(b\\)は現時点で得られているデータが周りに及ぼす影響の広さを表しています。\\(b\\)が小さいと、隣接する点が互いに強く影響を及ぼし合うため、精度は下がるが汎化性能は上がるかもしれません。逆に、\\(b\\)が大きいと、個々の点にのみフィットする不自然な結果になります。これは右下の図になります（\\(b=\\frac{1}{0.03},\\beta=25\\)）。御覧の通り、\\(\\beta\\)が大きいのでoverfitting気味であり、なおかつ\\(b\\)も大きいので個々の点のみにfitし、無茶苦茶なグラフになっています。左下のグラフが最もよさそうです。\\(b=\\frac{1}{0.3},\\beta=2\\)となっています。試しに、このグラフのx区間を[0,2]へ伸ばしてみましょう。すると、以下のようなグラフがかけます。\ngrid.newpage() # make a palet\rpushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2\rprint(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))\rprint(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) \rこれを見ればわかるように、左下以外のグラフはすぐに95%信頼区間のバンドが広がり、データ点がないところではまったく使い物にならないことがわかります。一方、左下のグラフは1.3~1.4ぐらいまではそこそこのバンドがかけており、我々が直感的に理解する関数とも整合的な点を平均値が通っているように思えます。また、観測可能なデータ点から離れすぎるとパラメータに何を与えようと平均０、分散１の正規分布になることもわかるがわかります。\rさて、このようにパラメータの値に応じて、アウトサンプルの予測精度が異なることを示したわけですが、ここで問題となるのはこれらハイパーパラメータをどのようにして推計するかです。これは対数尤度関数\\(\\ln p(\\textbf{t}|a,b)\\)を最大にするハイパーパラメータを勾配法により求めます((\\(\\beta\\)は少しタイプが異なるようで、発展的な議論では他のチューニング方法をとる模様。まだ、そのレベルにはいけていないのでここではカリブレートすることにします。))。\\(p(\\textbf{t}) = N(\\textbf{y}|0,\\textbf{C}_{N})\\)なので、対数尤度関数は\n\\[\r\\displaystyle \\ln p(\\textbf{t}|a,b,\\beta) = -\\frac{1}{2}\\ln|\\textbf{C}_{N}| - \\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1}\\textbf{k}\r\\]\nとなります。あとは、これをパラメータで微分し、得られた連立方程式を解くことで最尤推定量が得られます。ではまず導関数を導出してみます。\n\\[\r\\displaystyle \\frac{\\partial}{\\partial \\theta_{i}} \\ln p(\\textbf{t}|\\theta) = -\\frac{1}{2}Tr(\\textbf{C}_{N}^{-1}\\frac{\\partial \\textbf{C}_{N}}{\\partial \\theta_{i}}) + \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1}\r\\frac{\\partial\\textbf{C}_{N}}{\\partial\\theta_{i}}\\textbf{C}_{N}^{-1}\\textbf{t}\r\\]\nここで、\\(\\theta\\)はパラメータセットで、\\(\\theta_{i}\\)は\\(i\\)番目のパラメータを表しています。この導関数が理解できない方はこちらの補論にある(C.21)式と(C.22)式をご覧になると良いと思います。今回はガウスカーネルを用いているため、\n\\[\r\\displaystyle \\frac{\\partial k(x,x\u0026#39;)}{\\partial a} = \\exp(-b(x-x\u0026#39;)^{2}) \\\\\r\\displaystyle \\frac{\\partial k(x,x\u0026#39;)}{\\partial b} = -a(x-x\u0026#39;)^{2}\\exp(-b(x-x\u0026#39;)^{2})\r\\]\nを上式に代入すれば良いだけです。ただ、今回は勾配法により最適なパラメータを求めます。以下、実装のコードです（かなり迷走しています）。\ng \u0026lt;- function(xmin, xmax, input, train){\r# initial value\rbeta = 100\rb = 1\ra = 1\rlearning_rate = 0.1\ritermax \u0026lt;- 1000\rif (class(input) == \u0026quot;numeric\u0026quot;){\rN \u0026lt;- length(input)\r} else\r{\rN \u0026lt;- NROW(input)\r}\rkernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel\rderivative_a \u0026lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2)\rderivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)\rdloglik_a \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y }\rdloglik_b \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y }\r# loglikelihood function\rlikelihood \u0026lt;- function(b,a,x,y){\rkernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2)\rK \u0026lt;- outer(x, x, kernel)\rC_N \u0026lt;- K + diag(N)/beta\ritermax \u0026lt;- 1000\rl \u0026lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y\rreturn(l)\r}\rK \u0026lt;- outer(input, input, kernel) C_N \u0026lt;- K + diag(N)/beta\rfor (i in 1:itermax){\rkernel \u0026lt;- function(x1, x2) a*exp(-b*(x1-x2)^2)\rderivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)\rdloglik_b \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y }\rK \u0026lt;- outer(input, input, kernel) # calc gram matrix\rC_N \u0026lt;- K + diag(N)/beta\rl \u0026lt;- 0\rif(abs(l-likelihood(b,a,input,train))\u0026lt;0.0001\u0026amp;i\u0026gt;2){\rbreak\r}else{\ra \u0026lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input))\rb \u0026lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input))\r}\rl \u0026lt;- likelihood(b,a,input,train)\r}\rK \u0026lt;- outer(input, input, kernel)\rC_N \u0026lt;- K + diag(length(input))/beta\rm \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel))))\rx \u0026lt;- seq(xmin,xmax,length=100)\routput \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),\rtx=input,ty=train),\raes(x=x1,y=m)) + geom_line() +\rgeom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +\rgeom_point(aes(x=tx,y=ty))\rreturn(output)\r}\rprint(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))\rたしかに、良さそうな感じがします（笑）\rとりあえず、今日はここまで。\n\r","date":1543708800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1543708800,"objectID":"eb19f4601e090b622a80cdb28ae21cf8","permalink":"/post/post1/","publishdate":"2018-12-02T00:00:00Z","relpermalink":"/post/post1/","section":"post","summary":"万能すぎて逆に面白くないと話題のガウス回帰を実装してみました。","tags":["R"],"title":"ガウス回帰の実装をやってみた","type":"post"},{"authors":null,"categories":["統計学"],"content":"\r\r\r\r1. Unrestricted VARについて\r2. BVARについて\r\r具体的な推定方法（カルマンフィルタ）\rカルマンフィルタの初期値をどのようにきめるか\rハイパーパラメータの決定方法とその評価尺度\r\r3. Rでの実装\r\r\r　おはこんばんにちは。日次GDP推計を休日に進めているのですが、今日は少し勉強編でBVARについての記事を書きたいと思います。このBVARはFRBアトランタ連銀のGDPNowでも使用されていることから、日次GDP推計との親和性も高いと思われます。そもそも、時系列でアウトサンプルの予測精度を上げたいということになると真っ先に思いつくのがBVARです。Doan, Litterman and Sims(1984)で提案されたこのモデルは予測精度が良いので、非常に有効な手段になると思われます。BVARはBayesian Vector Autoregressionの略で、ベクトル自己回帰モデル（VAR）の派生版です。VARとネットで調べるとまずValue at Risk（VaR）が出てくると思いますが、それとは違います。よく見るとaが小文字になっていることに気づくかと思います。\rさて、BVARの説明をこれから行おうとするのですが、その前にまず基本的なVARの説明からしたいと思います。ただし、歴史的な背景（大型マクロ計量モデルからの経緯など）には触れません。あくまで、BVARを説明するうえで必要な知識について触れたいと思います。\n1. Unrestricted VARについて\r　まず、注意点を一点。この投稿では、もっとも基本的なVARのことをUnrestricted VAR（UVAR）と呼ぶことにします。UVARはSims(1980)の論文が有名です。1\nこのモデルには、理論的な基礎づけは原則ありません。あくまで実証的なモデルです。UVARは一般系は以下のような形をしています。\n\\[\rY_{t} = A_{0} + A_{1}Y_{t-1} + ... + A_{K}Y_{t-K} + U_{t}, ~~ U_{t} ～ N(0,\\Omega)\r\\]\r\\[\rY_{t} = \\left(\r\\begin{array}{cccc}\ry_{1,t} \\\\\ry_{2,t} \\\\\r\\vdots \\\\\ry_{J,t} \\\\\r\\end{array}\r\\right),\rA_{0} = \\left(\r\\begin{array}{cccc}\ra_{10} \\\\\ra_{20} \\\\\r\\vdots \\\\\ra_{J0} \\\\\r\\end{array}\r\\right),\rA_{k} = \\left(\r\\begin{array}{cccc}\ra_{11,k} \u0026amp; a_{12,k} \u0026amp; \\ldots \u0026amp; a_{1J,k} \\\\\ra_{21,k} \u0026amp; a_{22,k} \u0026amp; \\ldots \u0026amp; a_{2J,k} \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\ra_{J1,k} \u0026amp; a_{J2,k} \u0026amp; \\ldots \u0026amp; a_{JJ,k}\r\\end{array}\r\\right),\rU_{t} = \\left(\r\\begin{array}{cccc}\ru_{1,t} \\\\\ru_{2,t} \\\\\r\\vdots \\\\\ru_{J,t} \\\\\r\\end{array}\r\\right)\r\\]\rここで、\\(t\\)は時点、\\(J\\)は変数の数、\\(K\\)はラグ数を表しています。上式を見ると、UVARは自己回帰＋他変数のラグでt期の変数\\(y_{j,t}\\)を説明しようとするモデルであると言えます。しばしば、経済の実証分析で使用され、インサンプルの当てはまりが良いことも知られています（GDP、消費、投資、金利、マネーサプライの５変数VARで金融政策の波及経路を分析したり･･･）。推定するパラメータの個数は、回帰式1本だけでJK+1個（定数項込み）の係数を含むので、J本になればJ(JK+1)個になります。また、\\(\\Omega\\)がJ(J+1)/2個のパラメータを持っているので、合計J(JK+1)+J(J+1)/2個のパラメータを推定することになり、かなりパラメータ数が多い印象です（これは後々重要になってきます）。具体的な推計方法ですが、UVARは同時方程式体系ではないのでそこまで面倒ではありません。UVAR自体はSeemingly Unrestricted Regression Equation（SUR）の一種でそれぞれの方程式は誤差項の相関を通じて関係してはいますが（\\(\\Omega\\)の部分）、全ての回帰式が同じ説明変数を持つため、各方程式を最小二乗法（OLS）によって推定するだけで良いことが知られています。\rこの事実を説明してみましょう（BVARが気になる方は読み飛ばしてもらって構いません）。説明のために今、UVARをSURの一般系に書き直します。上式はt期のVAR(K)システムですが、UVARを推定する際はこれらJ本の方程式がサンプル数Tセット分存在するので、実際のシステム体系は以下のようになります。\n\\[\r\\left(\r\\begin{array}{cccc}\rY_{1} \\\\\rY_{2} \\\\\r\\vdots \\\\\rY_{J} \\\\\r\\end{array}\r\\right) = \\left(\r\\begin{array}{cccc}\rX_{1} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\\r0 \u0026amp; X_{2} \u0026amp; \\ldots \u0026amp; 0 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; X_{J}\r\\end{array}\r\\right) \\left(\r\\begin{array}{cccc}\rA_{1} \\\\\rA_{2} \\\\\r\\vdots \\\\\rA_{J} \\\\\r\\end{array}\r\\right) +\r\\left(\r\\begin{array}{cccc}\rU_{1} \\\\\rU_{2} \\\\\r\\vdots \\\\\rU_{J} \\\\\r\\end{array}\r\\right) = \\overline{X}A + U (1式)\r\\]\rここで、\n\\[\rY_{j} = \\left(\r\\begin{array}{cccc}\ry_{t,j} \\\\\ry_{t+1,j} \\\\\r\\vdots \\\\\ry_{T,j} \\\\\r\\end{array}\r\\right) ,\rX_{j} = X = \\left(\r\\begin{array}{cccc}\r1 \u0026amp; y_{t-1,1} \u0026amp; \\ldots \u0026amp; y_{t-K,1} \u0026amp; \\ldots \u0026amp; y_{t-K,J} \\\\\r1 \u0026amp; y_{t,1} \u0026amp; \\ldots \u0026amp; y_{t-K+1,1} \u0026amp; \\ldots \u0026amp; y_{t-K+1,J} \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r1 \u0026amp; y_{T-1,1} \u0026amp; \\ldots \u0026amp; y_{T-K,1} \u0026amp; \\ldots \u0026amp; y_{T-K,J} \\\\\r\\end{array}\r\\right),\rA_{j} = \\left(\r\\begin{array}{cccc}\ra_{00,j} \\\\\ra_{11,j} \\\\\r\\vdots \\\\\ra_{JK,j} \\\\\r\\end{array}\r\\right)\r\\]\rです。上式とは違い、変数順で並べられていることに注意してください（つまり、各方程式を並べる優先順位は１番目にj、２番目にtとなっている）。また、\\(X_{j}\\)が全ての変数\\(j\\)について等しいことに注目してください（jに依存していません、VARなので当たり前ですが）。これが後々非常に重要になってきます。\\(U\\)の分散共分散行列は\n\\[\rE[UU\u0026#39;|X_{1}, X_{2},..,X_{J}] = \\Omega = \\left(\r\\begin{array}{cccc}\r\\sigma_{11}I \u0026amp; \\sigma_{12}I \u0026amp; \\ldots \u0026amp; \\sigma_{1J}I \\\\\r\\sigma_{21}I \u0026amp; \\sigma_{22}I \u0026amp; \\ldots \u0026amp; \\sigma_{2J}I \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r\\sigma_{J1}I \u0026amp; \\sigma_{J2}I \u0026amp; \\ldots \u0026amp; \\sigma_{JJ}I \\\\\r\\end{array}\r\\right)\r\\]\nとなっており、それぞれの変数は誤差項の相関を通じて関係しています（ただし、同じ変数内の異なる時点間の相関はないと仮定します）。このような場合、一般化最小二乗法（GLS）を用いて推計を行うことになりますが、これら方程式体系において説明変数が同じであるならば、GLS推定量とOLS推定量は同値になります。それを確かめてみましょう。上述した方程式体系(1)のGLS推定量は以下になります（参考）。\n\\[\rA_{GLS} = (\\overline{X}\u0026#39;\\Omega^{-1}\\overline{X})^{-1}\\overline{X}\u0026#39;\\Omega^{-1}Y = (\\overline{X}\u0026#39;(\\Sigma^{-1}\\otimes I)\\overline{X})^{-1}\\overline{X}\u0026#39;(\\Sigma^{-1}\\otimes I)Y\r\\]\nここで、\\(\\Sigma\\)は誤差項の分散共分散行列のうち、スカラーである分散、共分散を取り出した行列です。先ほど確認したように、\\(X_{1}=X_{2}=...=X_{J}=X\\)なので\\(\\overline{X}=I \\otimes X\\)であり、その転置も\\(\\overline{X}\u0026#39;=I \\otimes X\\)となります。よって、このの性質を用いると、\n\\[\r\\begin{eqnarray}\rA_{GLS} \u0026amp;=\u0026amp; [(I \\otimes X\u0026#39;)(\\Sigma^{-1}\\otimes I)(I \\otimes X)]^{-1}(I \\otimes X\u0026#39;)(\\Sigma^{-1}\\otimes I)Y \\\\\r\u0026amp;=\u0026amp; [(\\Sigma^{-1}\\otimes X\u0026#39;)(I \\otimes X)]^{-1}(\\Sigma^{-1}\\otimes X\u0026#39;)y \\\\\r\u0026amp;=\u0026amp; (\\Sigma^{-1}\\otimes(X\u0026#39;X))^{-1}(\\Sigma^{-1}\\otimes X\u0026#39;)y \\\\\r\u0026amp;=\u0026amp; (I \\otimes (X\u0026#39;X)^{-1}X\u0026#39;)y \\\\\r\u0026amp;=\u0026amp; (\\overline{X}\u0026#39;\\overline{X})^{-1}\\overline{X}\u0026#39;Y \\\\\r\u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r(X\u0026#39;X)^{-1}X\u0026#39; \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\\r0 \u0026amp; (X\u0026#39;X)^{-1}X\u0026#39; \u0026amp; \\ldots \u0026amp; 0 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; (X\u0026#39;X)^{-1}X\u0026#39;\r\\end{array}\r\\right) \\left(\r\\begin{array}{cccc}\rY_{1} \\\\\rY_{2} \\\\\r\\vdots \\\\\rY_{J} \\\\\r\\end{array}\r\\right)\\\\ \u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r(X\u0026#39;X)^{-1}X\u0026#39;Y_{1} \\\\\r(X\u0026#39;X)^{-1}X\u0026#39;Y_{2} \\\\\r\\vdots \\\\\r(X\u0026#39;X)^{-1}X\u0026#39;Y_{J} \\\\\r\\end{array}\r\\right) \\\\\r\u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\rA_{1,OLS} \\\\\rA_{2,OLS} \\\\\r\\vdots \\\\\rA_{J,OLS} \\\\\r\\end{array}\r\\right)\r\\end{eqnarray}\r\\]\nとなり、各経済変数の方程式を別個にOLS推計していけばよいことがわかります。\rOLS推定に際し、VARの次数Kの選択を選択する必要がありますが、次数KはAIC（BIC）を評価軸に探索的に決定します。つまり、いろいろな値をKに設定し、OLS推計を行い、計算されたVAR(K)のAIC(BIC)のうちで最も値が大きいモデルの次数を真のモデルの次数Kとして採用するということです。RにもVARselect()という関数があり、引数にラグの探索最大数とデータを渡すことで最適な次数を計算してくれます（便利）。\rこのようにUVARはOLS推計で各変数間の相互依存関係をデータから推計できる手軽な手法です。私が知っている分野ですと財政政策乗数の推計に使用されていました。GDP、消費、投資、政府支出の４変数でVARを推定し、推定したVARでインパルス応答を見ることで１単位の財政支出の増加がGDP等に与える影響を定量的にシミュレートすることができたりします（指導教官が論文を書いてました）。そもそも私の専門のDSGEも誘導系に書き直せばVAR形式になり、インパルス応答などは基本的に一緒です。また、経済変数は慣性が強いので（特に我が国の場合）、インサンプルのモデルの当てはまりもいいです。ただし、あくまでインサンプルです。アウトサンプルの当てはまりはそれほど良い印象はありません。なぜなら、推定パラメータが多すぎるからです。UVARの予測に関する問題点はover-parametrizationです。例えば、先ほどの４変数UVARでラグが６期だったとすると、推定すべきパラメータは３１個になります。よって、データ数にもよりますが、パラメータ数がデータ数に近づくとインサンプルの補間に近づき、過学習を引き起こす危険性があります。日次GDP推計は大量の変数を使用するのでこの問題は非常に致命的になります。BVARはこの問題を解決することに主眼を置いています。\n\r2. BVARについて\r　UVARの問題点はover-parametrizationであると述べました。BVARはこの問題を防ぐために不必要な説明変数（のパラメータ）をそぎ落とそうとします。ただ、不要なパラメータを推定する前に０と仮置き（カリブレート）するのではなく、１階の自己に関わるパラメータは１周り、その他変数のパラメータは０周りに正規分布するという形の制約を与えます。このモデルの最大の仮定は「各経済変数は多かれ少なかれドリフト付き１階のランダムウォークに従う」というものであり、上述したような事前分布を先験的に与えた上で推定を行うのです。砕けた言い方をすると、BVARの考え方は「経済変数の挙動は基本はランダムウォークだけど他変数のラグに予測力向上に資するものがあればそれも取り入れるよ」というものなんです。さて、前置きが長くなりましたが、具体的な説明に移りたいと思います。\n具体的な推定方法（カルマンフィルタ）\r　上述した事前分布に加えて、BVARがUVARと異なる点はパラメータがtime-varyingであるということです。なんとなく、パラメータがずっと固定よりもサンプルが増えるたびにその値が更新されるほうが予測精度が上がりそうですよね（笑）。推定手法としてはUVARの時のようにOLSをそれぞれにかけることはせず、カルマンフィルタ と呼ばれるアルゴリズムを用いて推定を行います。BVARは以下のような状態空間モデルとして定義されます（各ベクトル、行列の次元はUVAR時と同じです）。\n\\[\rY_{t} = X_{t}B_{t} + u_{t} \\\\\rB_{t} = \\Phi B_{t-1} + \\epsilon_{t} \\\\\rB_{t} = \\left(\r\\begin{array}{cccc}\r\\beta_{00,t} \\\\\r\\beta_{11,t} \\\\\r\\vdots \\\\\r\\beta_{JK,t} \\\\\r\\end{array}\r\\right), \\Phi = \\left(\r\\begin{array}{cccc}\r\\phi_{00} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\\r0 \u0026amp; \\phi_{11} \u0026amp; \\ldots \u0026amp; 0 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\phi_{JK}\r\\end{array}\r\\right),\ru_{t} = \\left(\r\\begin{array}{cccc}\ru_{1,t} \\\\\ru_{2,t} \\\\\r\\vdots \\\\\ru_{J,t} \\\\\r\\end{array}\r\\right),\r\\epsilon_{t} = \\left(\r\\begin{array}{cccc}\r\\epsilon_{00,t} \\\\\r\\epsilon_{11,t} \\\\\r\\vdots \\\\\r\\epsilon_{JK,t} \\\\\r\\end{array}\r\\right)\r\\]\r状態空間モデルは観測可能なデータ（ex.経済統計）を用いて、観測不可能なデータ（ex.リスクプレミアムや限界消費性向等）を推定します。観測可能なデータと不可能なデータを関連付ける方程式を観測方程式、観測不可能なデータの挙動をモデル化した方程式を遷移方程式と呼びます。ここでは、1本目が観測方程式、2本目が遷移方程式となります。御覧の通り、観測方程式は通常のVARの形をしている一方、遷移方程式はAR(1)となっており、これによってパラメータ\\(B_{t}\\)は過去の値を引きずりながら\\(\\epsilon_{t}\\)によって確率的に変動します。\\(\\Phi\\)は自己回帰係数です。誤差項\\(u_{t}\\)と\\(\\epsilon_{t}\\)は平均０の正規分布に従い、その分散は\n\\[\r\\begin{eqnarray}\rVar(u_{t}) \u0026amp;=\u0026amp; \\sigma_{u}^{2} \\\\\rVar(\\epsilon_{t}) \u0026amp;=\u0026amp; \\sigma_{\\epsilon}^{2}R\r\\end{eqnarray}\r\\]\rで与えられます。ここで、\\(\\Phi,R,\\sigma_{u}^{2},\\sigma_{\\epsilon}^{2}\\)は既知であるとします。今、t-1期までのデータが入手可能であるとすると、そのデータをカルマンフィルタアルゴリズムで推定した\\(B_{t-1|t-1}\\)とその分散共分散行列\\(P_{t-1|t-1}\\)を用いて、t期の予想値を以下のように計算します。\n\\[\r\\begin{eqnarray}\rB_{t|t-1} \u0026amp;=\u0026amp; \\Phi B_{t-1|t-1} \\\\\rP_{t|t-1} \u0026amp;=\u0026amp; \\Phi P_{t|t-1} \\Phi\u0026#39; + \\sigma_{\\epsilon}^{2}R\r\\end{eqnarray}\r\\]\r観測可能な変数の予測値はこの値を用いて計算します。\n\\[\r\\hat{Y_{t}} = X_{t} B_{t|t-1}\r\\]\r次にt期の観測値が得られると次の更新方程式を用いて\\(B_{t|t}, P_{t|t}\\)を計算します。\n\\[\r\\begin{eqnarray}\rB_{t|t} \u0026amp;=\u0026amp; B_{t|t-1} + P_{t|t-1}X_{t}\u0026#39;(X_{t}P_{t|t-1}X_{t}\u0026#39; + \\sigma_{u}^{2})^{-1}(Y_{t}-X_{t}B_{t|t-1}) \\\\\rP_{t|t} \u0026amp;=\u0026amp; P_{t|t-1} + P_{t|t-1}X_{t}\u0026#39;(X_{t}P_{t|t-1}X_{t}\u0026#39; + \\sigma_{u}^{2})^{-1}X_{t}P_{t|t-1}\r\\end{eqnarray}\r\\]\r\\(\\sigma_{\\epsilon}^{2}R = 0\\)かつ\\(\\Phi = I\\)の場合は逐次最小二乗法に一致します。要は入手できるサンプル増えるたびにOLSをやり直していくことと同値だということです。こうして推計を行うのがBVARなのですが、カルマンフィルタは漸化式なので初期値\\(B_{0|0}, P_{0|0}\\)を決めてやる必要があります。BVARの２つ目の特徴は初期値の計算に混合推定法を用いているところであり、ここに前述した事前分布が関係してきます。\n\rカルマンフィルタの初期値をどのようにきめるか\r初期値\\(B_{0|0}, P_{0|0}\\)をどうやって計算するのかを考えた際にすぐ思いつく方法としては、カルマンフィルタのスタート地点tの前に、初期値推計期間をある程度用意し、\\(Y = XB + \\epsilon\\)で\\(B_{0|0}, P_{0|0}\\)を推定する方法があります。つまり、観測方程式をGLS推計し、そのパラメータを初期値とする方法です。その際のGLS推定量は\n\\[\r\\hat{B} = (X\u0026#39;V^{-1}X)^{-1}X\u0026#39;V^{-1}Y\r\\]\nです。これでもいいんですが、これではただ時変パラメータを推計しているだけで先ほど述べたover-parametrizationの問題にはアプローチできていません。そこで、ここではパラメータ\\(B\\)に対してなにか先験的な情報が得られているとしましょう。つまり、先述した「経済変数の挙動は基本はランダムウォークだけど他変数のラグに予測力向上に資するものがあればそれも取り入れるよ」という予想です。これを定式化してみましょう。つまり、パラメータ\\(B\\)に対して以下の制約式を課します。\n\\[\rr = RB + \\nu\r\\]\rここで、\\(E(\\nu) = 0,Var(\\nu) = V_{0}\\)です。また、\\(r\\)は\\(RB\\)の予想値であり、\\(V_{0}\\)はその予想値の周りでのばらつきを表しています。取っ付きにくいかもしれませんが、\\(r\\)は１階自己回帰係数に関わる部分は1、それ以外は0となるベクトルで、\\(R\\)は単位行列\\(I\\)だと思ってもらえばいいです。つまり、\n\\[\r\\begin{eqnarray}\r\\left(\r\\begin{array}{cccc}\r0 \\\\\r1 \\\\\r0 \\\\\r\\vdots \\\\\r1 \\\\\r\\vdots \\\\\r0 \\\\\r\\end{array}\r\\right)\r\u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r\\beta_{0,1}^{1} \\\\\r\\beta_{1,1}^{1} \\\\\r\\beta_{2,1}^{1} \\\\\r\\vdots \\\\\r\\beta_{j,1}^{j} \\\\\r\\vdots \\\\\r\\beta_{J,K}^{J} \\\\\r\\end{array}\r\\right)\r+\r\\left(\r\\begin{array}{cccc}\r\\nu_{0,1}^{1} \\\\\r\\nu_{1,1}^{1} \\\\\r\\nu_{2,1}^{1} \\\\\r\\vdots \\\\\r\\nu_{j,1}^{j} \\\\\r\\vdots \\\\\r\\nu_{J,K}^{J} \\\\\r\\end{array}\r\\right)\r\\end{eqnarray}\r\\]\rみたいな感じです。ここで\\(\\beta_{j,k}^{i}\\)はi番目の方程式のj番目の変数のk次ラグにかかるパラメータを表しています。混合推定では、正規分布に従う観測値とこの事前分布が独立であるという仮定の下で観測方程式\\(Y = XB + \\epsilon\\)と$ r = RB + $を以下のように組み合わせます。\n\\[\r\\left(\r\\begin{array}{cccc}\rY \\\\\rr \\end{array}\r\\right) = \\left(\r\\begin{array}{cccc}\rX \\\\\rR \\end{array}\r\\right)B + \\left(\r\\begin{array}{cccc}\r\\epsilon \\\\\r\\nu \\end{array}\r\\right)\r\\]\nここで、\n\\[\rE\\left(\r\\begin{array}{cccc}\r\\epsilon \\\\\r\\nu \\end{array}\r\\right) = 0 \\\\\rVar\\left(\r\\begin{array}{cccc}\r\\epsilon \\\\\r\\nu \\end{array}\r\\right) = \\left(\r\\begin{array}{cccc}\r\\sigma^{2}V \u0026amp; 0 \\\\\r0 \u0026amp; V_{0} \\\\\r\\end{array}\r\\right)\r\\]\nそして、このシステム体系をGLSで推計するのです。こうすることで、事前分布を考慮した初期値の推定を行うことができます。GLS推定量は以下のようになります。\n\\(\\displaystyle \\hat{B}_{M} = (\\frac{1}{\\sigma^{2}}X\u0026#39;V^{-1}X + R\u0026#39;V_{0}^{-1}R)^{-1}(\\frac{1}{\\sigma^{2}}X\u0026#39;V^{-1}y + R\u0026#39;V_{0}^{-1}r)\\)\nご覧になればわかるように、GLS推定量は上記2本の連立方程式(実際には行列なので何本もありますが)それぞれのGLS推定量を按分したような推定量になります。ここで、\\(\\sigma^{2}\\)は既知ではないので、いったんOLSで推計しその推定量を用います。問題は\\(V_{0}\\)の置き方です。\\(V_{0}\\)の各要素を小さくとれば、事前分布に整合的な推定量が得られます（つまり、ランダムウォーク）。逆に、大きくとれば通常のGLS推定量に近づいていきます。Doan, Litternam and Sims (1984)では以下のように分散を置いています。\n\\[\r\\begin{eqnarray}\r\\displaystyle Var(\\beta_{j,k}^{j}) \u0026amp;=\u0026amp; \\frac{\\pi_{5}・\\pi_{1}}{k・exp(\\pi_{4}w_{i}^{i})} \\\\\r\\displaystyle Var(\\beta_{j,k}^{i}) \u0026amp;=\u0026amp; \\frac{\\pi_{5}・\\pi_{2}・\\sigma_{i}^{2}}{k・exp(\\pi_{4}w_{j}^{i})・\\sigma_{j}^{2}} \\\\\rVar(c^{i}) \u0026amp;=\u0026amp; \\pi_{5}・\\pi_{3}・\\sigma_{i}^{2}\r\\end{eqnarray}\r\\]\nここで、\\(\\beta_{j,k}^{j}\\)は\\(i\\)番目の方程式の自己回帰パラメータ、\\(\\beta_{j,k}^{i}\\)は\\(i\\)番目方程式の他変数ラグ項にかかるパラメータ、\\(c^{i}\\)は定数項です。そして、\\(\\pi_{1},\\pi_{2},\\pi_{3},\\pi_{4},\\pi_{5}\\)はハイパーパラメータと呼ばれるもので、カルマンフィルタにかけるパラメータの初期値の事前分布の分布の広がりを決定するパラメータとなっています。これらは初期値の推定を行う前に値を指定する必要があります。各ハイパーパラメータの具体的な特徴は以下の通りです。\\(\\pi_{1}\\)は\\(\\beta_{j,k}^{j}\\)の分散にのみ出現することから自己回帰係数に影響を与えるパラメータとなっています。具体的には、\\(\\pi_{1}\\)が大きくなればなるほど自己回帰係数は事前分布から大きく離れた辺りを取りうることになります。\\(\\pi_{2}\\)は\\(\\beta_{j,k}^{i}\\)の分散にのみ出現することから他変数のラグ項に影響を与えるパラメータとなっています。こちらも値が大きくなればなるほど係数は事前分布から大きく離れた辺りを取りうることになります。\\(\\pi_{3}\\)は定数項の事前分布に影響を与えるパラメータでこちらも考え方は同じです。\\(\\pi_{4}\\)はラグ項の分散(つまり定数項以外)に影響を与えるパラメータで、値が大きくなるにつれ、初期値は事前分布に近づいていきます。最後に\\(\\pi_{5}\\)ですが、こちらは全体にかかるパラメータで、値が大きくなるにつれ、初期値は事前分布から遠ざかります。\n　上式には、ハイパーパラメータ以外にもパラメータや変数が存在します。\\(\\sigma_{i}, \\sigma_{j}\\)は\\(y_{j,t}, y_{i,t}\\)をそれぞれAR(m)でフィッティングをかけた時の残差の標準偏差の推定値です。変数間のスケーリングの違いを考慮するために、\\(\\sigma_{i}\\)を\\(\\sigma_{j}\\)で割ったものを使用しています。本来ならば、VARの残差の標準偏差を使用すべきなのですが、推定する前にわかるわけもないので、それぞれAR(m)で推定をかけ、その標準偏差を使用しています((ランダムウォークが先験情報なので整合性は取れているような気がします))。\\(w_{j}^{i}\\)は完全に恣意的なパラメータで、DLSではrelative weightsと呼ばれているものです。i番目の方程式のj番目の変数のラグ項にかかるパラメータが０であるかどうかについて、分析者の先験情報を反映するためのパラメータです。分散の式を見ればわかるように、relative weightが大きくなれば分散は小さくなり、推定値は事前分布に近づいていきます。DLSでは、ほとんどの変数は\\(w_{i}^{i}=0,w_{j}^{i}=1\\)でよいと主張されています。つまり、自己ラグにかかる事前分布の分散に関しては確信をもってランダムウォークであるといえる一方、他変数ラグについては予測力向上に役立つもののあることを考え、値を１と置いているのです。一方、為替レートや株価はランダムウォーク色が強いということから大きい値を使用しています。最後に、\\(Var(\\beta_{j,k}^{j})\\)と\\(Var(\\beta_{j,k}^{i})\\)には分母にkがついています。つまり、ラグ次数kが大きくなればなるほど、その係数は０に近づいていくことを先験的情報として仮定していることになります((このおかげでVARの次数をこちらで指定する必要がなくなります。適当に大きい次数を指定しておけば、必要のない次数の大きいパラメータに関しては事前と値が０になるので。))。\rこれらがいわゆるMinnesota Priorの正体です((実はこれに加えて自己回帰パラメータの総和が１、他変数のラグ項にかかるパラメータの総和が０となる制約を課すのですが、話が複雑になりすぎるので今回は割愛しました))。初期値が事前分布に近づけばBVARはランダムウォークに近づきますし、離れるとUVARに近づきます。現実はその間となるのですが、なにを評価尺度としてハイパーパラメータの値を決めるかというと、それは当てはまりの良さということになります。\n\rハイパーパラメータの決定方法とその評価尺度\r当てはまりの良さと言ってもいろいろありますが、DLSはその時点で観測可能なデータから予測できるk期先の予測値の当てはまりの良さを基準としています。DLSでは以下の予測誤差ベクトル\\(\\hat{\\epsilon}_{t+k|t}\\)のクロス積和を最小化することを目的関数として、ハイパーパラメータのチューニングを行っています。\n\\[\r\\hat{\\epsilon}_{t+k|t} = \\hat{Y}_{t+k|t}-Y_{t+k}\r\\]\rここで、\\(\\hat{Y}_{t+k|t}\\)はカルマンフィルタによるk期先の予測値です（kをいくつ先にすれば良いかは不明）。このクロス積は\\(\\hat{\\epsilon}_{t+k|t}\\hat{\\epsilon}\u0026#39;_{t+k|t}\\)であり、これをフィルタリングをかけるt=1期からサンプル期間であるt=T期まで計算していくので、最終的にはT個の予測誤差ベクトルを得ることになり、以下のようなこれらの総和を最小化します。\n\\[\r\\displaystyle \\sum_{t=1}^{T-k}\\hat{\\epsilon}_{t+k|t}\\hat{\\epsilon}\u0026#39;_{t+k|t}\r\\]\rより厳密にはこのクロス積和の対数値を最小化するハイパーパラメータの値をグリッドサーチやランダムサーチで探索していくことになります((ここは機械学習等で用いられているベイズ最適化を利用するとより高速に収束させることが可能かもです。))。\rとまあ、BVARの推定方法はこんな感じです。他のVARと違い、恣意的であり、また推定方法が機械学習に近い点が特徴ではないかと思います。そもそも、BVARは予測に特化したVARですから、他のVARとは別物と考える方が良いかもです。\n（追記　2019/4/29）\n\r\r3. Rでの実装\rここまでをRで実装したいと思います。まずは、varsパッケージに準備されているCanadaデータを使用して、通常のVARの推定をやってみます。これはカナダの1980～2000年の労働生産性、雇用、失業率、実質賃金をまとめたものになります。\nlibrary(vars)\rdata(Canada)\rCanada.var \u0026lt;- VAR(Canada,p=VARselect(Canada,lag.max = 4)$selection[1])\rsummary(Canada.var)\r## ## VAR Estimation Results:\r## ========================= ## Endogenous variables: e, prod, rw, U ## Deterministic variables: const ## Sample size: 81 ## Log Likelihood: -150.609 ## Roots of the characteristic polynomial:\r## 1.004 0.9283 0.9283 0.7437 0.7437 0.6043 0.6043 0.5355 0.5355 0.2258 0.2258 0.1607\r## Call:\r## VAR(y = Canada, p = VARselect(Canada, lag.max = 4)$selection[1])\r## ## ## Estimation results for equation e: ## ================================== ## e = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## e.l1 1.75274 0.15082 11.622 \u0026lt; 2e-16 ***\r## prod.l1 0.16962 0.06228 2.723 0.008204 ** ## rw.l1 -0.08260 0.05277 -1.565 0.122180 ## U.l1 0.09952 0.19747 0.504 0.615915 ## e.l2 -1.18385 0.23517 -5.034 3.75e-06 ***\r## prod.l2 -0.10574 0.09425 -1.122 0.265858 ## rw.l2 -0.02439 0.06957 -0.351 0.727032 ## U.l2 -0.05077 0.24534 -0.207 0.836667 ## e.l3 0.58725 0.16431 3.574 0.000652 ***\r## prod.l3 0.01054 0.06384 0.165 0.869371 ## rw.l3 0.03824 0.05365 0.713 0.478450 ## U.l3 0.34139 0.20530 1.663 0.100938 ## const -150.68737 61.00889 -2.470 0.016029 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## ## Residual standard error: 0.3399 on 68 degrees of freedom\r## Multiple R-Squared: 0.9988, Adjusted R-squared: 0.9985 ## F-statistic: 4554 on 12 and 68 DF, p-value: \u0026lt; 2.2e-16 ## ## ## Estimation results for equation prod: ## ===================================== ## prod = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## e.l1 -0.14880 0.28913 -0.515 0.6085 ## prod.l1 1.14799 0.11940 9.615 2.65e-14 ***\r## rw.l1 0.02359 0.10117 0.233 0.8163 ## U.l1 -0.65814 0.37857 -1.739 0.0866 . ## e.l2 -0.18165 0.45083 -0.403 0.6883 ## prod.l2 -0.19627 0.18069 -1.086 0.2812 ## rw.l2 -0.20337 0.13337 -1.525 0.1319 ## U.l2 0.82237 0.47034 1.748 0.0849 . ## e.l3 0.57495 0.31499 1.825 0.0723 . ## prod.l3 0.04415 0.12239 0.361 0.7194 ## rw.l3 0.09337 0.10285 0.908 0.3672 ## U.l3 0.40078 0.39357 1.018 0.3121 ## const -195.86985 116.95813 -1.675 0.0986 . ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## ## Residual standard error: 0.6515 on 68 degrees of freedom\r## Multiple R-Squared: 0.98, Adjusted R-squared: 0.9765 ## F-statistic: 277.5 on 12 and 68 DF, p-value: \u0026lt; 2.2e-16 ## ## ## Estimation results for equation rw: ## =================================== ## rw = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## e.l1 -4.716e-01 3.373e-01 -1.398 0.167 ## prod.l1 -6.500e-02 1.393e-01 -0.467 0.642 ## rw.l1 9.091e-01 1.180e-01 7.702 7.63e-11 ***\r## U.l1 -7.941e-04 4.417e-01 -0.002 0.999 ## e.l2 6.667e-01 5.260e-01 1.268 0.209 ## prod.l2 -2.164e-01 2.108e-01 -1.027 0.308 ## rw.l2 -1.457e-01 1.556e-01 -0.936 0.353 ## U.l2 -3.014e-01 5.487e-01 -0.549 0.585 ## e.l3 -1.289e-01 3.675e-01 -0.351 0.727 ## prod.l3 2.140e-01 1.428e-01 1.498 0.139 ## rw.l3 1.902e-01 1.200e-01 1.585 0.118 ## U.l3 1.506e-01 4.592e-01 0.328 0.744 ## const -1.167e+01 1.365e+02 -0.086 0.932 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## ## Residual standard error: 0.7601 on 68 degrees of freedom\r## Multiple R-Squared: 0.9989, Adjusted R-squared: 0.9987 ## F-statistic: 5239 on 12 and 68 DF, p-value: \u0026lt; 2.2e-16 ## ## ## Estimation results for equation U: ## ================================== ## U = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## e.l1 -0.61773 0.12508 -4.939 5.39e-06 ***\r## prod.l1 -0.09778 0.05165 -1.893 0.062614 . ## rw.l1 0.01455 0.04377 0.332 0.740601 ## U.l1 0.65976 0.16378 4.028 0.000144 ***\r## e.l2 0.51811 0.19504 2.656 0.009830 ** ## prod.l2 0.08799 0.07817 1.126 0.264279 ## rw.l2 0.06993 0.05770 1.212 0.229700 ## U.l2 -0.08099 0.20348 -0.398 0.691865 ## e.l3 -0.03006 0.13627 -0.221 0.826069 ## prod.l3 -0.01092 0.05295 -0.206 0.837180 ## rw.l3 -0.03909 0.04450 -0.879 0.382733 ## U.l3 0.06684 0.17027 0.393 0.695858 ## const 114.36732 50.59802 2.260 0.027008 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## ## Residual standard error: 0.2819 on 68 degrees of freedom\r## Multiple R-Squared: 0.9736, Adjusted R-squared: 0.969 ## F-statistic: 209.2 on 12 and 68 DF, p-value: \u0026lt; 2.2e-16 ## ## ## ## Covariance matrix of residuals:\r## e prod rw U\r## e 0.11550 -0.03161 -0.03681 -0.07034\r## prod -0.03161 0.42449 0.05589 0.01494\r## rw -0.03681 0.05589 0.57780 0.03660\r## U -0.07034 0.01494 0.03660 0.07945\r## ## Correlation matrix of residuals:\r## e prod rw U\r## e 1.0000 -0.14276 -0.1425 -0.73426\r## prod -0.1428 1.00000 0.1129 0.08136\r## rw -0.1425 0.11286 1.0000 0.17084\r## U -0.7343 0.08136 0.1708 1.00000\rplot(predict(Canada.var,n.ahead=20,ci=0.95))\rパッケージが整備されているので簡単に実行できました。上で見たようにやっていることは別々のOLSを4本推定しているだけです。\r次にBVARです。まず、カルマンフィルタと事前分布を定義します。カルマンフィルタは以前記事でご紹介したものと同じです。\nlibrary(seasonal)\rkalmanfiter \u0026lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){\r#-------------------------------------------------------------------\r# Implemention of Kalman filter\r# y - observed variable\r# I - the number of unobserved variable\r# t - parameter of endogenous variable in state equation\r# z - parameter of endogenous variable in observable equation\r# c - constant in state equaion\r# R - parameter of exogenous variable in state equation\r# Q - var-cov matrix of exogenous variable in state equation\r# d - constant in observable equaion\r# S - parameter of exogenous variable in observable equation\r# h - var-cov matrix of exogenous variable in observable equation\r# a_int - initial value of endogenous variable\r# sig_int - initial value of variance of endogenous variable\r#-------------------------------------------------------------------\rlibrary(MASS)\r# 1.Define Variable\rif (class(y)!=\u0026quot;matrix\u0026quot;){\ry \u0026lt;- as.matrix(y)\r}\rN \u0026lt;- NROW(y) # sample size\rL \u0026lt;- NCOL(y) # the number of observable variable a_pre \u0026lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable\ra_fil \u0026lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable\rsig_pre \u0026lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable\rsig_fil \u0026lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable\ry_pre \u0026lt;- array(0,dim = c(L,1,N)) # prediction of observed variable\rF_pre \u0026lt;- array(0,dim = c(L,L,N)) # auxiliary variable k \u0026lt;- array(0,dim = c(I,L,N)) # kalman gain\rif (any(is.na(a_int))){\ra_int \u0026lt;- matrix(0,nrow = I,ncol = 1)\r}\rif (any(is.na(sig_int))){\rsig_int \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(R))){\rR \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(Q))){\rQ \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif(any(is.na(S))){\rS \u0026lt;- matrix(1,nrow = L,ncol = L)\r}\rif (any(is.na(h))){\rH \u0026lt;- array(0,dim = c(L,L,N))\rfor(i in 1:N){\rdiag(H[,,i]) = 1\r}\r}else if (class(h)!=\u0026quot;array\u0026quot;){\rH \u0026lt;- array(h,dim = c(NROW(h),NCOL(h),N))\r}\r# fill infinite if observed data is NA\rfor(i in 1:N){\rmiss \u0026lt;- is.na(y[i,])\rdiag(H[,,i])[miss] \u0026lt;- 1e+32\r}\ry[is.na(y)] \u0026lt;- 0\r# 2.Set Initial Value\ra_fil[,,1] \u0026lt;- a_int\rsig_fil[,,1] \u0026lt;- sig_int\r# 3.Implement Kalman filter\rfor (i in 1:N){\rif(class(z)==\u0026quot;array\u0026quot;){\rZ \u0026lt;- z[,,i]\r}else{\rZ \u0026lt;- z\r}\ra_pre[,,i] \u0026lt;- t%*%a_fil[,,i] + c\rsig_pre[,,i] \u0026lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)\ry_pre[,,i] \u0026lt;- Z%*%a_pre[,,i] + d\rF_pre[,,i] \u0026lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)\rk[,,i] \u0026lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])\ra_fil[,,i+1] \u0026lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])\rsig_fil[,,i+1] \u0026lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])\r}\r# 4.Aggregate results\rresult \u0026lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)\rnames(result) \u0026lt;- c(\u0026quot;state prediction\u0026quot;, \u0026quot;state filtered\u0026quot;, \u0026quot;state var prediction\u0026quot;, \u0026quot;state var filtered\u0026quot;, \u0026quot;observable prediction\u0026quot;, \u0026quot;kalman gain\u0026quot;,\r\u0026quot;parameter of state eq\u0026quot;, \u0026quot;parameter of observable eq\u0026quot;)\rreturn(result)\r}\r事前分布を計算する関数を定義します。まず、引数を計算しやすい形に変換しておきます。\n # set pi\rpi \u0026lt;- abs(rnorm(5)) # hyperparameter\rx \u0026lt;- Canada # dataset\rorder \u0026lt;- 4 # order\rlibrary(MASS)\r# 1. Process row data\rfor (i in 1:5){\reval(parse(text = paste0(\u0026quot;pi\u0026quot;,i,\u0026quot;=pi[[\u0026quot;,i,\u0026quot;]]\u0026quot;)))\r}\rif (class(x)!=\u0026quot;matrix\u0026quot;){\rx \u0026lt;- as.matrix(x)\r}\rdependent \u0026lt;- as.matrix(x[(order+1):NROW(x),1])\rfor (i in 2:NCOL(x)){\rdependent \u0026lt;- rbind(dependent,as.matrix(x[(order+1):NROW(x),i]))\r}\rexplanatory \u0026lt;- embed(cbind(1,x),order)[,-((order*NCOL(x)+order-NCOL(x)):(order*NCOL(x)+order))]\rexplanatory \u0026lt;- diag(1,nrow = NCOL(x),ncol = NCOL(x))%x%as.matrix(explanatory)\rnpara \u0026lt;- NCOL(explanatory) # the number of parameters/ ncol(x)*(ncol(x)*order + 1(const))\rdependentは非説明変数、explanatoryは説明変数です。ちょうど1式を再現した形になります。\n\\[\r\\left(\r\\begin{array}{cccc}\rY_{1} \\\\\rY_{2} \\\\\r\\vdots \\\\\rY_{J} \\\\\r\\end{array}\r\\right) = \\left(\r\\begin{array}{cccc}\rX_{1} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\\r0 \u0026amp; X_{2} \u0026amp; \\ldots \u0026amp; 0 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; X_{J}\r\\end{array}\r\\right) \\left(\r\\begin{array}{cccc}\rA_{1} \\\\\rA_{2} \\\\\r\\vdots \\\\\rA_{J} \\\\\r\\end{array}\r\\right) +\r\\left(\r\\begin{array}{cccc}\rU_{1} \\\\\rU_{2} \\\\\r\\vdots \\\\\rU_{J} \\\\\r\\end{array}\r\\right)\r\\]\n# 2. Make Prior matrixes\rr \u0026lt;- as.matrix(numeric(npara)) # a part of dependent variable in mixed estimation\riter1 \u0026lt;- numeric(NCOL(x))\rn \u0026lt;- 2\rfor (i in 1:length(iter1)){\riter1[i] \u0026lt;- n\rn \u0026lt;- n + order*NCOL(x) + 1\r}\rfor (i in iter1){\rr[i] \u0026lt;- 1\r}\rR \u0026lt;- diag(1,nrow = npara,ncol = npara) # a part of explanatory variables in mixed estimation\rV \u0026lt;- diag(1,nrow = NROW(explanatory),ncol = NROW(explanatory)) # coefficient matrix of var-cov matrix in mixed estimation\rV0 \u0026lt;- matrix(0,nrow = npara,ncol = npara) # Prior for var matrix\r# PRIOR FOR VAR OF CONSTANT\rsigi \u0026lt;- numeric(NCOL(x)) # var of AR(m)\rfor (i in 1:NCOL(x)){\rAR \u0026lt;- ar(x[,i],order.max = order)\rsigi[i] \u0026lt;- AR$var.pred\r}\rsigma_AR \u0026lt;- diag(sigi,nrow = NCOL(x),ncol = NCOL(x))\riter2 \u0026lt;- seq(1,npara,(NCOL(x)*order+1))\rn \u0026lt;- 1\rfor (i in iter2){\rV0[i,i] \u0026lt;- pi5*pi3*sigi[n]\rn \u0026lt;- n + 1\r}\r# PRIOR FOR VAR OF AUTOREGRESSIVE PARAMETER\rwi \u0026lt;- 0 # prior weight on s.d. of autoregressive parameter\rk \u0026lt;- 0 # decay parameter\riter3 \u0026lt;- numeric((NCOL(x)*order))\rn \u0026lt;- 2;iter3[1] \u0026lt;- n\rfor (i in 2:length(iter3)){\rif ((i-1)%%order == 0){\rn \u0026lt;- n + NCOL(x) + 2 # const\riter3[i] \u0026lt;- n }else{\rn \u0026lt;- n + NCOL(x)\riter3[i] \u0026lt;- n\r}\r}\rfor (i in iter3) {\rk \u0026lt;- k + 1\rV0[i,i] \u0026lt;- pi5*pi1/(k*exp(pi4*wi))\rif (k == order){\rk = 0\r}\r}\r# PRIOR FOR VAR OF DISTRIBUTED LAG PARAMETER\rwj \u0026lt;- 1 # prior weight on s.d. of distributed lag parameter\rk \u0026lt;- 1 # decay parameter\rcount \u0026lt;- 0\rn \u0026lt;- 1\rndis \u0026lt;- npara - NCOL(x) - length(iter3) # the number of distibuted lag in each equation\rl \u0026lt;- rep(1:NCOL(x),length=npara)\rtemp \u0026lt;- numeric(length(iter3))\rj \u0026lt;- 1;temp[1] \u0026lt;- j\rfor (i in 2:length(temp)){\rif ((i-1)%%order == 0){\rj \u0026lt;- j + NCOL(x) + 1 # const\rtemp[i] \u0026lt;- j }else{\rj \u0026lt;- j + NCOL(x)\rtemp[i] \u0026lt;- j\r}\r}\rl \u0026lt;- l[-c(temp)]; l \u0026lt;- l[1:(ndis)]\riter4 \u0026lt;- 1:npara; iter4 \u0026lt;- iter4[-c(iter2,iter3)]\rfor (i in iter4){\rcount \u0026lt;- count + 1\rV0[i,i] \u0026lt;- pi5*pi2*sigi[n]/(k*exp(pi4*wj)*sigi[l[count]])\rif(count%%(order-1) == 0){\rk \u0026lt;- k + 1\r}\rif (count%%(ndis/NCOL(x)) == 0){\rn = n + 1\rk = 1\r}\r}\r# 3. Estimate OLS\rbeta \u0026lt;- ginv((t(explanatory)%*%explanatory))%*%t(explanatory)%*%dependent\rsig \u0026lt;- as.numeric((NROW(explanatory)-NCOL(explanatory))^(-1)*\rt(dependent-explanatory%*%beta)%*%ginv(V)%*%\r(dependent-explanatory%*%beta))\r# 4. Implement Mixed Estimation for initial values\rbeta_M \u0026lt;- ginv((1/sig)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)%*%\r((sig)^(-1)*t(explanatory)%*%ginv(V)%*%dependent + t(R)%*%ginv(V0)%*%r)\rsig_M \u0026lt;- sig*ginv((sig)^(-1)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)\r# for kalman filter\robservable \u0026lt;- as.matrix(x[(order+1):NROW(x),]) # dependent variable\rcoefficient \u0026lt;- array(0,dim = c(nrow = NCOL(x),ncol = npara,(NROW(x)-order))) # explanatory variable\rfor(i in 1:(NROW(x)-order)){\rfor (k in 1:NCOL(x)){\rcoefficient[k,(1+(k-1)*npara/NCOL(x)):(k*npara/NCOL(x)),i] \u0026lt;- explanatory[i,1:(npara/NCOL(x))]\r}\r}\rresult \u0026lt;- list(observable, coefficient, beta_M, sig_M, sigma_AR, V0, order, npara)\rnames(result) \u0026lt;- c(\u0026quot;observable\u0026quot;, \u0026quot;coefficient\u0026quot;,\u0026quot;beta_M\u0026quot;, \u0026quot;sig_M\u0026quot;,\u0026quot;sigma_AR\u0026quot;, \u0026quot;V0\u0026quot;,\u0026quot;order\u0026quot;,\u0026quot;npara\u0026quot;)\rreturn(result)\r}\rMPrior \u0026lt;- function(x,order,pi){\r#-------------------------------------------------------------------\r# Make Minnesota Prior\r# x - dataset\r# order - max lag of autoregression\r# pi - a set of hyper-parameter of prior matrix\r#-------------------------------------------------------------------\rlibrary(MASS)\r# 1. Process row data\rfor (i in 1:5){\reval(parse(text = paste0(\u0026quot;pi\u0026quot;,i,\u0026quot;=pi[[\u0026quot;,i,\u0026quot;]]\u0026quot;)))\r}\rif (class(x)!=\u0026quot;matrix\u0026quot;){\rx \u0026lt;- as.matrix(x)\r}\rdependent \u0026lt;- as.matrix(x[1:(NROW(x)-order),1])\rfor (i in 2:NCOL(x)){\rdependent \u0026lt;- rbind(dependent,as.matrix(x[(order+1):NROW(x),i]))\r}\rexplanatory \u0026lt;- cbind(1,embed(x,order)[-1,])\rexplanatory \u0026lt;- diag(1,nrow = NCOL(x),ncol = NCOL(x))%x%as.matrix(explanatory)\rnpara \u0026lt;- NCOL(explanatory) # the number of parameters/ ncol(x)*(ncol(x)*order + 1(const))\r# 2. Make Prior matrixes\rr \u0026lt;- as.matrix(numeric(npara)) # a part of dependent variable in mixed estimation\riter1 \u0026lt;- numeric(NCOL(x))\rn \u0026lt;- 2\rfor (i in 1:length(iter1)){\riter1[i] \u0026lt;- n\rn \u0026lt;- n + order*NCOL(x) + 1\r}\rfor (i in iter1){\rr[i] \u0026lt;- 1\r}\rR \u0026lt;- diag(1,nrow = npara,ncol = npara) # a part of explanatory variables in mixed estimation\rV \u0026lt;- diag(1,nrow = NROW(explanatory),ncol = NROW(explanatory)) # coefficient matrix of var-cov matrix in mixed estimation\rV0 \u0026lt;- matrix(0,nrow = npara,ncol = npara) # Prior for var matrix\r# PRIOR FOR VAR OF CONSTANT\rsigi \u0026lt;- numeric(NCOL(x)) # var of AR(m)\rfor (i in 1:NCOL(x)){\rAR \u0026lt;- ar(x[,i],order.max = order)\rsigi[i] \u0026lt;- AR$var.pred\r}\rsigma_AR \u0026lt;- diag(sigi,nrow = NCOL(x),ncol = NCOL(x))\riter2 \u0026lt;- seq(1,npara,(NCOL(x)*order+1))\rn \u0026lt;- 1\rfor (i in iter2){\rV0[i,i] \u0026lt;- pi5*pi3*sigi[n]\rn \u0026lt;- n + 1\r}\r# PRIOR FOR VAR OF AUTOREGRESSIVE PARAMETER\rwi \u0026lt;- 0 # prior weight on s.d. of autoregressive parameter\rk \u0026lt;- 0 # decay parameter\riter3 \u0026lt;- numeric((NCOL(x)*order))\rn \u0026lt;- 2;iter3[1] \u0026lt;- n\rfor (i in 2:length(iter3)){\rif ((i-1)%%order == 0){\rn \u0026lt;- n + NCOL(x) + 2 # const\riter3[i] \u0026lt;- n }else{\rn \u0026lt;- n + NCOL(x)\riter3[i] \u0026lt;- n\r}\r}\rfor (i in iter3) {\rk \u0026lt;- k + 1\rV0[i,i] \u0026lt;- pi5*pi1/(k*exp(pi4*wi))\rif (k == order){\rk = 0\r}\r}\r# PRIOR FOR VAR OF DISTRIBUTED LAG PARAMETER\rwj \u0026lt;- 1 # prior weight on s.d. of distributed lag parameter\rk \u0026lt;- 1 # decay parameter\rcount \u0026lt;- 0\rn \u0026lt;- 1\rndis \u0026lt;- npara - NCOL(x) - length(iter3) # the number of distibuted lag in each equation\rl \u0026lt;- rep(1:NCOL(x),length=npara)\rtemp \u0026lt;- numeric(length(iter3))\rj \u0026lt;- 1;temp[1] \u0026lt;- j\rfor (i in 2:length(temp)){\rif ((i-1)%%order == 0){\rj \u0026lt;- j + NCOL(x) + 1 # const\rtemp[i] \u0026lt;- j }else{\rj \u0026lt;- j + NCOL(x)\rtemp[i] \u0026lt;- j\r}\r}\rl \u0026lt;- l[-c(temp)]; l \u0026lt;- l[1:(ndis)]\riter4 \u0026lt;- 1:npara; iter4 \u0026lt;- iter4[-c(iter2,iter3)]\rfor (i in iter4){\rcount \u0026lt;- count + 1\rV0[i,i] \u0026lt;- pi5*pi2*sigi[n]/(k*exp(pi4*wj)*sigi[l[count]])\rif(count%%(order-1) == 0){\rk \u0026lt;- k + 1\r}\rif (count%%(ndis/NCOL(x)) == 0){\rn = n + 1\rk = 1\r}\r}\r# 3. Estimate OLS\rbeta \u0026lt;- ginv((t(explanatory)%*%explanatory))%*%t(explanatory)%*%dependent\rsig \u0026lt;- as.numeric((NROW(explanatory)-NCOL(explanatory))^(-1)*\rt(dependent-explanatory%*%beta)%*%ginv(V)%*%\r(dependent-explanatory%*%beta))\r# 4. Implement Mixed Estimation for initial values\rbeta_M \u0026lt;- ginv((1/sig)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)%*%\r((sig)^(-1)*t(explanatory)%*%ginv(V)%*%dependent + t(R)%*%ginv(V0)%*%r)\rsig_M \u0026lt;- sig*ginv((sig)^(-1)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)\r# for kalman filter\robservable \u0026lt;- as.matrix(x[(order+1):NROW(x),]) # dependent variable\rcoefficient \u0026lt;- array(0,dim = c(nrow = NCOL(x),ncol = npara,(NROW(x)-order))) # explanatory variable\rfor(i in 1:(NROW(x)-order)){\rfor (k in 1:NCOL(x)){\rcoefficient[k,(1+(k-1)*npara/NCOL(x)):(k*npara/NCOL(x)),i] \u0026lt;- explanatory[i,1:(npara/NCOL(x))]\r}\r}\rresult \u0026lt;- list(observable, coefficient, beta_M, sig_M, sigma_AR, V0, order, npara)\rnames(result) \u0026lt;- c(\u0026quot;observable\u0026quot;, \u0026quot;coefficient\u0026quot;,\u0026quot;beta_M\u0026quot;, \u0026quot;sig_M\u0026quot;,\u0026quot;sigma_AR\u0026quot;, \u0026quot;V0\u0026quot;,\u0026quot;order\u0026quot;,\u0026quot;npara\u0026quot;)\rreturn(result)\r}\rBVAR \u0026lt;- function(m){\r#-------------------------------------------------------------------\r# Estimation of Bayesian Vector Auto-Regression\r# m - dataset\r#-------------------------------------------------------------------\r# 1. Process data\robservable \u0026lt;- m$observable\rcoefficient \u0026lt;- m$coefficient\rbeta_M \u0026lt;- m$beta_M\rsig_M \u0026lt;- m$sig_M\rsigma_AR \u0026lt;- m$sigma_AR\rV0 \u0026lt;- m$V0\rorder \u0026lt;- m$order\rnpara \u0026lt;- m$npara\r# 2. Run Kalmanfilter\rresults \u0026lt;- kalmanfiter(observable,npara,diag(1,nrow = npara,ncol = npara),\rcoefficient,0,NA,V0,0,NA,sigma_AR,beta_M,sig_M)\rreturn(results)\r}\rm \u0026lt;- MPrior(Canada,3,abs(rnorm(5)))\rresults \u0026lt;- BVAR(m)\rBVAR_Prediction \u0026lt;- function(results,horizon){\rstate_pred \u0026lt;- results$`state prediction`\rstate_fil \u0026lt;- results$`state filtered`\robs_pred \u0026lt;- results$`observable prediction`\rA \u0026lt;- results$`parameter of state eq`\rnparaall \u0026lt;- dim(state_pred)[1]\rnum \u0026lt;- dim(obs_pred)[1]\rnpara \u0026lt;- order*num+1\rorder \u0026lt;- (nparaall-num)/(num^2)\rsamplesize \u0026lt;- dim(state_pred)[3]\ry \u0026lt;- matrix(0,horizon,num)\rB \u0026lt;- matrix(0,horizon,nparaall)\rx \u0026lt;- results$`parameter of observable eq`[1,1:npara,samplesize]\rB[1,] \u0026lt;- state_fil[,1,samplesize]\rfor (i in 1:(horizon-1)) {\rB[i+1,] \u0026lt;- A%*%B[i,]\ry[i+1,] \u0026lt;- B[i+1,]%*%(diag(1,num)%x%x)\rx \u0026lt;- c(1,y[i+1,],x[-c(1,seq(from=(npara-order),to=npara))])\r}\rresult \u0026lt;- list(B,y)\rnames(result) \u0026lt;- c(\u0026quot;B\u0026quot;,\u0026quot;y\u0026quot;)\rreturn(result)\r}\rpre \u0026lt;- BVAR_Prediction(results,20)\rBVAR_OPT \u0026lt;- function(x){\rint_pi \u0026lt;- rnorm(5)\r}\r\r\rSims, Christopher A, 1980. “Macroeconomics and Reality,” Econometrica, Econometric Society, vol. 48(1), pages 1-48, January.↩︎\n\r\r\r","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1556496000,"objectID":"20dd4f483681dd9058ecc0bf2b0fcdd8","permalink":"/post/post5/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/post/post5/","section":"post","summary":"Bayesian VAR（BVAR）と呼ばれる、VARの仲でも予測精度の向上を目材して開発されたモデルを勉強してみました。","tags":["BVAR"],"title":"BVARについて","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r\r1. Gianonne et. al. (2008)版マルチファクターモデル\r2. Rで実装する\r\r\rおはこんばんにちは。\r前回、統計ダッシュボードからAPI接続で統計データを落とすという記事を投稿しました。\r今回はそのデータを、Gianonne et. al. (2008)のマルチファクターモデルにかけ、四半期GDPの予測を行いたいと思います。\n1. Gianonne et. al. (2008)版マルチファクターモデル\r元論文\n前回の投稿でも書きましたが、この論文はGiannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標を2つのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しています。\rまず、このモデルについてご紹介します。このモデルでは2段階推計を行います。まず主成分分析により経済統計を統計間の相関が0となるファクターへ変換します（参考）。そして、その後の状態空間モデルでの推計で必要になるパラメータをOLS推計し、そのパラメータを使用してカルマンフィルタ＆カルマンスムーザーを回し、ファクターを推計しています。では、具体的な説明に移ります。\r統計データを\\(x_{i,t|v_j}\\)と定義します。ここで、\\(i=1,...,n\\)は経済統計を表し（つまり\\(n\\)が全統計数）、\\(t=1,...,T_{iv_j}\\)は統計\\(i\\)のサンプル期間の時点を表しています（つまり、\\(T_{iv_j}\\)は統計\\(i\\)のその時点での最新データ日付を表す）。また、\\(v_j\\)はある時点\\(j\\)（2005年など）で得られる情報集合（vintage）を表しています。統計データ\\(x_{i,t|v_j}\\)は以下のようにファクター\\(f_{r,t}\\)の線形結合で表すことができます（ここで\\(r\\)はファクターの数を表す）。\n\\[\rx_{i,t|v\\_j} = \\mu_i + \\lambda_{i1}f_{1,t} + ... + \\lambda_{ir}f_{r,t} + \\xi_{i,t|v_j} \\tag{1}\r\\]\n\\(\\mu\\_i\\)は定数項、\\(\\lambda\\_{ir}\\)はファクターローディング、\\(\\xi\\_{i,t|v\\_j}\\)はホワイトノイズの誤差項を表しています。これを行列形式で書くと以下のようになります。\n\\[\rx_{t|v_j} = \\mu + \\Lambda F_t + \\xi_{t|v_j} = \\mu + \\chi_t + \\xi_{t|v_j} \\tag{2}\r\\]\nここで、\\(x_{t|v_j} = (x_{1,t|v_j}, ..., x_{n,t|v_j} )^{\\mathrm{T}}\\)、\\(\\xi_{t|v_j}=(\\xi_{1,t|v_j}, ..., \\xi_{n,t|v_j})^{\\mathrm{T}}\\)、\\(F_t = (f_{1,t}, ..., f_{r,t})^{\\mathrm{T}}\\)であり、\\(\\Lambda\\)は各要素が$ _{ij}\\(の\\)nr\\(行列のファクターローディングを表しています。また、\\)t = F_t\\(です。よって、ファクター\\) F_t\\(を推定するためには、データ\\)x{i,t|v_j}$を以下のように基準化したうえで、分散共分散行列を計算し、その固有値問題を解けばよいという事になります。\n\\[\r\\displaystyle z_{it} = \\frac{1}{\\hat{\\sigma}_i}(x_{it} - \\hat{\\mu}_{it}) \\tag{3}\r\\]\nここで、\\(\\displaystyle \\hat{\\mu}_{it} = 1/T \\sum_{t=1}^T x_{it}\\)であり、\\(\\hat{\\sigma}_i = \\sqrt{1/T \\sum_{t=1}^T (x_{it}-\\hat{\\mu_{it}})^2}\\)です（ここで\\(T\\)はサンプル期間）。分散共分散行列\\(S\\)を以下のように定義します。\n\\[\r\\displaystyle S = \\frac{1}{T} \\sum_{t=1}^T z_t z_t^{\\mathrm{T}} \\tag{4}\r\\]\r次に、\\(S\\)のうち、固有値を大きい順に\\(r\\)個取り出し、それを要素にした$ r r\\(対角行列を\\) D\\(、それに対応する固有ベクトルを\\)n r\\(行列にしたものを\\) V\\(と定義します。ファクター\\) _t$は以下のように推計できます。\n\\[\r\\tilde{F}_t = V^{\\mathrm{T}} z_t \\tag{5}\r\\]\rファクターローディング\\(\\Lambda\\)と誤差項の共分散行列\\(\\Psi = \\mathop{\\mathbb{E}} [\\xi_t\\xi^{\\mathrm{T}}_t]\\)は\\(\\tilde{F}_t\\)を\\(z_t\\)に回帰することで推計します。\n\\[\r\\displaystyle \\hat{\\Lambda} = \\sum_{t=1}^T z_t \\tilde{F}^{\\mathrm{T}}_t (\\sum_{t=1}^T\\tilde{F}_t\\tilde{F}^{\\mathrm{T}}_t)^{-1} = V \\tag{6}\r\\]\n\\[\r\\hat{\\Psi} = diag(S - VDV) \\tag{7}\r\\]\n注意して頂きたいのは、ここで推計した\\(\\tilde{F}_t\\)は、以下の状態空間モデルでの推計に必要なパラメータを計算するための一時的な推計値であるという事です（２段階推計の１段階目という事）。\n\\[\rx_{t|v_j} = \\mu + \\Lambda F\\_t + \\xi_{t|v_j} = \\mu + \\chi_t + \\xi_{t|v_j} \\tag{2}\r\\]\n\\[\rF\\_t = AF\\_{t-1} + u\\_t \\tag{8}\r\\]\rここで、\\(u_t\\)は平均0、分散\\(H\\)のホワイトノイズです。再掲している(2)式が観測方程式、(8)式が遷移方程式となっています。推定すべきパラメータは\\(\\Lambda\\)、\\(\\Psi\\)以外に\\(A\\)と\\(H\\)があります（\\(\\mu=0\\)としています）。\\(A\\)は主成分分析により計算した\\(\\tilde{F}_t\\)をVAR(1)にかけることで推定します。\n\\[\r\\hat{A} = \\sum_{t=2}^T\\tilde{F}_t\\tilde{F}_{t-1}^{\\mathrm{T}} (\\sum_{t=2}^T\\tilde{F}_{t-1}\\tilde{F}_{t-1}^{\\mathrm{T}})^{-1} \\tag{9}\r\\]\r\\(H\\)は今推計したVAR(1)の誤差項の共分散行列から計算します。これで必要なパラメータの推定が終わりました。次にカルマンフィルタを回します。カルマンフィルタに関してはこちらを参考にしてください。わかりやすいです。これで最終的に\\(\\hat{F}_{t|v_j}\\)の推計ができるわけです。\rGDPがこれらのファクターで説明可能であり（つまり固有の変動がない）、GDPと月次経済指標がjointly normalであれば以下のような単純なOLS推計でGDPを予測することができます。もちろん月次経済指標の方が早く公表されるので、内生性の問題はないと考えられます。\n\\[\r\\hat{y}_{3k|v_j} = \\alpha + \\beta^{\\mathrm{T}} \\hat{F}_{3k|v_j} \\tag{10}\r\\]\nここで、\\(3k\\)は四半期の最終月を示しています（3月、6月など）\\(\\hat{y}_{3k|v_j}\\)は\\(j\\)時点で得られる情報集合\\(v_j\\)での四半期GDPを表しており、\\(\\hat{F}_{3k|v_j}\\)はその時点で推定したファクターを表しています（四半期最終月の値だけを使用している点に注意）。これで推計方法の説明は終わりです。\n\r2. Rで実装する\rでは実装します。前回記事で得られたデータ（dataset）が読み込まれている状態からスタートします。まず、主成分分析でファクターを計算します。なお、前回の記事で3ファクターの累積寄与度が80%を超えたため、今回もファクター数は3にしています。\n#------------------------\r# Giannone et. al. 2008 #------------------------\rlibrary(xts)\rlibrary(MASS)\rlibrary(tidyverse)\r# 主成分分析でファクターを計算\rf \u0026lt;- 3 # ファクター数を定義\ra \u0026lt;- which(dataset1$publication == \u0026quot;2012-04-01\u0026quot;) # サンプル開始期間を2012年に設定。\rdataset2 \u0026lt;- dataset1[a:nrow(dataset1),]\rrownames(dataset2) \u0026lt;- dataset2$publication\rdataset2 \u0026lt;- dataset2[,-2]\rz \u0026lt;- scale(dataset2) # zは基準化されたサンプルデータ\rfor (i in 1:nrow(z)){\reval(parse(text = paste(\u0026quot;S_i \u0026lt;- z[i,]%*%t(z[i,])\u0026quot;,sep = \u0026quot;\u0026quot;)))\rif (i==1){\rS \u0026lt;- S_i\r}else{\rS \u0026lt;- S + S_i\r}\r}\rS \u0026lt;- (1/nrow(z))*S # 分散共分散行列を計算 (4)式\rgamma \u0026lt;- eigen(S) D \u0026lt;- diag(gamma$values[1:f])\rV \u0026lt;- gamma$vectors[,1:f]\rF_t \u0026lt;- matrix(0,nrow(z),f)\rfor (i in 1:nrow(z)){\reval(parse(text = paste(\u0026quot;F_t[\u0026quot;,i,\u0026quot;,]\u0026lt;- z[\u0026quot;,i,\u0026quot;,]%*%V\u0026quot;,sep = \u0026quot;\u0026quot;))) # (5)式を実行\r}\rF_t.xts \u0026lt;- xts(F_t,order.by = as.Date(row.names(z)))\rplot.zoo(F_t.xts,col = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;yellow\u0026quot;,\u0026quot;purple\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 時系列プロット\rlambda_hat \u0026lt;- V\rpsi \u0026lt;- diag(S-V%*%D%*%t(V)) # (7)式\rR \u0026lt;- diag(diag(cov(z-z%*%V%*%t(V)))) \r推計したファクター\\(\\tilde{F}\\_t\\)の時系列プロットは以下のようになり、前回princomp関数で計算したファクターと完全一致します（じゃあprincompでいいやんと思われるかもしれませんが実装しないと勉強になりませんので）。\n次に、VAR(1)を推計し、パラメータを取り出します。\n# VAR(1)モデルを推計\ra \u0026lt;- matrix(0,f,f)\rb \u0026lt;- matrix(0,f,f)\rfor(t in 2:nrow(z)){\ra \u0026lt;- a + F_t[t,]%*%t(F_t[t-1,])\rb \u0026lt;- b + F_t[t-1,]%*%t(F_t[t-1,])\r}\rb_inv \u0026lt;- solve(b)\rA_hat \u0026lt;- a%*%b_inv # (9)式\re \u0026lt;- numeric(f)\rfor (t in 2:nrow(F_t)){\re \u0026lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat\r}\rH \u0026lt;- t(e)%*%e\rQ \u0026lt;- diag(1,f,f)\rQ[1:f,1:f] \u0026lt;- H\rVAR(1)に関してもvar関数とパラメータの数値が一致することを確認済みです。いよいよカルマンフィルタを実行します。\n# カルマンフィルタを実行\rRR \u0026lt;- array(0,dim = c(ncol(z),ncol(z),nrow(z))) # RRは観測値の分散行列（相関はないと仮定）\rfor(i in 1:nrow(z)){\rmiss \u0026lt;- is.na(z[i,])\rR_temp \u0026lt;- diag(R)\rR_temp[miss] \u0026lt;- 1e+32 # 欠損値の分散は無限大にする\rRR[,,i] \u0026lt;- diag(R_temp)\r}\rzz \u0026lt;- z; zz[is.na(z)] \u0026lt;- 0 # 欠損値（NA）に0を代入（計算結果にはほとんど影響しない）。\ra_t \u0026lt;- matrix(0,nrow(zz),f) # a_tは状態変数の予測値\ra_tt \u0026lt;- matrix(0,nrow(zz),f) # a_ttは状態変数の更新後の値\ra_tt[1,] \u0026lt;- F_t[1,] # 状態変数の初期値には主成分分析で推計したファクターを使用\rsigma_t \u0026lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の予測値\rsigma_tt \u0026lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の更新値\rp \u0026lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))\rsigma_tt[,,1] \u0026lt;- matrix(p,3,3) # 状態変数の分散の初期値はVAR(1)の推計値から計算\ry_t \u0026lt;- matrix(0,nrow(zz),ncol(zz)) # y_tは観測値の予測値\rK_t \u0026lt;- array(0,dim = c(f,ncol(zz),nrow(zz))) # K_tはカルマンゲイン\rdata.m \u0026lt;- as.matrix(dataset2)\r# カルマンフィルタを実行\rfor (t in 2:nrow(zz)){\ra_t[t,] \u0026lt;- A_hat%*%a_tt[t-1,]\rsigma_t[,,t] \u0026lt;- A_hat%*%sigma_tt[,,t-1]%*%t(A_hat) + Q\ry_t[t,] \u0026lt;- as.vector(V%*%a_t[t,])\rS_t \u0026lt;- V%*%sigma_tt[,,t-1]%*%t(V)+RR[,,t]\rGG \u0026lt;- t(V)%*%diag(1/diag(RR[,,t]))%*%V\rSinv \u0026lt;- diag(1/diag(RR[,,t])) - diag(1/diag(RR[,,t]))%*%V%*%ginv(diag(nrow(A_hat))+sigma_t[,,t]%*%GG)%*%sigma_t[,,t]%*%t(V)%*%diag(1/diag(RR[,,t]))\rK_t[,,t] \u0026lt;- sigma_t[,,t]%*%t(V)%*%Sinv\ra_tt[t,] \u0026lt;- a_t[t,] + K_t[,,t]%*%(zz[t,]-y_t[t,])\rsigma_tt[,,t] \u0026lt;- sigma_t[,,t] - K_t[,,t]%*%V%*%sigma_tt[,,t-1]%*%t(V)%*%t(K_t[,,t])\r}\rF.xts \u0026lt;- xts(a_tt,order.by = as.Date(rownames(data.m)))\rplot.zoo(F.xts, col = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;yellow\u0026quot;,\u0026quot;purple\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 得られた推計値を時系列プロット\rカルマンフィルタにより推計したファクターの時系列プロットが以下です。遷移方程式がAR(1)だったからかかなり平準化された値となっています。\nでは、この得られたファクターをOLSにかけます。\n# 得られたファクターとGDPをOLSにかける\rF_q \u0026lt;- as.data.frame(a_tt[seq(3,nrow(a_tt),3),]) # 四半期の終わり月の値だけを引っ張ってくる colnames(F_q) \u0026lt;- c(\u0026quot;factor1\u0026quot;,\u0026quot;factor2\u0026quot;,\u0026quot;factor3\u0026quot;)\rcolnames(GDP) \u0026lt;- c(\u0026quot;publication\u0026quot;,\u0026quot;GDP\u0026quot;)\rt \u0026lt;- which(GDP$publication==\u0026quot;2012-04-01\u0026quot;)\rt2 \u0026lt;- which(GDP$publication==\u0026quot;2015-01-01\u0026quot;) # 2012-2q~2015-1qまでのデータが学習データ、それ以降がテストデータ\rGDP_q \u0026lt;- GDP[t:nrow(GDP),]\rdataset.q \u0026lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),])\rtest \u0026lt;- lm(GDP~factor1 + factor2 + factor3,data=dataset.q)\rsummary(test)\r## ## Call:\r## lm(formula = GDP ~ factor1 + factor2 + factor3, data = dataset.q)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1350.34 -361.67 -31.63 375.61 1105.07 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 125729.4 4134.2 30.412 1.07e-08 ***\r## factor1 -199.0 1651.3 -0.121 0.907 ## factor2 -1699.7 960.2 -1.770 0.120 ## factor3 -2097.6 3882.5 -0.540 0.606 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 897.5 on 7 degrees of freedom\r## Multiple R-squared: 0.783, Adjusted R-squared: 0.69 ## F-statistic: 8.419 on 3 and 7 DF, p-value: 0.0101\rout_of_sample \u0026lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成\rtest.pred \u0026lt;- predict(test, out_of_sample, interval=\u0026quot;prediction\u0026quot;)\rpred.GDP.xts \u0026lt;- xts(cbind(test.pred[,1],out_of_sample$GDP),order.by = out_of_sample$publication)\rplot.zoo(pred.GDP.xts,col = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 予測値と実績値を時系列プロット\rOLSの推計結果はfactor1（赤）とfactor2（青）が有意との結果。前回の投稿でも言及したように、factor1（赤）はリスクセンチメントを表していそうなので、係数の符号が負であることは頷ける。ただし、factor2（青）も符号が負なのではなぜなのか…。このファクターは生産年齢人口など経済の潜在能力を表していると思っていたのに。かなり謎。まあとりあえず予測に移りましょう。このモデルを使用したGDPの予測値と実績値の推移はいかのようになりました。直近の精度は悪くない？\nというか、これ完全に単位根の問題を無視してOLSしてしまっているな。ファクターもGDPも完全に単位根を持つけど念のため単位根検定をかけてみます。\nlibrary(tseries)\radf.test(F_q$factor1)\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor1\r## Dickey-Fuller = -2.8191, Lag order = 2, p-value = 0.2603\r## alternative hypothesis: stationary\radf.test(F_q$factor2)\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor2\r## Dickey-Fuller = -2.6749, Lag order = 2, p-value = 0.3153\r## alternative hypothesis: stationary\radf.test(F_q$factor3)\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor3\r## Dickey-Fuller = -2.8928, Lag order = 2, p-value = 0.2323\r## alternative hypothesis: stationary\radf.test(GDP_q$GDP)\r## ## Augmented Dickey-Fuller Test\r## ## data: GDP_q$GDP\r## Dickey-Fuller = 1.5034, Lag order = 3, p-value = 0.99\r## alternative hypothesis: stationary\rはい。全部単位根もってました…。階差をとったのち、単位根検定を行います。\nGDP_q \u0026lt;- GDP_q %\u0026gt;% mutate(growth.rate=(GDP/lag(GDP)-1)*100)\rF_q \u0026lt;- F_q %\u0026gt;% mutate(f1.growth.rate=(factor1/lag(factor1)-1)*100,\rf2.growth.rate=(factor2/lag(factor2)-1)*100,\rf3.growth.rate=(factor3/lag(factor3)-1)*100)\radf.test(GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)])\r## ## Augmented Dickey-Fuller Test\r## ## data: GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)]\r## Dickey-Fuller = -0.31545, Lag order = 3, p-value = 0.9838\r## alternative hypothesis: stationary\radf.test(F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)])\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)]\r## Dickey-Fuller = -2.7762, Lag order = 2, p-value = 0.2767\r## alternative hypothesis: stationary\radf.test(F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)])\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)]\r## Dickey-Fuller = -2.6156, Lag order = 2, p-value = 0.3379\r## alternative hypothesis: stationary\radf.test(F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)])\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)]\r## Dickey-Fuller = -2.9893, Lag order = 2, p-value = 0.1955\r## alternative hypothesis: stationary\rfactor1だけは5%有意水準で帰無仮説を棄却できない…。困りました。有意水準を10%ということにして、とりあえず階差でOLSしてみます。\ndataset.q \u0026lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),])\rcolnames(dataset.q) \u0026lt;- c(\u0026quot;publication\u0026quot;,\u0026quot;GDP\u0026quot;,\u0026quot;growth.rate\u0026quot;,\u0026quot;factor1\u0026quot;,\u0026quot;factor2\u0026quot;,\u0026quot;factor3\u0026quot;,\u0026quot;f1.growth.rate\u0026quot;,\u0026quot;f2.growth.rate\u0026quot;,\u0026quot;f3.growth.rate\u0026quot;)\rtest1 \u0026lt;- lm(growth.rate~f1.growth.rate + f2.growth.rate + f3.growth.rate,data=dataset.q)\rsummary(test1)\r## ## Call:\r## lm(formula = growth.rate ~ f1.growth.rate + f2.growth.rate + ## f3.growth.rate, data = dataset.q)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.6940 -0.2411 0.2041 0.4274 1.0904 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|)\r## (Intercept) 8.978e-02 4.588e-01 0.196 0.851\r## f1.growth.rate -6.353e-03 1.375e-02 -0.462 0.660\r## f2.growth.rate 6.155e-04 6.026e-03 0.102 0.922\r## f3.growth.rate -9.249e-05 5.152e-04 -0.180 0.863\r## ## Residual standard error: 0.956 on 6 degrees of freedom\r## (1 observation deleted due to missingness)\r## Multiple R-squared: 0.04055, Adjusted R-squared: -0.4392 ## F-statistic: 0.08452 on 3 and 6 DF, p-value: 0.966\r推計結果がわるくなりました…。予測値を計算し、実績値とプロットしてみます。\nout_of_sample1 \u0026lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成\rtest1.pred \u0026lt;- predict(test1, out_of_sample1, interval=\u0026quot;prediction\u0026quot;)\rpred1.GDP.xts \u0026lt;- xts(cbind(test1.pred[,1],out_of_sample1$growth.rate),order.by = out_of_sample1$publication)\rplot.zoo(pred1.GDP.xts,col = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 予測値と実績値を時系列プロット\rん～、これはやり直しですね。今日はここまでで勘弁してください…。\n\r","date":1531699200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1531699200,"objectID":"fbb0ce36043730cfcf2788a681f8758a","permalink":"/post/post6/","publishdate":"2018-07-16T00:00:00Z","relpermalink":"/post/post6/","section":"post","summary":"前回集めた経済データをGiannone et al (2008)のマルチファクターモデルで推定し、四半期GDPを予測したいと思います。","tags":["R","カルマンフィルタ"],"title":"Gianonne et. al. (2008)のマルチファクターモデルで四半期GDPを予想してみた","type":"post"},{"authors":null,"categories":["マクロ経済学","Webスクレイピング"],"content":"\r\r\r\r1. 先行研究と具体的にやりたいこと\r2. 統計ダッシュボードからのデータの収集\r3. 得られたデータを主成分分析にかけてみる\r\r\rおはこんばんちわ。\n最近、競馬ばっかりやってましたが、そろそろ本業のマクロの方もやらないとなということで今回は日次GDP推計に使用するデータを総務省が公開している統計ダッシュボードから取ってきました。\rそもそも、前の記事では四半期GDP速報の精度が低いことをモチベーションに高頻度データを用いてより精度の高い予測値をはじき出すモデルを作れないかというテーマで研究を進めていました。しかし、先行研究を進めていくうちに、どうやら大規模な経済指標を利用することで日次で四半期GDPの予測値を計算することが可能であることが判明しました。しかも、精度も良い(米国ですが)ということで、なんとかこの方向で研究を進めていけないかということになりました。\n1. 先行研究と具体的にやりたいこと\r先行研究\nGiannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標をいくつかのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しました。なお、ファクターの計算にはカルマンスムージングを用いています(詳しい推計方法は論文\u0026amp;Technical Appendixを参照)。理論的な定式化は無いのですが、なかなか当たります。そもそも私がこの研究に興味を持ったのは、以下の本を立ち読みした際に参考文献として出てきたからで、いよいよ運用機関などでも使用され始めるのかと思い、やっておこうと思った次第です。\n実践 金融データサイエンス 隠れた構造をあぶり出す6つのアプローチ\rとりあえずはGiannoneの日本版をやろうかなと思っています。実はこの後に、ファクターモデルとDSGEを組み合わせたモデルがありましてそこまで発展させたいなーなんて思っておりますが。とにかく、ファクターを計算するための経済統計が必要ですので、今回はそれを集めてきたというのがこの記事の趣旨です。\n\r2. 統計ダッシュボードからのデータの収集\r政府や日銀が公表しているデータの一部は統計ダッシュボードから落とすことができます。これは総務省統計局が提供しているもので、これまで利用しにくかった経済統計をより身近に使用してもらおうというのが一応のコンセプトとなっています。似たものに総務省統計局が提供しているestatがありますが、日銀の公表データがなかったり、メールアドレスの登録が必要だったりと非常に使い勝手が悪いです(個人的感想)。ただ、estatにはestatapiというRパッケージがあり、データを整形するのは比較的容易であると言えます。今回、統計ダッシュボードを選択した理由はそうは言っても日銀のデータがないのはダメだろうという理由で、データの整形に関しては関数を組みました。\rそもそも統計ダッシュボードは経済統計をグラフなどで見て楽しむ？ものですが、私のような研究をしたい者を対象にAPIを提供してくれています。取得できるデータは大きく分けて6つあります。\nやり方は簡単で、ベースのurlと欲しい統計のIDをGET関数で渡すことによって、データを取得することができます。公式にも以下のように書かれています。\n\r基本的な使い方としては、まず①「統計メタ情報（系列）取得」で取得したいデータの[系列コード]を検索し、 その後⑥「統計データ取得」で[系列コード]を検索条件に指定し、その系列の情報を取得します。\r（②③④⑤は補助的な情報として独立して取得できるようにしています。データのみ必要な場合は当該機能は不要です。）\r具体的な使い方は、以下の「WebAPIの詳細仕様」に記載する[ベースURL]に検索条件となる[パラメータ]を“\u0026amp;”で連結し、HTTPリクエスト（GET）を送信することで目的のデータを取得できます。\r各パラメータは「パラメータ名=値」のように名称と値を’=‘で結合し、複数のパラメータを指定する場合は「パラメータ名=値\u0026amp;パラメータ名=値\u0026amp;…」のようにそれぞれのパラメータ指定を’\u0026amp;’で結合してください。\rまた、パラメータ値は必ずURLエンコード(文字コードUTF-8)してから結合してください。\n\r今回も以下の文献を参考にデータを取ってきたいと思います。\nRによるスクレイピング入門\rまず、最初にこのAPIからデータを取得し、得られた結果を分析しやすいように整形する関数を定義したいと思います。\nlibrary(httr)\rlibrary(estatapi)\rlibrary(dplyr)\rlibrary(XML)\rlibrary(stringr)\rlibrary(xts)\rlibrary(GGally)\rlibrary(ggplot2)\rlibrary(seasonal)\rlibrary(dlm)\rlibrary(vars)\rlibrary(MASS)\r# 関数を定義\rget_dashboard \u0026lt;- function(ID){\rbase_url \u0026lt;- \u0026quot;https://dashboard.e-stat.go.jp/api/1.0/JsonStat/getData?\u0026quot;\rres \u0026lt;- GET(\rurl = base_url,\rquery = list(\rIndicatorCode=ID\r)\r)\rresult \u0026lt;- content(res)\rx \u0026lt;- result$link$item[[1]]$value\rx \u0026lt;- t(do.call(\u0026quot;data.frame\u0026quot;,x))\rdate_x \u0026lt;- result$link$item[[1]]$dimension$Time$category$label\rdate_x \u0026lt;- t(do.call(\u0026quot;data.frame\u0026quot;,date_x))\rdate_x \u0026lt;- str_replace_all(date_x, pattern=\u0026quot;年\u0026quot;, replacement=\u0026quot;/\u0026quot;)\rdate_x \u0026lt;- str_replace_all(date_x, pattern=\u0026quot;月\u0026quot;, replacement=\u0026quot;\u0026quot;)\rdate_x \u0026lt;- as.Date(gsub(\u0026quot;([0-9]+)/([0-9]+)\u0026quot;, \u0026quot;\\\\1/\\\\2/1\u0026quot;, date_x))\rdate_x \u0026lt;- as.Date(date_x, format = \u0026quot;%m/%d/%Y\u0026quot;)\rdate_x \u0026lt;- as.numeric(date_x)\rdate_x \u0026lt;- as.Date(date_x, origin=\u0026quot;1970-01-01\u0026quot;)\r#x \u0026lt;- cbind(x,date_x)\rx \u0026lt;- data.frame(x)\rx[,1] \u0026lt;- as.character(x[,1])%\u0026gt;%as.numeric(x[,1])\rcolnames(x) \u0026lt;- c(result$link$item[[1]]$label)\rx \u0026lt;- x %\u0026gt;% mutate(\u0026quot;publication\u0026quot; = date_x)\rreturn(x)\r}\rまずベースのurlを定義しています。今回はデータが欲しいので⑥統計データのベースurlを使用します（\r参考）。次にベースurlと統計ID（IndicatorCode）をGET関数で渡し、結果を取得しています。統計IDについてはエクセルファイルで公開されています。得られた結果の中身（リスト形式）をresultに格納し、リストの深層にある原数値データ（value）をxに格納します。原数値データもリスト形式なので、それをdo.callでデータフレームに変換しています。次に、データ日付を取得します。resultの中を深くたどるとTime→category→labelというデータがあり、そこに日付データが保存されているので、それをdate_xに格納し、同じようにデータフレームへ変換します。データの仕様上、日付は「yyyy年mm月」になっていますが、これだとRは日付データとして読み取ってくれないので、str_replace_all等で変換したのち、Date型に変換しています。列名にデータ名（result→link→item[[1]]→label）をつけ、データ日付をxに追加したら完成です。\rそのほか、data_connectという関数も定義しています。これはデータ系列によれば、たとえば推計方法の変更などで1980年～2005年の系列と2003年～2018年までの系列の2系列があるようなデータも存在し、この2系列を接続するための関数です。これは単純に接続しているだけなので、説明は省略します。\ndata_connect \u0026lt;- function(x){\ra \u0026lt;- min(which(x[,ncol(x)] != \u0026quot;NA\u0026quot;))\rb \u0026lt;- x[a,ncol(x)]/x[a,1]\rc \u0026lt;- x[1:a-1,1]*b\rreturn(c)\r}\rでは、実際にデータを取得していきます。今回取得するデータは月次データとなっています。これは統計dashboardが月次以下のデータがとれないからです。なので、例えば日経平均などは月末の終値を引っ張っています。ただし、GDPは四半期データとなっています。さきほど定義したget_dashboardの使用方法は簡単で、引数に統計ダッシュボードで公開されている統計IDを入力するだけでデータが取れます。今回使用するデータを以下の表にまとめました。\n# データを取得\rNikkei \u0026lt;- get_dashboard(\u0026quot;0702020501000010010\u0026quot;)\rcallrate \u0026lt;- get_dashboard(\u0026quot;0702020300000010010\u0026quot;)\rTOPIX \u0026lt;- get_dashboard(\u0026quot;0702020590000090010\u0026quot;)\rkikai \u0026lt;- get_dashboard(\u0026quot;0701030000000010010\u0026quot;)\rkigyo.bukka \u0026lt;- get_dashboard(\u0026quot;0703040300000090010\u0026quot;)\rmoney.stock1 \u0026lt;- get_dashboard(\u0026quot;0702010201000010030\u0026quot;)\rmoney.stock2 \u0026lt;- get_dashboard(\u0026quot;0702010202000010030\u0026quot;)\rmoney.stock \u0026lt;- dplyr::full_join(money.stock1,money.stock2,by=\u0026quot;publication\u0026quot;)\rc \u0026lt;- data_connect(money.stock)\ra \u0026lt;- min(which(money.stock[,ncol(money.stock)] != \u0026quot;NA\u0026quot;))\rmoney.stock[1:a-1,ncol(money.stock)] \u0026lt;- c\rmoney.stock \u0026lt;- money.stock[,c(2,3)]\rcpi \u0026lt;- get_dashboard(\u0026quot;0703010401010090010\u0026quot;)\rexport.price \u0026lt;- get_dashboard(\u0026quot;0703050301000090010\u0026quot;)\rimport.price \u0026lt;- get_dashboard(\u0026quot;0703060301000090010\u0026quot;)\rimport.price$`輸出物価指数（総平均）（円ベース）2015年基準` \u0026lt;- NULL\rpublic.expenditure1 \u0026lt;- get_dashboard(\u0026quot;0802020200000010010\u0026quot;)\rpublic.expenditure2 \u0026lt;- get_dashboard(\u0026quot;0802020201000010010\u0026quot;)\rpublic.expenditure \u0026lt;- dplyr::full_join(public.expenditure1,public.expenditure2,by=\u0026quot;publication\u0026quot;)\rc \u0026lt;- data_connect(public.expenditure)\ra \u0026lt;- min(which(public.expenditure[,ncol(public.expenditure)] != \u0026quot;NA\u0026quot;))\rpublic.expenditure[1:a-1,ncol(public.expenditure)] \u0026lt;- c\rpublic.expenditure \u0026lt;- public.expenditure[,c(2,3)]\rexport.service \u0026lt;- get_dashboard(\u0026quot;1601010101000010010\u0026quot;)\rworking.population \u0026lt;- get_dashboard(\u0026quot;0201010010000010020\u0026quot;)\ryukoukyuujinn \u0026lt;- get_dashboard(\u0026quot;0301020001000010010\u0026quot;)\rhours_worked \u0026lt;- get_dashboard(\u0026quot;0302010000000010000\u0026quot;)\rnominal.wage \u0026lt;- get_dashboard(\u0026quot;0302020000000010000\u0026quot;) iip \u0026lt;- get_dashboard(\u0026quot;0502070101000090010\u0026quot;)\rshukka.shisu \u0026lt;- get_dashboard(\u0026quot;0502070102000090010\u0026quot;)\rzaiko.shisu \u0026lt;- get_dashboard(\u0026quot;0502070103000090010\u0026quot;)\rsanji.sangyo \u0026lt;- get_dashboard(\u0026quot;0603100100000090010\u0026quot;)\rretail.sells \u0026lt;- get_dashboard(\u0026quot;0601010201010010000\u0026quot;)\rGDP1 \u0026lt;- get_dashboard(\u0026quot;0705020101000010000\u0026quot;)\rGDP2 \u0026lt;- get_dashboard(\u0026quot;0705020301000010000\u0026quot;)\rGDP \u0026lt;- dplyr::full_join(GDP1,GDP2,by=\u0026quot;publication\u0026quot;)\rc \u0026lt;- data_connect(GDP)\ra \u0026lt;- min(which(GDP[,ncol(GDP)] != \u0026quot;NA\u0026quot;))\rGDP[1:a-1,ncol(GDP)] \u0026lt;- c\rGDP \u0026lt;- GDP[,c(2,3)]\ryen \u0026lt;- get_dashboard(\u0026quot;0702020401000010010\u0026quot;)\rhousehold.consumption \u0026lt;- get_dashboard(\u0026quot;0704010101000010001\u0026quot;)\rJGB10y \u0026lt;- get_dashboard(\u0026quot;0702020300000010020\u0026quot;)\r今取得したデータは原数値系列のデータが多いので、それらは季節調整をかけます。なぜ季節調整済みのデータを取得しないのかというとそれらのデータは何故か極端にサンプル期間が短くなってしまうからです。ここらへんは使い勝手が悪いです。\n# 季節調整をかける\rSys.setenv(X13_PATH = \u0026quot;C:\\\\Program Files\\\\WinX13\\\\x13as\u0026quot;)\rcheckX13()\rseasoning \u0026lt;- function(data,i,start.y,start.m){\rtimeseries \u0026lt;- ts(data[,i],frequency = 12,start=c(start.y,start.m))\rm \u0026lt;- seas(timeseries)\rsummary(m$data)\rreturn(m$series$s11)\r}\rk \u0026lt;- seasoning(kikai,1,2005,4)\rkikai$`機械受注額（船舶・電力を除く民需）` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(kigyo.bukka,1,1960,1)\rkigyo.bukka$`国内企業物価指数（総平均）2015年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(cpi,1,1970,1)\rcpi$`消費者物価指数（生鮮食品を除く総合）2015年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(export.price,1,1960,1)\rexport.price$`輸出物価指数（総平均）（円ベース）2015年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(import.price,1,1960,1)\rimport.price$`輸入物価指数（総平均）（円ベース）2015年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(public.expenditure,2,2004,4)\rpublic.expenditure$公共工事受注額 \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(export.service,1,1996,1)\rexport.service$`貿易・サービス収支` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(yukoukyuujinn,1,1963,1)\ryukoukyuujinn$有効求人倍率 \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(hours_worked,1,1990,1)\rhours_worked$総実労働時間 \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(nominal.wage,1,1990,1)\rnominal.wage$現金給与総額 \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(iip,1,1978,1)\riip$`鉱工業生産指数　2010年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(shukka.shisu,1,1990,1)\rshukka.shisu$`鉱工業出荷指数　2010年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(zaiko.shisu,1,1990,1)\rzaiko.shisu$`鉱工業在庫指数　2010年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(sanji.sangyo,1,1988,1)\rsanji.sangyo$`第３次産業活動指数　2010年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(retail.sells,1,1980,1)\rretail.sells$`小売業販売額（名目）` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(household.consumption,1,2010,1)\rhousehold.consumption$`二人以上の世帯　消費支出（除く住居等）` \u0026lt;- as.numeric(k)\rGDP.ts \u0026lt;- ts(GDP[,2],frequency = 4,start=c(1980,1))\rm \u0026lt;- seas(GDP.ts)\rGDP$`国内総生産（支出側）（実質）2011年基準` \u0026lt;- m$series$s11\rここでは詳しく季節調整のかけ方は説明しません。x13arimaを使用しています。上述のコードを回す際はx13arimaがインストールされている必要があります。以下の記事を参考にしてください。\n[http://sinhrks.hatenablog.com/entry/2014/11/09/003632]\nでは、データ日付を基準に落としてきたデータを結合し、データセットを作成します。\n# データセットに結合\rdataset \u0026lt;- dplyr::full_join(kigyo.bukka,callrate,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,kikai,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,Nikkei,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,money.stock,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,cpi,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,export.price,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,import.price,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,public.expenditure,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,export.service,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,working.population,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,yukoukyuujinn,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,hours_worked,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,nominal.wage,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,iip,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,shukka.shisu,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,zaiko.shisu,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,sanji.sangyo,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,retail.sells,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,yen,by=\u0026quot;publication\u0026quot;)\rcolnames(dataset) \u0026lt;- c(\u0026quot;DCGPI\u0026quot;,\u0026quot;publication\u0026quot;,\u0026quot;callrate\u0026quot;,\u0026quot;Machinery_Orders\u0026quot;,\r\u0026quot;Nikkei225\u0026quot;,\u0026quot;money_stock\u0026quot;,\u0026quot;CPI\u0026quot;,\u0026quot;export_price\u0026quot;,\r\u0026quot;import_price\u0026quot;,\u0026quot;public_works_order\u0026quot;,\r\u0026quot;trade_service\u0026quot;,\u0026quot;working_population\u0026quot;,\r\u0026quot;active_opening_ratio\u0026quot;,\u0026quot;hours_worked\u0026quot;,\r\u0026quot;wage\u0026quot;,\u0026quot;iip_production\u0026quot;,\u0026quot;iip_shipment\u0026quot;,\u0026quot;iip_inventory\u0026quot;,\r\u0026quot;ITIA\u0026quot;,\u0026quot;retail_sales\u0026quot;,\u0026quot;yen\u0026quot;)\r最後に列名をつけています。datasetはそれぞれのデータの公表開始時期が異なるために大量のNAを含むデータフレームとなっているので、NAを削除するために最もデータの開始時期が遅い機械受注統計に合わせてデータセットを再構築します。\na \u0026lt;- min(which(dataset$Machinery_Orders != \u0026quot;NA\u0026quot;))\rdataset1 \u0026lt;- dataset[a:nrow(dataset),]\rdataset1 \u0026lt;- na.omit(dataset1)\rrownames(dataset1) \u0026lt;- dataset1$publication\rdataset1 \u0026lt;- dataset1[,-2]\rdataset1.xts \u0026lt;- xts(dataset1,order.by = as.Date(rownames(dataset1)))\rこれでとりあえずデータの収集は終わりました。\n\r3. 得られたデータを主成分分析にかけてみる\r本格的な分析はまた今後にしたいのですが、データを集めるだけでは面白くないので、Gianonneらのように主成分分析を行いたいと思います。主成分分析をこれまでに学んだことのない方は以下を参考にしてください。個人的にはわかりやすいと思っています。\n主成分分析(Principal Component Analysis, PCA)～データセットの見える化・可視化といったらまずはこれ！～\nでは主成分分析を実行してみます。Rではprincomp関数を使用することで非常に簡単に主成分分析を行うことができます。\n# 主成分分析を実行\rfactor.pca \u0026lt;- princomp(~.,cor = TRUE,data = dataset1) # cor = TRUEでデータの基準化を自動で行ってくれる。\rsummary(factor.pca)\r## Importance of components:\r## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\r## Standard deviation 3.2538097 1.8606047 1.5142917 1.05061250 0.88155352\r## Proportion of Variance 0.5293639 0.1730925 0.1146540 0.05518933 0.03885683\r## Cumulative Proportion 0.5293639 0.7024563 0.8171103 0.87229965 0.91115648\r## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10\r## Standard deviation 0.7504599 0.63476742 0.48794520 0.413374289 0.358909911\r## Proportion of Variance 0.0281595 0.02014648 0.01190453 0.008543915 0.006440816\r## Cumulative Proportion 0.9393160 0.95946246 0.97136699 0.979910904 0.986351721\r## Comp.11 Comp.12 Comp.13 Comp.14\r## Standard deviation 0.296125594 0.254294037 0.233119328 0.1394055697\r## Proportion of Variance 0.004384518 0.003233273 0.002717231 0.0009716956\r## Cumulative Proportion 0.990736239 0.993969512 0.996686743 0.9976584386\r## Comp.15 Comp.16 Comp.17 Comp.18\r## Standard deviation 0.1356026290 0.1104356585 0.0861468897 0.070612034\r## Proportion of Variance 0.0009194036 0.0006098017 0.0003710643 0.000249303\r## Cumulative Proportion 0.9985778423 0.9991876440 0.9995587083 0.999808011\r## Comp.19 Comp.20\r## Standard deviation 0.053565104 3.115371e-02\r## Proportion of Variance 0.000143461 4.852767e-05\r## Cumulative Proportion 0.999951472 1.000000e+00\rscreeplot(factor.pca)\rpc \u0026lt;- predict(factor.pca,dataset1)[,1:3] # 主成分を計算\rpc.xts \u0026lt;- xts(pc,order.by = as.Date(rownames(dataset1)))\rplot.zoo(pc.xts,col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;purple\u0026quot;,\u0026quot;yellow\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 主成分を時系列プロット\r第3主成分まででデータの約80％が説明できる結果を得たので、第3主成分までのプロットをお見せします。第1主成分（赤）はリーマンショックや東日本大震災、消費税増税のあたりで急上昇しています。ゆえに経済全体のリスクセンチメントを表しているのではないかと思っています。第2主成分（青）と第3主成分（緑）はリーマンショックのあたりで大きく落ち込んでいることは共通していますが2015年～現在の動きが大きく異なっています。また、第2主成分（青）はサンプル期間を通して過去トレンドを持つことから日本経済の潜在能力のようなものを表しているのではないでしょうか（そうするとリーマンショックまで上昇傾向にあることが疑問なのですが）。第3主成分（緑）はいまだ解読不能です（物価＆為替動向を表しているのではないかと思っています）。とりあえず今日はこれまで。次回はGianonne et. al.(2008)の日本版の再現を行いたいと思います。\n\r","date":1531526400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1531526400,"objectID":"4d722651b532773776680a7d0533fd3f","permalink":"/post/post7/","publishdate":"2018-07-14T00:00:00Z","relpermalink":"/post/post7/","section":"post","summary":"Rでデータ集めをします。データ分析はデータ集めと前処理が7割を占めるといわれる中、データ集めを自動化すべくウェブスクレイピングを行いました。","tags":["R","API"],"title":"日次GDP推計に使用する経済統計を統計ダッシュボードから集めてみた","type":"post"},{"authors":null,"categories":["競馬","Webスクレイピング"],"content":"\r\r\r\r1. Rvestとは\r2. レース結果をスクレイピングしてみる\r\r\rみなさん、おはこんばんにちは。\n競馬のレース結果を的中させるモデルを作ろうということで研究をはじめましたが、まずはデータを自分で取ってくるところからやろうとおもいます。どこからデータを取ってくるのかという点が重要になるわけですが、データ先としてはdatascisotistさんがまとめられた非常にわかりやすい記事があります。どこからデータが取れるのかというと大きく分けて二つで、①JRA提供のJRA-VANや電子競馬新聞でおなじみの？JRJDといったデータベース、②netkeiba、yahoo競馬とといった競馬情報サイト、となっています。②の場合は自分でコードを書き、クローリングを行う必要があります。今回は②を選択し、yahoo競馬のデータをクローリングで落としてきたいと思います。Rでクローリングを行うパッケージとしては、rvest, httr, XMLがありますが、今回は1番簡単に使えるrvestを用います。yahoo競馬では以下のように各レース結果が表にまとめられています（5月の日本ダービーの結果）。\nyahoo競馬\n各馬のざっくりとした特徴やレース結果（通過順位等含む）、オッズが掲載されています。とりあえず、このぐらい情報があれば良いのではないかと思います（オッズの情報はもう少し欲しいのですが）。ただ、今後は少しずつ必要になった情報を拡充していこうとも思っています。1986年までのレース結果が格納されており、全データ数は50万件を超えるのではないかと思っています。ただ、単勝オッズが利用できるのは1994年からのようなので今回は1994年から直近までのデータを落としてきます。今回のゴールは、このデータをcsvファイル or SQLに格納することです。\n1. Rvestとは\rRvestとは、webスクレイピングパッケージの一種でdplyrでおなじみのHadley Wickhamさんによって作成されたパッケージです。たった数行でwebスクレイピングができる優れものとなっており、操作が非常に簡単であるのが特徴です。今回は以下の本を参考にしました。\nRによるスクレイピング入門\rそもそも、htmlも大学1年生にやった程度でほとんど忘れていたのですが、この本はそこも非常にわかりやすく解説されており、非常に実践的な本だと思います。\n\r2. レース結果をスクレイピングしてみる\r実際にyahoo競馬からデータを落としてみたいと思います。コードは以下のようになっています。ご留意頂きたいのはこのコードをそのまま使用してスクレイピングを行うことはご遠慮いただきたいという事です。webスクレイピングは高速でサイトにアクセスするため、サイトへの負荷が大きくなる可能性があります。スクレイピングを行う際は、時間を空けるコーディングするなどその点に留意をして行ってください（最悪訴えられる可能性がありますが、こちらは一切の責任を取りません）。\n# rvestによる競馬データのwebスクレイピング\r#install.packages(\u0026quot;rvest\u0026quot;)\r#if (!require(\u0026quot;pacman\u0026quot;)) install.packages(\u0026quot;pacman\u0026quot;)\rpacman::p_load(qdapRegex)\rlibrary(rvest)\rlibrary(stringr)\rlibrary(dplyr)\r使用するパッケージはqdapRegex、rvest、stringr、dplyrです。qdapRegexはカッコ内の文字を取り出すために使用しています。\nkeiba.yahoo \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp/schedule/list/2016/?month=\u0026quot;,k))\rrace_url \u0026lt;- keiba.yahoo %\u0026gt;%\rhtml_nodes(\u0026quot;a\u0026quot;) %\u0026gt;%\rhtml_attr(\u0026quot;href\u0026quot;) # 全urlを取得\r# レース結果のをurlを取得\rrace_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026quot;result\u0026quot;)==1] # 「result」が含まれるurlを抽出\rまず、read_htmlでyahoo競馬のレース結果一覧のhtml構造を引っ張ってきます（リンクは2016年1月の全レース）。ここで、kと出ているのは月を表し、k=1であれば2016年1月のレース結果を引っ張ってくるということです。keiba.yahooを覗いてみると以下のようにそのページ全体のhtml構造が格納されているのが分かります。\nkeiba.yahoo\r## $node\r## \u0026lt;pointer: (nil)\u0026gt;\r## ## $doc\r## \u0026lt;pointer: (nil)\u0026gt;\r## ## attr(,\u0026quot;class\u0026quot;)\r## [1] \u0026quot;xml_document\u0026quot; \u0026quot;xml_node\u0026quot;\rrace_urlにはyahoo.keibaのうちの2016年k月にあった全レース結果のリンクを格納しています。html_nodeとはhtml構造のうちどの要素を引っ張るかを指定し、それを引っ張る関数で、簡単に言えばほしいデータの住所を入力する関数であると認識しています（おそらく正しくない）。ここではa要素を引っ張ることにしています。注意すべきことは、html_nodeは欲しい情報をhtml形式で引っ張ることです。なので、テキストデータとしてリンクを保存するためにはhtml_attrを使用する必要があります。html_attrの引数として、リンク属性を表すhrefを渡しています。これでレース結果のurlが取れたと思いきや、実はこれでは他のリンクもとってしまっています。一番わかりやすいのが広告のリンクです。こういったリンクは除外する必要があります。レース結果のurlには“result”が含まれているので、この文字が入っている要素だけを抽出したのが一番最後のコードです。\nfor (i in 1:length(race_url)){\rrace1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,race_url[i])) # レース結果のurlを取得\r# レース結果をスクレイピング\rrace_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_table()\rrace_result \u0026lt;- do.call(\u0026quot;data.frame\u0026quot;,race_result) # リストをデータフレームに変更\rcolnames(race_result) \u0026lt;- c(\u0026quot;order\u0026quot;,\u0026quot;frame_number\u0026quot;,\u0026quot;horse_number\u0026quot;,\u0026quot;horse_name/age\u0026quot;,\u0026quot;time/margin\u0026quot;,\u0026quot;passing_rank/last_3F\u0026quot;,\u0026quot;jockey/weight\u0026quot;,\u0026quot;popularity/odds\u0026quot;,\u0026quot;trainer\u0026quot;) #　列名変更\rさて、いよいよレース結果のスクレイピングを行います。さきほど取得したリンク先のhtml構造を一つ一つ取得し、その中で必要なテキスト情報を引っ張るという作業をRに実行させます（なのでループを使う）。race_1にはあるレース結果ページのhtml構造が格納されおり、race_resultにはその結果が入っています。html_nodesの引数に入っているxpathですが、これはXLMフォーマットのドキュメントから効率的に要素を抜き出す言語です。先ほど説明した住所のようなものと思っていただければ良いと思います。その横に書いてある//table[@id = 'raceScore']が住所です。これはwebブラウザから簡単に探すことができます。Firefoxの説明になりますが、ほかのブラウザでも同じような機能があると思います。スクレイプしたい画面でCtrl+Shift+Cを押すと下のような画面が表示されます。\nこのインスペクターの横のマークをクリックすると、カーソルで指した部分のhtml構造（住所）が表示されます。この場合だと、レース結果はtable属性のidがraceScoreの場所に格納されていることが分かります。なので、上のコードではxpath=のところにそれを記述しているのです。そして、レース結果は表（table）形式でドキュメント化されているので、html_tableでごっそりとスクレイプしました。基本的にリスト形式で返されるので、それをデータフレームに変換し、適当に列名をつけています。\n# 通過順位と上り3Fのタイム\rrace_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- race_result[-6]\r# タイムと着差\rrace_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026quot;)))\rrace_result \u0026lt;- race_result[-5]\r# 馬名、馬齢、馬体重\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;[ァ-ヴー・]+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\d{3}\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026quot;)))\rrace_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\r# ジョッキー\rrace_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026quot;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026quot;)))\rrace_result \u0026lt;- race_result[-4]\r# オッズと人気\rrace_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\(.+\\\\)\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\d+[^(\\\\d+.\\\\d)]\u0026quot;)))\rrace_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\rここまででデータは取得できたわけなのですが、そのデータは綺麗なものにはなっていません。 上のコードでは、その整形作業を行っています。現在、取得したデータは以下のようになっています。\nhead(race_result)\r{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"order\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"frame_number\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"horse_number\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"trainer\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"passing_rank\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"last_3F\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"time\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"margin\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_name\"],\"name\":[9],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_age\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight\"],\"name\":[11],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight_change\"],\"name\":[12],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"jockey\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"odds\"],\"name\":[14],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"popularity\"],\"name\":[15],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_date\"],\"name\":[16],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_name\"],\"name\":[17],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"2\",\"3\":\"2\",\"4\":\"河野 通文\",\"5\":\"06-07-06-06\",\"6\":\"35.1\",\"7\":\"3.19.7\",\"8\":\"character(0)\",\"9\":\"センゴクシルバー\",\"10\":\"牡6\",\"11\":\"478\",\"12\":\"+2\",\"13\":\"田中 勝春\",\"14\":\"2.3\",\"15\":\"1\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"1\"},{\"1\":\"2\",\"2\":\"6\",\"3\":\"9\",\"4\":\"武 邦彦\",\"5\":\"06-05-04-04\",\"6\":\"35.7\",\"7\":\"3.19.9\",\"8\":\"1 1/4馬身\",\"9\":\"ジャムシード\",\"10\":\"牡6\",\"11\":\"480\",\"12\":\"0\",\"13\":\"柴田 政人\",\"14\":\"14\",\"15\":\"7\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"2\"},{\"1\":\"3\",\"2\":\"7\",\"3\":\"10\",\"4\":\"白井 寿昭\",\"5\":\"08-07-06-06\",\"6\":\"35.7\",\"7\":\"3.20.1\",\"8\":\"1 1/2馬身\",\"9\":\"ホクセツギンガ\",\"10\":\"牡6\",\"11\":\"500\",\"12\":\"+4\",\"13\":\"小屋敷 昭\",\"14\":\"7.5\",\"15\":\"3\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"3\"},{\"1\":\"4\",\"2\":\"7\",\"3\":\"11\",\"4\":\"矢野 進\",\"5\":\"03-03-03-02\",\"6\":\"36.3\",\"7\":\"3.20.4\",\"8\":\"1 3/4馬身\",\"9\":\"サマーワイン\",\"10\":\"牝5\",\"11\":\"422\",\"12\":\"-4\",\"13\":\"木幡 初広\",\"14\":\"32.6\",\"15\":\"9\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"1\",\"3\":\"1\",\"4\":\"秋山 史郎\",\"5\":\"12-12-12-12\",\"6\":\"35.6\",\"7\":\"3.20.5\",\"8\":\"1/2馬身\",\"9\":\"ダイワジェームス\",\"10\":\"牡6\",\"11\":\"472\",\"12\":\"+6\",\"13\":\"大塚 栄三郎\",\"14\":\"5.9\",\"15\":\"2\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"5\"},{\"1\":\"6\",\"2\":\"5\",\"3\":\"7\",\"4\":\"須貝 彦三\",\"5\":\"11-10-06-06\",\"6\":\"36.1\",\"7\":\"3.20.5\",\"8\":\"ハナ\",\"9\":\"シゲノランボー\",\"10\":\"牡7\",\"11\":\"438\",\"12\":\"-6\",\"13\":\"須貝 尚介\",\"14\":\"8.7\",\"15\":\"4\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rご覧のように、\\nが入っていたり、通過順位と上り3ハロンのタイムが一つのセルに入っていたりとこのままでは分析ができません。不要なものを取り除いたり、データを二つに分割する作業が必要になります。今回の記事ではこの部分について詳しくは説明しません。この部分は正規表現を駆使する必要がありますが、私自身全く詳しくないからです。今回も手探りでやりました。\n# レース情報\rrace_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026quot;\\\\d+年\\\\d+月\\\\d+日\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;)))\r# ファイル格納\rif (k ==1 \u0026amp;\u0026amp; i == 1){\rdataset \u0026lt;- race_result\r} else {\rdataset \u0026lt;- rbind(dataset,race_result)\r}# if文の終わり\r} # iループの終わり\rwrite.csv(race_result,\u0026quot;race_result.csv\u0026quot;)\r最後に、レース日時とレース名を抜き出し、データを一時的に格納するコードとcsvファイルに書き出すコードを書いて終了です。完成データセットは以下のような状態になっています。\nhead(dataset)\r{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"order\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"frame_number\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"horse_number\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"trainer\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"passing_rank\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"last_3F\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"time\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"margin\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_name\"],\"name\":[9],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_age\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight\"],\"name\":[11],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight_change\"],\"name\":[12],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"jockey\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"odds\"],\"name\":[14],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"popularity\"],\"name\":[15],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_date\"],\"name\":[16],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_name\"],\"name\":[17],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"8\",\"3\":\"11\",\"4\":\"森安 弘昭\",\"5\":\"01-01-01-01\",\"6\":\"35.7\",\"7\":\"2.00.7\",\"8\":\"character(0)\",\"9\":\"ヒダカハヤト\",\"10\":\"牡8\",\"11\":\"488\",\"12\":\"-2\",\"13\":\"大塚 栄三郎\",\"14\":\"29\",\"15\":\"10\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"1\"},{\"1\":\"2\",\"2\":\"5\",\"3\":\"6\",\"4\":\"矢野 進\",\"5\":\"06-06-04-03\",\"6\":\"35.3\",\"7\":\"2.00.8\",\"8\":\"3/4馬身\",\"9\":\"ステージチャンプ\",\"10\":\"牡5\",\"11\":\"462\",\"12\":\"+14\",\"13\":\"岡部 幸雄\",\"14\":\"3.2\",\"15\":\"1\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"2\"},{\"1\":\"3\",\"2\":\"4\",\"3\":\"4\",\"4\":\"新関 力\",\"5\":\"03-03-02-02\",\"6\":\"35.8\",\"7\":\"2.01.1\",\"8\":\"1 3/4馬身\",\"9\":\"マキノトウショウ\",\"10\":\"牡5\",\"11\":\"502\",\"12\":\"+6\",\"13\":\"的場 均\",\"14\":\"6.9\",\"15\":\"4\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"3\"},{\"1\":\"4\",\"2\":\"8\",\"3\":\"12\",\"4\":\"大和田 稔\",\"5\":\"02-02-03-03\",\"6\":\"35.7\",\"7\":\"2.01.1\",\"8\":\"ハナ\",\"9\":\"ペガサス\",\"10\":\"牡5\",\"11\":\"464\",\"12\":\"+4\",\"13\":\"安田 富男\",\"14\":\"5.7\",\"15\":\"2\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"7\",\"3\":\"9\",\"4\":\"田中 和夫\",\"5\":\"07-07-07-05\",\"6\":\"35.8\",\"7\":\"2.01.6\",\"8\":\"3馬身\",\"9\":\"シャマードシンボリ\",\"10\":\"牡7\",\"11\":\"520\",\"12\":\"+4\",\"13\":\"田中 剛\",\"14\":\"29.6\",\"15\":\"12\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"5\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"3\",\"4\":\"中島 敏文\",\"5\":\"09-10-10-09\",\"6\":\"35.5\",\"7\":\"2.01.7\",\"8\":\"3/4馬身\",\"9\":\"モンタミール\",\"10\":\"牡7\",\"11\":\"474\",\"12\":\"+4\",\"13\":\"蓑田 早人\",\"14\":\"10.4\",\"15\":\"6\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r以上です。次回はこのデータセットを使用して、分析を行っていきます。次回までには1994年からのデータを全てスクレイピングしてきます。\n【追記（2018/6/10）】\n上述したスクリプトを用いて、スクレイピングを行ったところエラーが出ました。どうやらレース結果の中には強風などで中止になったものも含まれているらしく、そこでエラーが出る様子（race_resultがcharacter(0)になってしまう）。なので、この部分を修正したスクリプトを以下で公開しておきます。こちらは私の PC環境では正常に作動しています。\n# rvestによる競馬データのwebスクレイピング\r#install.packages(\u0026quot;rvest\u0026quot;)\r#if (!require(\u0026quot;pacman\u0026quot;)) install.packages(\u0026quot;pacman\u0026quot;)\rinstall.packages(\u0026quot;beepr\u0026quot;)\rpacman::p_load(qdapRegex)\rlibrary(rvest)\rlibrary(stringr)\rlibrary(dplyr)\rlibrary(beepr)\r# pathの設定\rsetwd(\u0026quot;C:/Users/assiy/Dropbox/競馬統計解析\u0026quot;)\rfor(year in 1994:2018){\r# yahoo競馬のレース結果一覧ページの取得\rfor (k in 1:12){\rkeiba.yahoo \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp/schedule/list/\u0026quot;, year,\u0026quot;/?month=\u0026quot;,k))\rrace_url \u0026lt;- keiba.yahoo %\u0026gt;%\rhtml_nodes(\u0026quot;a\u0026quot;) %\u0026gt;%\rhtml_attr(\u0026quot;href\u0026quot;) # 全urlを取得\r# レース結果のをurlを取得\rrace_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026quot;result\u0026quot;)==1] # 「result」が含まれるurlを抽出\rfor (i in 1:length(race_url)){\rSys.sleep(10)\rprint(str_c(\u0026quot;現在、\u0026quot;, year, \u0026quot;年\u0026quot;, k, \u0026quot;月\u0026quot;, i,\u0026quot;番目のレースの保存中です\u0026quot;))\rrace1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,race_url[i])) # レース結果のurlを取得\r# レースが中止でなければ処理を実行\rif (identical(race1 %\u0026gt;%\rhtml_nodes(xpath = \u0026quot;//div[@class = \u0026#39;resultAtt mgnBL fntSS\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text(),character(0)) == TRUE){\r# レース結果をスクレイピング\rrace_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_table()\rrace_result \u0026lt;- do.call(\u0026quot;data.frame\u0026quot;,race_result) # リストをデータフレームに変更\rcolnames(race_result) \u0026lt;- c(\u0026quot;order\u0026quot;,\u0026quot;frame_number\u0026quot;,\u0026quot;horse_number\u0026quot;,\u0026quot;horse_name/age\u0026quot;,\u0026quot;time/margin\u0026quot;,\u0026quot;passing_rank/last_3F\u0026quot;,\u0026quot;jockey/weight\u0026quot;,\u0026quot;popularity/odds\u0026quot;,\u0026quot;trainer\u0026quot;) #　列名変更\r# 通過順位と上り3Fのタイム\rrace_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- race_result[-6]\r# タイムと着差\rrace_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026quot;)))\rrace_result \u0026lt;- race_result[-5]\r# 馬名、馬齢、馬体重\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;[ァ-ヴー・]+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\d{3}\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026quot;)))\rrace_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\r# ジョッキー\rrace_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026quot;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026quot;)))\rrace_result \u0026lt;- race_result[-4]\r# オッズと人気\rrace_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\(.+\\\\)\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\d+[^(\\\\d+.\\\\d)]\u0026quot;)))\rrace_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\r# レース情報\rrace_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026quot;\\\\d+年\\\\d+月\\\\d+日\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;)))\r## ファイル貯めるのかく\rif (k == 1 \u0026amp;\u0026amp; i == 1 \u0026amp;\u0026amp; year == 1994){\rdataset \u0026lt;- race_result\r} else {\rdataset \u0026lt;- rbind(dataset,race_result)\r} # if文2の終わり\r} # if文1の終わり\r} # iループの終わり\r} # kループの終わり\rbeep()\r} # yearループの終わり\rwrite.csv(dataset,\u0026quot;race_result.csv\u0026quot;, row.names = FALSE)\rこれを回すのに16時間かかりました（笑）データ数は想定していたよりは少なく、97939になりました。\n\r","date":1526688000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1528588800,"objectID":"e816798bf62f83b8a57cccf9397181f5","permalink":"/post/post9/","publishdate":"2018-05-19T00:00:00Z","relpermalink":"/post/post9/","section":"post","summary":"今、競馬×データサイエンスが熱いです。ウマナリティクスなるものがあり、これまでのレース結果からなんらかのモデルを作成し、順位予想や回収率を高める馬券購入方法を考えようとする人が一定数いるようです。今回は競馬をデータ解析するためのデータを取得します。rvestを用いて、ごりごりにクローリングを行いました。","tags":["前処理","R"],"title":"rvestでyahoo競馬にある過去のレース結果をクローリングしてみた","type":"post"},{"authors":null,"categories":[],"content":"\r\r\rどうもはじめまして。\r東京にある資産運用会社に勤める新卒1年目の葦原彩人と申します。\n簡単に自己紹介をしたいと思います。\r大学院卒の24歳です。\r専攻はマクロ経済学で特にDSGEモデル、状態空間モデル、カルマンフィルタ、ベイズ推定、MCMCなんかをやっていました。\r指導教官の研究サポートとして自然言語処理をやったこともあります。\rもともと学者志望でしたが金銭的な問題で就職することになりました。\r今は営業サポートの下働きをしています。\nこのブログは研究への興味関心を捨てることができない私が趣味として研究を進めていく際の備忘録という位置付けになるのだと思います。\n現在進めている研究は以下の２つです。\n馬券版ファクターモデル\r四半期GDP予測モデル\r\r1は、馬券市場が株式市場よりも投機的に魅力のある市場であるという観点から、回収率100%超えを目指す馬券ポートフォリオを構築するモデルを作成できないかというものです。株式にはファーマフレンチのようなファクターモデルがありますが、それを応用して馬券版ファクターモデルなるものを作れないかと思っています。\n2は最近の四半期GDP速報の精度が低いという問題意識から、精度の高い予測モデルを新たに構築できないかというものです。特にこれまでのようなマクロ経済理論に基づいたモデルではなく、機械学習を用い、通常マクロ経済学の研究で使用しないようなデータを織り込んだモデルを作成する方向で研究を進めていきたいと思っています。\nまだ、研究は始めたばかりなのですが、\r興味を持ってもらえるように頑張りたいと思います…\rでは、最初はこの辺で。\nよろしくお願いします。\n","date":1526601600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1526601600,"objectID":"ceb64c07c657dc4dad3f41bf7df6c579","permalink":"/post/post4/","publishdate":"2018-05-18T00:00:00Z","relpermalink":"/post/post4/","section":"post","summary":"どうもはじめまして。このブログを始めるに当たってまずは自己紹介をしたいと思います。","tags":[],"title":"はじめまして","type":"post"},{"authors":["Ayato Ashihara"],"categories":null,"content":" Click the Cite button above to get Bibtex and Cite ME !!   ","date":1522454400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1522454400,"objectID":"8eb0b83c007669e1fcaec626e70937a6","permalink":"/publication/%E7%B4%80%E8%A6%812019/","publishdate":"2018-03-31T00:00:00Z","relpermalink":"/publication/%E7%B4%80%E8%A6%812019/","section":"publication","summary":"Japan  has  experienced  a  long-lasting  stagnation  since  the  early  1990s.  According  to  the  Reference Dates of Business Cycle, the Cabinet Office of Japan, there are four recession periods between 1987 and 2010,  and  three  of  them  are  considered  to  be  financially-related.  This  implies  that  the  stagnation  wastriggered by financial factors. Nonetheless, many studies using Dynamic Stochastic General Equilibrium models claim that a decline in Total Factor Productivity is the main driver of the stagnation. To resolve this contradiction,  this study estimates the Japanese economy by a New-Keynesian  DSGE  model augmented with  financial  friction  used  in  Christiano,  Motto  and  Rostagno  (2014),  where “risk  shock”  is newly incorporated into the model that refers to uncertainty in the financial market. According to our estimationresults, the estimated risk shock can explain the overall fluctuations of GDP and investment, and thus it isconsidered to be the main driver of the stagnation. We also find that it is highly correlated with the Business condition  Diffusion  Index,  the  Financial  Position  Diffusion  Index  and  the  Lending  Attitude  Index  of  Financial Institutions in Tankan released by the Bank of Japan. Therefore, we conclude that the estimated risk  shock  can  be  interpreted  as  the  firms’   distrust  toward  their  business  conditions,  and  it  delayed  theirinvestment decisions, then causing the prolonged economic contraction.","tags":"","title":"Does Financial Risk Explain Japan’s Great Stagnation?","type":"publication"},{"authors":["Ayato Ashihara"],"categories":null,"content":" Click the Cite button above to get Bibtex and Cite ME !!   ","date":1488844800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1488844800,"objectID":"e1ffa3282e6f7fc6edad66690c0b404c","permalink":"/publication/ael2018/","publishdate":"2017-03-07T00:00:00Z","relpermalink":"/publication/ael2018/","section":"publication","summary":"Using Japanese financial data that provide enough observations under the good and bad regimes of financial conditions, we find that fiscal multipliers are smaller in the bad regime than in the good regime.","tags":[""],"title":"Is fiscal expansion more effective in a financial crisis?","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]