[{"authors":null,"categories":null,"content":"奈良県出身の院卒3年目の26歳。専攻はマクロ経済（主にDynamic Stochastic General Equilibruim model）、時系列解析やテキストマイニングにも手を出していました。最近はデータラングリングに興味があり、(インフラ)エンジニアとしてのキャリアを進むため、資格勉強中。現在は某資産運用会社にて運用企画グループに所属。広瀬すずより有村架純よりお姉ちゃんのほうがタイプ。\n","date":1522454400,"expirydate":-62135596800,"kind":"term","lang":"ja","lastmod":1522454400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/ayato-ashihara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/ayato-ashihara/","section":"authors","summary":"奈良県出身の院卒3年目の26歳。専攻はマクロ経済（主にDyn","tags":null,"title":"Ayato Ashihara","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"ja","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["単発","Tips"],"content":"\r\r\r1. Mecab(RMeCab)とは？\r\r固有名詞に弱いデフォルトのMeCab\r新語に強いNEologd辞書\r\r2. インストール手順\r\rA. Windows Subsystem for Linux(WSL)のインストール\rB. Ubuntu Linuxのインストール\rC. Ubuntu LinuxにMeCabをインストール\rD. Ubuntu for LinuxにNEologd辞書をインストール\rE. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー\rF. Windows内で辞書ファイルのコンパイル(SHIFT-JIS)を行う。\r\r3. まとめ\r\r\r皆さんおはこんばんにちは。\r最近在宅勤務で運動不足ですが、平日休日問わず研究活動をしています。\r学術誌投稿を目指したBlog記事には書けない内容なので、更新はストップしてしまっていますがちゃんと活動はしています。\n今回はその研究の中で利用しているMeCabにまつわるTipsのご紹介です。MeCabというと形態素解析ソフトということはこのブログを読まれている方はお分かりかと思いますが、その辞書にNEologd辞書を使用できるようにしてみたというのが内容です。MeCabってなに？という方もいらっしゃるかもしれませんので、かなり簡単にご紹介します。\n1. Mecab(RMeCab)とは？\rMeCabは日本語テキストマイニングで使用される形態素解析ソフトです。英語などの言語とは異なり、日本語は単語毎にスペースを置かないため、テキストマイニングを行う際、そのままでは文章を単語単位に区切って集計するといったことができません。例えば、「これはペンです。」という文章はそのままでは「これはペンです。」という1つの単語として認識されてしまいます。ですが、単語の頻度分析を行う際などは、「これ/は/ペン/です/。」という風に文章を単語レベルにまで分割し、「ペン」という特徴量を取得したいわけです。それができるのがRMeCabで、この処理は形態素解析と呼ばれます。1そして、RMeCabとはこのMeCabのラッパーになります。RからMeCabを使用するにはRMeCabを使用する必要があります。使用方法は簡単で、RMeCabC関数に文章を渡すだけです。\nlibrary(magrittr)\rlibrary(RMeCab)\rRMeCabC(\u0026quot;これはペンです。\u0026quot;) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 助動詞 記号 ## \u0026quot;これ\u0026quot; \u0026quot;は\u0026quot; \u0026quot;ペン\u0026quot; \u0026quot;です\u0026quot; \u0026quot;。\u0026quot;\rRMeCabC関数は結果をリストで返してくるので、unlistでcharacterに変換しています。\n固有名詞に弱いデフォルトのMeCab\r便利なように思えるのですが、デフォルトのMeCabは固有名詞に弱いという弱点があります。例えば、「欅坂46が赤いきつねを食べている。」という文章を形態素解析してみましょう。\nRMeCabC(\u0026quot;欅坂46が赤いきつねを食べている。\u0026quot;) %\u0026gt;% unlist()\r## 名詞 名詞 名詞 助詞 形容詞 名詞 助詞 動詞 ## \u0026quot;欅\u0026quot; \u0026quot;坂\u0026quot; \u0026quot;46\u0026quot; \u0026quot;が\u0026quot; \u0026quot;赤い\u0026quot; \u0026quot;きつね\u0026quot; \u0026quot;を\u0026quot; \u0026quot;食べ\u0026quot; ## 助詞 動詞 記号 ## \u0026quot;て\u0026quot; \u0026quot;いる\u0026quot; \u0026quot;。\u0026quot;\r予想していた結果は「欅坂46/が/赤いきつね/を/食べ/て/いる/。」でしょう。ただ、固有名詞を上手く形態素解析することができていないため、不必要なところで分割がなされており、「赤い/きつね/を/食べ」という部分については「赤い（動物の）きつね」を食べているかのような解析結果になっています。赤いきつねの「きつね」と動物の「きつね」を同等に扱ってしまうので、問題があります。また、経済においても以下のように日経平均株価が分割され、新聞社の「日経」と株価の「日経」が、大統領の「トランプ」とカードの「トランプ」が区別できないといったことが想定されます。\nRMeCabC(\u0026quot;今週の日経平均株価は、上値抵抗線を突破して上昇！\u0026quot;) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 名詞 名詞 助詞 記号 名詞 名詞 名詞 助詞 ## \u0026quot;今週\u0026quot; \u0026quot;の\u0026quot; \u0026quot;日経\u0026quot; \u0026quot;平均\u0026quot; \u0026quot;株価\u0026quot; \u0026quot;は\u0026quot; \u0026quot;、\u0026quot; \u0026quot;上値\u0026quot; \u0026quot;抵抗\u0026quot; \u0026quot;線\u0026quot; \u0026quot;を\u0026quot; ## 名詞 動詞 助詞 名詞 記号 ## \u0026quot;突破\u0026quot; \u0026quot;し\u0026quot; \u0026quot;て\u0026quot; \u0026quot;上昇\u0026quot; \u0026quot;！\u0026quot;\rRMeCabC(\u0026quot;トランプ政権による経済活動再開の指針発表が評価される\u0026quot;) %\u0026gt;% unlist()\r## 名詞 名詞 助詞 名詞 名詞 名詞 助詞 ## \u0026quot;トランプ\u0026quot; \u0026quot;政権\u0026quot; \u0026quot;による\u0026quot; \u0026quot;経済\u0026quot; \u0026quot;活動\u0026quot; \u0026quot;再開\u0026quot; \u0026quot;の\u0026quot; ## 名詞 名詞 助詞 名詞 動詞 動詞 ## \u0026quot;指針\u0026quot; \u0026quot;発表\u0026quot; \u0026quot;が\u0026quot; \u0026quot;評価\u0026quot; \u0026quot;さ\u0026quot; \u0026quot;れる\u0026quot;\rこれはデフォルトでMeCabが使用している辞書に原因があります。IPA辞書(ipadic)と呼ばれる物で、奈良先端科学技術大学院大学が公開している茶筌と呼ばれる形態素解析ソフト用に作られました。こちらを見ると辞書の更新は2007年でストップしており、新語や流行語がアップデートされていないために上記の固有名詞が上手く形態素解析できないことがわかります。\n\r新語に強いNEologd辞書\r最近MeCabでよく使用されている辞書にNEologd辞書というものがあります。NEologd辞書とは、Web上から得た新語に対応しており、頻繁に更新されるMeCab用のシステム辞書です。2Twitterのアカウントには、\n\r特色は語彙の多さと更新頻度、新語の採録の速さ、読み仮名の正確さ、表記揺れへの対応。おもな解析対象はWeb上のニュース記事や流行した出来事。\rおもな用途は文書分類、文書ベクトル作成、単語埋め込みベクトル作成、読み仮名付与。\n\rと記載されており、上述したIPA辞書の弱点を補完する辞書となっています。今回の記事はその辞書のインストール方法についてですが、インストールした辞書の威力を先にお見せしておきます。実行するには、RMeCab関数に辞書ファイル(.dicファイル)のパスを渡してやれば良いです。\ndic_directory \u0026lt;- \u0026quot;C:\\\\hogehoge\\\\mecab-user-dict-seed.yyyymmdd.dic\u0026quot;\rRMeCabC(\u0026quot;欅坂46が赤いきつねを食べている。\u0026quot;,dic=dic_directory) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 助詞 動詞 助詞 ## \u0026quot;欅坂46\u0026quot; \u0026quot;が\u0026quot; \u0026quot;赤いきつね\u0026quot; \u0026quot;を\u0026quot; \u0026quot;食べ\u0026quot; \u0026quot;て\u0026quot; ## 動詞 記号 ## \u0026quot;いる\u0026quot; \u0026quot;。\u0026quot;\rRMeCabC(\u0026quot;今週の日経平均株価は、上値抵抗線を突破して上昇！\u0026quot;,dic=dic_directory) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 助詞 記号 ## \u0026quot;今週\u0026quot; \u0026quot;の\u0026quot; \u0026quot;日経平均株価\u0026quot; \u0026quot;は\u0026quot; \u0026quot;、\u0026quot; ## 名詞 名詞 名詞 助詞 名詞 ## \u0026quot;上値\u0026quot; \u0026quot;抵抗\u0026quot; \u0026quot;線\u0026quot; \u0026quot;を\u0026quot; \u0026quot;突破\u0026quot; ## 動詞 助詞 名詞 記号 ## \u0026quot;し\u0026quot; \u0026quot;て\u0026quot; \u0026quot;上昇\u0026quot; \u0026quot;！\u0026quot;\rRMeCabC(\u0026quot;トランプ政権による経済活動再開の指針発表が評価される\u0026quot;,dic=dic_directory) %\u0026gt;% unlist()\r## 名詞 助詞 名詞 名詞 助詞 ## \u0026quot;トランプ政権\u0026quot; \u0026quot;による\u0026quot; \u0026quot;経済活動\u0026quot; \u0026quot;再開\u0026quot; \u0026quot;の\u0026quot; ## 名詞 名詞 助詞 名詞 動詞 ## \u0026quot;指針\u0026quot; \u0026quot;発表\u0026quot; \u0026quot;が\u0026quot; \u0026quot;評価\u0026quot; \u0026quot;さ\u0026quot; ## 動詞 ## \u0026quot;れる\u0026quot;\r上値抵抗線は1語カウントしてほしいところではありますが、それ以外は上手く形態素解析できていそうです。\rこのNEologd辞書ですが、Windowsのインストールが想定されていません。つまり、Windowsユーザーは直接インストールすることができないのです。Windowsユーザーは少々ややこしい手順を踏まなければなりません。今回はそのややこしい手順を解説する記事です。Linuxでインストールを行い、それをWindows環境にコピー、その後辞書ファイルを作成します。\n\r\r2. インストール手順\rA. Windows Subsystem for Linux(WSL)のインストール\rWindows Power Shellを管理者権限で開き、\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\rを実行する。WSLをインストールすることができます。\npowerShellでの実行の様子\n\r\rB. Ubuntu Linuxのインストール\rMicrosoft Storeよりubuntuをダウンロードする。\nubuntuの画面\n\rインストールが完了したらubuntuを起動し、初期設定を完了させる(ID、パスワード)。\n\rC. Ubuntu LinuxにMeCabをインストール\rubuntuのコマンドプロンプトで、\nsudo apt-get update\rsudo apt install mecab\rsudo apt install libmecab-dev\rsudo apt install mecab-ipadic-utf8\rを入力し、MeCabをインストール。\nubuntuでmecabをインストール\n\r\rD. Ubuntu for LinuxにNEologd辞書をインストール\rubuntsuのコマンドプロンプトで\nsudo apt install make\rgit clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\rcd mecab-ipadic-neologd\rsudo bin/install-mecab-ipadic-neologd -n -a\rを実行。NEologd辞書ファイルが(/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/)にインストールできます。\n\rE. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー\rubuntuのコマンドプロンプトで\nexplorer.exe .\rを入力。エクスプローラーが立ち上がるので、/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/(ディレクトリ)をWindowsの任意のディレクトリにコピーする。完了したらUbuntuは閉じる。\nエクスプローラーでubuntu内部ディレクトリを確認する図\n\r\rF. Windows内で辞書ファイルのコンパイル(SHIFT-JIS)を行う。\rコピーしたディレクトリ内の以下のcsvを辞書ファイル(.dic)にコンパイルします。(yyyymmddは辞書の最終更新日なので変更してください)\n-mecab-ipadic-neologd-buildmecab-ipadic-2.7.0-20070801-neologd-yyyymmdd-mecab-user-dict-seed.yyyymmdd.csv\rその際、元ファイルはエンコーディングがUTF-8となっているので、SHIFT-JISへ変換することに注意です。これをしないと、RMeCabを実行したときに結果が文字化けします。コンパイルにはMeCabのmecab-dict-indexというバイナリファイルを使用します。自分は以下のディレクトリに存在しました。\nC:\r-Program Files (x86)\r-MeCab\r-bin\r-mecab-dict-index.exe\nコマンドプロンプトを立ち上げ、\nC:\\Program Files (x86)\\MeCab\\bin\\mecab-dict-index.exe -d .../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmdd(ファイルの保存場所) -u NEologd.yyyymmdd.dic -f utf-8 -t shift-jis mecab-ipadic-neologd\\buildmecab\\ipadic-2.7.0-20070801-neologd-yyyymmdd\\mecab-user-dict-seed.yyyymmdd.csv \rを適宜変更の上、入力。.../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmddに辞書がコンパイルされ、NEologd.yyyymmdd.dicができます。\n\r\r3. まとめ\r以上で、NEologd辞書が使用できるようになります。非常に強力なツールなので使用してみてください。なお、辞書がアップデートされた際は同じ手続きを行う必要があります。\n\rUbuntu for Linuxを使用しなくてもダウンロードできそうな方法ありました。\nhttps://qiita.com/zincjp/items/c61c441426b9482b5a48\nただ、自分は実行していないので実際にできるかはわかりません。辞書が更新されたら、やってみたいと思います。\n\r\rMeCab内部の仕組みについてはこちらの記事が参考になります。↩︎\n\r2020/4/19時点の最近版は2020/3/15更新の辞書です。↩︎\n\r\r\r","date":1587254400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1587254400,"objectID":"9a9dac6ab7ada9192e2cf9879cae5535","permalink":"/post/post17/","publishdate":"2020-04-19T00:00:00Z","relpermalink":"/post/post17/","section":"post","summary":"新語に強いNEologd辞書をMecabにインストールしてみました。","tags":["R","テキストマイニング"],"title":"WindowsにNEologd辞書をインストールして、RMeCabを実行する方法","type":"post"},{"authors":null,"categories":["競馬"],"content":"\r\r\r1.データインポート\r2. 予測モデルの作成\r3. shapでの結果解釈\r4. 最後に\r\r\rおはこんばんにちは。かなり久しぶりではありますが、Pythonの勉強をかねて以前yahoo.keibaで収集した競馬のレース結果データから、レース結果を予想するモデルを作成したいと思います。\n1.データインポート\rまず、前回sqliteに保存したレース結果データをpandasデータフレームへ保存します。\nconn = sqlite3.connect(r\u0026#39;C:\\hogehoge\\horse_data.db\u0026#39;)\rsql = r\u0026#39;SELECT * FROM race_result\u0026#39;\rdf = pd.read_sql(con=conn,sql=sql)\rデータの中身を確認してみましょう。列は以下のようになっています。orderが着順となっています。\ndf.columns\r## Index([\u0026#39;order\u0026#39;, \u0026#39;frame_number\u0026#39;, \u0026#39;horse_number\u0026#39;, \u0026#39;trainer\u0026#39;, \u0026#39;passing_rank\u0026#39;,\r## \u0026#39;last_3F\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;margin\u0026#39;, \u0026#39;horse_name\u0026#39;, \u0026#39;horse_age\u0026#39;, \u0026#39;horse_sex\u0026#39;,\r## \u0026#39;horse_weight\u0026#39;, \u0026#39;horse_weight_change\u0026#39;, \u0026#39;brinker\u0026#39;, \u0026#39;jockey\u0026#39;,\r## \u0026#39;jockey_weight\u0026#39;, \u0026#39;jockey_weight_change\u0026#39;, \u0026#39;odds\u0026#39;, \u0026#39;popularity\u0026#39;,\r## \u0026#39;race_date\u0026#39;, \u0026#39;race_course\u0026#39;, \u0026#39;race_name\u0026#39;, \u0026#39;race_distance\u0026#39;, \u0026#39;type\u0026#39;,\r## \u0026#39;race_turn\u0026#39;, \u0026#39;race_condition\u0026#39;, \u0026#39;race_weather\u0026#39;, \u0026#39;colour\u0026#39;, \u0026#39;owner\u0026#39;,\r## \u0026#39;farm\u0026#39;, \u0026#39;locality\u0026#39;, \u0026#39;horse_birthday\u0026#39;, \u0026#39;father\u0026#39;, \u0026#39;mother\u0026#39;, \u0026#39;prize\u0026#39;,\r## \u0026#39;http\u0026#39;],\r## dtype=\u0026#39;object\u0026#39;)\rorderの中身を確認してみると、括弧（）がついている物が多く、また取消や中止、失格などが存在するため、文字型に認識されていることがわかります。ちなみに括弧（）内の順位は入線順位というやつで、他馬の走行を妨害したりして順位が降着させられたことを意味します（http://www.jra.go.jp/judge/）。\ndf.loc[:,\u0026#39;order\u0026#39;].unique()\r## array([\u0026#39;1\u0026#39;, \u0026#39;7\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;8\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;15\u0026#39;, \u0026#39;6\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;11\u0026#39;, \u0026#39;14\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;13\u0026#39;,\r## \u0026#39;4\u0026#39;, \u0026#39;16\u0026#39;, \u0026#39;9\u0026#39;, \u0026#39;10\u0026#39;, \u0026#39;取消\u0026#39;, \u0026#39;中止\u0026#39;, \u0026#39;除外\u0026#39;, \u0026#39;17\u0026#39;, \u0026#39;18\u0026#39;, \u0026#39;4(3)\u0026#39;, \u0026#39;2(1)\u0026#39;,\r## \u0026#39;3(2)\u0026#39;, \u0026#39;6(4)\u0026#39;, \u0026#39;失格\u0026#39;, \u0026#39;9(8)\u0026#39;, \u0026#39;16(6)\u0026#39;, \u0026#39;12(12)\u0026#39;, \u0026#39;13(9)\u0026#39;, \u0026#39;6(3)\u0026#39;,\r## \u0026#39;10(7)\u0026#39;, \u0026#39;6(5)\u0026#39;, \u0026#39;9(3)\u0026#39;, \u0026#39;11(8)\u0026#39;, \u0026#39;13(2)\u0026#39;, \u0026#39;12(9)\u0026#39;, \u0026#39;14(7)\u0026#39;,\r## \u0026#39;10(1)\u0026#39;, \u0026#39;16(8)\u0026#39;, \u0026#39;14(6)\u0026#39;, \u0026#39;10(3)\u0026#39;, \u0026#39;12(1)\u0026#39;, \u0026#39;13(6)\u0026#39;, \u0026#39;7(1)\u0026#39;,\r## \u0026#39;12(6)\u0026#39;, \u0026#39;6(2)\u0026#39;, \u0026#39;11(2)\u0026#39;, \u0026#39;15(6)\u0026#39;, \u0026#39;13(10)\u0026#39;, \u0026#39;14(4)\u0026#39;, \u0026#39;7(5)\u0026#39;,\r## \u0026#39;17(4)\u0026#39;, \u0026#39;9(7)\u0026#39;, \u0026#39;16(14)\u0026#39;, \u0026#39;12(11)\u0026#39;, \u0026#39;14(2)\u0026#39;, \u0026#39;8(2)\u0026#39;, \u0026#39;9(5)\u0026#39;,\r## \u0026#39;11(5)\u0026#39;, \u0026#39;12(7)\u0026#39;, \u0026#39;11(1)\u0026#39;, \u0026#39;12(8)\u0026#39;, \u0026#39;7(4)\u0026#39;, \u0026#39;5(4)\u0026#39;, \u0026#39;13(12)\u0026#39;,\r## \u0026#39;14(3)\u0026#39;, \u0026#39;10(2)\u0026#39;, \u0026#39;11(10)\u0026#39;, \u0026#39;18(3)\u0026#39;, \u0026#39;10(4)\u0026#39;, \u0026#39;15(8)\u0026#39;, \u0026#39;8(3)\u0026#39;,\r## \u0026#39;5(1)\u0026#39;, \u0026#39;10(5)\u0026#39;, \u0026#39;7(3)\u0026#39;, \u0026#39;5(2)\u0026#39;, \u0026#39;9(1)\u0026#39;, \u0026#39;13(3)\u0026#39;, \u0026#39;16(11)\u0026#39;,\r## \u0026#39;11(3)\u0026#39;, \u0026#39;18(15)\u0026#39;, \u0026#39;11(6)\u0026#39;, \u0026#39;10(6)\u0026#39;, \u0026#39;14(12)\u0026#39;, \u0026#39;12(5)\u0026#39;, \u0026#39;15(14)\u0026#39;,\r## \u0026#39;17(8)\u0026#39;, \u0026#39;18(6)\u0026#39;, \u0026#39;4(2)\u0026#39;, \u0026#39;18(10)\u0026#39;, \u0026#39;16(7)\u0026#39;, \u0026#39;13(1)\u0026#39;, \u0026#39;16(10)\u0026#39;,\r## \u0026#39;15(7)\u0026#39;, \u0026#39;9(4)\u0026#39;, \u0026#39;15(5)\u0026#39;, \u0026#39;12(3)\u0026#39;, \u0026#39;8(7)\u0026#39;, \u0026#39;15(2)\u0026#39;, \u0026#39;12(10)\u0026#39;,\r## \u0026#39;14(9)\u0026#39;, \u0026#39;3(1)\u0026#39;, \u0026#39;6(1)\u0026#39;, \u0026#39;14(5)\u0026#39;, \u0026#39;15(4)\u0026#39;, \u0026#39;11(4)\u0026#39;, \u0026#39;12(4)\u0026#39;,\r## \u0026#39;16(4)\u0026#39;, \u0026#39;9(2)\u0026#39;, \u0026#39;13(5)\u0026#39;, \u0026#39;12(2)\u0026#39;, \u0026#39;15(1)\u0026#39;, \u0026#39;4(1)\u0026#39;, \u0026#39;14(13)\u0026#39;,\r## \u0026#39;14(1)\u0026#39;, \u0026#39;13(7)\u0026#39;, \u0026#39;5(3)\u0026#39;, \u0026#39;8(6)\u0026#39;, \u0026#39;15(13)\u0026#39;, \u0026#39;7(2)\u0026#39;, \u0026#39;15(11)\u0026#39;,\r## \u0026#39;10(9)\u0026#39;, \u0026#39;11(9)\u0026#39;, \u0026#39;8(4)\u0026#39;, \u0026#39;15(3)\u0026#39;, \u0026#39;13(4)\u0026#39;, \u0026#39;16(12)\u0026#39;, \u0026#39;16(5)\u0026#39;,\r## \u0026#39;18(11)\u0026#39;, \u0026#39;10(8)\u0026#39;, \u0026#39;18(8)\u0026#39;, \u0026#39;14(8)\u0026#39;, \u0026#39;16(9)\u0026#39;, \u0026#39;8(5)\u0026#39;, \u0026#39;8(1)\u0026#39;,\r## \u0026#39;14(11)\u0026#39;, \u0026#39;9(6)\u0026#39;, \u0026#39;16(13)\u0026#39;, \u0026#39;16(15)\u0026#39;, \u0026#39;11(11)\u0026#39;, \u0026#39;15(10)\u0026#39;, \u0026#39;7(6)\u0026#39;],\r## dtype=object)\rまずここを修正しましょう。括弧を除去してint型に型変更し、入線順位は新たな列arriving orderとして追加します。\ndf[\u0026#39;arriving order\u0026#39;] = df[df.order.str.contains(r\u0026#39;\\d*\\(\\d*\\)\u0026#39;,regex=True)][\u0026#39;order\u0026#39;].replace(r\u0026#39;\\d+\\(\u0026#39;,r\u0026#39;\u0026#39;,regex=True).replace(r\u0026#39;\\)\u0026#39;,r\u0026#39;\u0026#39;,regex=True).astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;arriving order\u0026#39;].unique()\r## array([nan, 3., 1., 2., 4., 8., 6., 12., 9., 7., 5., 10., 14.,\r## 11., 15., 13.])\rdf[\u0026#39;order\u0026#39;] = df[\u0026#39;order\u0026#39;].replace(r\u0026#39;\\(\\d+\\)\u0026#39;,r\u0026#39;\u0026#39;,regex=True)\rdf = df[lambda df: ~df.order.str.contains(r\u0026#39;(取消|中止|除外|失格)\u0026#39;,regex=True)]\r## C:\\Users\\aashi\\Anaconda3\\envs\\umanalytics\\lib\\site-packages\\pandas\\core\\strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\r## return func(self, *args, **kwargs)\rdf[\u0026#39;order\u0026#39;] = df[\u0026#39;order\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;order\u0026#39;].unique()\r## array([ 1., 7., 2., 8., 5., 15., 6., 12., 11., 14., 3., 13., 4.,\r## 16., 9., 10., 17., 18.])\rきれいなfloat型に処理することができました。では、次にラスト3Fのタイムの前処理に移ります。前走のラスト3Fのタイムを予測に使用します。\nimport numpy as np\rdf[\u0026#39;last_3F\u0026#39;] = df[\u0026#39;last_3F\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False).astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;last_3F\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;last_3F\u0026#39;].shift(-1)\r前走のレースと順位、追加順位もデータセットへ含めましょう。\ndf[\u0026#39;prerace\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;race_name\u0026#39;].shift(-1)\rdf[\u0026#39;preorder\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;order\u0026#39;].shift(-1)\rdf[\u0026#39;prepassing\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;passing_rank\u0026#39;].shift(-1)\r出走時点で獲得している累積賞金額も追加します。\ndf[\u0026#39;preprize\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;prize\u0026#39;].shift(-1)\rdf[\u0026#39;preprize\u0026#39;] = df[\u0026#39;preprize\u0026#39;].fillna(0)\rdf[\u0026#39;margin\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;margin\u0026#39;].shift(-1)\rその他、欠損値やデータ型の修正、カテゴリデータのラベルエンコーディングです。\ndf[\u0026#39;horse_weight\u0026#39;] = df[\u0026#39;horse_weight\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;margin\u0026#39;] = df[\u0026#39;margin\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False)\rdf[\u0026#39;horse_age\u0026#39;] = df[\u0026#39;horse_age\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;horse_weight_change\u0026#39;] = df[\u0026#39;horse_weight_change\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;jockey_weight\u0026#39;] = df[\u0026#39;jockey_weight\u0026#39;].astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;race_distance\u0026#39;] = df[\u0026#39;race_distance\u0026#39;].replace(r\u0026#39;m\u0026#39;,r\u0026#39;\u0026#39;,regex=True).astype(\u0026#39;float64\u0026#39;)\rdf[\u0026#39;race_turn\u0026#39;] = df[\u0026#39;race_turn\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False)\rdf.loc[df[\u0026#39;order\u0026#39;]!=1,\u0026#39;order\u0026#39;] = 0\rdf[\u0026#39;race_turn\u0026#39;] = df[\u0026#39;race_turn\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;colour\u0026#39;] = df[\u0026#39;colour\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;prepassing\u0026#39;] = df[\u0026#39;prepassing\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;prerace\u0026#39;] = df[\u0026#39;prerace\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;father\u0026#39;] = df[\u0026#39;father\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rdf[\u0026#39;mother\u0026#39;] = df[\u0026#39;mother\u0026#39;].fillna(\u0026#39;missing\u0026#39;)\rfrom sklearn import preprocessing\rcat_list = [\u0026#39;trainer\u0026#39;, \u0026#39;horse_name\u0026#39;, \u0026#39;horse_sex\u0026#39;, \u0026#39;brinker\u0026#39;, \u0026#39;jockey\u0026#39;, \u0026#39;race_course\u0026#39;, \u0026#39;race_name\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;race_turn\u0026#39;, \u0026#39;race_condition\u0026#39;, \u0026#39;race_weather\u0026#39;, \u0026#39;colour\u0026#39;, \u0026#39;father\u0026#39;, \u0026#39;mother\u0026#39;, \u0026#39;prerace\u0026#39;, \u0026#39;prepassing\u0026#39;]\rfor column in cat_list:\rtarget_column = df[column]\rle = preprocessing.LabelEncoder()\rle.fit(target_column)\rlabel_encoded_column = le.transform(target_column)\rdf[column] = pd.Series(label_encoded_column).astype(\u0026#39;category\u0026#39;)\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\rimport pandas_profiling as pdq\rprofile = pdq.ProfileReport(df)\rprofile\r\r2. 予測モデルの作成\rではLightGBMで予測モデルを作ってみます。optunaのLightGBMを使用して、ハイパーパラメータチューニングを行い、学習したモデルを用いて計算したテストデータの予測値と実績値のconfusion matrixならびに正解率を算出します。\nimport optuna.integration.lightgbm as lgb\rfrom sklearn.model_selection import train_test_split\ry = df[\u0026#39;order\u0026#39;]\rx = df.drop([\u0026#39;order\u0026#39;,\u0026#39;passing_rank\u0026#39;,\u0026#39;time\u0026#39;,\u0026#39;odds\u0026#39;,\u0026#39;popularity\u0026#39;,\u0026#39;owner\u0026#39;,\u0026#39;farm\u0026#39;,\u0026#39;locality\u0026#39;,\u0026#39;horse_birthday\u0026#39;,\u0026#39;http\u0026#39;,\u0026#39;prize\u0026#39;,\u0026#39;race_date\u0026#39;,\u0026#39;margin\u0026#39;],axis=1)\rX_train, X_test, y_train, y_test = train_test_split(x, y)\rX_train, x_val, y_train, y_val = train_test_split(X_train, y_train)\rlgb_train = lgb.Dataset(X_train, y_train)\rlgb_eval = lgb.Dataset(x_val, y_val)\rlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\rlgbm_params = {\r\u0026#39;objective\u0026#39;: \u0026#39;binary\u0026#39;,\r\u0026#39;boost_from_average\u0026#39;: False\r}\rbest_params, history = {}, []\rmodel = lgb.train(lgbm_params, lgb_train, categorical_feature = cat_list,valid_sets = lgb_eval, num_boost_round=100,early_stopping_rounds=20,best_params=best_params,tuning_history=history, verbose_eval=False)\rbest_params\rdef calibration(y_proba, beta):\rreturn y_proba / (y_proba + (1 - y_proba) / beta)\rsampling_rate = y_train.sum() / len(y_train)\ry_proba = model.predict(X_test, num_iteration=model.best_iteration)\ry_proba_calib = calibration(y_proba, sampling_rate)\ry_pred = np.vectorize(lambda x: 1 if x \u0026gt; 0.49 else 0)(y_proba_calib)\r可視化パートです。\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\rimport matplotlib.pyplot as plt\rimport seaborn as sns\r# AUC (Area Under the Curve) を計算する\rfpr, tpr, thresholds = roc_curve(y_test, y_pred)\rauc = auc(fpr, tpr)\r# ROC曲線をプロット\rplt.plot(fpr, tpr, label=\u0026#39;ROC curve (area = %.2f)\u0026#39;%auc)\rplt.legend()\rplt.title(\u0026#39;ROC curve\u0026#39;)\rplt.xlabel(\u0026#39;False Positive Rate\u0026#39;)\rplt.ylabel(\u0026#39;True Positive Rate\u0026#39;)\rplt.grid(True)\rplt.show()\rplt.close()\r# Confusion Matrixを生成\rConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()\r## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000004F873388\u0026gt;\rplt.show()\rplt.close()\raccuracy_score(y_test, y_pred)\r## 0.9300814465677578\rprecision_score(y_test, y_pred)\r## 0.9426605504587156\raccuracy_score（予測精度）が90%を超え、precision_Score（適合率、陽=1着と予想したデータの正解率）もいい感じです。\nrecall_score(y_test, y_pred)\r## 0.01211460236986382\rf1_score(y_test, y_pred)\r## 0.02392177405273267\r一方、recall_score(再現性、陽=1着のサンプルのうち実際に正解した割合)が低く偽陰性が高いことが確認できます。その結果、F1値も低くなっていますね。競馬予測モデルの場合、偽陰性が高いことは偽陽性が高いことよりはましなのですが、回収率を上げるためには偽陰性を下げることを頑張らなければいけません。これは今後の課題ですね。次節ではshapley値を使って要因分解をしたいと思います。。\n\r3. shapでの結果解釈\rimport shap\rshap.initjs()\r## \u0026lt;IPython.core.display.HTML object\u0026gt;\rexplainer = shap.TreeExplainer(model)\r## Setting feature_perturbation = \u0026quot;tree_path_dependent\u0026quot; because no background data was given.\rshap_values = explainer.shap_values(X_test)\r## LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\rまず、各特徴量の重要度を見ることにします。summary_plotメソッドを使用します。\nshap.summary_plot(shap_values, X_test)\r横軸は各特徴量の平均的な重要度を表しています(shap値の絶対値)。preprize(前走までの賞金獲得金額)やhorse_age、preorder(前走の着順)などが予測に重要であることが分かります。特にpreprizeの重要度は1着の予測、1着以外の予測どちらに対しても大きいです。horse_ageも同様です。ただ、これでは重要というだけで定性的な評価はできません。例えば、preprizeが大きい→1位になる確率が上昇といった関係が確認できれば、それは重要な情報になり得ます。次にそれを確認します。summary_plotメソッドを使用します。\nshap.summary_plot(shap_values[1], X_test)\r上図も各特徴量の重要度を表しています(今回は絶対値ではありません)。今回はそれぞれの特徴量の重要度がバイオリンプロットによって表されており、かつ特徴量の値の大きさで色分けがされています。例えば、preprizeだと横軸が0以上の部分でのみ赤色の分布が発生しており、ここからpreprizeの特徴量が大きい、つまり前走までの獲得賞金額が多いと平均的に1着の確率が上がるという当たり前の解釈をすることができます。\r他にも、horse_age,preorder,last_3Fは特徴量が小さくなるほど1着になる確率があがることも読み取れます。horse_weight, jokey_weightは大きくなるほど1着になる確率が上がるようです。一方、その他は特に定性的な関係を読み取ることはできません。\n次に、特徴量と確率の関係をより詳しく確認してみましょう。先ほど、preprizeは特徴量が大きくなるほど1着になる確率が上昇するということがわかりました。ただ、その確率の上昇は1次関数的に増加するのか、指数的に増大するのか、それとも\\(\\log x\\)のように逓減していくのか、わかりません。dependence_plotを使用してそれを確認してみましょう。\nshap.dependence_plot(ind=\u0026quot;preprize\u0026quot;, shap_values=shap_values[1], features=X_test)\r上図は学習したLightGBMをpreprizeの関数として見たときの概形をplotしたものです。先に確認したとおり、やはり特徴量が大きくなるにつれ、1着になる確率が上昇していきます。ただ、その上昇は徐々に逓減していき、2000万円を超えるところでほぼ頭打ちとなります。また、上図ではhorse_ageでの色分けを行っており、preprizeとの関係性も確認できるようになっています。やはり、直感と同じく、preprizeが高い馬の中でもhorse_ageが若い馬の1着確率が高くなることが見て取れます。\npreorderのdependence_plotも確認してみましょう。\nshap.dependence_plot(ind=\u0026quot;preorder\u0026quot;, shap_values=shap_values[1], features=X_test)\rやはり、前走の着順が上位になるほど1着確率が高まることがここからも分かります。また、その確率は6着以上とそれ以外で水準感が変わることも分かります。last_3Fのタイムとの関係性も確認していますが、こちらはあまり関連性はなさそうです。\n\r4. 最後に\rLightGBMを使用し、競馬の予測モデルを作成してみました。さすがLightGBMといった感じで、予測精度は高かったです。また、shap値を使用した重要特徴量の検出も上手くいきました。これによって、LightGBMの気持ちを理解し、より良い特徴量の発見を進めていくことでモデリングの精度を高めていこうと思います。\n\r","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1582934400,"objectID":"eb4dfbeb63c342ef9c2eb0c104bfc7e1","permalink":"/post/post16/","publishdate":"2020-02-29T00:00:00Z","relpermalink":"/post/post16/","section":"post","summary":"Kagglerが大好きなLightGBMで競馬の順位予想してみました。","tags":["Python","前処理","機械学習"],"title":"LightGBMを使用して競馬結果を予想してみる","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r1. データ収集\r2. 月次解析パート\r3. 日次解析パート\r\r\rおはこんばんにちは。とある理由で10年物長期金利のフィッティングを行いたいと思いました。というわけで、USデータを用いて解析していきます。まず、データを収集しましょう。quantmodパッケージを用いて、FREDからデータを落とします。getsymbols(キー,from=開始日,src=\"FRED\", auto.assign=TRUE)で簡単にできちゃいます。ちなみにキーはFREDのHPで確認できます。\n1. データ収集\rlibrary(quantmod)\r# data name collected\rsymbols.name \u0026lt;- c(\u0026quot;10-Year Treasury Constant Maturity Rate\u0026quot;,\u0026quot;Effective Federal Funds Rate\u0026quot;,\u0026quot;\rConsumer Price Index for All Urban Consumers: All Items\u0026quot;,\u0026quot;Civilian Unemployment Rate\u0026quot;,\u0026quot;3-Month Treasury Bill: Secondary Market Rate\u0026quot;,\u0026quot;Industrial Production Index\u0026quot;,\u0026quot;\r10-Year Breakeven Inflation Rate\u0026quot;,\u0026quot;Trade Weighted U.S. Dollar Index: Broad, Goods\u0026quot;,\u0026quot;\rSmoothed U.S. Recession Probabilities\u0026quot;,\u0026quot;Moody\u0026#39;s Seasoned Baa Corporate Bond Yield\u0026quot;,\u0026quot;5-Year, 5-Year Forward Inflation Expectation Rate\u0026quot;,\u0026quot;Personal Consumption Expenditures\u0026quot;)\r# Collect economic data\rsymbols \u0026lt;- c(\u0026quot;GS10\u0026quot;,\u0026quot;FEDFUNDS\u0026quot;,\u0026quot;CPIAUCSL\u0026quot;,\u0026quot;UNRATE\u0026quot;,\u0026quot;TB3MS\u0026quot;,\u0026quot;INDPRO\u0026quot;,\u0026quot;T10YIEM\u0026quot;,\u0026quot;TWEXBMTH\u0026quot;,\u0026quot;RECPROUSM156N\u0026quot;,\u0026quot;BAA\u0026quot;,\u0026quot;T5YIFRM\u0026quot;,\u0026quot;PCE\u0026quot;)\rgetSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, src = \u0026quot;FRED\u0026quot;, auto.assign = TRUE)\r## [1] \u0026quot;GS10\u0026quot; \u0026quot;FEDFUNDS\u0026quot; \u0026quot;CPIAUCSL\u0026quot; \u0026quot;UNRATE\u0026quot; ## [5] \u0026quot;TB3MS\u0026quot; \u0026quot;INDPRO\u0026quot; \u0026quot;T10YIEM\u0026quot; \u0026quot;TWEXBMTH\u0026quot; ## [9] \u0026quot;RECPROUSM156N\u0026quot; \u0026quot;BAA\u0026quot; \u0026quot;T5YIFRM\u0026quot; \u0026quot;PCE\u0026quot;\rmacro_indicator \u0026lt;- merge(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE)\rrm(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE,USEPUINDXD)\r\r2. 月次解析パート\rデータは\rこちら\rから参照できます。では、推計用のデータセットを作成していきます。被説明変数は10-Year Treasury Constant Maturity Rate(GS10)です。説明変数は以下の通りです。\n\r\r説明変数名\rキー\r代理変数\r\r\r\rFederal Funds Rate\rFEDFUNDS\r短期金利\r\rConsumer Price Index\rCPIAUCSL\r物価\r\rUnemployment Rate\rUNRATE\r雇用関連\r\r3-Month Treasury Bill\rTB3MS\r短期金利\r\rIndustrial Production Index\rINDPRO\r景気\r\rBreakeven Inflation Rate\rT10YIEM\r物価\r\rTrade Weighted Dollar Index\rTWEXBMTH\r為替\r\rRecession Probabilities\rRECPROUSM156N\r景気\r\rMoody’s Seasoned Baa Corporate Bond Yield\rBAA\rリスクプレミアム\r\rInflation Expectation Rate\rT5YIFRM\r物価\r\rPersonal Consumption Expenditures\rPCE\r景気\r\rEconomic Policy Uncertainty Index\rUSEPUINDXD\r政治\r\r\r\rかなり適当な変数選択ではあるんですが、マクロモデリング的に長期金利ってどうやってモデル化するかというとちゃんとやってない場合が多いです。DSGEでは効率市場仮説に従って10年先までの短期金利のパスをリンクしたものと長期金利が等しくなると定式化するのが院生時代のモデリングでした（マクロファイナンスの界隈ではちゃんとやってそう）。そういうわけで、短期金利を説明変数に加えています。そして、短期金利に影響を与えるであろう物価にかかる指標も3つ追加しました。加えて、景気との相関が強いことはよく知られているので景気に関するデータも追加しました。これらはそもそもマクロモデルでは短期金利は以下のようなテイラールールに従うとモデリングすることが一般的であることが背景にあります。\n\\[\rr_t = \\rho r_{t-1} + \\alpha \\pi_{t} + \\beta y_{t}\r\\]\rここで、\\(r_t\\)は政策金利（短期金利）、\\(\\pi_t\\)はインフレ率、\\(y_t\\)はoutputです。\\(\\rho, \\alpha, \\beta\\)はdeep parameterと呼ばれるもので、それぞれ慣性、インフレ率への金利の感応度、outputに対する感応度を表しています。\\(\\rho=0,\\beta=0\\)の時、\\(\\alpha\u0026gt;=1\\)でなければ合理的期待均衡解が得られないことは「テイラーの原理」として有名です。\rその他、Corporate bondとの裁定関係も存在しそうなMoody's Seasoned Baa Corporate Bond Yieldも説明変数に追加しています。また、欲を言えばVIX指数と財政に関する指標を追加したいところです。財政に関する指数はQuateryかAnnualyなので今回のようなmonthlyの推計には使用することができません。この部分は最もネックなところです。なにか考え付いたら再推計します。\nでは、推計に入ります。今回は説明変数が多いのでlasso回帰を行い、有効な変数を絞り込みたいと思います。また、比較のためにOLSもやります。説明変数は被説明変数の1期前の値を使用します。おそらく、1期前でもデータの公表時期によっては翌月の推計に間に合わない可能性もありますが、とりあえずこれでやってみます。\n# make dataset\rtraindata \u0026lt;- na.omit(merge(macro_indicator[\u0026quot;2003-01-01::2015-12-31\u0026quot;][,1],stats::lag(macro_indicator[\u0026quot;2003-01-01::2015-12-31\u0026quot;][,-1],1)))\rtestdata \u0026lt;- na.omit(merge(macro_indicator[\u0026quot;2016-01-01::\u0026quot;][,1],stats::lag(macro_indicator[\u0026quot;2016-01-01::\u0026quot;][,-1],1)))\r# fitting OLS\rtrial1 \u0026lt;- lm(GS10~.,data = traindata)\rsummary(trial1)\r## ## Call:\r## lm(formula = GS10 ~ ., data = traindata)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.76208 -0.21234 0.00187 0.21595 0.70493 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 14.3578405 4.3524691 3.299 0.001226 ** ## FEDFUNDS -0.2011132 0.1438774 -1.398 0.164335 ## CPIAUCSL -0.0702011 0.0207761 -3.379 0.000938 ***\r## UNRATE -0.2093502 0.0796052 -2.630 0.009477 ** ## TB3MS 0.2970160 0.1413796 2.101 0.037410 * ## INDPRO -0.0645376 0.0260343 -2.479 0.014339 * ## T10YIEM 1.1484487 0.1769925 6.489 1.32e-09 ***\r## TWEXBMTH -0.0317345 0.0118155 -2.686 0.008091 ** ## RECPROUSM156N -0.0099083 0.0021021 -4.713 5.72e-06 ***\r## BAA 0.7793520 0.0868628 8.972 1.49e-15 ***\r## T5YIFRM -0.4551318 0.1897695 -2.398 0.017759 * ## PCE 0.0009087 0.0002475 3.672 0.000339 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.2981 on 143 degrees of freedom\r## Multiple R-squared: 0.9203, Adjusted R-squared: 0.9142 ## F-statistic: 150.1 on 11 and 143 DF, p-value: \u0026lt; 2.2e-16\r自由度修正済み決定係数高めですね。2015/12/31までのモデルを使って、アウトサンプルのデータ(2016/01/01~)を予測し、平均二乗誤差を計算します。\nest.OLS.Y \u0026lt;- predict(trial1,testdata[,-1])\rY \u0026lt;- as.matrix(testdata[,1])\rmse.OLS \u0026lt;- sum((Y - est.OLS.Y)^2) / length(Y)\rmse.OLS\r## [1] 0.1431734\r次にlasso回帰です。Cross Validationを行い、\\(\\lambda\\)を決めるglmnetパッケージのcv.glmnet関数を使用します。\n# fitting lasso regression\rlibrary(glmnet)\rtrial2 \u0026lt;- cv.glmnet(as.matrix(traindata[,-1]),as.matrix(traindata[,1]),family=\u0026quot;gaussian\u0026quot;,alpha=1)\rplot(trial2)\rtrial2$lambda.min\r## [1] 0.000436523\rcoef(trial2,s=trial2$lambda.min)\r## 12 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## 1\r## (Intercept) 12.0180676188\r## FEDFUNDS -0.1041665345\r## CPIAUCSL -0.0574470880\r## UNRATE -0.1919723880\r## TB3MS 0.2110222475\r## INDPRO -0.0610115260\r## T10YIEM 1.1688912397\r## TWEXBMTH -0.0242324285\r## RECPROUSM156N -0.0095154487\r## BAA 0.7600115062\r## T5YIFRM -0.4575241038\r## PCE 0.0007486169\rUnemployment Rate、3-Month Treasury Bill、Breakeven Inflation Rate、Moody's Seasoned Baa Corporate Bond Yield、Inflation Expectation Rateの回帰係数が大きくなるという結果ですね。失業率以外は想定内の結果です。ただ、今回の結果を見る限り景気との相関は低そうです（逆向きにしか効かない？）。MSEを計算します。\nest.lasso.Y \u0026lt;- predict(trial2, newx = as.matrix(testdata[,-1]), s = trial2$lambda.min, type = \u0026#39;response\u0026#39;)\rmse.lasso \u0026lt;- sum((Y - est.lasso.Y)^2) / length(Y)\rmse.lasso\r## [1] 0.1318541\rlasso回帰のほうが良い結果になりました。lasso回帰で計算した予測値と実績値を時系列プロットしてみます。\nlibrary(tidyverse)\rggplot(gather(data.frame(actual=Y[,1],lasso_prediction=est.lasso.Y[,1],OLS_prediction=est.OLS.Y,date=as.POSIXct(rownames(Y))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +\rgeom_line(size=1.5) +\rscale_x_datetime(breaks = \u0026quot;6 month\u0026quot;,date_labels = \u0026quot;%Y-%m\u0026quot;) +\rscale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,3.5))\r方向感はいい感じです。一方で、2016年1月からや2018年12月以降の急激な金利低下は予測できていません。この部分については何か変数を考えるorローリング推計を実施する、のいずれかをやってみないと精度が上がらなそうです。\n\r3. 日次解析パート\r月次での解析に加えて日次での解析もやりたいとおもいます。日次データであればデータの公表は市場が閉まり次第の場合が多いので、いわゆるjagged edgeの問題が起こりにくいと思います。まずは日次データの収集から始めます。\n# data name collected\rsymbols.name \u0026lt;- c(\u0026quot;10-Year Treasury Constant Maturity Rate\u0026quot;,\u0026quot;Effective Federal Funds Rate\u0026quot;,\u0026quot;\r6-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar\u0026quot;,\u0026quot;NASDAQ Composite Index\u0026quot;,\u0026quot;3-Month Treasury Bill: Secondary Market Rate\u0026quot;,\u0026quot;Economic Policy Uncertainty Index for United States\u0026quot;,\u0026quot;\r10-Year Breakeven Inflation Rate\u0026quot;,\u0026quot;Trade Weighted U.S. Dollar Index: Broad, Goods\u0026quot;,\u0026quot;Moody\u0026#39;s Seasoned Baa Corporate Bond Yield\u0026quot;,\u0026quot;5-Year, 5-Year Forward Inflation Expectation Rate\u0026quot;)\r# Collect economic data\rsymbols \u0026lt;- c(\u0026quot;DGS10\u0026quot;,\u0026quot;DFF\u0026quot;,\u0026quot;USD6MTD156N\u0026quot;,\u0026quot;NASDAQCOM\u0026quot;,\u0026quot;DTB3\u0026quot;,\u0026quot;USEPUINDXD\u0026quot;,\u0026quot;T10YIE\u0026quot;,\u0026quot;DTWEXB\u0026quot;,\u0026quot;DBAA\u0026quot;,\u0026quot;T5YIFR\u0026quot;)\rgetSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, src = \u0026quot;FRED\u0026quot;, auto.assign = TRUE)\r## [1] \u0026quot;DGS10\u0026quot; \u0026quot;DFF\u0026quot; \u0026quot;USD6MTD156N\u0026quot; \u0026quot;NASDAQCOM\u0026quot; \u0026quot;DTB3\u0026quot; ## [6] \u0026quot;USEPUINDXD\u0026quot; \u0026quot;T10YIE\u0026quot; \u0026quot;DTWEXB\u0026quot; \u0026quot;DBAA\u0026quot; \u0026quot;T5YIFR\u0026quot;\rNASDAQCOM.r \u0026lt;- ROC(na.omit(NASDAQCOM))\rmacro_indicator.d \u0026lt;- merge(DGS10,DFF,USD6MTD156N,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)\rrm(DGS10,DFF,USD6MTD156N,NASDAQCOM,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)\r次にデータセットを構築します。学習用と訓練用にデータを分けます。実際の予測プロセスを考え、2営業日前のデータを説明変数に用いています。\n# make dataset\rtraindata.d \u0026lt;- na.omit(merge(macro_indicator.d[\u0026quot;1980-01-01::2010-12-31\u0026quot;][,1],stats::lag(macro_indicator.d[\u0026quot;1980-01-01::2010-12-31\u0026quot;][,-1],2)))\rtestdata.d \u0026lt;- na.omit(merge(macro_indicator.d[\u0026quot;2010-01-01::\u0026quot;][,1],stats::lag(macro_indicator.d[\u0026quot;2010-01-01::\u0026quot;][,-1],2)))\r# fitting OLS\rtrial1.d \u0026lt;- lm(DGS10~.,data = traindata.d)\rsummary(trial1.d)\r## ## Call:\r## lm(formula = DGS10 ~ ., data = traindata.d)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.82445 -0.12285 0.00469 0.14332 0.73789 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -3.5051208 0.1690503 -20.734 \u0026lt; 2e-16 ***\r## DFF 0.0818269 0.0239371 3.418 0.000653 ***\r## USD6MTD156N -0.0135771 0.0233948 -0.580 0.561799 ## NASDAQCOM -0.3880217 0.4367334 -0.888 0.374483 ## DTB3 0.1227984 0.0280283 4.381 1.29e-05 ***\r## USEPUINDXD -0.0006611 0.0001086 -6.087 1.58e-09 ***\r## T10YIE 0.6980971 0.0355734 19.624 \u0026lt; 2e-16 ***\r## DTWEXB 0.0270128 0.0012781 21.135 \u0026lt; 2e-16 ***\r## DBAA 0.2988122 0.0182590 16.365 \u0026lt; 2e-16 ***\r## T5YIFR 0.3374944 0.0381111 8.856 \u0026lt; 2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.2173 on 1114 degrees of freedom\r## Multiple R-squared: 0.8901, Adjusted R-squared: 0.8892 ## F-statistic: 1002 on 9 and 1114 DF, p-value: \u0026lt; 2.2e-16\r依然決定係数は高めです。\nest.OLS.Y.d \u0026lt;- predict(trial1.d,testdata.d[,-1])\rY.d \u0026lt;- as.matrix(testdata.d[,1])\rmse.OLS.d \u0026lt;- sum((Y.d - est.OLS.Y.d)^2) / length(Y.d)\rmse.OLS.d\r## [1] 0.8003042\r次にlasso回帰です。CVで\\(\\lambda\\)を決定。\n# fitting lasso regression\rtrial2.d \u0026lt;- cv.glmnet(as.matrix(traindata.d[,-1]),as.matrix(traindata.d[,1]),family=\u0026quot;gaussian\u0026quot;,alpha=1)\rplot(trial2.d)\rtrial2.d$lambda.min\r## [1] 0.001472377\rcoef(trial2.d,s=trial2.d$lambda.min)\r## 10 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## 1\r## (Intercept) -3.4186530904\r## DFF 0.0707022021\r## USD6MTD156N . ## NASDAQCOM -0.2675513858\r## DTB3 0.1204092358\r## USEPUINDXD -0.0006506183\r## T10YIE 0.6915446819\r## DTWEXB 0.0270389569\r## DBAA 0.2861031504\r## T5YIFR 0.3376304446\rliborの係数値が0になりました。MSEはOLSの方が高い結果に。\nest.lasso.Y.d \u0026lt;- predict(trial2.d, newx = as.matrix(testdata.d[,-1]), s = trial2.d$lambda.min, type = \u0026#39;response\u0026#39;)\rmse.lasso.d \u0026lt;- sum((Y.d - est.lasso.Y.d)^2) / length(Y.d)\rmse.lasso.d\r## [1] 0.8378427\r予測値をプロットします。\nggplot(gather(data.frame(actual=Y.d[,1],lasso_prediction=est.lasso.Y.d[,1],OLS_prediction=est.OLS.Y.d,date=as.POSIXct(rownames(Y.d))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +\rgeom_line(size=1.5) +\rscale_x_datetime(breaks = \u0026quot;2 year\u0026quot;,date_labels = \u0026quot;%Y-%m\u0026quot;) +\rscale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,5))\r月次と同じく、OLSとlassoで予測値に差はほとんどありません。なかなかいい感じに変動を捉えることができていますが、2011年のUnited States federal government credit-rating downgradesによる金利低下や2013年の景気回復に伴う金利上昇は捉えることができていません。日次の景気指標はPOSデータくらいしかないんですが、強いて言うなら最近使用した夜間光の衛星画像データなんかは使えるかもしれません。時間があればやってみます。とりあえず、いったんこれでこの記事は終わりたいと思います。ここまで読み進めていただき、ありがとうございました。\n\r","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1563235200,"objectID":"4bae285f397707d2b9754abbeb8ebce3","permalink":"/post/post14/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/post/post14/","section":"post","summary":"単なる思い付きです。","tags":["R","時系列解析","金融"],"title":"10年物長期金利をフィッティングしてみる","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r1. Earth Engineを使うための事前準備\r2. Python APIを用いた衛星画像データの取得\r\rImage…\rImageCollection…\rFeatureCollection…\r\r\r\r皆さんおはこんばんにちわ。前回、GPLVMモデルを用いたGDP予測モデルを構築しました。ただ、ナウキャスティングというからにはオルタナティブデータを用いた解析を行いたいところではあります。ふと、以下の記事を見つけました。\n焦点：ナウキャストのＧＤＰ推計、世界初の衛星画像利用　利用拡大も\nこちらは東京大学の渡辺努先生が人工衛星画像を用いてGDP予測モデルを開発したというものです。記事には\n\r米国の海洋大気庁が運営する気象衛星「スオミＮＰＰ」が日本上空を通過する毎日午前１時３０分時点の画像を購入し、縦、横７２０メートル四方のマス目ごとの明るさを計測する。同じ明るさでも、農地、商業用地、工業用地など土地の用途によって経済活動の大きさが異なるため、国土地理院の土地利用調査を参照。土地の用途と、明るさが示す経済活動の相関を弾き出し、この結果を考慮した上で、明るさから経済活動の大きさを試算する。\r（中略）衛星画像のように誰もが入手可能な公表データであれば、政府、民間の区別なく分析が可能であるため、渡辺氏はこれを「統計の民主化」と呼び、世界的な潮流になると予想している。\n\rと書かれており、衛星写真を用いた分析に興味を惹かれました。 衛星写真って誰でも利用可能か？というところですが、GoogleがEarth Engineというサービスを提供していることがわかりました。\nhttps://earthengine.google.com/\n\r（拙訳）Google Earth Engineは、数ペタバイトの衛星画像群と地理空間データセットを惑星規模の解析機能と組み合わせ、科学者、研究者、開発者が変化を検出し、傾向を射影し、地球の変容を定量化することを可能にします。\n\r研究・教育・非営利目的ならば、なんと無料で衛星写真データを解析することができます。具体的に何ができるのかは以下の動画を見てください。\n\r今回はそんなEath Engineのpython APIを用いて衛星画像データを取得し、解析していきたいと思います。\n1. Earth Engineを使うための事前準備\rEarth Engineを使用するためには、Google Accountを使って申請を行う必要があります。先ほどの画像の右上の「Sign Up」からできます。申請を行って、Gmailに以下のようなメールが来るととりあえずEarth Engineは使用できるようになります。\nとりあえずというのはWEB上のEarth Engine コードエディタは使用できるということです。コードエディタというのは以下のようなもので、ブラウザ上でデータを取得したり、解析をしたり、解析結果をMAPに投影したりすることができる便利ツールです。Earth Engineの本体はむしろこいつで、APIは副次的なものと考えています。\n真ん中のコードエディタにコードを打っていきますが、言語はjavascriptです(APIはpythonとjavascript両方あるんですけどね)。解析結果をMAPに投影したり、reference（左）を参照したり、Consoleに吐き出したデータを確認することができるのでかなり便利です。が、データを落とした後で高度な解析を行いたい場合はpythonを使ったほうが慣れているので今回はAPIを使用しています。\r話が脱線しました。さて、Earth Engineの承認を得たら、pipでearthengine-apiをインストールしておきます。そして、コマンドプロンプト上で、earthengine authenticateと打ちます。そうすると、勝手にブラウザが立ち上がり、以下のようにpython apiのauthenticationを行う画面がでますので「次へ」を押下します。\n次に以下のような画面にいきますので、そのまま承認します。これでauthenticationの完成です。pythonからAPIが使えます。\n\r2. Python APIを用いた衛星画像データの取得\rPython APIを使用する準備ができました。ここからは衛星画像データを取得していきます。以下にあるようにEarth Engineにはたくさんのデータセットが存在します。\nhttps://developers.google.com/earth-engine/datasets/\n今回はVIIRS Stray Light Corrected Nighttime Day/Night Band Composites Version 1というデータセットを使用します。このデータセットは世界中の夜間光の光量を月次単位で平均し、提供するものです。サンプル期間は2014-01~現在です。\nEarth Engineにはいくつかの固有なデータ型が存在します。覚えておくべきものは以下の3つです。\nImage…\rある１時点におけるrasterデータです。imageオブジェクトはいくつかのbandで構成されています。このbandはデータによって異なりますが、おおよそのデータはbandそれぞれがRGB値を表していたりします。Earth Engineを使用する上で最も基本的なデータです。\n\rImageCollection…\rImageオブジェクトを時系列に並べたオブジェクトです。今回は時系列解析をするのでこのデータを使用します。\n\rFeatureCollection…\rGeoJSON Featureです。地理情報を表すGeometryオブジェクトやそのデータのプロパティ（国名等）が格納されています。今回は日本の位置情報を取得する際に使用しています。\nではコーディングしていきます。まず、日本の地理情報のFeatureCollectionオブジェクトを取得します。地理情報はFusion Tablesに格納されていますので、IDで引っ張りCountryがJapanのものを抽出します。ee.FeatureCollection()の引数にIDを入力すれば簡単に取得できます。\nimport ee\rfrom dateutil.parser import parse\ree.Initialize()\r# get Japan geometory as FeatureCollection from fusion table\rjapan = ee.FeatureCollection(\u0026#39;ft:1tdSwUL7MVpOauSgRzqVTOwdfy17KDbw-1d9omPw\u0026#39;).filter(ee.Filter.eq(\u0026#39;Country\u0026#39;, \u0026#39;Japan\u0026#39;))\r次に夜間光の衛星画像を取得してみます。こちらもee.ImageCollection()にデータセットのIDを渡すと取得できます。なお、ここではbandを月次の平均光量であるavg_radに抽出しています。\n# get night-light data from earth engine from 2014-01-01 to 2019-01-01\rdataset = ee.ImageCollection(\u0026#39;NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\u0026#39;).filter(ee.Filter.date(\u0026#39;2014-01-01\u0026#39;,\u0026#39;2019-01-01\u0026#39;)).select(\u0026#39;avg_rad\u0026#39;)\r取得した衛星画像を日本周辺に切り出し、画像ファイルとして出力してみましょう。画像ファイルの出力はimageオブジェクトで可能です（そうでないと画像がたくさん出てきてしまいますからね。。。）。今取得したのはImageCollectionオブジェクトですからImageオブジェクトへ圧縮してやる必要があります（上がImageCollectionオブジェクト、下が圧縮されたImageオブジェクト）。\nここでは、ImageCollectionオブジェクトの中にあるのImageオブジェクトの平均値をとってサンプル期間の平均的な画像を出力してみたいと思います。ImageCollection.mean()でできます。また、.visualize({min:0.5})でピクセル値が0.5以上でフィルターをかけています。こうしないと雲と思われるものやゴミ？みたいなものがついてしまいます。次に、ここまで加工した画像データをダウンロードするurlを.getDownloadURLメソッドで取得しています。その際、regionで切り出す範囲をポリゴン値で指定し、scaleでデータの解像度を指定しています（scaleが小さすぎると処理が重すぎるらしくエラーが出て処理できません）。\ndataset.mean().visualize(min=0.5).getDownloadURL(dict(name=\u0026#39;thumbnail\u0026#39;,region=[[[120.3345348936478, 46.853488838010854],[119.8071911436478, 24.598157870729043],[148.6353161436478, 24.75788466523463],[149.3384411436478, 46.61252884462868]]],scale=5000))\r取得した画像が以下です。\nやはり、東京を中心とした関東圏、大阪を中心とした関西圏、愛知、福岡、北海道（札幌周辺）の光量が多く、経済活動が活発であることがわかります。また、陸内よりも沿岸部で光量が多い地域があることがわかります。これは経済活動とは直接関係しない現象のような気もします。今回は分析対象外ですが、北緯38度を境に北側が真っ暗になるのが印象的です。これは言うまでもなく北朝鮮と韓国の境界線ですから、両国の経済活動水準の差が視覚的にコントラストされているのでしょう。今回使用したデータセットは2014年からのものですが、他のデータセットでは1990年代からのデータが取得できるものもあります（その代わり最近のデータは取れませんが）。それらを用いて朝鮮半島や中国の経済発展を観察するのも面白いかもしれません。\nさて、画像は取得できましたがこのままでは解析ができません。ここからは夜間光をピクセル値にマッピングしたデータを取得し、数値的な解析を試みます。ただ、先ほどとはデータ取得の手続きが少し変わります。というのも、今度は日本各地で各ピクセル単位ごとにさまざまな値をとる夜間光を集約し、1つの代用値にしなければならないからです。ピクセルごとの数値を手に入れたところで解析するには手に余ってしまいますからね。イメージは以下のような感じです（Earth Engineサイトから引用）。\n先ほど取得した夜間光のImageCollectionのある1時点の衛星画像が左です。その中に日本というRegionが存在し、それをee.Reducerによって定量的に集約（aggregate）します。Earth Engine APIには.reduceRegions()メソッドが用意されていますのでそれを用いればいいです。引数は、reducer=集約方法（ここでは合計値）、collection=集約をかけるregion（FeatureCollectionオブジェクト）、scale=解像度、です。以下では、ImageCollection（dataset）の中にある1番目のImageオブジェクトに.reduceRegions()メソッドをかけています。\n# initialize output box\rtime0 = dataset.first().get(\u0026#39;system:time_start\u0026#39;);\rfirst = dataset.first().reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(\u0026#39;time_start\u0026#39;, time0)\r我々は時系列データが欲しいわけですから、ImageCollection内にあるImageそれぞれに対して同じ処理を行う必要があります。Earth Engineにはiterateという便利な関数があり、引数に処理したい関数を渡せばfor文いらずでこの処理を行ってくれます。ここではImageオブジェクトにreduceRegionsメソッドを処理したComputed Objectを以前に処理したものとmergeするmyfuncという関数を定義し、それをiterateに渡しています。最後に、先ほどと同じく生成したデータをgetDownloadURLメソッドを用いてurlを取得しています（ファイル形式はcsv）。\n# define reduceRegions function for iteration\rdef myfunc(image,first):\radded = image.reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(\u0026#39;time_start\u0026#39;, image.get(\u0026#39;system:time_start\u0026#39;))\rreturn ee.FeatureCollection(first).merge(added)\r# implement iteration\rnightjp = dataset.filter(ee.Filter.date(\u0026#39;2014-02-01\u0026#39;,\u0026#39;2019-01-01\u0026#39;)).iterate(myfunc,first)\r# get url to download\ree.FeatureCollection(nightjp).getDownloadURL(filetype=\u0026#39;csv\u0026#39;,selectors=ee.FeatureCollection(nightjp).first().propertyNames().getInfo())\rCSVファイルのurlが取得できました。この時系列をプロットして今日は終わりにしたいと思います。\rデータを読み込むとこんな感じです。\nimport pandas as pd\rimport matplotlib.pyplot as plt\rimport os\ros.environ[\u0026#39;QT_QPA_PLATFORM_PLUGIN_PATH\u0026#39;] = \u0026#39;C:/Users/aashi/Anaconda3/Library/plugins/platforms\u0026#39;\rplt.style.use(\u0026#39;ggplot\u0026#39;)\rnightjp_csv.head()\r## system:index sum Country Unnamed: 3 Unnamed: 4\r## 0 2014/1/1 881512.4572 Japan NaN NaN\r## 1 2014/2/1 827345.3551 Japan NaN NaN\r## 2 2014/3/1 729110.4619 Japan NaN NaN\r## 3 2014/4/1 612665.8866 Japan NaN NaN\r## 4 2014/5/1 661434.5027 Japan NaN NaN\rplt.plot(pd.to_datetime(nightjp_csv[\u0026#39;system:index\u0026#39;]),nightjp_csv[\u0026#39;sum\u0026#39;])\rかなり季節性がありますね。冬場は日照時間が少ないこともあって光量が増えているみたいです。それにしても急激な増え方ですが。次回はこのデータと景況感の代理変数となる経済統計を元に統計解析を行いたいと思います。おたのしみに。\n\r\r","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1563235200,"objectID":"59f787335065eeaf96823fbe7b34c9e7","permalink":"/post/post12/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/post/post12/","section":"post","summary":"オルタナティブデータを用いた解析やってみました。","tags":["Python","前処理","Earth Engine"],"title":"Google Earth Engine APIで衛星画像データを取得し、景況感をナウキャスティングしてみる","type":"post"},{"authors":null,"categories":["競馬"],"content":"2回目になりますが、またrvestで過去のレース結果を落としてみたいと思います。過去の記事を見てないという人は先にそちらをご覧になられることをお勧めします。\n今回データを取り直そうと思ったのは、競馬の分析をした際により多くの項目を説明変数に加えて、分析をしたいと思ったからです。なので、今回は前回のRスクリプトに追記を行う形でプログラムを作成しました。新たに追加したデータ項目は以下の14個です。\n芝かダートか\r右回りか左回りか\rレースコンディション（良や稍重など）\r天候\r馬の毛色（栗毛、鹿毛など）\r馬主\r生産者\r産地\r生年月日\r父馬\r母馬\rそのレースまでの獲得賞金（2003年から入手可能）\rジョッキーの体重\rジョッキーの体重の増減\r\r実はまだデータ収集は終わっていなくて、Rのプログラムがずっと実行中になっています（3日くらい回しています）。しかし、プログラム自体はきっちり回っているのでスクリプトの紹介をしていこうと思います。もしかしたら追記で結果を書くかもしれません。\n1. スクリプトの中身\rまずはパッケージの呼び出しです。\n# rvestによる競馬データのwebスクレイピング\r#install.packages(\u0026quot;rvest\u0026quot;)\r#if (!require(\u0026quot;pacman\u0026quot;)) install.packages(\u0026quot;pacman\u0026quot;)\r#install.packages(\u0026quot;beepr\u0026quot;)\r#install.packages(\u0026quot;RSQLite\u0026quot;)\rpacman::p_load(qdapRegex)\rlibrary(rvest)\rlibrary(stringr)\rlibrary(dplyr)\rlibrary(beepr)\rlibrary(RSQLite)\rかなりwarnningが出るのでそれを禁止し、SQLiteに接続しています\n# warnning禁止\roptions(warn=-1)\r# SQLiteへの接続\rcon = dbConnect(SQLite(), \u0026quot;horse_data.db\u0026quot;, synchronous=\u0026quot;off\u0026quot;)\r1994年からしかオッズが取れないので、1994年から直近までのデータを取得します。yahoo競馬では月ごとにレースがまとめられているので、それを変数として使用しながらデータをとっていきます。基本的には、該当年、該当月のレース結果一覧へアクセスし、そのページ上の各日の個々の競馬場ごとのタイムテーブルへのリンクを取得します。個々の競馬場でレースはだいたい12ほどあるので、そのリンクを取得し、各レースのレース結果ページにアクセスします。そして、レース結果を取得していきます。まず、各日の個々の競馬場ごとのタイムテーブルへのリンクの取得方法です。\nfor(year in 1994:2019){\rstart.time \u0026lt;- Sys.time() # 計算時間を図る\r# yahoo競馬のレース結果一覧ページの取得\rfor (k in 1:12){ # kは月を表す\rtryCatch(\r{\rkeiba.yahoo \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp/schedule/list/\u0026quot;, year,\u0026quot;/?month=\u0026quot;,k)) # 該当年、該当月のレース結果一覧にアクセス\rSys.sleep(2)\rrace_lists \u0026lt;- keiba.yahoo %\u0026gt;%\rhtml_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) # 全urlを取得\r# 競馬場ごとの各日のレースリストを取得\rrace_lists \u0026lt;- race_lists[str_detect(race_lists, pattern=\u0026quot;race/list/\\\\d+/\u0026quot;)==1] # 「result」が含まれるurlを抽出\r}\r, error = function(e){signal \u0026lt;- 1}\r)\rここでは、取得したリンクのurlにresultという文字が含まれているものだけを抽出しています。要はそれが各競馬場のレーステーブルへのリンクとなります。ここからは取得した競馬場のレーステーブルのリンクを用いて、そのページにアクセスし、全12レースそれぞれのレース結果が掲載されているページのリンクを取得していきます。\n for (j in 1:length(race_lists)){ # jは当該年月にあったレーステーブルへのリンクを表す\rtryCatch(\r{\rrace_list \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,race_lists[j]))\rrace_url \u0026lt;- race_list %\u0026gt;% html_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) # 全urlを取得\r# レース結果のurlを取得\rrace_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026quot;result\u0026quot;)==1] # 「result」が含まれるurlを抽出\r}\r, error = function(e){signal \u0026lt;- 1}\r)\r各レース結果へのリンクが取得できたので、ここからはいよいよレース結果の取得とその整形パートに入ります。かなり長ったらしく複雑なコードになってしまいました。レース結果は以下のようなテーブル属性に格納されているので、まずそれを単純に引っ張ってきます。\n for (i in 1:length(race_url)){ # iは当該年月当該競馬場で開催されたレースを表す\rprint(str_c(\u0026quot;現在、\u0026quot;, year, \u0026quot;年\u0026quot;, k, \u0026quot;月\u0026quot;,j, \u0026quot;グループ、\u0026quot;, i,\u0026quot;番目のレースの保存中です\u0026quot;))\rtryCatch(\r{\rrace1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,race_url[i])) # レース結果のurlを取得\rsignal \u0026lt;- 0\rSys.sleep(2)\r}\r, error = function(e){signal \u0026lt;- 1}\r)\r# レースが中止orこれまでの過程でエラーでなければ処理を実行\rif (identical(race1 %\u0026gt;%\rhtml_nodes(xpath = \u0026quot;//div[@class = \u0026#39;resultAtt mgnBL fntSS\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text(),character(0)) == TRUE \u0026amp;\u0026amp; signal == 0){\r# レース結果をスクレイピング\rrace_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_table()\rrace_result \u0026lt;- do.call(\u0026quot;data.frame\u0026quot;,race_result) # リストをデータフレームに変更\rcolnames(race_result) \u0026lt;- c(\u0026quot;order\u0026quot;,\u0026quot;frame_number\u0026quot;,\u0026quot;horse_number\u0026quot;,\u0026quot;horse_name/age\u0026quot;,\u0026quot;time/margin\u0026quot;,\u0026quot;passing_rank/last_3F\u0026quot;,\u0026quot;jockey/weight\u0026quot;,\u0026quot;popularity/odds\u0026quot;,\u0026quot;trainer\u0026quot;) #　列名変更\rtableをただ取得しただけでは以下のように、一つのセルに複数の情報が入っていたりと分析には使えないデータとなっています。なので、これを成型する必要が出てきます。\n # 通過順位と上り3Fのタイム\rrace_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- race_result[-6]\r# タイムと着差\rrace_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026quot;)))\rrace_result$margin[race_result$order==1] \u0026lt;- \u0026quot;トップ\u0026quot;\rrace_result$margin[race_result$margin==\u0026quot;character(0)\u0026quot;] \u0026lt;- \u0026quot;大差\u0026quot;\rrace_result$margin[race_result$order==0] \u0026lt;- NA\rrace_result \u0026lt;- race_result[-5]\r# 馬名、馬齢、馬体重\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;[ァ-ヴー・]+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026quot;)))\rrace_result$horse_sex \u0026lt;- str_extract(race_result$horse_age, pattern = \u0026quot;牡|牝|せん\u0026quot;)\rrace_result$horse_age \u0026lt;- str_extract(race_result$horse_age, pattern = \u0026quot;\\\\d\u0026quot;)\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\d{3}\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026quot;)))\rrace_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- dplyr::mutate(race_result,brinker=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;B\u0026quot;)))\rrace_result$brinker[race_result$brinker!=\u0026quot;B\u0026quot;] \u0026lt;- \u0026quot;N\u0026quot;\rrace_result \u0026lt;- race_result[-4]\r# ジョッキー\rrace_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026quot;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,jockey_weight=as.character(str_extract_all(race_result$`jockey/weight`,\u0026quot;\\\\d{2}\u0026quot;)))\rrace_result$jockey_weight_change \u0026lt;- 0\rrace_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026quot;☆\u0026quot;)==1] \u0026lt;- 1\rrace_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026quot;△\u0026quot;)==1] \u0026lt;- 2\rrace_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026quot;△\u0026quot;)==1] \u0026lt;- 3\rrace_result \u0026lt;- race_result[-4]\r# オッズと人気\rrace_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\(.+\\\\)\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\d+[^(\\\\d+.\\\\d)]\u0026quot;)))\rrace_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\r次に、今取得したtable以外の情報も取り込むことにします。具体的には、レース名や天候、馬場状態、日付、競馬場などです。これらの情報はレース結果ページの上部に掲載されています。\n # レース情報\rrace_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_distance \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//p[@id = \u0026#39;raceTitMeta\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026quot;\\\\d+年\\\\d+月\\\\d+日\u0026quot;)))\rrace_result$race_date \u0026lt;- str_replace_all(race_result$race_date,\u0026quot;年\u0026quot;,\u0026quot;/\u0026quot;)\rrace_result$race_date \u0026lt;- str_replace_all(race_result$race_date,\u0026quot;月\u0026quot;,\u0026quot;/\u0026quot;)\rrace_result$race_date \u0026lt;- as.Date(race_result$race_date)\rrace_course \u0026lt;- as.character(str_extract_all(race_date,pattern = \u0026quot;札幌|函館|福島|新潟|東京|中山|中京|京都|阪神|小倉\u0026quot;))\rrace_result$race_course \u0026lt;- race_course\rrace_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,race_distance=as.character(str_extract_all(race_distance,\u0026quot;\\\\d+m\u0026quot;)))\rrace_type=as.character(str_extract_all(race_distance,pattern = \u0026quot;芝|ダート\u0026quot;))\rrace_result$type \u0026lt;- race_type\rrace_turn \u0026lt;- as.character(str_extract_all(race_distance,pattern = \u0026quot;右|左\u0026quot;))\rrace_result$race_turn \u0026lt;- race_turn\rif(length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg ryou\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_condition \u0026lt;- \u0026quot;良\u0026quot;\r} else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg yayaomo\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_condition \u0026lt;- \u0026quot;稍重\u0026quot;\r} else if (length(race1 %\u0026gt;%\rhtml_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg omo\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_condition \u0026lt;- \u0026quot;重\u0026quot;\r} else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg furyou\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_condition \u0026lt;- \u0026quot;不良\u0026quot;\r} else race_result$race_condition \u0026lt;- \u0026quot;NA\u0026quot;\rif (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg hare\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_weather \u0026lt;- \u0026quot;晴れ\u0026quot;\r} else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg ame\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_weather \u0026lt;- \u0026quot;曇り\u0026quot;\r} else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//img[@class = \u0026#39;spBg kumori\u0026#39;]\u0026quot;)) == 1){\rrace_result$race_weather \u0026lt;- \u0026quot;雨\u0026quot;\r} else race_result$race_weather \u0026lt;- \u0026quot;その他\u0026quot;\r次は各馬の情報です。 実はさきほど取得したtableの馬名はリンクになっており、そのリンクをたどると各馬の情報が取得できます（毛色や生年月日など）。\n horse_url \u0026lt;- race1 %\u0026gt;% html_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) horse_url \u0026lt;- horse_url[str_detect(horse_url, pattern=\u0026quot;directory/horse\u0026quot;)==1] # 馬情報のリンクだけ抽出する\rfor (l in 1:length(horse_url)){\rtryCatch(\r{\rhorse1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,horse_url[l]))\rSys.sleep(0.5)\rhorse_name \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;dirTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026quot;) %\u0026gt;% html_text()\rhorse \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;dirTitName\u0026#39;]/ul\u0026quot;) %\u0026gt;% html_text()\rrace_result$colour[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;毛色：.+\u0026quot;)) race_result$owner[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;馬主：.+\u0026quot;))\rrace_result$farm[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;生産者：.+\u0026quot;))\rrace_result$locality[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;産地：.+\u0026quot;))\rrace_result$horse_birthday[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026quot;\\\\d+年\\\\d+月\\\\d+日\u0026quot;))\rrace_result$father[race_result$horse_name==horse_name] \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026quot;//td[@class = \u0026#39;bloodM\u0026#39;][@rowspan = \u0026#39;4\u0026#39;]\u0026quot;) %\u0026gt;% html_text()\rrace_result$mother[race_result$horse_name==horse_name] \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026quot;//td[@class = \u0026#39;bloodF\u0026#39;][@rowspan = \u0026#39;4\u0026#39;]\u0026quot;) %\u0026gt;% html_text()\r}\r, error = function(e){\rrace_result$colour[race_result$horse_name==horse_name] \u0026lt;- NA race_result$owner[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$farm[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$locality[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$horse_birthday[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$father[race_result$horse_name==horse_name] \u0026lt;- NA\rrace_result$mother[race_result$horse_name==horse_name] \u0026lt;- NA\r}\r)\r}\rrace_result$colour \u0026lt;- str_replace_all(race_result$colour,\u0026quot;毛色：\u0026quot;,\u0026quot;\u0026quot;)\rrace_result$owner \u0026lt;- str_replace_all(race_result$owner,\u0026quot;馬主：\u0026quot;,\u0026quot;\u0026quot;)\rrace_result$farm \u0026lt;- str_replace_all(race_result$farm,\u0026quot;生産者：\u0026quot;,\u0026quot;\u0026quot;)\rrace_result$locality \u0026lt;- str_replace_all(race_result$locality,\u0026quot;産地：\u0026quot;,\u0026quot;\u0026quot;)\r#race_result$horse_birthday \u0026lt;- str_replace_all(race_result$horse_birthday,\u0026quot;年\u0026quot;,\u0026quot;/\u0026quot;)\r#race_result$horse_birthday \u0026lt;- str_replace_all(race_result$horse_birthday,\u0026quot;月\u0026quot;,\u0026quot;/\u0026quot;)\r#race_result$horse_birthday \u0026lt;- as.Date(race_result$horse_birthday)\rrace_result \u0026lt;- dplyr::arrange(race_result,horse_number) # 馬番順に並べる\r次にそのレースまでに獲得した賞金額を落としに行きます。これはレース結果のページの出馬表と書かれたリンクをたどるとアクセスできます。ここに賞金があるのでそれを取得します。\n yosou_url \u0026lt;- race1 %\u0026gt;% html_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% html_attr(\u0026quot;href\u0026quot;) yosou_url \u0026lt;- yosou_url[str_detect(yosou_url, pattern=\u0026quot;denma\u0026quot;)==1]\rif (length(yosou_url)==1){\ryosou1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,yosou_url)) Sys.sleep(2)\ryosou \u0026lt;- yosou1 %\u0026gt;% html_nodes(xpath = \u0026quot;//td[@class = \u0026#39;txC\u0026#39;]\u0026quot;) %\u0026gt;% as.character()\rprize \u0026lt;- yosou[grepl(\u0026quot;万\u0026quot;,yosou)==TRUE] %\u0026gt;% str_extract_all(\u0026quot;\\\\d+万\u0026quot;)\rprize \u0026lt;- t(do.call(\u0026quot;data.frame\u0026quot;,prize)) %\u0026gt;% as.character()\rrace_result$prize \u0026lt;- prize\rrace_result$prize \u0026lt;- str_replace_all(race_result$prize,\u0026quot;万\u0026quot;,\u0026quot;\u0026quot;) %\u0026gt;% as.numeric()\r} else race_result$prize \u0026lt;- NA\r取得した各レース結果を格納するdatasetというデータフレームを作成し、データを格納していきます。1年ごとにそれをSQLite\rへ保存していきます。\n ## ファイル貯めるのかく\rif (k == 1 \u0026amp;\u0026amp; i == 1 \u0026amp;\u0026amp; j == 1){\rdataset \u0026lt;- race_result\r} else {\rdataset \u0026lt;- rbind(dataset,race_result)\r} # if文2の終わり\r}else\r{\rprint(\u0026quot;保存できませんでした\u0026quot;) }# if文1の終わり\r} # iループの終わり\r} # jループ終わり\r} # kループの終わり\rbeep(3)\rwrite.csv(dataset,\u0026quot;race_result2.csv\u0026quot;, row.names = FALSE)\rif (year == 1994){\rdbWriteTable(con, \u0026quot;race_result\u0026quot;, dataset)\r} else {\rdbWriteTable(con, \u0026quot;temp\u0026quot;, dataset)\rdbSendQuery(con, \u0026quot;INSERT INTO race_result select * from temp\u0026quot;)\rdbSendQuery(con, \u0026quot;DROP TABLE temp\u0026quot;)\r} # ifの終わり\r} # yearループの終わり\rend.time \u0026lt;- Sys.time()\rprint(str_c(\u0026quot;処理時間は\u0026quot;,end.time-start.time,\u0026quot;です。\u0026quot;))\rbeep(5)\roptions(warn = 1)\rdbDisconnect(con)\r以上です。取れたデータは以下のようになりました。\nhead(race_result)\r## order frame_number horse_number trainer passing_rank last_3F time\r## 1 10 1 1 田中 剛 09-09 39.0 1.14.3\r## 2 16 1 2 天間 昭一 11-11 40.3 1.15.7\r## 3 15 2 3 田中 清隆 14-14 39.4 1.15.1\r## 4 9 2 4 中舘 英二 08-08 39.1 1.14.3\r## 5 12 3 5 根本 康広 11-11 39.0 1.14.4\r## 6 4 3 6 杉浦 宏昭 04-04 38.4 1.13.2\r## margin horse_name horse_age horse_sex horse_weight\r## 1 アタマ サトノジョニー 3 牡 512\r## 2 3 1/2馬身 ツギノイッテ 3 牡 464\r## 3 3馬身 ギュウホ 3 牡 444\r## 4 2 1/2馬身 セイウンメラビリア 3 牝 466\r## 5 クビ サバイバルトリック 3 牝 450\r## 6 アタマ ステイホット 3 牝 474\r## horse_weight_change brinker jockey jockey_weight jockey_weight_change\r## 1 +30 N 松岡 正海 56 0\r## 2 +8 N 西田 雄一郎 56 0\r## 3 +8 N 杉原 誠人 56 0\r## 4 +10 N 村田 一誠 54 0\r## 5 -2 N 野中 悠太郎 51 0\r## 6 -2 N 大野 拓弥 54 0\r## odds popularity race_date race_course race_name race_distance type\r## 1 40.3 9 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 2 340.9 16 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 3 283.1 14 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 4 299.7 15 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 5 26.7 8 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 6 2.4 1 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## race_turn race_condition race_weather colour owner\r## 1 右 良 晴れ 栗毛 株式会社 サトミホースカンパニー\r## 2 右 良 晴れ 黒鹿毛 西村 新一郎\r## 3 右 良 晴れ 鹿毛 有限会社 ミルファーム\r## 4 右 良 晴れ 青鹿毛 西山 茂行\r## 5 右 良 晴れ 黒鹿毛 福田 光博\r## 6 右 良 晴れ 栗毛 小林 善一\r## farm locality horse_birthday father\r## 1 千代田牧場 新ひだか町 2016年1月29日 オルフェーヴル\r## 2 織笠 時男 青森県 2016年4月17日 スクワートルスクワート\r## 3 神垣 道弘 新ひだか町 2016年4月19日 ジャングルポケット\r## 4 石郷岡 雅樹 新冠町 2016年4月21日 キンシャサノキセキ\r## 5 原田牧場 日高町 2016年4月30日 リーチザクラウン\r## 6 社台ファーム 千歳市 2016年3月13日 キャプテントゥーレ\r## mother prize\r## 1 スパークルジュエル 0\r## 2 エプソムアイリス 0\r## 3 デライトシーン 0\r## 4 ドリームシップ 0\r## 5 フリーダムガール 180\r## 6 ステイアライヴ 455\r\r","date":1562976000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1562976000,"objectID":"d5c2d4bb023ae8c645620087122f1f62","permalink":"/post/post11/","publishdate":"2019-07-13T00:00:00Z","relpermalink":"/post/post11/","section":"post","summary":"前回のデータ収集では足りないデータがあったので再度スクレイピングしてみました。","tags":["R","Webスクレイピング","前処理","SQL"],"title":"rvestでyahoo競馬にある過去のレース結果をスクレイピングしてみた（2回目）","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r\r1. GPLVMとは\r2. 最もPrimitiveなGP-LVM\r3. Rでの実装\r\r\rおはこんばんにちは。\rずいぶん前にGianonne et al (2008)のマルチファクターモデルで四半期GDPの予想を行いました。\r結果としては、ある程度は予測精度が出ていたものの彼らの論文ほどは満足のいくものではありませんでした。原因としてはクロスセクショナルなデータ不足が大きいと思われ、現在収集方法についてもEXCELを用いて改修中です。しかし一方で、マルチファクターモデルの改善も考えたいと思っています。前回は月次経済統計を主成分分析（実際にはカルマンフィルタ）を用いて次元削減を行い、主成分得点を説明変数としてGDPに回帰しました。今回はこの主成分分析のド発展版であるGaussian Process Latent Variable Model(GPLVM)を用いてファクターを計算し、それをGDPに回帰したいと思います。\n1. GPLVMとは\rGPLVMとは、Gaussian Process Modelの一種です。以前、Gaussian Process Regressionの記事を書きました。\n最も基本的なGaussian Process Modelは上の記事のようなモデルで、非説明変数\\(Y=(y_{1},y_{2},...,y_{n})\\)と説明変数\\(X=(\\textbf{x}_{1},\\textbf{x}_{2},...,\\textbf{x}_{n})\\)があり、以下のような関係式で表される際にそのモデルを直接推定することなしに新たな説明変数\\(X\\)の入力に対し、非説明変数\\(Y\\)の予測値をはじき出すというものでした。\n\\[\r\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(\\textbf{x}_{i})\r\\]\nここで、\\(\\textbf{x}_{i}\\)は\\(i\\)番目の説明変数ベクトル、\\(\\phi(・)\\)は非線形関数、 \\(\\textbf{w}^{T}\\)は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、\\(\\phi(\\textbf{x}_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\\)を想定しています（\\(x_{1,i}\\)は\\(i\\)番目の入力データ\\(\\textbf{x}_{i}\\)の１番目の変数）。詳しくは過去記事を参照してください。\n今回やるGPLVMは説明変数ベクトルが観測できない潜在変数（Latent Variable）であるところが特徴です。以下のスライドが非常にわかりやすいですが、GP-LVMは確率的主成分分析（PPCA）の非線形版という位置付けになっています。\n資料\nでは具体的な説明に移ります。GPLVMは主成分分析の発展版ですので、主に次元削減のために行われることを想定しています。つまり、データセットがあったとして、サンプルサイズ\\(n\\)よりも変数の次元\\(p\\)が大きいような場合を想定しています。\n\r2. 最もPrimitiveなGP-LVM\r先述したようにGPLVMはPPCAの非線形版です。なので、GPLVMを説明するスタートはPPCAになります。観測可能な\\(D\\)次元データセットを\\(\\{\\textbf{y}_{n}\\}_{n=1}^{N}\\)とします。そして、潜在変数を\\(\\textbf{x}_{n}\\)とおきます。今、データセットと潜在変数の間には以下のような関係があるとします。\n\\[\r\\textbf{y}_{n} = \\textbf{W}\\textbf{x}_{n} + \\epsilon_{n}\r\\]\nここで、\\(\\textbf{W}\\)はウェイト行列、\\(\\epsilon_{n}\\)はかく乱項で\\(N(0,\\beta^{-1}\\textbf{I})\\)に従います（被説明変数が多次元になることに注意）。また、\\(\\textbf{x}_{n}\\)は\\(N(0,\\textbf{I})\\)に従います。このとき、\\(\\textbf{y}_{n}\\)の尤度を\\(\\textbf{x}_{n}\\)を周辺化することで表現すると、\n\\[\r\\begin{eqnarray*}\r\\displaystyle p(\\textbf{y}_{n}|\\textbf{W},\\beta) \u0026amp;=\u0026amp; \\int p(\\textbf{y}_{n}|\\textbf{x}_{n},\\textbf{W},\\beta)N(0,\\textbf{I})d\\textbf{x}_{n} \\\\\r\\displaystyle \u0026amp;=\u0026amp; \\int N(\\textbf{W}\\textbf{x}_{n},\\beta^{-1}\\textbf{I})N(0,\\textbf{I})d\\textbf{x}_{n} \\\\\r\u0026amp;=\u0026amp; N(0,\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}) \\\\\r\\displaystyle \u0026amp;=\u0026amp; \\frac{1}{(2\\pi)^{DN/2}|\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}|^{N/2}}\\exp(\\frac{1}{2}\\textbf{tr}( (\\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{YY}^{T}))\r\\end{eqnarray*}\r\\]\nとなります。ここで、\\(p(\\textbf{y}_{n}|\\textbf{x}_{n},\\textbf{W},\\beta)=N(\\textbf{W}\\textbf{x}_{n},\\beta^{-1}\\textbf{I})\\)です。平均と分散は以下から求めました。\n\\[\r\\begin{eqnarray*}\rE(\\textbf{y}_{n}|\\textbf{W},\\beta) \u0026amp;=\u0026amp; E(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n}) \\\\\r\u0026amp;=\u0026amp; E(\\textbf{W}\\textbf{x}_{n}) + E(\\epsilon_{n}) \\\\\r\u0026amp;=\u0026amp; \\textbf{W}E(\\textbf{x}_{n}) + E(\\epsilon_{n}) = 0\r\\end{eqnarray*}\r\\]\n\\[\r\\begin{eqnarray*}\rE[(\\textbf{y}_{n}|\\textbf{W},\\beta)(\\textbf{y}_{n}|\\textbf{W},\\beta)^{T}] \u0026amp;=\u0026amp; E[ (\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n} - 0)(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n} - 0)^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ (\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n})(\\textbf{W}\\textbf{x}_{n} + \\epsilon_{n})^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ \\textbf{W}\\textbf{x}_{n}(\\textbf{W}\\textbf{x}_{n})^{T} + \\textbf{W}\\textbf{x}_{n}\\epsilon_{n}^{T} + \\epsilon_{n}\\textbf{W}\\textbf{x}_{n}^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ \\textbf{W}\\textbf{x}_{n}(\\textbf{W}\\textbf{x}_{n})^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ \\textbf{W}\\textbf{x}_{n}\\textbf{x}_{n}^{T}\\textbf{W}^{T} + \\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026amp;=\u0026amp; E[ \\textbf{W}\\textbf{x}_{n}\\textbf{x}_{n}^{T}\\textbf{W}^{T}] + E[\\epsilon_{n}\\epsilon_{n}^{T} ] \\\\\r\u0026amp;=\u0026amp; \\textbf{W}\\textbf{W}^{T} + \\beta^{-1}\\textbf{I}\r\\end{eqnarray*}\r\\]\n\\(\\textbf{W}\\)を求めるためには\\(\\textbf{y}_{n}\\)がi.i.d.と仮定し、以下のようなデータセット全体の尤度を最大化すれば良いことになります。\n\\[\r\\displaystyle p(\\textbf{Y}|\\textbf{W},\\beta) = \\prod_{n=1}^{N}p(\\textbf{y}_{n}|\\textbf{W},\\beta)\r\\]\nここで、\\(\\textbf{Y}\\)は\\(N×D\\)の計画行列です。このように、PPCAでは\\(\\textbf{x}_{n}\\)を周辺化し、\\(\\textbf{W}\\)を最適化します。逆に、Lawrence(2004)では\\(\\textbf{W}\\)を周辺化し、\\(\\textbf{x}_{n}\\)します（理由は後述）。\\(\\textbf{W}\\)を周辺化するために、\\(\\textbf{W}\\)に事前分布を与えましょう。\n\\[\r\\displaystyle p(\\textbf{W}) = \\prod_{i=1}^{D}N(\\textbf{w}_{i}|0,\\alpha^{-1}\\textbf{I})\r\\]\nここで、\\(\\textbf{w}_{i}\\)はウェイト行列\\(\\textbf{W}\\)の\\(i\\)番目の列です。では、\\(\\textbf{W}\\)を周辺化して\\(\\textbf{Y}\\)の尤度関数を導出してみます。やり方はさっきとほぼ同じなので省略します。\n\\[\r\\displaystyle p(\\textbf{Y}|\\textbf{X},\\beta) = \\frac{1}{(2\\pi)^{DN/2}|K|^{D/2}}\\exp(\\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T}))\r\\]\nここで、\\(\\textbf{K}=\\alpha^2\\textbf{X}\\textbf{X}^{T} + \\beta^{-1}\\textbf{I}\\)は\\(p(\\textbf{Y}|\\textbf{X},\\beta)\\)の分散共分散行列で、\\(\\textbf{X}=(\\textbf{x}_{1},\\textbf{x}_{2},...,\\textbf{x}_{N})^{T}\\)は入力ベクトルです。対数尤度は\n\\[\r\\displaystyle L = - \\frac{DN}{2}\\ln{2\\pi} - \\frac{1}{2}\\ln{|\\textbf{K}|} - \\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T})\r\\]\n周辺化のおかげでウェイト\\(\\textbf{W}\\)が消えたのでこれを\\(X\\)で微分してみましょう。\n\\[\r\\displaystyle\\frac{\\partial L}{ \\partial \\textbf{X}} = \\alpha^2 \\textbf{K}^{-1}\\textbf{Y}\\textbf{Y}^{T}\\textbf{K}^{-1}\\textbf{X} - \\alpha^2 D\\textbf{K}^{-1}\\textbf{X}\r\\]\nここから、\n\\[\r\\displaystyle \\frac{1}{D}\\textbf{Y}\\textbf{Y}^{T}\\textbf{K}^{-1}\\textbf{X} = \\textbf{X}\r\\]\nここで、特異値分解を用いると\n\\[\r\\textbf{X} = \\textbf{ULV}^{T}\r\\]\nとなります。\\(\\textbf{U} = (\\textbf{u}_{1},\\textbf{u}_{2},...,\\textbf{u}_{q})\\)は\\(N×q\\)直交行列、\\(\\textbf{L} = diag(l_{1},l_{2},..., l_{q})\\)は\\(q×q\\)の特異値を対角成分に並べた行列、\\(\\textbf{V}\\)は\\(q×q\\)直交行列です。これを先ほどの式に代入すると、\n\\[\r\\begin{eqnarray*}\r\\textbf{K}^{-1}\\textbf{X} \u0026amp;=\u0026amp; (\\alpha^2\\textbf{X}\\textbf{X}^{T} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{X} \\\\\r\u0026amp;=\u0026amp; \\textbf{X}(\\alpha^2\\textbf{X}^{T}\\textbf{X} + \\beta^{-1}\\textbf{I})^{-1} \\\\\r\u0026amp;=\u0026amp; \\textbf{ULV}^{T}(\\alpha^2\\textbf{VLU}^{T}\\textbf{ULV}^{T} + \\beta^{-1}\\textbf{I})^{-1} \\\\\r\u0026amp;=\u0026amp; \\textbf{ULV}^{T}\\textbf{V}(\\alpha^2\\textbf{LU}^{T}\\textbf{UL} + \\beta^{-1}\\textbf{I}^{-1})\\textbf{V}^{T} \\\\\r\u0026amp;=\u0026amp; \\textbf{UL}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{V}^{T}\r\\end{eqnarray*}\r\\]\nなので、\n\\[\r\\begin{eqnarray*}\r\\displaystyle \\frac{1}{D}\\textbf{Y}\\textbf{Y}^{T}\\textbf{UL}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1})\\textbf{V}^{T} \u0026amp;=\u0026amp; \\textbf{ULV}^{T}\\\\\r\\displaystyle \\textbf{Y}\\textbf{Y}^{T}\\textbf{UL} \u0026amp;=\u0026amp; D\\textbf{U}(\\alpha^2\\textbf{L}^{2} + \\beta^{-1}\\textbf{I})^{-1}\\textbf{L} \\\\\r\\end{eqnarray*}\r\\]\nとなります。\\(l_{j}\\)が0でなければ、\\(\\textbf{Y}\\textbf{Y}^{T}\\textbf{u}_{j} = D(\\alpha^2 l_{j}^{2} + \\beta^{-1})\\textbf{u}_{j}\\)となり、\\(\\textbf{U}\\)のそれぞれの列は\\(\\textbf{Y}\\textbf{Y}^{T}\\)の固有ベクトルであり、対応する固有値\\(\\lambda_{j}\\)は\\(D(\\alpha^2 l_{j}^{2} + \\beta^{-1})\\)となります。つまり、未知であった\\(X=ULV\\)が実は\\(\\textbf{Y}\\textbf{Y}^{T}\\)の固有値問題から求めることが出来るというわけです。\\(l_{j}\\)は上式を利用して、\n\\[\r\\displaystyle l_{j} = (\\frac{\\lambda_{j}}{D\\alpha^2} - \\frac{1}{\\beta\\alpha^2})^{1/2}\r\\]\nと\\(\\textbf{Y}\\textbf{Y}^{T}\\)の固有値\\(\\lambda_{j}\\)とパラメータから求められることがわかります。よって、\\(X=ULV\\)は\n\\[\r\\textbf{X} = \\textbf{U}_{q}\\textbf{L}\\textbf{V}^{T}\r\\]\nとなります。ここで、\\(\\textbf{U}_{q}\\)は\\(\\textbf{Y}\\textbf{Y}^{T}\\)の固有ベクトルを\\(q\\)個取り出したものです。\\(\\beta\\)が限りなく大きければ（=観測誤差が限りなく小さければ）通常のPCAと一致します。\n以上がPPCAです。GPLVMはPPCAで確率モデルとして想定していた以下のモデルを拡張します。\n\\[\r\\textbf{y}_{n} = \\textbf{W}^{T}\\textbf{x}_{n} + \\epsilon_{n}\r\\]\n具体的には、通常のガウス過程と同様、\n\\[\r\\displaystyle \\textbf{y}_{n} = \\textbf{W}^{T}\\phi(\\textbf{x}_{n})+ \\epsilon_{n}\r\\]\nという風に基底関数\\(\\phi(\\textbf{x}_{n})\\)をかませて拡張します。\\(\\phi(・)\\)は平均\\(\\textbf{0}\\)、分散共分散行列\\(\\textbf{K}_{\\textbf{x}}\\)のガウス過程と仮定します。分散共分散行列\\(\\textbf{K}_{\\textbf{x}}\\)は\n\\[\r\\textbf{K}_{\\textbf{x}} = \\alpha^2\\phi(\\textbf{x})\\phi(\\textbf{x})^T\r\\]\nであり、入力ベクトル\\(\\textbf{X}\\)を\\(\\phi(\\textbf{・})\\)で非線形変換した特徴量\\(\\phi(\\textbf{x})\\)が近いほど、出力値\\(\\textbf{Y}\\)も近くなりやすいという性質があることになります。GPLVMではこの性質を逆に利用しています。つまり、出力値\\(Y_i\\)と\\(Y_j\\)が近い→\\(\\phi(\\textbf{x}_i)\\)と\\(\\phi(\\textbf{x}_j)\\)が近い（内積が大きい）→\\(\\textbf{K}_{x,ij}\\)が大きい→観測不可能なデータ\\(X_{i}\\)と\\(X_{j}\\)は近い値（or同じようなパターン）をとる。\rこの議論からもわかるように、\\(\\textbf{K}_{\\textbf{x}}\\)は入力ベクトル\\(\\textbf{X}\\)それぞれの距離を表したものになります。分散共分散行列の計算には入力ベクトル\\(\\textbf{X}\\)を基底関数\\(\\phi(\\textbf{・})\\)で非線形変換した後、内積を求めるといったことをする必要はなく、カーネル関数を計算するのみでOKです。今回は王道中の王道RBFカーネルを使用していますので、これを例説明します。\nRBFカーネル（スカラーに対する）\r\\[\r\\theta_{1}\\exp(-\\frac{1}{\\theta_{2}}(x-x^T)^2)\r\\]\nこのRBFカーネルは以下の基底関数と対応しています。\n\\[\r\\phi(x)_h = \\tau\\exp(-\\frac{1}{r}(x-h)^2)\r\\]\n例えば、この基底関数で入力\\(x\\)を変換したものを\\(2H^2+1\\)個並べた関数を\n\\[\r\\phi(x) = (\\phi(x)_{-H^2}, ..., \\phi(x)_{0},...,\\phi(x)_{H^2})\r\\]\n入力\\(x\\)の特徴量だとすると\\(x\u0026#39;\\)との共分散\\(K_{x}(x,x\u0026#39;)\\)は内積の和なので\n\\[\rK_{x}(x,x\u0026#39;) = \\sum_{h=-H^2}^{H^2}\\phi_{h}(x)\\phi_{h}(x\u0026#39;)\r\\]\nとなります。ここで、\\(H \\to \\infty\\)とし、グリッドを極限まで細かくしてみます。\n\\[\r\\begin{eqnarray*}\rK_{x}(x,x\u0026#39;) \u0026amp;=\u0026amp; \\lim_{H \\to \\infty}\\sum_{h=-H^2}^{H^2}\\phi_{h}(x)\\phi_{h}(x\u0026#39;) \\\\\r\u0026amp;\\to\u0026amp;\\int_{-\\infty}^{\\infty}\\tau\\exp(-\\frac{1}{r}(x-h)^2)\\tau\\exp(-\\frac{1}{r}(x\u0026#39;-h)^2)dh \\\\\r\u0026amp;=\u0026amp; \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{r}\\{(x-h)^2+(x\u0026#39;-h)^2\\})dh \\\\\r\u0026amp;=\u0026amp; \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{r}\\{2(h-\\frac{x+x\u0026#39;}{2})^2+\\frac{1}{2}(x-x\u0026#39;)^2\\})dh \\\\\r\\end{eqnarray*}\r\\]\nとなります。\\(h\\)に関係のない部分を積分の外に出します。\n\\[\r\\begin{eqnarray*}\r\u0026amp;=\u0026amp; \\tau^2 \\int_{-\\infty}^{\\infty}\\exp(-\\frac{2}{r}(h-\\frac{x+x\u0026#39;}{2})^2)dh\\exp(-\\frac{1}{2r}(x-x\u0026#39;)^2) \\\\\r\\end{eqnarray*}\r\\]\n残った積分を見ると、正規分布の正規化定数と等しいことがわかります。\n\\[\r\\begin{eqnarray*}\r\\int_{-\\infty}^{\\infty}\\exp(-\\frac{1}{2\\sigma}(h-\\frac{x+x\u0026#39;}{2})^2)dh \u0026amp;=\u0026amp; \\int_{-\\infty}^{\\infty}\\exp(-\\frac{2}{r}(h-\\frac{x+x\u0026#39;}{2})^2)dh\\\\\r\\sigma \u0026amp;=\u0026amp; \\frac{r}{4}\r\\end{eqnarray*}\r\\]\nとなるので、ガウス積分の公式を用いて\n\\[\r\\begin{eqnarray*}\r\u0026amp;=\u0026amp; \\tau^2 \\sqrt{\\frac{\\pi r}{2}}\\exp(-\\frac{1}{2r}(x-x\u0026#39;)^2)　\\\\\r\u0026amp;=\u0026amp; \\theta_{1}\\exp(-\\frac{1}{\\theta_{2}}(x-x’)^2)\r\\end{eqnarray*}\r\\]\nとなり、RBFカーネルと等しくなることがわかります。よって、RBFカーネルで計算した共分散は上述した基底関数で入力\\(x\\)を無限次元へ拡張した特徴量ベクトルの内積から計算した共分散と同値になることがわかります。つまり、入力\\(x\\)と\\(x\u0026#39;\\)のスカラーの計算のみで\\(K_{x}(x,x\u0026#39;)\\)ができてしまうという夢のような計算効率化が可能になるわけです。無限次元特徴量ベクトルの回帰問題なんて普通計算できませんからね。。。カーネル関数は偉大です。\r前の記事にも載せましたが、RBFカーネルで分散共分散行列を計算したガウス過程のサンプルパスは以下通りです（\\(\\theta_1=1,\\theta_2=0.5\\)）。\n# Define Kernel function\rKernel_Mat \u0026lt;- function(X,sigma,beta){\rN \u0026lt;- NROW(X)\rK \u0026lt;- matrix(0,N,N)\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta\r}\r}\rreturn(K)\r}\rN \u0026lt;- 10 # max value of X\rM \u0026lt;- 1000 # sample size\rX \u0026lt;- matrix(seq(1,N,length=M),M,1) # create X\rtestK \u0026lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix\rlibrary(MASS)\rP \u0026lt;- 6 # num of sample path\rY \u0026lt;- matrix(0,M,P) # define Y\rfor(i in 1:P){\rY[,i] \u0026lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y\r}\r# Plot\rmatplot(x=X,y=Y,type = \u0026quot;l\u0026quot;,lwd = 2)\r非常に滑らかな関数となっていることがわかります。RBFのほかにもカーネル関数は存在します。カーネル関数を変えると基底関数が変わりますから、サンプルパスは大きく変わることになります。\nGPLVMの推定方法に話を進めましょう。PPCAの時と同じく、以下の尤度関数を最大化する観測不能な入力\\(x\\)を推定値とします。\n\\[\r\\displaystyle L = - \\frac{DN}{2}\\ln{2\\pi} - \\frac{1}{2}\\ln{|\\textbf{K}|} - \\frac{1}{2}\\textbf{tr}(\\textbf{K}^{-1}\\textbf{YY}^{T})\r\\]\nただ、PPCAとは異なり、その値は解析的に求めることができません。尤度関数の導関数は今や複雑な関数であり、展開することができないからです。よって、共役勾配法を用いて数値的に計算するのが主流なようです（自分は準ニュートン法で実装）。解析的、数値的のどちらにせよ導関数を求めておくことは必要なので、導関数を求めてみます。\n\\[\r\\frac{\\partial L}{\\partial \\textbf{K}_x} = \\frac{1}{2}(\\textbf{K}_x^{-1}\\textbf{Y}\\textbf{Y}^T\\textbf{K}_x^{-1}-D\\textbf{K}_x^{-1})\r\\]\nなので、チェーンルールから\n\\[\r\\frac{\\partial L}{\\partial \\textbf{x}} = \\frac{\\partial L}{\\partial \\textbf{K}_x}\\frac{\\partial \\textbf{K}_x}{\\partial \\textbf{x}}\r\\]\nなので、カーネル関数を決め、\\(\\textbf{x}\\)に初期値を与えてやれば勾配法によって尤度\\(L\\)が最大となる点を探索することができます。今回使用するRBFカーネルで\\(\\frac{\\partial \\textbf{K}_x}{\\partial x_{nj}}\\)を計算してみます。\n\\[\r\\begin{eqnarray*}\r\\frac{\\partial \\textbf{K}_x(\\textbf{x}_n,\\textbf{x}_n\u0026#39;)}{x_{nj}}\u0026amp;=\u0026amp;\\frac{\\partial\\theta_{1}\\exp(-\\frac{|\\textbf{x}_n-\\textbf{x}_n\u0026#39;|^2}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026amp;=\u0026amp; \\frac{\\partial\\theta_{1}\\exp(-\\frac{(\\textbf{x}_n-\\textbf{x}_n\u0026#39;)^T(\\textbf{x}_n-\\textbf{x}_n\u0026#39;)}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026amp;=\u0026amp; \\frac{\\partial\\theta_{1}\\exp(-\\frac{-(\\textbf{x}_n^T\\textbf{x}_n-2\\textbf{x}_n\u0026#39;^T\\textbf{x}_n+\\textbf{x}_n\u0026#39;^T\\textbf{x}_n\u0026#39;)}{\\theta_{2}})}{\\partial x_{nk}} \\\\\r\u0026amp;=\u0026amp; -2\\textbf{K}_x(\\textbf{x}_n\\textbf{x}_n\u0026#39;)\\frac{(x_{nj}-x_{n\u0026#39;j})}{\\theta_2}\r\\end{eqnarray*}\r\\]\n\\(j\\)番目の潜在変数の\\(n\\)番目のサンプルそれぞれに導関数を計算し、それを分散共分散行列と同じ行列に整理したものと\\(\\frac{\\partial L}{\\partial \\textbf{K}_x}\\)との要素ごとの積を足し合わせたものが勾配となります。\n\r3. Rでの実装\rGPLVMをRで実装します。使用するデータは以前giannoneの記事で使用したものと同じものです。\nESTIMATE_GPLVM \u0026lt;- function(Y,P,sigma){\r# 1. Set initial value\rY \u0026lt;- as.matrix(Y)\reigenvector \u0026lt;- eigen(cov(Y))$vectors\rX \u0026lt;- Y%*%eigenvector[,1:P] # initial value\rN \u0026lt;- NROW(Y) # Sample Size\rD \u0026lt;- NCOL(Y) # Dimention of dataset\rX0 \u0026lt;- c(as.vector(X))\rsigma \u0026lt;- var(matrix(Y,dim(Y)[1]*dim(Y)[2],1))\r# 2. Define log likelihood function\rloglik \u0026lt;- function(X0,Y,N,P,D,beta,sigma){\rX \u0026lt;- matrix(X0,N,P)\rK \u0026lt;- matrix(0,N,N)\rscale \u0026lt;- diag(sqrt(3/((apply(X, 2, max) -apply(X, 2, min))^2)))\rX \u0026lt;- X%*%scale\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- sigma*exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)\r}\r}\rL \u0026lt;- - D*N/2*log(2*pi) - D/2*log(det(K)) - 1/2*sum(diag(ginv(K)%*%Y%*%t(Y))) #loglikelihood\rreturn(L)\r}\r# 3. Define derivatives of log likelihood function\rdloglik \u0026lt;- function(X0,P,D,N,Y,beta,sigma){\rX \u0026lt;- matrix(X0,N,P)\rK \u0026lt;- matrix(0,N,N)\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)\r}\r}\rinvK \u0026lt;- ginv(K)\rdLdK \u0026lt;- invK%*%Y%*%t(Y)%*%invK - D*invK\rdLdx \u0026lt;- matrix(0,N,P)\rfor (j in 1:P){\rfor(i in 1:N){\rdKdx \u0026lt;- matrix(0,N,N)\rfor (k in 1:N){\rdKdx[i,k] \u0026lt;- dKdx[k,i] \u0026lt;- -exp(-(t(X[i,]-X[k,])%*%(X[i,]-X[k,]))*0.5)*((X[i,j]-X[k,j])*0.5)\r}\rdLdx[i,j] \u0026lt;- sum(dLdK*dKdx)\r}\r}\rreturn(dLdx)\r}\r# 4. Optimization\rres \u0026lt;- optim(X0, loglik, dloglik, Y = Y, N=N, P=P, D=D, beta = exp(2), sigma = sigma,\rmethod = \u0026quot;BFGS\u0026quot;, control = list(fnscale = -1,trace=1000,maxit=10000))\routput \u0026lt;- matrix(res$par,N,P)\rresult \u0026lt;- list(output,res,P)\rnames(result) \u0026lt;- c(\u0026quot;output\u0026quot;,\u0026quot;res\u0026quot;,\u0026quot;P\u0026quot;)\rreturn(result)\r}\rGPLVM_SELECT \u0026lt;- function(Y){\rD \u0026lt;- NCOL(Y)\rlibrary(stringr)\rfor (i in 1:D){\rif (i == 1){\rresult \u0026lt;- ESTIMATE_GPLVM(Y,i)\rP \u0026lt;- 2\rprint(str_c(\u0026quot;STEP\u0026quot;, i, \u0026quot; loglikelihood \u0026quot;, as.numeric(result$res$value)))\r}else{\rtemp \u0026lt;- ESTIMATE_GPLVM(Y,i)\rprint(str_c(\u0026quot;STEP\u0026quot;, i, \u0026quot; loglikelihood \u0026quot;, as.numeric(temp$res$value)))\rif (result$res$value \u0026lt; temp$res$value){\rresult \u0026lt;- temp\rP \u0026lt;- i\r}\r}\r}\rprint(str_c(\u0026quot;The optimal number of X is \u0026quot;, P))\rprint(str_c(\u0026quot;loglikelihood \u0026quot;, as.numeric(result$res$value)))\rreturn(result)\r}\rresult \u0026lt;- ESTIMATE_GPLVM(scale(Y),5)\r## initial value 613.864608 ## final value 609.080329 ## converged\rlibrary(tidyverse)\rggplot(gather(as.data.frame(result$output),key = name,value = value),\raes(x=rep(dataset1$publication,5),y=value,colour=name)) + geom_line(size=1) +\rxlab(\u0026quot;Date\u0026quot;) +\rggtitle(\u0026quot;5 economic factors\u0026quot;)\rlibrary(xts)\rX.xts \u0026lt;- xts(result$output,order.by = dataset1$publication)\rX.q.xts \u0026lt;- apply.quarterly(X.xts,mean)\rX.3m.xts \u0026lt;- X.xts[endpoints(X.xts,on=\u0026quot;quarters\u0026quot;),]\rif (months(index(X.q.xts)[NROW(X.q.xts)]) %in% c(\u0026quot;3月\u0026quot;,\u0026quot;6月\u0026quot;,\u0026quot;9月\u0026quot;,\u0026quot;12月\u0026quot;)){\r} else X.q.xts \u0026lt;- X.q.xts[-NROW(X.q.xts),]\rif (months(index(X.3m.xts)[NROW(X.3m.xts)]) %in% c(\u0026quot;3月\u0026quot;,\u0026quot;6月\u0026quot;,\u0026quot;9月\u0026quot;,\u0026quot;12月\u0026quot;)){\r} else X.3m.xts \u0026lt;- X.3m.xts[-NROW(X.3m.xts),]\rcolnames(X.xts) \u0026lt;- c(\u0026quot;factor1\u0026quot;,\u0026quot;factor2\u0026quot;,\u0026quot;factor3\u0026quot;,\u0026quot;factor4\u0026quot;,\u0026quot;factor5\u0026quot;) GDP$publication \u0026lt;- GDP$publication + months(2)\rGDP.q \u0026lt;- GDP[GDP$publication\u0026gt;=index(X.q.xts)[1] \u0026amp; GDP$publication\u0026lt;=index(X.q.xts)[NROW(X.q.xts)],]\rrg \u0026lt;- lm(scale(GDP.q$GDP)~X.q.xts[-54])\rrg2 \u0026lt;- lm(scale(GDP.q$GDP)~X.3m.xts[-54])\rsummary(rg)\r## ## Call:\r## lm(formula = scale(GDP.q$GDP) ~ X.q.xts[-54])\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.31774 -0.06927 -0.03224 0.06690 0.31062 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.0445000 0.0317777 1.400 0.177 ## X.q.xts[-54]X.1 -0.3181108 0.0106335 -29.916 \u0026lt; 2e-16 ***\r## X.q.xts[-54]X.2 0.0004621 0.0175913 0.026 0.979 ## X.q.xts[-54]X.3 0.1737612 0.0249186 6.973 9.09e-07 ***\r## X.q.xts[-54]X.4 -0.0060035 0.0376324 -0.160 0.875 ## X.q.xts[-54]X.5 0.0646009 0.0430678 1.500 0.149 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.1617 on 20 degrees of freedom\r## Multiple R-squared: 0.9791, Adjusted R-squared: 0.9739 ## F-statistic: 187.3 on 5 and 20 DF, p-value: 4.402e-16\rsummary(rg2)\r## ## Call:\r## lm(formula = scale(GDP.q$GDP) ~ X.3m.xts[-54])\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.35613 -0.11430 0.00258 0.15171 0.36506 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.003798 0.041789 -0.091 0.928 ## X.3m.xts[-54]1 -0.312411 0.014008 -22.303 1.34e-15 ***\r## X.3m.xts[-54]2 0.022873 0.025672 0.891 0.384 ## X.3m.xts[-54]3 0.152350 0.031080 4.902 8.62e-05 ***\r## X.3m.xts[-54]4 -0.019237 0.047809 -0.402 0.692 ## X.3m.xts[-54]5 -0.032419 0.055662 -0.582 0.567 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 0.21 on 20 degrees of freedom\r## Multiple R-squared: 0.9647, Adjusted R-squared: 0.9559 ## F-statistic: 109.3 on 5 and 20 DF, p-value: 8.102e-14\r決定係数が大幅に改善しました。\r3ヶ月の平均をとったファクターの方がパフォーマンスが良さそうなので、こちらで実際のGDPと予測値のプロットを行ってみます。\nggplot(gather(data.frame(fit=rg$fitted.values,actual=scale(GDP.q$GDP),Date=GDP.q$publication),key,value,-Date),aes(y=value,x=Date,colour=key)) +\rgeom_line(size=1) +\rggtitle(\u0026quot;fit v.s. actual GDP\u0026quot;)\rいかがでしょうか。個人的にはかなりフィッティングできている印象があります（もはや経済理論など不要なのでしょうか）。ただ、最も新しい値を除いては未来の値が情報量として加味された上で推計されていることになりますから、フェアではありません。正しく予測能力を検証するためには四半期ごとに逐次的に回帰を行う必要があります。\nというわけで、アウトサンプルの予測力がどれほどあるのかをテストしてみたいと思います。まず、2005年4月から2007年3月までの月次統計データでファクターを計算し、データ頻度を四半期に集約します。そして、2007年1Qのデータを除いて、GDPに回帰します。回帰したモデルの係数を用いて、2007年1Qのファクターデータで同時点のGDPの予測値を計算し、それを実績値と比較します。次は2005年4月から2007年6月までのデータを用いて･･･という感じでアウトサンプルの予測を行ってみます。\nlibrary(lubridate)\rtest_df \u0026lt;- data.frame()\rfor (i in as.list(seq(as.Date(\u0026quot;2015-04-01\u0026quot;),as.Date(\u0026quot;2019-03-01\u0026quot;),by=\u0026quot;quarter\u0026quot;))){\rday(i) \u0026lt;- days_in_month(i)\rtraindata \u0026lt;- dataset1[dataset1$publication\u0026lt;=i,]\rX_train \u0026lt;- ESTIMATE_GPLVM(scale(traindata[,-2]),5)\rX_train.xts \u0026lt;- xts(X_train$output,order.by = traindata$publication)\rX_train.q.xts \u0026lt;- apply.quarterly(X_train.xts,mean)\rif (months(index(X_train.q.xts)[NROW(X_train.q.xts)]) %in% c(\u0026quot;3月\u0026quot;,\u0026quot;6月\u0026quot;,\u0026quot;9月\u0026quot;,\u0026quot;12月\u0026quot;)){\r} else X_train.q.xts \u0026lt;- X_train.q.xts[-NROW(X_train.q.xts),]\rcolnames(X_train.q.xts) \u0026lt;- c(\u0026quot;factor1\u0026quot;,\u0026quot;factor2\u0026quot;,\u0026quot;factor3\u0026quot;,\u0026quot;factor4\u0026quot;,\u0026quot;factor5\u0026quot;) GDP_train.q \u0026lt;- scale(GDP[GDP$publication\u0026gt;=index(X_train.q.xts)[1] \u0026amp; GDP$publication\u0026lt;=index(X_train.q.xts)[NROW(X_train.q.xts)],2])\rrg_train \u0026lt;- lm(GDP_train.q[-NROW(GDP_train.q)]~.,data=X_train.q.xts[-NROW(X_train.q.xts)])\rsummary(rg_train)\rtest_df \u0026lt;- rbind(test_df,data.frame(predict(rg_train,X_train.q.xts[NROW(X_train.q.xts)],interval = \u0026quot;prediction\u0026quot;,level=0.90),GDP=GDP_train.q[NROW(GDP_train.q)]))\r}\r計算できました。グラフにしてみましょう。先ほどのグラフよりは精度が悪くなりました。特にリーマンの後は予測値の信頼区間（90%）が大きく拡大しており、不確実性が増大していることもわかります。2010年以降に関しては実績値は信頼区間にほど入っており、予測モデルとしての性能はまあまあなのかなと思います。ただ、リーマンのような金融危機もズバッと当てるところにロマンがあると思うので元データの改善を図りたいと思います。この記事はいったんここで終了です。\nggplot(gather(data.frame(test_df,Date=as.Date(rownames(test_df))),,,-c(lwr,upr,Date)),aes(y=value,x=Date,colour=key)) +\rgeom_ribbon(aes(ymax=upr,ymin=lwr,fill=\u0026quot;band\u0026quot;),alpha=0.1,linetype=\u0026quot;blank\u0026quot;) +\rgeom_line(size=1) +\rggtitle(\u0026quot;out-sample test\u0026quot;)\r\r","date":1558828800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1558828800,"objectID":"453e55777bb25992fabfa629782065a2","permalink":"/post/post8/","publishdate":"2019-05-26T00:00:00Z","relpermalink":"/post/post8/","section":"post","summary":"最近私が注目しているのがガウス過程。今回はそのガウス過程の中でも、主成分分析のようにデータセットの次元削減のために使用されるGaussian Process Latent Variable Model（GPLVM）の紹介をします。","tags":["R","ガウス過程","GPLVM"],"title":"GPLVMでマルチファクターモデルを構築してみた","type":"post"},{"authors":null,"categories":["仕事関連"],"content":"\r\r\r\r1. 最小分散ポートフォリオ\r2. 分散共分散行列をどのように求めるか\r\rA. Constant conditional correlation (CCC) model\rB. Dynamic Conditional Correlation (DCC) model\rC. Dynamic Equicorrelation (DECO) model\r\r3. テスト用データの収集\r\r\rおはこんばんにちは。勤め先で、アセットアロケーションに関するワークショップに参加したので、この分野は完全なる専門外ですがシミュレーションをしてみたいと思います。今回は、最小分散ポートフォリオ(minimum variance portfolio)を基本ポートフォリオとしたうえで、その分散共分散行列（予測値）をどのように推計するのかという点について先行研究を参考にエクササイズしていきたいと思います。先行研究は以下の論文です（オペレーションリサーチのジャーナルでした）。\nAsset Allocation with Correlation: A Composite Trade-Off\n1. 最小分散ポートフォリオ\r最小分散ポートフォリオの詳しい説明はここでは割愛しますが、要は各資産（内株、外株、内債、外債、オルタナ）のリターンの平均と分散を計算し、それらを縦軸平均値、横軸分散の二次平面にプロットしたうえで、投資可能範囲を計算し、その集合の中で最も分散が小さくなるポートフォリオの事らしいです（下図参照）。\nminimum variance portfolio\n\r先行研究のCarroll et. al. (2017)では、\n\rthis paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.\n\rと記載されており、現状でも期待リターンの推計は難しく、それを必要としない最小分散ポートフォリオは有益で実践的な手法であるといえます。最小分散ポートフォリオの目的関数は、その名の通り「分散を最小化すること」です。今、各資産のリターンを集めたベクトルを[tex:r]、各資産の保有ウェイトを\\(\\theta\\)、ポートフォリオリターンを\\(R_{p}\\)で表すことにすると、ポートフォリオ全体の分散\\(var(R_{p})\\)は以下のように記述できます。\n\\[\rvar(R_{p}) = var(r^{T}\\theta) = E( (r^{T}\\theta)(r^{T}\\theta)^{T}) = \\theta^{T}\\Sigma\\theta\r\\]\nここで\\(Sigma\\)は\\(r\\)の分散共分散行列です。よって、最小化問題は以下になります。\n\\[\r\\min_{\\theta}(\\theta^{T}\\Sigma\\theta) \\\\\rs.t 1^{T}\\theta = 1\r\\]\nここでは、フルインベストメントを制約条件に加えています。ラグランジュ未定乗数法を用いてこの問題を解いてみましょう。ラグランジュ関数\\(L\\)は以下のようになります。\n\\[\rL = \\theta^{T}\\Sigma\\theta + \\lambda(1^{T}\\theta - 1)\r\\]\n1階の条件は、\n\\[\r\\displaystyle\\frac{\\partial L}{\\partial \\theta} = 2\\Sigma\\theta + 1\\lambda = 0 \\\\\r\\displaystyle \\frac{\\partial L}{\\partial \\lambda} = 1^{T} \\theta = 1\r\\]\n1本目の式を\\(\\theta\\)について解くと、\n\\[\r\\theta = \\Sigma^{-1}1\\lambda^{*}\r\\]\nとなります。ここで、\\(\\lambda^{*}=-1/2\\lambda\\)です。これを2本目の式に代入し、\\(\\lambda^{*}\\)について解きます。\n\\[\r1^{T}\\Sigma1\\lambda^{*} = 1 \\\\\r\\displaystyle \\lambda^{*} = \\frac{1}{1^{T}\\Sigma^{-1}1}\r\\]\n\\(\\theta = \\Sigma^{-1}1\\lambda^{*}\\)だったので、\\(\\lambda^{*}\\)を消去すると、\n\\[\r\\displaystyle \\theta_{gmv} = \\frac{\\Sigma^{-1}1}{1^{T}\\Sigma^{-1}1}\r\\]\nとなり、最適なウェイトを求めることができました。とりあえず、これをRで実装しておきます。\ngmv \u0026lt;- function(r_dat,r_cov){\rlibrary(MASS)\ri \u0026lt;- matrix(1,NCOL(r_dat),1)\rr_weight \u0026lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i)\rwr_dat \u0026lt;- r_dat*as.numeric(r_weight)\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;) return(result)\r}\rnlgmv \u0026lt;- function(r_dat,r_cov){\rqp.out \u0026lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))),\rbvec=c(1,rep(0,NCOL(r_dat))),meq=1)\rr_weight \u0026lt;- qp.out$solution\rwr_dat \u0026lt;- r_dat*r_weight\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;)\rreturn(result)\r}\r入力は各資産のリターンと分散共分散行列になっています。出力はウェイト、リターン、リスクです。nlgmvは最小分散ポートフォリオの空売り制約バージョンです。解析的な解は得られないので、数値的に買いを求めています。\n\r2. 分散共分散行列をどのように求めるか\r最小分散ポートフォリオの計算式は求めることができました。次は、その入力である分散共分散行列をどうやって求めるのかについて分析したいと思います。一番原始的な方法はその時点以前に利用可能なリターンデータを標本として分散共分散行列を求め、その値を固定して最小分散ポートフォリオを求めるというヒストリカルなアプローチかと思います（つまりウェイトも固定）。ただ、これはあくまで過去の平均値を将来の予想値に使用するため、いろいろ問題が出てくるかと思います。専門外の私が思いつくものとしては、前日ある資産Aのリターンが大きく下落したという場面で明日もこの資産の分散は大きくなることが予想されるにも関わらず、平均値を使用するため昨日の効果が薄められてしまうことでしょうか。それに、ウェイトを最初から変更しないというのも時間がたつにつれ、最適点から離れていく気がします。ただ、ではどう推計するのかついてはこの分野でも試行錯誤が行われているようです。Carroll et. al. (2017)でも、\n\rThe estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.\n\rと述べられていました。この論文では以下のようなモデルを用いて推計が行われています。いずれも、分散共分散行列を時変としているところに特徴があります。\nA. Constant conditional correlation (CCC) model\r元論文はこちら。\nまず、分散共分散行列と相関行列の関係性から、\\(\\Sigma_{t} = D_{t}R_{t}D_{t}\\)となります。ここで、\\(R_{t}\\)は相関行列、\\(D_{t}\\)は\\(diag(\\sigma_{1,t},...,\\sigma_{N,t})\\)で各資産\\(tt\\)期の標準偏差\\(\\sigma_{i,t}\\)を対角成分に並べた行列です。ここから、\\(D_{t}\\)と\\(R_{t}\\)を分けて推計していきます。まず、\\(D_{t}\\)ですが、こちらは以下のような多変量GARCHモデル(1,1)で推計します。\n\\[\rr_{t} = \\mu + u_{t} \\\\\ru_{t} = \\sigma_{t}\\epsilon \\\\\r\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}u_{t-1}^{2} + \\alpha_{2}\\sigma_{t-1}^{2} \\\\\r\\epsilon_{t} = NID(0,1) \\\\\rE(u_{t}|u_{t-1}) = 0\r\\]\nここで、\\(\\mu\\)はリターンの標本平均です。\\(\\alpha_{i}\\)は推定すべきパラメータ。\\(D_{t}\\)をGARCHで推計しているので、リターンの分布が正規分布より裾野の厚い分布に従い、またリターンの変化は一定ではなく前日の分散に依存する関係をモデル化しているといえるのではないでしょうか。とりあえずこれで\\(D_{t}\\)の推計はできたということにします。次に\\(R_{t}\\)の推計ですが、このモデルではリターンを標本として求めるヒストリカルなアプローチを取ります。つまり、\\(R_{t}\\)は定数です。よって、リターン変動の大きさは時間によって変化するが、各資産の相対的な関係性は不変であるという仮定を置いていることになります。\n\rB. Dynamic Conditional Correlation (DCC) model\r元論文はこちら。\nこちらのモデルでは、\\(D_{t}\\)を求めるところまでは①と同じですが、[\\(R_{t}\\)の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、\\(tex:t\\)期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列\\(R_{t}\\)は、\n\\[\rR_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2}\r\\]\nです。ここで、\\(Q_{t}\\)は\\(t\\)期での条件付分散共分散行列で以下のように定式化されます。\n\\[\rQ_{t} = \\bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\\epsilon_{i,t-1}\\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1}\r\\]\nここで、\\(\\bar{Q}\\)はヒストリカルな方法で計算した分散共分散行列であり、\\(a,b\\)はパラメータです。この方法では、先ほどとは異なり、リターン変動の大きさが時間によって変化するだけでなく、各資産の相対的な関係性も通時的に変化していくという仮定を置いていることになります。金融危機時には全資産のリターンが下落し、各資産の相関が正になる事象も観測されていることから、この定式化は魅力的であるということができるのではないでしょうか。\n\rC. Dynamic Equicorrelation (DECO) model\r元論文はこちら。\nこの論文はまだきっちり読めていないのですが、相関行列\\(R_{t}\\)の定義から\n\\[\rR_{t} = (1-\\rho_{t})I_{N} + \\rho_{t}1\r\\]\nとなるようです。ここで、\\(\\rho_{t}\\)はスカラーでequicorrelationの程度を表す係数です。equicorrelationとは平均的なペアワイズ相関の事であると理解しています。つまりは欠損値がなければ普通の相関と変わりないんじゃないかと。ただ、資産が増えればそのような問題にも対処する必要があるのでその点ではよい推定量のようです。\\(\\rho_{t}\\)は以下のように求めることができます。\n\\[\r\\displaystyle \\rho_{t} = \\frac{1}{N(N-1)}(\\iota^{T}R_{t}^{DCC}\\iota - N) = \\frac{2}{N(N-1)}\\sum_{i\u0026gt;j}\\frac{q_{ij,t}}{\\sqrt{q_{ii,t} q_{jj,t}}}\r\\]\nここで、\\(\\iota\\)はN×1ベクトルで要素は全て1です。また、\\(q_{ij,t}\\)は\\(Q_{t}\\)のi,j要素です。\nさて、分散共分散行列のモデル化ができたところで、ここまでをRで実装しておきます。\ncarroll \u0026lt;- function(r_dat,FLG){\rlibrary(rmgarch)\rif(FLG == \u0026quot;benchmark\u0026quot;){\rH \u0026lt;- cov(r_dat)\r}else{\r#1. define variables\rN \u0026lt;- NCOL(r_dat) # the number of assets\r#2. estimate covariance matrix\rbasic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = \u0026#39;sGARCH\u0026#39;), distribution.model = \u0026#39;norm\u0026#39;)\rmulti_garch = multispec(replicate(N, basic_garch))\rdcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = \u0026quot;mvnorm\u0026quot;,model = \u0026quot;DCC\u0026quot;)\rfit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE))\rforecast_dcc_garch \u0026lt;- dccforecast(fit_dcc_garch)\rif (FLG == \u0026quot;CCC\u0026quot;){\r#Constant conditional correlation (CCC) model\rD \u0026lt;- sigma(forecast_dcc_garch)\rR_ccc \u0026lt;- cor(r_dat)\rH \u0026lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1])\rcolnames(H) \u0026lt;- colnames(r_dat)\rrownames(H) \u0026lt;- colnames(r_dat)\r}\relse{\r#Dynamic Conditional Correlation (DCC) model\rH \u0026lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1])\rif (FLG == \u0026quot;DECO\u0026quot;){\r#Dynamic Equicorrelation (DECO) model\rone \u0026lt;- matrix(1,N,N)\riota \u0026lt;- rep(1,N)\rQ_dcc \u0026lt;- rcor(forecast_dcc_garch,type=\u0026quot;Q\u0026quot;)[[1]][,,1]\rrho \u0026lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N))\rD \u0026lt;- sigma(forecast_dcc_garch)\rR_deco \u0026lt;- (1-rho)*diag(1,N,N) + rho*one\rH \u0026lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1])\rcolnames(H) \u0026lt;- colnames(r_dat)\rrownames(H) \u0026lt;- colnames(r_dat)\r}\r}\r}\rreturn(H)\r}\r本来であれば、パッケージを使用するべきではないのですが、今日はエクササイズなので推計結果だけを追い求めたいと思います。GARCHについては再来週ぐらいに記事を書く予定です。\rこれで準備ができました。この関数にリターンデータを入れて、分散共分散行列を計算し、それを用いて最小分散ポートフォリオを計算することができるようになりました。\n\r\r3. テスト用データの収集\rデータは以下の記事を参考にしました。\n(Introduction to Asset Allocation)[https://www.r-bloggers.com/introduction-to-asset-allocation/]\n使用したのは、以下のインデックスに連動するETF(iShares)の基準価額データです。\n\rS\u0026amp;P500\rNASDAQ100\rMSCI Emerging Markets\rRussell 2000\rMSCI EAFE\rUS 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index)\rU.S. Real Estate(the Dow Jones US Real Estate Index)\rgold bullion market\r\rまず、データ集めです。\nlibrary(quantmod)\r#**************************\r# ★8 ASSETS SIMULATION\r# SPY - S\u0026amp;P 500 # QQQ - Nasdaq 100\r# EEM - Emerging Markets\r# IWM - Russell 2000\r# EFA - EAFE\r# TLT - 20 Year Treasury\r# IYR - U.S. Real Estate\r# GLD - Gold\r#**************************\r# load historical prices from Yahoo Finance\rsymbol.names = c(\u0026quot;S\u0026amp;P 500\u0026quot;,\u0026quot;Nasdaq 100\u0026quot;,\u0026quot;Emerging Markets\u0026quot;,\u0026quot;Russell 2000\u0026quot;,\u0026quot;EAFE\u0026quot;,\u0026quot;20 Year Treasury\u0026quot;,\u0026quot;U.S. Real Estate\u0026quot;,\u0026quot;Gold\u0026quot;)\rsymbols = c(\u0026quot;SPY\u0026quot;,\u0026quot;QQQ\u0026quot;,\u0026quot;EEM\u0026quot;,\u0026quot;IWM\u0026quot;,\u0026quot;EFA\u0026quot;,\u0026quot;TLT\u0026quot;,\u0026quot;IYR\u0026quot;,\u0026quot;GLD\u0026quot;)\rgetSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, auto.assign = TRUE)\r#gn dates for all symbols \u0026amp; convert to monthly\rhist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)\rmonth.ends = endpoints(hist.prices, \u0026#39;day\u0026#39;)\rhist.prices = Cl(hist.prices)[month.ends, ]\rcolnames(hist.prices) = symbols\r# remove any missing data\rhist.prices = na.omit(hist.prices[\u0026#39;1995::\u0026#39;])\r# compute simple returns\rhist.returns = na.omit( ROC(hist.prices, type = \u0026#39;discrete\u0026#39;) )\r# compute historical returns, risk, and correlation\ria = list()\ria$expected.return = apply(hist.returns, 2, mean, na.rm = T)\ria$risk = apply(hist.returns, 2, sd, na.rm = T)\ria$correlation = cor(hist.returns, use = \u0026#39;complete.obs\u0026#39;, method = \u0026#39;pearson\u0026#39;)\ria$symbols = symbols\ria$symbol.names = symbol.names\ria$n = length(symbols)\ria$hist.returns = hist.returns\r# convert to annual, year = 12 months\rannual.factor = 12\ria$expected.return = annual.factor * ia$expected.return\ria$risk = sqrt(annual.factor) * ia$risk\rrm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)\rリターンをプロットするとこんな感じです。\nPerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = \u0026quot;パフォーマンスサマリー\u0026quot;)\r次に、バックテストのコーディングを行います。一気にコードを公開します。\n# BACK TEST\rbacktest \u0026lt;- function(r_dat,FLG,start_date,span,learning_term,port){\r#-----------------------------------------\r# BACKTEST\r# r_dat - return data(xts object) # FLG - flag(CCC,DCC,DECO)\r# start_date - start date for backtest\r# span - rebalance frequency\r# learning_term - learning term (days)\r# port - method of portfolio optimization\r#-----------------------------------------\rlibrary(stringi)\rinitial_dat \u0026lt;- r_dat[stri_c(as.Date(start_date)-learning_term,\u0026quot;::\u0026quot;,as.Date(start_date))]\rfor (i in NROW(initial_dat):NROW(r_dat)) {\rif (i == NROW(initial_dat)){\rH \u0026lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG)\rif (port == \u0026quot;nlgmv\u0026quot;){\rresult \u0026lt;- nlgmv(initial_dat,H)\r}else if (port == \u0026quot;risk parity\u0026quot;){\rresult \u0026lt;- risk_parity(initial_dat,H)\r}\rweight \u0026lt;- t(result$weight)\rcolnames(weight) \u0026lt;- colnames(initial_dat)\rp_return \u0026lt;- initial_dat[NROW(initial_dat),]*result$weight\r} else {\rif (i %in% endpoints(r_dat,span)){\rH \u0026lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG)\rif (port == \u0026quot;nlgmv\u0026quot;){\rresult \u0026lt;- nlgmv(test_dat,H)\r}else if (port == \u0026quot;risk parity\u0026quot;){\rresult \u0026lt;- risk_parity(test_dat,H)\r}\r}\rweight \u0026lt;- rbind(weight,t(result$weight))\rp_return \u0026lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight)\r}\rif (i != NROW(r_dat)){\rterm \u0026lt;- stri_c(index(r_dat[i+1,])-learning_term,\u0026quot;::\u0026quot;,index(r_dat[i+1,])) test_dat \u0026lt;- r_dat[term]\r}\r}\rp_return$portfolio \u0026lt;- xts(apply(p_return,1,sum),order.by = index(p_return))\rweight.xts \u0026lt;- xts(weight,order.by = index(p_return))\rresult \u0026lt;- list(p_return,weight.xts)\rnames(result) \u0026lt;- c(\u0026quot;return\u0026quot;,\u0026quot;weight\u0026quot;)\rreturn(result)\r}\rCCC \u0026lt;- backtest(hist.returns,\u0026quot;CCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDCC \u0026lt;- backtest(hist.returns,\u0026quot;DCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDECO \u0026lt;- backtest(hist.returns,\u0026quot;DECO\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rbenchmark \u0026lt;- backtest(hist.returns,\u0026quot;benchmark\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rresult \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)\rcolnames(result) \u0026lt;- c(\u0026quot;CCC\u0026quot;,\u0026quot;DCC\u0026quot;,\u0026quot;DECO\u0026quot;,\u0026quot;benchmark\u0026quot;)\r計算結果をグラフにしてみます。\nPerformanceAnalytics::charts.PerformanceSummary(result,main = \u0026quot;BACKTEST\u0026quot;)\r空売り制約を課したので、上で定義した最小分散ポートフォリオは使用していません。どうやらこれだけで解析的に解くのは難しいらしく、数値的に解くことにしています。リバランス期間を週次にしたので、自前のPCでは計算に時間がかかりましたが、結果が計算できました。\nリーマン以降はどうやらベンチマークである等ウェイトポートフォリオよりをアウトパフォームしているようです。特に、DECOはいい感じです。そもそもDECOとDCCはほぼ変わらないパフォーマンスであると思っていたのですが、どうやら自分の理解が足らないらしく、論文の読み返す必要があるようです。Equicorrelationの意味をもう一度考えてみたいと思います。それぞれの組入比率の推移は以下のようになりました。\n# plot allocation weighting\rd_allocation \u0026lt;- function(ggweight,title){\r#install.packages(\u0026quot;tidyverse\u0026quot;)\rlibrary(tidyverse)\rggweight \u0026lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method)\rggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) +\rgeom_area(colour=\u0026quot;black\u0026quot;,size=.1) +\rscale_y_continuous(limits = c(0,1)) +\rlabs(title=title) + facet_grid(method~.)\r}\rgmv_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)))\r# plot allocation weighting\rd_allocation(gmv_weight,\u0026quot;GMV Asset Allocation\u0026quot;)\rリーマンの際にTLT、つまり米国債への比率を増やしているようです。CCCとDCCはそれ以外の部分でも米国債への比率が高く、よく挙げられる最小分散ポートフォリオの問題点がここでも発生しているようです。一方、DECOがやはり個性的な組入比率の推移をしており、ここらを考えてももう一度論文を読み返してみる必要がありそうです。\n追記（2019/3/3）\rこれまでは、最小分散ポートフォリオで分析をしていましたが、リスクパリティの結果も見たいなと言うことで、そのコードも書いてみました。\nrisk_parity \u0026lt;- function(r_dat,r_cov){\rfn \u0026lt;- function(weight, r_cov) {\rN \u0026lt;- NROW(r_cov)\rrisks \u0026lt;- weight * (r_cov %*% weight)\rg \u0026lt;- rep(risks, times = N) - rep(risks, each = N)\rreturn(sum(g^2))\r}\rdfn \u0026lt;- function(weight,r_cov){\rout \u0026lt;- weight\rfor (i in 0:length(weight)) {\rup \u0026lt;- dn \u0026lt;- weight\rup[i] \u0026lt;- up[i]+.0001\rdn[i] \u0026lt;- dn[i]-.0001\rout[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002\r}\rreturn(out)\r}\rstd \u0026lt;- sqrt(diag(r_cov)) x0 \u0026lt;- 1/std/sum(1/std)\rres \u0026lt;- nloptr::nloptr(x0=x0,\reval_f=fn,\reval_grad_f=dfn,\reval_g_eq=function(weight,r_cov) { sum(weight) - 1 },\reval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) },\rlb=rep(0,length(std)),ub=rep(1,length(std)),\ropts = list(\u0026quot;algorithm\u0026quot;=\u0026quot;NLOPT_LD_SLSQP\u0026quot;,\u0026quot;print_level\u0026quot; = 0,\u0026quot;xtol_rel\u0026quot;=1.0e-8,\u0026quot;maxeval\u0026quot; = 1000),\rr_cov = r_cov)\rr_weight \u0026lt;- res$solution\rnames(r_weight) \u0026lt;- colnames(r_cov)\rwr_dat \u0026lt;- r_dat*r_weight\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;)\rreturn(result)\r}\rCCC \u0026lt;- backtest(hist.returns,\u0026quot;CCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDCC \u0026lt;- backtest(hist.returns,\u0026quot;DCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDECO \u0026lt;- backtest(hist.returns,\u0026quot;DECO\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rbenchmark \u0026lt;- backtest(hist.returns,\u0026quot;benchmark\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rresult \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)\rcolnames(result) \u0026lt;- c(\u0026quot;CCC\u0026quot;,\u0026quot;DCC\u0026quot;,\u0026quot;DECO\u0026quot;,\u0026quot;benchmark\u0026quot;)\r結果はこんな感じ。\nPerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026quot;BACKTEST COMPARISON\u0026quot;)\rlibrary(plotly)\r# plot allocation weighting\rriskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026quot;benchmark\u0026quot;,Date=index(benchmark$weight)))\rPerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026quot;BACKTEST COMPARISON\u0026quot;)\rriskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026quot;benchmark\u0026quot;,Date=index(benchmark$weight)))\r# plot allocation weighting\rd_allocation(riskparity_weight, \u0026quot;Risk Parity Asset Allocation\u0026quot;)\rどの手法もbenchmarkをアウトパーフォームできているという好ましい結果になりました。\rやはり、分散共分散行列の推計がうまくいっているようです。また、DECOのパフォーマンスがよいのは、相関行列に各資産ペアの相関係数の平均値を用いているため、他の手法よりもリスク資産の組み入れが多くなったからだと思われます。ウェイトは以下の通りです。\nとりあえず、今日はここまで。\n\r","date":1550361600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1551571200,"objectID":"bc248846d8caf05f6b02c4f3564abd29","permalink":"/post/post2/","publishdate":"2019-02-17T00:00:00Z","relpermalink":"/post/post2/","section":"post","summary":"社内ワークショップがあり、休日は国会図書館に籠り、先行研究を漁った結果、微妙なアセットアロケーションモデルを構築できました。","tags":["R"],"title":"Asset Allocation ModelをRで組んでみた。","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r\r1. カルマンフィルタとは？\r2. カルマンフィルタアルゴリズムの導出\r3. Rで実装する。\r4. カルマンスムージング\r\r\rおはこんばんにちは。かなり久しぶりの投稿となってしまいました。決して研究をさぼっていたのではなく、BVARのコーディングに手こずっていました。あと少しで完成します。さて、今回はBVARやこの前のGiannnone et a (2008)のような分析でも大活躍のカルマンフィルタを実装してしまいたいと思います。このブログではパッケージソフトに頼らず、基本的に自分で一から実装を行い、研究することをポリシーとしていますので、これから頻繁に使用するであろうカルマンフィルタを関数として実装してしまうことは非常に有益であると考えます。今回はRで実装をしましたので、そのご報告をします。\n1. カルマンフィルタとは？\rまず、カルマンフィルタに関する簡単な説明を行います。非常にわかりやすい記事があるので、こちらを読んでいただいたほうがより分かりやすいかと思います。\nカルマンフィルタとは、状態空間モデルを解くアルゴリズムの一種です。状態空間モデルとは、手元の観測可能な変数から観測できない変数を推定するモデルであり、以下のような形をしています。\n\\[\rY_{t} = Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} \\\\\r\\alpha_{t} = T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}\r\\]\nここで、\\(Y_{t}\\)は\\(g×1\\)ベクトルの観測可能な変数(観測変数)、\\(Z_{t}\\)は\\(g×k\\)係数行列、\\(\\alpha_{t}\\)は\\(k×1\\)ベクトルの観測不可能な変数(状態変数)、\\(T_{t}\\)は\\(k×k\\)係数行列です。また、\\(\\epsilon_{t}\\)は観測変数の誤差項、\\(\\eta_{t}\\)は状態変数の誤差項です。これらの誤差項はそれぞれ\\(N(0,H_{t})\\), \\(N(0,Q_{t})\\)に従います（\\(H_{t},Q_{t}\\)は分散共分散行列）。\\(d_{t}\\), \\(c_{t}\\)は定数項です。1本目の式は観測方程式、2本目の式は遷移方程式と呼ばれます。\r状態空間モデルを使用する例として、しばしば池の魚の数を推定する問題が使用されます。今、池の中の魚の全数が知りたいとして、その推定を考えます。観測時点毎に池の中の魚をすべて捕まえてその数を調べるのは現実的に困難なので、一定期間釣りをして釣れた魚をサンプルに全数を推定することを考えます。ここで、釣れた魚は観測変数、池にいる魚の全数は状態変数と考えることができます。今、経験的に釣れた魚の数と全数の間に以下のような関係があるとします。\n\\[\rY_{t} = 0.01\\alpha_{t} + 5 + \\epsilon_{t}\r\\]\rこれが観測方程式になります。また、魚の全数は過去の値からそれほど急速には変化しないと考えられるため、以下のようなランダムウォークに従うと仮定します。\n\\[\r\\alpha_{t} = \\alpha_{t-1} + 500 + \\eta_{t}\r\\]\rこれが遷移方程式になります。あとは、これをカルマンフィルタアルゴリズムを用いて計算すれば、観測できない魚の全数を推定することができます。\rこのように状態空間モデルは非常に便利なモデルであり、また応用範囲も広いです。例えば、販売額から潜在顧客数を推定したり、クレジットスプレッドやトービンのQ等経済モデル上の概念として存在する変数を推定する、BVARのようにVARや回帰式の時変パラメータ推定などにも使用されます。\n\r2. カルマンフィルタアルゴリズムの導出\rさて、非常に便利な状態空間モデルの説明はこれくらいにして、カルマンフィルタの説明に移りたいと思います。カルマンフィルタは状態空間モデルを解くアルゴリズムの一種であると先述しました。つまり、他にも状態空間モデルを解くアルゴリズムは存在します。カルマンフィルタアルゴリズムは一般に誤差項の正規性の仮定を必要としないフィルタリングアルゴリズムであり、観測方程式と遷移方程式の線形性の仮定さえあれば、線形最小分散推定量となります。カルマンフィルタアルゴリズムの導出にはいくつかの方法がありますが、今回はこの線形最小分散推定量としての導出を行います。まず、以下の３つの仮定を置きます。\n初期値\\(\\alpha_{0}\\)は正規分布\\(N(a_{0},\\Sigma_{0})\\)に従う確率ベクトルである(\\(a_{t}\\)は\\(\\alpha_{t}\\)の推定値)。\r誤差項\\(\\epsilon_{t}\\),\\(\\eta_{s}\\)は全ての\\(t\\),\\(s\\)で互いに独立で、初期値ベクトル\\(\\alpha_{0}\\)と無相関である（\\(E(\\epsilon_{t}\\eta_{s})=0\\), \\(E(\\epsilon_{t}\\alpha_{0})=0\\), \\(E(\\eta_{t}\\alpha_{0})=0\\)）。\r2より、\\(E(\\epsilon_{t}\\alpha_{t}\u0026#39;)=0\\)、\\(E(\\eta_{t}\\alpha_{t-1}\u0026#39;)=0\\)。\r\rまず、\\(t-1\\)期の情報集合\\(\\Omega_{t-1}\\)が既知の状態での\\(\\alpha_{t}\\)と\\(Y_{t}\\)の期待値（予測値）を求めてみましょう。上述した状態空間モデルと誤差項の期待値がどちらもゼロである事実を用いると、以下のように計算することができます。\n\\[\rE(\\alpha_{t}|\\Omega_{t-1}) = a_{t|t-1} = T_{t}a_{t-1|t-1} + c_{t}\rE(Y_{t}|\\Omega_{t-1}) = Y_{t|t-1} = Z_{t}a_{t|t-1} + d_{t}\r\\]\nここで、次に、これらの分散を求めます。\n\\[\r\\begin{eqnarray}\rE( (\\alpha_{t}-a_{t|t-1})(\\alpha_{t}-a_{t|t-1})\u0026#39;|\\Omega_{t-1}) \u0026amp;=\u0026amp; E( (T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}-a_{t|t-1})(T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}-a_{t|t-1})\u0026#39;|\\Omega_{t-1}) \\\\ \u0026amp;=\u0026amp; E(T_{t}\\alpha_{t-1}\\alpha_{t-1}\u0026#39;T_{t}\u0026#39; + R_{t}\\eta_{t}\\eta_{t}\u0026#39;R_{t}\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; E(T_{t}\\alpha_{t-1}\\alpha_{t-1}\u0026#39;T_{t}\u0026#39;|\\Omega_{t-1}) + E(R_{t}\\eta_{t}\\eta_{t}\u0026#39;R_{t}\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; T_{t}E(\\alpha_{t-1}\\alpha_{t-1}\u0026#39;|\\Omega_{t-1})T_{t}\u0026#39; + R_{t}E(\\eta_{t}\\eta_{t}\u0026#39;|\\Omega_{t-1})R_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; T_{t}\\Sigma_{t-1|t-1}T_{t}\u0026#39; + R_{t}Q_{t}R_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1}\r\\end{eqnarray}\r\\]\n\\[\r\\begin{eqnarray}\rE( (Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})\u0026#39;|\\Omega_{t-1}) \u0026amp;=\u0026amp; E( (Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t}-Y_{t|t-1})(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t}-Y_{t|t-1})\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; E(Z_{t}\\alpha_{t}\\alpha_{t}\u0026#39;Z_{t}\u0026#39; + S_{t}\\epsilon_{t}\\epsilon_{t}\u0026#39;S_{t}\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; E(Z_{t}\\alpha_{t}\\alpha_{t}\u0026#39;Z_{t}\u0026#39;|\\Omega_{t-1}) + E(S_{t}\\epsilon_{t}\\epsilon_{t}\u0026#39;S_{t}\u0026#39;|\\Omega_{t-1}) \\\\\r\u0026amp;=\u0026amp; Z_{t}E(\\alpha_{t}\\alpha_{t}\u0026#39;|\\Omega_{t-1})Z_{t}\u0026#39; + S_{t}E(\\epsilon_{t}\\epsilon_{t}\u0026#39;|\\Omega_{t-1})S_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; F_{t|t-1}\r\\end{eqnarray}\r\\]\nここで、\\(t\\)期の情報集合\\(\\Omega_{t}\\)が得られたとします（つまり、観測値\\(Y_{t}\\)を入手）。カルマンフィルタでは、\\(t\\)期の情報である観測値\\(Y_{t}\\)を用いて\\(a_{t|t-1}\\)を以下の方程式で更新します。\n\\[\rE(\\alpha_{t}|\\Omega_{t}) = a_{t|t} = a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1})\r\\]\rつまり、観測値と\\(Y_{t}\\)の期待値（予測値）の差をあるウェイト\\(k_{t}\\)（\\(k×g\\)行列）でかけたもので補正をかけるわけです。よって、観測値と予測値が完全に一致していた場合は補正は行われないことになります。ここで重要なのは、ウエイト\\(k_{t}\\)をどのように決めるのかです。\\(k_{t}\\)は更新後の状態変数の分散\\(E( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})\u0026#39;)= \\Sigma_{t|t}\\)を最小化するよう決定します。これが、カルマンフィルタが線形最小分散推定量である根拠です。最小化にあたっては以下のベクトル微分が必要になりますので、おさらいをしておきましょう。今回使用するのは以下の事実です。\n\\[\r\\displaystyle \\frac{\\partial a\u0026#39;b}{\\partial b} = \\frac{\\partial b\u0026#39;a}{\\partial b} = a \\\\\r\\displaystyle \\frac{\\partial b\u0026#39;Ab}{\\partial b} = 2Ab\r\\]\nここで、\\(a,b\\)はベクトル（それぞれ\\(n×1\\)ベクトル、\\(1×n\\)ベクトル）、\\(A\\)は\\(n×n\\)の対称行列です。まず、１つ目から証明していきます。\\(\\displaystyle y = a\u0026#39;b = b\u0026#39;a = \\sum_{i=1}^{n}a_{i}b_{i}\\)とします。\rこのとき、\\(\\frac{\\partial y}{\\partial b_{i}}=a_{i}\\)なので、\n\\[\r\\displaystyle \\frac{\\partial a\u0026#39;b}{\\partial b} = \\frac{\\partial b\u0026#39;a}{\\partial b} = a\r\\]\n次に２つ目です。\\(y = b\u0026#39;Ab = \\sum_{i=1}^{n}\\sum_{j=1}^{n}a_{ij}b_{i}b_{j}\\)とします。このとき、\n\\[\r\\displaystyle \\frac{\\partial y}{\\partial b_{i}} = \\sum_{j=1}^{n}a_{ij}b_{j} + \\sum_{j=1}^{n}a_{ji}b_{j} = 2\\sum_{j=1}^{n}a_{ij}b_{j} = 2a_{i}\u0026#39;b\r\\]\rよって、\n\\[\r\\displaystyle \\frac{\\partial y}{\\partial b} =\r\\left(\r\\begin{array}{cccc}\r\\frac{\\partial y}{\\partial b_{1}} \\\\\r\\vdots \\\\\r\\frac{\\partial y}{\\partial b_{n}} \\\\\r\\end{array}\r\\right) = 2\r\\left(\r\\begin{array}{cccc}\r\\sum_{j=1}^{n}a_{1j}b_{j} \\\\\r\\vdots \\\\\r\\sum_{j=1}^{n}a_{nj}b_{j} \\\\\r\\end{array}\r\\right) = 2\r\\left(\r\\begin{array}{cccc}\ra_{1}\u0026#39;b \\\\\r\\vdots \\\\\ra_{n}\u0026#39;b \\\\\r\\end{array}\r\\right)\r= 2Ab\r\\]\rさて、準備ができたので、更新後の状態変数の分散\\(E( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})\u0026#39;)\\)を求めてみましょう。\n\\[\r\\begin{eqnarray}\rE( (\\alpha_{t} - a_{t|t})(\\alpha_{t} - a_{t|t})\u0026#39;) \u0026amp;=\u0026amp; \\Sigma_{t|t} \\\\\r\u0026amp;=\u0026amp; E\\{ (\\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))(\\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))\u0026#39;\\} \\\\\r\u0026amp;=\u0026amp; E\\{ ( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + d_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )\\} \\\\\r\u0026amp;=\u0026amp; E\\{ ( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1}) )( (\\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\\alpha_{t} + S_{t}\\epsilon_{t} - Z_{t}a_{t|t-1}) )\u0026#39;\\} \\\\\r\u0026amp;=\u0026amp; E\\{ ( (I - k_{t}Z_{t})\\alpha_{t} - k_{t}S_{t}\\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})( (I - k_{t}Z_{t})\\alpha_{t} - k_{t}S_{t}\\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})\u0026#39; \\} \\\\\r\u0026amp;=\u0026amp; E\\{( (I - k_{t}Z_{t})(\\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\\epsilon_{t})( (I - k_{t}Z_{t})(\\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\\epsilon_{t})\u0026#39;\\} \\\\\r\u0026amp;=\u0026amp; (I - k_{t}Z_{t})\\Sigma_{t|t-1}(I - k_{t}Z_{t})\u0026#39; + k_{t}S_{t}H_{t}S_{t}\u0026#39;k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; (\\Sigma_{t|t-1} - k_{t}Z_{t}\\Sigma_{t|t-1})(I - k_{t}Z_{t})\u0026#39; + k_{t}S_{t}H_{t}S_{t}\u0026#39;k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}(k_{t}Z_{t})\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; + k_{t}S_{t}H_{t}S_{t}\u0026#39;k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39;)k_{t}\u0026#39; \\\\\r\\end{eqnarray}\r\\]\n１回目の式変形で、\\(a_{t|t}\\)に上述した更新式を代入し、２回目の式変形で観測方程式と上で計算した\\(E(Y_{t}|\\Omega_{t-1})\\)を代入しています。さて、更新後の状態変数の分散\\(\\Sigma_{t|t}\\)を\\(k_{t}\\)の関数として書き表すことができたので、これを\\(k_{t}\\)で微分し、0と置き、\\(\\Sigma_{t|t}\\)を最小化する\\(k_{t}\\)を求めます。先述した公式で、\\(a=\\Sigma_{t|t-1}Z_{t}\u0026#39;\\)、\\(b=k_{t}\u0026#39;\\)、\\(A=(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39;)\\)とすると（\\(A\\)は分散共分散行列の和なので対称行列）、\n\\[\r\\frac{\\partial \\Sigma_{t|t}}{\\partial k_{t}\u0026#39;} = -2(Z_{t}\\Sigma_{t|t-1})\u0026#39; + 2(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39;)k_{t} = 0\r\\]\nここから、\\(k_{t}\\)を解きなおすと、\n\\[\r\\begin{eqnarray}\rk_{t} \u0026amp;=\u0026amp; \\Sigma_{t|t-1}Z_{t}\u0026#39;(Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39;)^{-1} \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1}Z_{t}\u0026#39;F_{t|t-1}^{-1}\r\\end{eqnarray}\r\\]\n突然、\\(F_{t|t-1}\\)が出てきました。これは観測変数の予測値の分散\\(E((Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})\u0026#39;|\\Omega_{t-1})\\)でした。一方、\\(\\Sigma_{t|t-1}Z_{t}\\)は状態変数の予測値の分散を観測変数のスケールに調整したものです（観測空間に写像したもの）。つまり、カルマンゲイン\\(k_{t}\\)は状態変数と観測変数の予測値の分散比となっているのです。観測変数にはノイズがあり、観測方程式はいつも誤差０で満たされるわけではありません。また、状態方程式にも誤差項が存在します。状態の遷移も100%モデル通りにはいかないということです。\\(t\\)期の観測変数\\(Y_{t}\\)が得られたとして、それをどれほど信頼して状態変数を更新するかは観測変数のノイズが状態変数のノイズに比べてどれほど小さいかによります。つまり、相対的に観測方程式が遷移方程式よりも信頼できる場合には状態変数を大きく更新するのです。このように、カルマンフィルタでは、観測方程式と遷移方程式の相対的な信頼度によって、更新の度合いを毎期調整しています。その度合いが分散比であり、カルマンゲインだというわけです。ちなみに欠損値が発生した場合には、観測変数の分散を無限大にし、状態変数の更新を全く行わないという対処を行います。観測変数に信頼がないので当たり前の処置です。この場合は遷移方程式を100%信頼します。これがカルマンフィルタのコアの考え方になります。\r更新後の分散を計算しておきます。\n\\[\r\\begin{eqnarray}\r\\Sigma_{t|t} \u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + k_{t}F_{t|t-1}k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + (\\Sigma_{t|t-1}Z_{t}\u0026#39;F_{t|t-1}^{-1})F_{t|t-1}k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; - k_{t}Z_{t}\\Sigma_{t|t-1} + \\Sigma_{t|t-1}Z_{t}\u0026#39;k_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - k_{t}Z_{t}\\Sigma_{t|t-1} \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - k_{t}F_{t|t-1}F_{t|t-1}^{-1}Z_{t}\\Sigma_{t|t-1} \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}\u0026#39;\r\\end{eqnarray}\r\\]\nでは、最終的に導出されたアルゴリズムをまとめたいと思います。\n\\[\r\\begin{eqnarray}\ra_{t|t-1} \u0026amp;=\u0026amp; T_{t}a_{t-1|t-1} + c_{t} \\\\\r\\Sigma_{t|t-1} \u0026amp;=\u0026amp; T_{t}\\Sigma_{t-1|t-1}T_{t}\u0026#39; + R_{t}Q_{t}R_{t}\u0026#39; \\\\\rY_{t|t-1} \u0026amp;=\u0026amp; Z_{t}a_{t|t-1} + d_{t} \\\\\rF_{t|t-1} \u0026amp;=\u0026amp; Z_{t}\\Sigma_{t|t-1}Z_{t}\u0026#39; + S_{t}H_{t}S_{t}\u0026#39; \\\\\rk_{t} \u0026amp;=\u0026amp; \\Sigma_{t|t-1}Z_{t}\u0026#39;F_{t|t-1}^{-1} \\\\\ra_{t|t} \u0026amp;=\u0026amp; a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}) \\\\\r\\Sigma_{t|t} \u0026amp;=\u0026amp; \\Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}\u0026#39;\r\\end{eqnarray}\r\\]\r初期値\\(a_{0},\\Sigma_{0}\\)が所与の元で、まず状態変数の予測値\\(a_{1|0},\\Sigma_{1|0}\\)を計算します。その結果を用いて、次は観測変数の予測値\\(Y_{t|t-1},F_{t|t-1}\\)を計算し、カルマンゲイン\\(k_{t}\\)を得ます。\\(t\\)期の観測可能なデータを入手したら、更新方程式を用いて\\(a_{t|t},\\Sigma_{t|t}\\)を更新します。これをサンプル期間繰り返していくことになります。ちなみに、遷移方程式の誤差項\\(\\eta_{t}\\)と定数項\\(c_{t}\\)がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致します。つまり、新しいサンプルを入手するたびにOLSをやり直す推計方法ということです（今回はその証明は勘弁してください）。\n\r3. Rで実装する。\r以下がRでの実装コードです。\nkalmanfiter \u0026lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){\r#-------------------------------------------------------------------\r# Implemention of Kalman filter\r# y - observed variable\r# I - the number of unobserved variable\r# t - parameter of endogenous variable in state equation\r# z - parameter of endogenous variable in observable equation\r# c - constant in state equaion\r# R - parameter of exogenous variable in state equation\r# Q - var-cov matrix of exogenous variable in state equation\r# d - constant in observable equaion\r# S - parameter of exogenous variable in observable equation\r# h - var-cov matrix of exogenous variable in observable equation\r# a_int - initial value of endogenous variable\r# sig_int - initial value of variance of endogenous variable\r#-------------------------------------------------------------------\rlibrary(MASS)\r# 1.Define Variable\rif (class(y)!=\u0026quot;matrix\u0026quot;){\ry \u0026lt;- as.matrix(y)\r}\rN \u0026lt;- NROW(y) # sample size\rL \u0026lt;- NCOL(y) # the number of observable variable a_pre \u0026lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable\ra_fil \u0026lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable\rsig_pre \u0026lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable\rsig_fil \u0026lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable\ry_pre \u0026lt;- array(0,dim = c(L,1,N)) # prediction of observed variable\rF_pre \u0026lt;- array(0,dim = c(L,L,N)) # prediction of var-cov mat. of observable variable F_inv \u0026lt;- array(0,dim = c(L,L,N)) # inverse of F_pre\rk \u0026lt;- array(0,dim = c(I,L,N)) # kalman gain\rif (any(is.na(a_int))==TRUE){\ra_int \u0026lt;- matrix(0,nrow = I,ncol = 1)\r}\rif (any(is.na(sig_int))==TRUE){\rsig_int \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(R))==TRUE){\rR \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(Q))==TRUE){\rQ \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(S))==TRUE){\rS \u0026lt;- matrix(1,nrow = L,ncol = L)\r}\rif (any(is.na(h))==TRUE){\rH \u0026lt;- array(0,dim = c(L,L,N))\rfor(i in 1:N){\rdiag(H[,,i]) = 1\r}\r}else if (class(h)!=\u0026quot;array\u0026quot;){\rH \u0026lt;- array(h,dim = c(NROW(h),NCOL(h),N))\r}\r# fill infinite if observed data is NA\rfor(i in 1:N){\rmiss \u0026lt;- is.na(y[i,])\rdiag(H[,,i])[miss] \u0026lt;- 1e+32\r}\ry[is.na(y)] \u0026lt;- 0\r# 2.Set Initial Value\ra_fil[,,1] \u0026lt;- a_int\rsig_fil[,,1] \u0026lt;- sig_int\r# 3.Implement Kalman filter\rfor (i in 1:N){\rif(class(z)==\u0026quot;array\u0026quot;){\rZ \u0026lt;- z[,,i]\r}else{\rZ \u0026lt;- z\r}\ra_pre[,,i] \u0026lt;- t%*%a_fil[,,i] + c\rsig_pre[,,i] \u0026lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)\ry_pre[,,i] \u0026lt;- Z%*%a_pre[,,i] + d\rF_pre[,,i] \u0026lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)\rk[,,i] \u0026lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])\ra_fil[,,i+1] \u0026lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])\rsig_fil[,,i+1] \u0026lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])\r}\r# 4.Aggregate results\rresult \u0026lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)\rnames(result) \u0026lt;- c(\u0026quot;state prediction\u0026quot;, \u0026quot;state filtered\u0026quot;, \u0026quot;state var prediction\u0026quot;, \u0026quot;state var filtered\u0026quot;, \u0026quot;observable prediction\u0026quot;, \u0026quot;kalman gain\u0026quot;,\r\u0026quot;parameter of state eq\u0026quot;, \u0026quot;parameter of observable eq\u0026quot;)\rreturn(result)\r}\r案外簡単に書けるもんですね。これを使って、Giannone et al (2008)をやり直してみます。データセットは前回記事と変わりません。\n以下、分析用のRコードです。\n#------------------------\r# Giannone et. al. 2008 #------------------------\rlibrary(MASS)\rlibrary(xts)\r# ファクターを計算\rf \u0026lt;- 3\rz \u0026lt;- scale(dataset1)\rfor (i in 1:nrow(z)){\reval(parse(text = paste(\u0026quot;S_i \u0026lt;- z[i,]%*%t(z[i,])\u0026quot;,sep = \u0026quot;\u0026quot;)))\rif (i==1){\rS \u0026lt;- S_i\r}else{\rS \u0026lt;- S + S_i\r}\r}\rS \u0026lt;- (1/nrow(z))*S\rgamma \u0026lt;- eigen(S)\rD \u0026lt;- diag(gamma$values[1:f])\rV \u0026lt;- gamma$vectors[,1:f]\rF_t \u0026lt;- matrix(0,nrow(z),f)\rfor (i in 1:nrow(z)){\reval(parse(text = paste(\u0026quot;F_t[\u0026quot;,i,\u0026quot;,]\u0026lt;- z[\u0026quot;,i,\u0026quot;,]%*%V\u0026quot;,sep = \u0026quot;\u0026quot;)))\r}\rlambda_hat \u0026lt;- V\rpsi \u0026lt;- diag(diag(S-V%*%D%*%t(V)))\rR \u0026lt;- diag(diag(cov(z-z%*%V%*%t(V))))\ra \u0026lt;- matrix(0,f,f)\rb \u0026lt;- matrix(0,f,f)\rfor(t in 2:nrow(z)){\ra \u0026lt;- a + F_t[t,]%*%t(F_t[t-1,])\rb \u0026lt;- b + F_t[t-1,]%*%t(F_t[t-1,])\r}\rb_inv \u0026lt;- solve(b)\rA_hat \u0026lt;- a%*%b_inv\re \u0026lt;- numeric(f)\rfor (t in 2:nrow(F_t)){\re \u0026lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat\r}\rH \u0026lt;- t(e)%*%e\rQ \u0026lt;- diag(1,f,f)\rQ[1:f,1:f] \u0026lt;- H\rp \u0026lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))\rresult1 \u0026lt;- kalmanfiter(z,f,A_hat,lambda_hat,c=0,R=NA,Q=Q,d=0,S=NA,h=R,a_int=F_t[1,],sig_int=matrix(p,f,f))\rプロットしてみます。\nlibrary(ggplot2)\rlibrary(tidyverse)\rggplot(gather(data.frame(factor1=result1$`state filtered`[1,1,-dim(result1$`state filtered`)[3]],factor2=result1$`state filtered`[2,1,-dim(result1$`state filtered`)[3]],factor3=result1$`state filtered`[3,1,-dim(result1$`state filtered`)[3]],time=as.Date(rownames(dataset1))),key = factor,value = value,-time),aes(x=time,y=value,colour=factor)) + geom_line()\rgiannoneの記事を書いた際は、元論文のMATLABコードを参考にRで書いたのですが、通常のカルマンフィルタとは観測変数の分散共分散行列の逆数の計算方法が違うらしくグラフの形が異なっています。まあでも、概形はほとんど同じですが（なので、ちゃんと動いているはず）。\n\r4. カルマンスムージング\rカルマンフィルタの実装は以上で終了なのですが、誤差項の正規性を仮定すれば\\(T\\)期までの情報集合\\(\\Omega_{T}\\)を用いて、\\(a_{i|i}, \\Sigma_{i|i}(i = 1:T)\\)を\\(a_{i|T}, \\Sigma_{i|T}(i = 1:T)\\)へ更新することができます。これをカルマンスムージングと呼びます。これを導出してみましょう。その準備として、以下のような\\(\\alpha_{t|t}\\)と\\(\\alpha_{t+1|t}\\)の混合分布を計算しておきます。\n\\[\r\\begin{eqnarray}\r(\\alpha_{t|t},\\alpha_{t+1|t}) \u0026amp;=\u0026amp; N(\r\\left(\r\\begin{array}{cccc}\rE(\\alpha_{t|t}) \\\\\rE(\\alpha_{t+1|t})\r\\end{array}\r\\right),\r\\left(\r\\begin{array}{cccc}\rVar(\\alpha_{t|t}), Cov(\\alpha_{t|t},\\alpha_{t+1|t}) \\\\\rCov(\\alpha_{t+1|t},\\alpha_{t|t}), Var(\\alpha_{t+1|t})\r\\end{array}\r\\right)\r) \\\\\r\u0026amp;=\u0026amp; N(\r\\left(\r\\begin{array}{cccc}\ra_{t|t} \\\\\ra_{t+1|t}\r\\end{array}\r\\right),\r\\left(\r\\begin{array}{cccc}\r\\Sigma_{t|t}, \\Sigma_{t|t}T_{t}\u0026#39; \\\\\rT_{t}\\Sigma_{t|t}, \\Sigma_{t+1|t} \\end{array}\r\\right)\r)\r\\end{eqnarray}\r\\]\rここで、\\(Cov(\\alpha_{t|t},\\alpha_{t+1|t})\\)は\n\\[\r\\begin{eqnarray}\rCov(\\alpha_{t+1|t},\\alpha_{t|t}) \u0026amp;=\u0026amp; Cov(T_{t}\\alpha_{t-1} + c_{t} + R_{t}\\eta_{t}, \\alpha_{t|t}) \\\\\r\u0026amp;=\u0026amp; T_{t}Cov(\\alpha_{t|t},\\alpha_{t|t}) + Cov(c_{t},\\alpha_{t|t}) + Cov(R_{t}\\eta_{t},\\alpha_{t|t}) \\\\\r\u0026amp;=\u0026amp; T_{t}Var(\\alpha_{t|t}) = T_{t}\\Sigma_{t|t}\r\\end{eqnarray}\r\\]\nここで、条件付き多変量正規分布は以下のような分布をしていることを思い出しましょう（\r参考\r）。\n\\[\r(X_{1},X_{2}) = N(\r\\left(\r\\begin{array}{cccc}\r\\mu_{1} \\\\\r\\mu_{2}\r\\end{array}\r\\right),\r\\left(\r\\begin{array}{cccc}\r\\Sigma_{11}, \\Sigma_{12} \\\\\r\\Sigma_{21}, \\Sigma_{22}\r\\end{array}\r\\right)\r) \\\\\r\\]\r\\[\r(X_{1}|X_{2}=x_{2}) = N(\\mu_{1} + \\Sigma_{12}\\Sigma_{22}^{-1}(x_{2}-\\mu_{2}),\\Sigma_{11}-\\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21})\r\\]\nこれを用いて、\\((\\alpha_{t|t}|\\alpha_{t+1|t}=a_{t+1})\\)を計算してみましょう。\n\\[\r\\begin{eqnarray}\r(\\alpha_{t|t}|\\alpha_{t+1|t}=a_{t+1}) \u0026amp;=\u0026amp; N(a_{t|t} + \\Sigma_{t|t}T_{t}\u0026#39;\\Sigma_{t+1|t}^{-1}(a_{t+1}-a_{t+1|t}), \\Sigma_{t|t}-\\Sigma_{t|t}T_{t}\u0026#39;\\Sigma_{t+1|t}^{-1}T_{t}\\Sigma_{t|t}) \\\\\r\u0026amp;=\u0026amp;N(a_{t|t} + L_{t}(a_{t+1}-a_{t+1|t}), \\Sigma_{t|t}-L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39;)\r\\end{eqnarray}\r\\]\nただし、\\(a_{t+1}\\)の値は観測不可能なので、上式を用いて状態変数を更新することはできません。今、わかるのは\\(T\\)期における\\(a_{t+1|T}\\)の分布のみです。ということで、\\(a_{t+1}\\)を\\(a_{t+1|T}\\)で代用し、\\(\\alpha_{t|T}\\)の分布を求めてみます。では、計算していきます。\\(\\alpha_{t|T} = N(E(\\alpha_{t|T}),Var(\\alpha_{t|T}))\\)ですが、\n\\[\r\\begin{eqnarray}\rE(\\alpha_{t|T}) \u0026amp;=\u0026amp; E_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\rVar(\\alpha_{t|T}) \u0026amp;=\u0026amp; E_{\\alpha_{t+1|T}}(Var(\\alpha_{t|t}|\\alpha_{t+1|t} = \\alpha_{t+1|T})) + Var_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T}))\r\\end{eqnarray}\r\\]\nというように、\\(\\alpha_{t+1|T}\\)も確率変数となるので、繰り返し期待値の法則と繰り返し分散の法則を使用します（こちらを参照）。\n*繰り返し期待値の法則\r\\(E(x) = E_{Z}(E(X|Y=Z))\\)\n*繰り返し分散の法則\r\\(Var(X) = E_{Z}(Var(X|Y=Z))+Var_{Z}(E(X|Y=Z))\\)\nよって、\n\\[\r\\begin{eqnarray}\rE(\\alpha_{t|T}) \u0026amp;=\u0026amp; E_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\r\u0026amp;=\u0026amp; a_{t|t} + L_{t}(a_{t+1|T}-a_{t+1|t}) \\\\\rVar(\\alpha_{t|T}) \u0026amp;=\u0026amp; E_{\\alpha_{t+1|T}}(Var(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) + Var_{\\alpha_{t+1|T}}(E(\\alpha_{t|t}|\\alpha_{t+1|t}=\\alpha_{t+1|T})) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + E( (a_{t|t} + L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))(a_{t|t} + L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))\u0026#39;) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + E( (L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))(L_{t}(\\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))\u0026#39;) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + E( (L_{t}\\alpha_{t+1|T} - L_{t}a_{t+1|T})(L_{t}\\alpha_{t+1|T} - L_{t}(a_{t+1|T})\u0026#39;) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + E( L_{t}(\\alpha_{t+1|T} - a_{t+1|T})(\\alpha_{t+1|T} - a_{t+1|T})\u0026#39;L_{t}\u0026#39;) \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + L_{t}E( (\\alpha_{t+1|T} - a_{t+1|T})(\\alpha_{t+1|T} - a_{t+1|T})\u0026#39;)L_{t}\u0026#39; \\\\\r\u0026amp;=\u0026amp; \\Sigma_{t|t} - L_{t}\\Sigma_{t+1|t}L_{t}\u0026#39; + L_{t}\\Sigma_{t+1|T}L_{t}\u0026#39;\r\\end{eqnarray}\r\\]\nとなります。カルマンスムージングのアルゴリズムをまとめておきます。\n\\[\r\\begin{eqnarray}\rL_{t} \u0026amp;=\u0026amp; \\Sigma_{t|t}T_{t}\\Sigma_{t+1|t}^{-1} \\\\\ra_{t|T} \u0026amp;=\u0026amp; a_{t|t} + L_{t}(a_{t+1|T} - a_{t+1|t}) \\\\\r\\Sigma_{t|T} \u0026amp;=\u0026amp; \\Sigma_{t+1|t} + L_{t}(\\Sigma_{t+1|T}-\\Sigma_{t+1|t})L_{t}\u0026#39;\r\\end{eqnarray}\r\\]\rカルマンスムージングの特徴的な点は後ろ向きに計算をしていく点です。つまり、\\(T\\)期から1期に向けて計算を行っていきます。\\(L_{t}\\)に関してはそもそもカルマンフィルタを回した時点で計算可能ですが、\\(\\alpha_{t|T}\\)は\\(T\\)期までのデータが手元にないと計算できません。今、\\(T\\)期まで観測可能なデータが入手できたとしましょう。すると、２番目の方程式を用いて、\\(a_{T-1|T}\\)を計算します。ちなみに\\(a_{T|T}\\)はカルマンフィルタを回した時点ですでに手に入っているので、計算する必要はありません。同時に、３番目の式を用いて\\(\\Sigma_{T-1|T}\\)を計算します。そして、\\(a_{T-1|T},\\Sigma_{T-1|T}\\)と\\(L_{T-1}\\)を用いて\\(a_{T-2|T},\\Sigma_{T-2|T}\\)を計算、というように1期に向けて後ろ向きに計算をしていくのです。さきほど、遷移方程式の誤差項\\(\\eta_{t}\\)と定数項\\(c_{t}\\)がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致すると書きましたが、カルマンスムージングの場合は\\(T\\)期までのサンプルでOLSを行った結果と一致します。\rRで実装してみます。\nkalmansmoothing \u0026lt;- function(filter){\r#-------------------------------------------------------------------\r# Implemention of Kalman smoothing\r# t - parameter of endogenous variable in state equation\r# z - parameter of endogenous variable in observable equation\r# a_pre - prediction of state\r# a_fil - filtered value of state\r# sig_pre - prediction of var of state\r# sig_fil - filtered value of state\r#-------------------------------------------------------------------\rlibrary(MASS)\r# 1.Define variable\ra_pre \u0026lt;- filter$`state prediction`\ra_fil \u0026lt;- filter$`state filtered`\rsig_pre \u0026lt;- filter$`state var prediction`\rsig_fil \u0026lt;- filter$`state var filtered`\rt \u0026lt;- filter$`parameter of state eq`\rC \u0026lt;- array(0,dim = dim(sig_pre))\ra_sm \u0026lt;- array(0,dim = dim(a_pre))\rsig_sm \u0026lt;- array(0,dim = dim(sig_pre))\rN \u0026lt;- dim(C)[3]\ra_sm[,,N] \u0026lt;- a_fil[,,N]\rsig_sm[,,N] \u0026lt;- sig_fil[,,N]\rfor (i in N:2){\rC[,,i-1] \u0026lt;- sig_fil[,,i-1]%*%t(t)%*%ginv(sig_pre[,,i])\ra_sm[,,i-1] \u0026lt;- a_fil[,,i-1] + C[,,i-1]%*%(a_sm[,,i]-a_pre[,,i])\rsig_sm[,,i-1] \u0026lt;- sig_fil[,,i-1] + C[,,i-1]%*%(sig_sm[,,i]-sig_pre[,,i])%*%t(C[,,i-1])\r}\rresult \u0026lt;- list(a_sm,sig_sm,C)\rnames(result) \u0026lt;- c(\u0026quot;state smoothed\u0026quot;, \u0026quot;state var smoothed\u0026quot;, \u0026quot;c\u0026quot;)\rreturn(result)\r}\r先ほどのコードの続きでRコードを書いてみます。\nresult2 \u0026lt;- kalmansmoothing(result1)\rかなりシンプルですね。ちなみにグラフにしましたが、１個目とほぼ変わりませんでした。とりあえず、今日はここまで。\n\r","date":1549756800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1549756800,"objectID":"74e0e5783c70722e767e0eb0cf2bbb08","permalink":"/post/post3/","publishdate":"2019-02-10T00:00:00Z","relpermalink":"/post/post3/","section":"post","summary":"時系列解析には欠かせないカルマンフィルタ。Rでもパッケージが用意されていて非常に便利ですが、ここではいったん理解を優先し、カルマンフィルタの実装を行ってみたいと思います。","tags":["カルマンフィルタ","R"],"title":"カルマンフィルタの実装","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r\r1. GPRとは\r2. GPRの実装\r\r\r　おはこんばんにちは。昨日、Bayesian Vector Autoregressionの記事を書きました。\nその中でハイパーパラメータのチューニングの話が出てきて、なにか効率的にチューニングを行う方法はないかと探していた際にBayesian Optimizationを発見しました。日次GDPでも機械学習の手法を利用しようと思っているので、Bayesian Optimizationはかなり使える手法ではないかと思い、昨日徹夜で理解しました。\nその内容をここで実装しようとは思うのですが、Bayesian Optimizationではガウス回帰（Gaussian Pocess Regression,以下GPR）を使用しており、まずその実装を行おうと持ったのがこのエントリを書いた動機です。Bayesian Optimizationの実装はこのエントリの後にでも書こうかなと思っています。\n1. GPRとは\r　GRPとは簡単に言ってしまえば「ベイズ推定を用いた非線形回帰手法の１種」です。モデル自体は線形ですが、カーネルトリックを用いて入力変数を無限個非線形変換したものを説明変数として推定できるところが特徴です（カーネルになにを選択するかによります）。\nGPRが想定しているのは、学習データとして入力データと教師データがそれぞれN個得られており、また入力データに関しては\\(N+1\\)個目のデータも得られている状況です。この状況から、\\(N+1\\)個目の教師データを予測します。\n教師データにはノイズが含まれており、以下のような確率モデルに従います。\n\\[\rt_{i} = y_{i} + \\epsilon_{i}\r\\]\nここで、\\(t_{i}\\)は\\(i\\)番目の観測可能な教師データ（スカラー）、\\(y_{i}\\)は観測できない出力データ（スカラー）、\\(\\epsilon_{i}\\)は測定誤差で正規分布\\(N(0,\\beta^{-1})\\)に従います。\\(y_{i}\\)は以下のような確率モデルに従います。\n\\[\r\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\r\\]\nここで、\\(x_{i}\\)はi番目の入力データベクトル、\\(\\phi(・)\\)は非線形関数、 \\(\\textbf{w}^{T}\\)は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、\\(\\phi(x_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\\)を想定しています（\\(x_{1,i}\\)は\\(i\\)番目の入力データ\\(x_{i}\\)の１番目の変数）。教師データの確率モデルから、\\(i\\)番目の出力データ\\(y_{i}\\)が得られたうえで\\(t_{i}\\)が得られる条件付確率は、\n\\[\rp(t_{i}|y_{i}) = N(t_{i}|y_{i},\\beta^{-1})\r\\]\nとなります。\\(\\displaystyle \\textbf{t} = (t_{1},...,t_{n})^{T}\\)、\\(\\displaystyle \\textbf{y} = (y_{1},...,y_{n})^{T}\\)とすると、上式を拡張することで\n\\[\r\\displaystyle p(\\textbf{t}|\\textbf{y}) = N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N})\r\\]\nと書けます。また、事前分布として\\(\\textbf{w}\\)の期待値は0、分散は全て\\(\\alpha\\)と仮定します。\\(\\displaystyle \\textbf{y}\\)はガウス過程に従うと仮定します。ガウス過程とは、\\(\\displaystyle \\textbf{y}\\)の同時分布が多変量ガウス分布に従うもののことです。コードで書くと以下のようになります。\n# Define Kernel function\rKernel_Mat \u0026lt;- function(X,sigma,beta){\rN \u0026lt;- NROW(X)\rK \u0026lt;- matrix(0,N,N)\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta\r}\r}\rreturn(K)\r}\rN \u0026lt;- 10 # max value of X\rM \u0026lt;- 1000 # sample size\rX \u0026lt;- matrix(seq(1,N,length=M),M,1) # create X\rtestK \u0026lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix\rlibrary(MASS)\rP \u0026lt;- 6 # num of sample path\rY \u0026lt;- matrix(0,M,P) # define Y\rfor(i in 1:P){\rY[,i] \u0026lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y\r}\r# Plot\rmatplot(x=X,y=Y,type = \u0026quot;l\u0026quot;,lwd = 2)\r　Kernel_Matについては後述しますが、\\(\\displaystyle \\textbf{y}\\)の各要素\\(\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\\)の間の共分散行列\\(K\\)を入力\\(x\\)からカーネル法を用いて計算しています。そして、この\\(K\\)と平均0から、多変量正規乱数を6系列生成し、それをプロットしています。\n　これらの系列は共分散行列から計算されるので、各要素の共分散が正に大きくなればなるほど同じ値をとりやすくなるようモデリングされていることになります。また、グラフを見ればわかるように非常になめらかなグラフが生成されており、かつ非常に柔軟な関数を表現できていることがわかります。コードでは計算コストの関係上、入力を0から10に限定して1000個の入力点をサンプルし、作図を行っていますが、原理的には\\(x\\)は実数空間で定義されるものであるので、\\(p(\\textbf{y})\\)は無限次元の多変量正規分布に従います。\r以上のように、\\(\\displaystyle \\textbf{y}\\)はガウス過程に従うと仮定するので同時確率\\(p(\\textbf{y})\\)は平均0、分散共分散行列が\\(K\\)の多変量正規分布\\(N(\\textbf{y}|0,K)\\)に従います。ここで、\\(K\\)の各要素\\(K_{i,j}\\)は、\n\\[\r\\begin{eqnarray}\rK_{i,j} \u0026amp;=\u0026amp; cov[y_{i},y_{j}] = cov[\\textbf{w}\\phi(x_{i}),\\textbf{w}\\phi(x_{j})] \\\\\r\u0026amp;=\u0026amp;\\phi(x_{i})\\phi(x_{j})cov[\\textbf{w},\\textbf{w}]=\\phi(x_{i})\\phi(x_{j})\\alpha\r\\end{eqnarray}\r\\]\nです。ここで、\\(\\phi(x_{i})\\phi(x_{j})\\alpha\\)は\\(\\phi(x_{i})\\)の次元が大きくなればなるほど計算量が多くなります（つまり、非線形変換をかければかけるほど計算が終わらない）。しかし、カーネル関数\\(k(x,x\u0026#39;)\\)を用いると、計算量は高々入力データ\\(x_{i},x_{j}\\)のサンプルサイズの次元になるので、計算がしやすくなります。カーネル関数を用いて\\(K_{i,j} = k(x_{i},x_{j})\\)となります。カーネル関数としてはいくつか種類がありますが、以下のガウスカーネルがよく使用されます。\n\\[\rk(x,x\u0026#39;) = a \\exp(-b(x-x\u0026#39;)^{2})\r\\]\n\\(\\displaystyle \\textbf{y}\\)の同時確率が定義できたので、\\(\\displaystyle \\textbf{t}\\)の同時確率を求めることができます。\n\\[\r\\begin{eqnarray}\r\\displaystyle p(\\textbf{t}) \u0026amp;=\u0026amp; \\int p(\\textbf{t}|\\textbf{y})p(\\textbf{y}) d\\textbf{y} \\\\\r\\displaystyle \u0026amp;=\u0026amp; \\int N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N})N(\\textbf{y}|0,K)d\\textbf{y} \\\\\r\u0026amp;=\u0026amp; N(\\textbf{y}|0,\\textbf{C}_{N})\r\\end{eqnarray}\r\\]\nここで、\\(\\textbf{C}_{N} = K + \\beta^{-1}\\textbf{I}_{N}\\)です。なお、最後の式展開は正規分布の再生性を利用しています（証明は正規分布の積率母関数から容易に導けます）。要は、両者は独立なので共分散は2つの分布の共分散の和となると言っているだけです。個人的には、\\(p(\\textbf{y})\\)が先ほど説明したガウス過程の事前分布であり、\\(p(\\textbf{t}|\\textbf{y})\\)が尤度関数で、\\(p(\\textbf{t})\\)は事後分布をというようなイメージです。事前分布\\(p(\\textbf{y})\\)は制約の緩い分布でなめらかであることのみが唯一の制約です。\r\\(N\\)個の観測可能な教師データ\\(\\textbf{t}\\)と\\(t_{N+1}\\)の同時確率は、\n\\[\rp(\\textbf{t},t_{N+1}) = N(\\textbf{t},t_{N+1}|0,\\textbf{C}_{N+1})\r\\]\nここで、\\(\\textbf{C}_{N+1}\\)は、\n\\[\r\\textbf{C}_{N+1} = \\left(\r\\begin{array}{cccc}\r\\textbf{C}_{N} \u0026amp; \\textbf{k} \\\\\r\\textbf{k}^{T} \u0026amp; c \\\\\r\\end{array}\r\\right)\r\\]\nです。ここで、\\(\\textbf{k} = (k(x_{1},x_{N+1}),...,k(x_{N},x_{N+1}))\\)、\\(c = k(x_{N+1},x_{N+1})\\)です。\\(\\textbf{t}\\)と\\(t_{N+1}\\)の同時分布から条件付分布\\(p(t_{N+1}|\\textbf{t})\\)を求めることができます。\n\\[\rp(t_{N+1}|\\textbf{t}) = N(t_{N+1}|\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{t},c-\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{k})\r\\]\n条件付分布の計算においては、条件付多変量正規分布の性質を利用しています。上式を見ればわかるように、条件付分布\\(p(t_{N+1}|\\textbf{t})\\)は\\(N+1\\)個の入力データ、\\(N\\)個の教師データ、カーネル関数のパラメータ\\(a,b\\)が既知であれば計算可能となっていますので、任意の点を入力データとして与えてやれば、元のData Generating Processを近似することが可能になります。GPRの良いところは上で定義した確率モデル\\(\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\\)を直接推定しなくても予測値が得られるところです。確率モデルには\\(\\phi(x_{i})\\)があり、非線形変換により入力データを高次元ベクトルへ変換しています。よって、次元が高くなればなるほど\\(\\phi(x_{i})\\phi(x_{j})\\alpha\\)の計算量は大きくなっていきますが、GPRではカーネルトリックを用いているので高々入力データベクトルのサンプルサイズの次元の計算量で事足りることになります。\n\r2. GPRの実装\r　とりあえずここまでをRで実装してみましょう。PRMLのテストデータで実装しているものがあったので、それをベースにいじってみました。\nlibrary(ggplot2)\rlibrary(grid)\r# 1.Gaussian Process Regression\r# PRML\u0026#39;s synthetic data set\rcurve_fitting \u0026lt;- data.frame(\rx=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000),\rt=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502))\rf \u0026lt;- function(beta, sigma, xmin, xmax, input, train) {\rkernel \u0026lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function\rK \u0026lt;- outer(input, input, kernel); # calc gram matrix\rC_N \u0026lt;- K + diag(length(input))/beta\rm \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance\rx \u0026lt;- seq(xmin,xmax,length=100)\routput \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),\rtx=input,ty=train),\raes(x=x1,y=m)) + geom_line() +\rgeom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +\rgeom_point(aes(x=tx,y=ty))\rreturn(output)\r}\rgrid.newpage() # make a palet\rpushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2\rprint(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))\rprint(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2))\rprint(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))\rprint(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) \r\\(\\beta^{-1}\\)は測定誤差を表しています。\\(\\beta\\)が大きい（つまり、測定誤差が小さい）とすでに得られているデータとの誤差が少なくなるように予測値をはじき出すので、over fitting しやすくなります。上図の左上がそうなっています。左上は\\(\\beta=400\\)で、現時点で得られているデータに過度にfitしていることがわかります。逆に\\(\\beta\\)が小さいと教師データとの誤差を無視するように予測値をはじき出しますが、汎化性能は向上するかもしれません。右上の図がそれです。\\(\\beta=4\\)で、得られているデータ点を平均はほとんど通っていません。\\(b\\)は現時点で得られているデータが周りに及ぼす影響の広さを表しています。\\(b\\)が小さいと、隣接する点が互いに強く影響を及ぼし合うため、精度は下がるが汎化性能は上がるかもしれません。逆に、\\(b\\)が大きいと、個々の点にのみフィットする不自然な結果になります。これは右下の図になります（\\(b=\\frac{1}{0.03},\\beta=25\\)）。御覧の通り、\\(\\beta\\)が大きいのでoverfitting気味であり、なおかつ\\(b\\)も大きいので個々の点のみにfitし、無茶苦茶なグラフになっています。左下のグラフが最もよさそうです。\\(b=\\frac{1}{0.3},\\beta=2\\)となっています。試しに、このグラフのx区間を[0,2]へ伸ばしてみましょう。すると、以下のようなグラフがかけます。\ngrid.newpage() # make a palet\rpushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2\rprint(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))\rprint(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) \rこれを見ればわかるように、左下以外のグラフはすぐに95%信頼区間のバンドが広がり、データ点がないところではまったく使い物にならないことがわかります。一方、左下のグラフは1.3~1.4ぐらいまではそこそこのバンドがかけており、我々が直感的に理解する関数とも整合的な点を平均値が通っているように思えます。また、観測可能なデータ点から離れすぎるとパラメータに何を与えようと平均０、分散１の正規分布になることもわかるがわかります。\rさて、このようにパラメータの値に応じて、アウトサンプルの予測精度が異なることを示したわけですが、ここで問題となるのはこれらハイパーパラメータをどのようにして推計するかです。これは対数尤度関数\\(\\ln p(\\textbf{t}|a,b)\\)を最大にするハイパーパラメータを勾配法により求めます((\\(\\beta\\)は少しタイプが異なるようで、発展的な議論では他のチューニング方法をとる模様。まだ、そのレベルにはいけていないのでここではカリブレートすることにします。))。\\(p(\\textbf{t}) = N(\\textbf{y}|0,\\textbf{C}_{N})\\)なので、対数尤度関数は\n\\[\r\\displaystyle \\ln p(\\textbf{t}|a,b,\\beta) = -\\frac{1}{2}\\ln|\\textbf{C}_{N}| - \\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1}\\textbf{k}\r\\]\nとなります。あとは、これをパラメータで微分し、得られた連立方程式を解くことで最尤推定量が得られます。ではまず導関数を導出してみます。\n\\[\r\\displaystyle \\frac{\\partial}{\\partial \\theta_{i}} \\ln p(\\textbf{t}|\\theta) = -\\frac{1}{2}Tr(\\textbf{C}_{N}^{-1}\\frac{\\partial \\textbf{C}_{N}}{\\partial \\theta_{i}}) + \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1}\r\\frac{\\partial\\textbf{C}_{N}}{\\partial\\theta_{i}}\\textbf{C}_{N}^{-1}\\textbf{t}\r\\]\nここで、\\(\\theta\\)はパラメータセットで、\\(\\theta_{i}\\)は\\(i\\)番目のパラメータを表しています。この導関数が理解できない方はこちらの補論にある(C.21)式と(C.22)式をご覧になると良いと思います。今回はガウスカーネルを用いているため、\n\\[\r\\displaystyle \\frac{\\partial k(x,x\u0026#39;)}{\\partial a} = \\exp(-b(x-x\u0026#39;)^{2}) \\\\\r\\displaystyle \\frac{\\partial k(x,x\u0026#39;)}{\\partial b} = -a(x-x\u0026#39;)^{2}\\exp(-b(x-x\u0026#39;)^{2})\r\\]\nを上式に代入すれば良いだけです。ただ、今回は勾配法により最適なパラメータを求めます。以下、実装のコードです（かなり迷走しています）。\ng \u0026lt;- function(xmin, xmax, input, train){\r# initial value\rbeta = 100\rb = 1\ra = 1\rlearning_rate = 0.1\ritermax \u0026lt;- 1000\rif (class(input) == \u0026quot;numeric\u0026quot;){\rN \u0026lt;- length(input)\r} else\r{\rN \u0026lt;- NROW(input)\r}\rkernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel\rderivative_a \u0026lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2)\rderivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)\rdloglik_a \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y }\rdloglik_b \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y }\r# loglikelihood function\rlikelihood \u0026lt;- function(b,a,x,y){\rkernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2)\rK \u0026lt;- outer(x, x, kernel)\rC_N \u0026lt;- K + diag(N)/beta\ritermax \u0026lt;- 1000\rl \u0026lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y\rreturn(l)\r}\rK \u0026lt;- outer(input, input, kernel) C_N \u0026lt;- K + diag(N)/beta\rfor (i in 1:itermax){\rkernel \u0026lt;- function(x1, x2) a*exp(-b*(x1-x2)^2)\rderivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)\rdloglik_b \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y }\rK \u0026lt;- outer(input, input, kernel) # calc gram matrix\rC_N \u0026lt;- K + diag(N)/beta\rl \u0026lt;- 0\rif(abs(l-likelihood(b,a,input,train))\u0026lt;0.0001\u0026amp;i\u0026gt;2){\rbreak\r}else{\ra \u0026lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input))\rb \u0026lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input))\r}\rl \u0026lt;- likelihood(b,a,input,train)\r}\rK \u0026lt;- outer(input, input, kernel)\rC_N \u0026lt;- K + diag(length(input))/beta\rm \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel))))\rx \u0026lt;- seq(xmin,xmax,length=100)\routput \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),\rtx=input,ty=train),\raes(x=x1,y=m)) + geom_line() +\rgeom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +\rgeom_point(aes(x=tx,y=ty))\rreturn(output)\r}\rprint(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))\rたしかに、良さそうな感じがします（笑）\rとりあえず、今日はここまで。\n\r","date":1543708800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1543708800,"objectID":"eb19f4601e090b622a80cdb28ae21cf8","permalink":"/post/post1/","publishdate":"2018-12-02T00:00:00Z","relpermalink":"/post/post1/","section":"post","summary":"万能すぎて逆に面白くないと話題のガウス回帰を実装してみました。","tags":["R"],"title":"ガウス回帰の実装をやってみた","type":"post"},{"authors":null,"categories":["統計学"],"content":"\r\r\r\r1. Unrestricted VARについて\r2. BVARについて\r\r具体的な推定方法（カルマンフィルタ）\rカルマンフィルタの初期値をどのようにきめるか\rハイパーパラメータの決定方法とその評価尺度\r\r3. Rでの実装\r\r\r　おはこんばんにちは。日次GDP推計を休日に進めているのですが、今日は少し勉強編でBVARについての記事を書きたいと思います。このBVARはFRBアトランタ連銀のGDPNowでも使用されていることから、日次GDP推計との親和性も高いと思われます。そもそも、時系列でアウトサンプルの予測精度を上げたいということになると真っ先に思いつくのがBVARです。Doan, Litterman and Sims(1984)で提案されたこのモデルは予測精度が良いので、非常に有効な手段になると思われます。BVARはBayesian Vector Autoregressionの略で、ベクトル自己回帰モデル（VAR）の派生版です。VARとネットで調べるとまずValue at Risk（VaR）が出てくると思いますが、それとは違います。よく見るとaが小文字になっていることに気づくかと思います。\rさて、BVARの説明をこれから行おうとするのですが、その前にまず基本的なVARの説明からしたいと思います。ただし、歴史的な背景（大型マクロ計量モデルからの経緯など）には触れません。あくまで、BVARを説明するうえで必要な知識について触れたいと思います。\n1. Unrestricted VARについて\r　まず、注意点を一点。この投稿では、もっとも基本的なVARのことをUnrestricted VAR（UVAR）と呼ぶことにします。UVARはSims(1980)の論文が有名です。1\nこのモデルには、理論的な基礎づけは原則ありません。あくまで実証的なモデルです。UVARは一般系は以下のような形をしています。\n\\[\rY_{t} = A_{0} + A_{1}Y_{t-1} + ... + A_{K}Y_{t-K} + U_{t}, ~~ U_{t} ～ N(0,\\Omega)\r\\]\r\\[\rY_{t} = \\left(\r\\begin{array}{cccc}\ry_{1,t} \\\\\ry_{2,t} \\\\\r\\vdots \\\\\ry_{J,t} \\\\\r\\end{array}\r\\right),\rA_{0} = \\left(\r\\begin{array}{cccc}\ra_{10} \\\\\ra_{20} \\\\\r\\vdots \\\\\ra_{J0} \\\\\r\\end{array}\r\\right),\rA_{k} = \\left(\r\\begin{array}{cccc}\ra_{11,k} \u0026amp; a_{12,k} \u0026amp; \\ldots \u0026amp; a_{1J,k} \\\\\ra_{21,k} \u0026amp; a_{22,k} \u0026amp; \\ldots \u0026amp; a_{2J,k} \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\ra_{J1,k} \u0026amp; a_{J2,k} \u0026amp; \\ldots \u0026amp; a_{JJ,k}\r\\end{array}\r\\right),\rU_{t} = \\left(\r\\begin{array}{cccc}\ru_{1,t} \\\\\ru_{2,t} \\\\\r\\vdots \\\\\ru_{J,t} \\\\\r\\end{array}\r\\right)\r\\]\rここで、\\(t\\)は時点、\\(J\\)は変数の数、\\(K\\)はラグ数を表しています。上式を見ると、UVARは自己回帰＋他変数のラグでt期の変数\\(y_{j,t}\\)を説明しようとするモデルであると言えます。しばしば、経済の実証分析で使用され、インサンプルの当てはまりが良いことも知られています（GDP、消費、投資、金利、マネーサプライの５変数VARで金融政策の波及経路を分析したり･･･）。推定するパラメータの個数は、回帰式1本だけでJK+1個（定数項込み）の係数を含むので、J本になればJ(JK+1)個になります。また、\\(\\Omega\\)がJ(J+1)/2個のパラメータを持っているので、合計J(JK+1)+J(J+1)/2個のパラメータを推定することになり、かなりパラメータ数が多い印象です（これは後々重要になってきます）。具体的な推計方法ですが、UVARは同時方程式体系ではないのでそこまで面倒ではありません。UVAR自体はSeemingly Unrestricted Regression Equation（SUR）の一種でそれぞれの方程式は誤差項の相関を通じて関係してはいますが（\\(\\Omega\\)の部分）、全ての回帰式が同じ説明変数を持つため、各方程式を最小二乗法（OLS）によって推定するだけで良いことが知られています。\rこの事実を説明してみましょう（BVARが気になる方は読み飛ばしてもらって構いません）。説明のために今、UVARをSURの一般系に書き直します。上式はt期のVAR(K)システムですが、UVARを推定する際はこれらJ本の方程式がサンプル数Tセット分存在するので、実際のシステム体系は以下のようになります。\n\\[\r\\left(\r\\begin{array}{cccc}\rY_{1} \\\\\rY_{2} \\\\\r\\vdots \\\\\rY_{J} \\\\\r\\end{array}\r\\right) = \\left(\r\\begin{array}{cccc}\rX_{1} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\\r0 \u0026amp; X_{2} \u0026amp; \\ldots \u0026amp; 0 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; X_{J}\r\\end{array}\r\\right) \\left(\r\\begin{array}{cccc}\rA_{1} \\\\\rA_{2} \\\\\r\\vdots \\\\\rA_{J} \\\\\r\\end{array}\r\\right) +\r\\left(\r\\begin{array}{cccc}\rU_{1} \\\\\rU_{2} \\\\\r\\vdots \\\\\rU_{J} \\\\\r\\end{array}\r\\right) = \\overline{X}A + U (1式)\r\\]\rここで、\n\\[\rY_{j} = \\left(\r\\begin{array}{cccc}\ry_{t,j} \\\\\ry_{t+1,j} \\\\\r\\vdots \\\\\ry_{T,j} \\\\\r\\end{array}\r\\right) ,\rX_{j} = X = \\left(\r\\begin{array}{cccc}\r1 \u0026amp; y_{t-1,1} \u0026amp; \\ldots \u0026amp; y_{t-K,1} \u0026amp; \\ldots \u0026amp; y_{t-K,J} \\\\\r1 \u0026amp; y_{t,1} \u0026amp; \\ldots \u0026amp; y_{t-K+1,1} \u0026amp; \\ldots \u0026amp; y_{t-K+1,J} \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ldots \u0026amp; \\ldots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r1 \u0026amp; y_{T-1,1} \u0026amp; \\ldots \u0026amp; y_{T-K,1} \u0026amp; \\ldots \u0026amp; y_{T-K,J} \\\\\r\\end{array}\r\\right),\rA_{j} = \\left(\r\\begin{array}{cccc}\ra_{00,j} \\\\\ra_{11,j} \\\\\r\\vdots \\\\\ra_{JK,j} \\\\\r\\end{array}\r\\right)\r\\]\rです。上式とは違い、変数順で並べられていることに注意してください（つまり、各方程式を並べる優先順位は１番目にj、２番目にtとなっている）。また、\\(X_{j}\\)が全ての変数\\(j\\)について等しいことに注目してください（jに依存していません、VARなので当たり前ですが）。これが後々非常に重要になってきます。\\(U\\)の分散共分散行列は\n\\[\rE[UU\u0026#39;|X_{1}, X_{2},..,X_{J}] = \\Omega = \\left(\r\\begin{array}{cccc}\r\\sigma_{11}I \u0026amp; \\sigma_{12}I \u0026amp; \\ldots \u0026amp; \\sigma_{1J}I \\\\\r\\sigma_{21}I \u0026amp; \\sigma_{22}I \u0026amp; \\ldots \u0026amp; \\sigma_{2J}I \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r\\sigma_{J1}I \u0026amp; \\sigma_{J2}I \u0026amp; \\ldots \u0026amp; \\sigma_{JJ}I \\\\\r\\end{array}\r\\right)\r\\]\nとなっており、それぞれの変数は誤差項の相関を通じて関係しています（ただし、同じ変数内の異なる時点間の相関はないと仮定します）。このような場合、一般化最小二乗法（GLS）を用いて推計を行うことになりますが、これら方程式体系において説明変数が同じであるならば、GLS推定量とOLS推定量は同値になります。それを確かめてみましょう。上述した方程式体系(1)のGLS推定量は以下になります（参考）。\n\\[\rA_{GLS} = (\\overline{X}\u0026#39;\\Omega^{-1}\\overline{X})^{-1}\\overline{X}\u0026#39;\\Omega^{-1}Y = (\\overline{X}\u0026#39;(\\Sigma^{-1}\\otimes I)\\overline{X})^{-1}\\overline{X}\u0026#39;(\\Sigma^{-1}\\otimes I)Y\r\\]\nここで、\\(\\Sigma\\)は誤差項の分散共分散行列のうち、スカラーである分散、共分散を取り出した行列です。先ほど確認したように、\\(X_{1}=X_{2}=...=X_{J}=X\\)なので\\(\\overline{X}=I \\otimes X\\)であり、その転置も\\(\\overline{X}\u0026#39;=I \\otimes X\\)となります。よって、このの性質を用いると、\n\\[\r\\begin{eqnarray}\rA_{GLS} \u0026amp;=\u0026amp; [(I \\otimes X\u0026#39;)(\\Sigma^{-1}\\otimes I)(I \\otimes X)]^{-1}(I \\otimes X\u0026#39;)(\\Sigma^{-1}\\otimes I)Y \\\\\r\u0026amp;=\u0026amp; [(\\Sigma^{-1}\\otimes X\u0026#39;)(I \\otimes X)]^{-1}(\\Sigma^{-1}\\otimes X\u0026#39;)y \\\\\r\u0026amp;=\u0026amp; (\\Sigma^{-1}\\otimes(X\u0026#39;X))^{-1}(\\Sigma^{-1}\\otimes X\u0026#39;)y \\\\\r\u0026amp;=\u0026amp; (I \\otimes (X\u0026#39;X)^{-1}X\u0026#39;)y \\\\\r\u0026amp;=\u0026amp; (\\overline{X}\u0026#39;\\overline{X})^{-1}\\overline{X}\u0026#39;Y \\\\\r\u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r(X\u0026#39;X)^{-1}X\u0026#39; \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\\r0 \u0026amp; (X\u0026#39;X)^{-1}X\u0026#39; \u0026amp; \\ldots \u0026amp; 0 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; (X\u0026#39;X)^{-1}X\u0026#39;\r\\end{array}\r\\right) \\left(\r\\begin{array}{cccc}\rY_{1} \\\\\rY_{2} \\\\\r\\vdots \\\\\rY_{J} \\\\\r\\end{array}\r\\right)\\\\ \u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r(X\u0026#39;X)^{-1}X\u0026#39;Y_{1} \\\\\r(X\u0026#39;X)^{-1}X\u0026#39;Y_{2} \\\\\r\\vdots \\\\\r(X\u0026#39;X)^{-1}X\u0026#39;Y_{J} \\\\\r\\end{array}\r\\right) \\\\\r\u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\rA_{1,OLS} \\\\\rA_{2,OLS} \\\\\r\\vdots \\\\\rA_{J,OLS} \\\\\r\\end{array}\r\\right)\r\\end{eqnarray}\r\\]\nとなり、各経済変数の方程式を別個にOLS推計していけばよいことがわかります。\rOLS推定に際し、VARの次数Kの選択を選択する必要がありますが、次数KはAIC（BIC）を評価軸に探索的に決定します。つまり、いろいろな値をKに設定し、OLS推計を行い、計算されたVAR(K)のAIC(BIC)のうちで最も値が大きいモデルの次数を真のモデルの次数Kとして採用するということです。RにもVARselect()という関数があり、引数にラグの探索最大数とデータを渡すことで最適な次数を計算してくれます（便利）。\rこのようにUVARはOLS推計で各変数間の相互依存関係をデータから推計できる手軽な手法です。私が知っている分野ですと財政政策乗数の推計に使用されていました。GDP、消費、投資、政府支出の４変数でVARを推定し、推定したVARでインパルス応答を見ることで１単位の財政支出の増加がGDP等に与える影響を定量的にシミュレートすることができたりします（指導教官が論文を書いてました）。そもそも私の専門のDSGEも誘導系に書き直せばVAR形式になり、インパルス応答などは基本的に一緒です。また、経済変数は慣性が強いので（特に我が国の場合）、インサンプルのモデルの当てはまりもいいです。ただし、あくまでインサンプルです。アウトサンプルの当てはまりはそれほど良い印象はありません。なぜなら、推定パラメータが多すぎるからです。UVARの予測に関する問題点はover-parametrizationです。例えば、先ほどの４変数UVARでラグが６期だったとすると、推定すべきパラメータは３１個になります。よって、データ数にもよりますが、パラメータ数がデータ数に近づくとインサンプルの補間に近づき、過学習を引き起こす危険性があります。日次GDP推計は大量の変数を使用するのでこの問題は非常に致命的になります。BVARはこの問題を解決することに主眼を置いています。\n\r2. BVARについて\r　UVARの問題点はover-parametrizationであると述べました。BVARはこの問題を防ぐために不必要な説明変数（のパラメータ）をそぎ落とそうとします。ただ、不要なパラメータを推定する前に０と仮置き（カリブレート）するのではなく、１階の自己に関わるパラメータは１周り、その他変数のパラメータは０周りに正規分布するという形の制約を与えます。このモデルの最大の仮定は「各経済変数は多かれ少なかれドリフト付き１階のランダムウォークに従う」というものであり、上述したような事前分布を先験的に与えた上で推定を行うのです。砕けた言い方をすると、BVARの考え方は「経済変数の挙動は基本はランダムウォークだけど他変数のラグに予測力向上に資するものがあればそれも取り入れるよ」というものなんです。さて、前置きが長くなりましたが、具体的な説明に移りたいと思います。\n具体的な推定方法（カルマンフィルタ）\r　上述した事前分布に加えて、BVARがUVARと異なる点はパラメータがtime-varyingであるということです。なんとなく、パラメータがずっと固定よりもサンプルが増えるたびにその値が更新されるほうが予測精度が上がりそうですよね（笑）。推定手法としてはUVARの時のようにOLSをそれぞれにかけることはせず、カルマンフィルタ と呼ばれるアルゴリズムを用いて推定を行います。BVARは以下のような状態空間モデルとして定義されます（各ベクトル、行列の次元はUVAR時と同じです）。\n\\[\rY_{t} = X_{t}B_{t} + u_{t} \\\\\rB_{t} = \\Phi B_{t-1} + \\epsilon_{t} \\\\\rB_{t} = \\left(\r\\begin{array}{cccc}\r\\beta_{00,t} \\\\\r\\beta_{11,t} \\\\\r\\vdots \\\\\r\\beta_{JK,t} \\\\\r\\end{array}\r\\right), \\Phi = \\left(\r\\begin{array}{cccc}\r\\phi_{00} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\\r0 \u0026amp; \\phi_{11} \u0026amp; \\ldots \u0026amp; 0 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; \\phi_{JK}\r\\end{array}\r\\right),\ru_{t} = \\left(\r\\begin{array}{cccc}\ru_{1,t} \\\\\ru_{2,t} \\\\\r\\vdots \\\\\ru_{J,t} \\\\\r\\end{array}\r\\right),\r\\epsilon_{t} = \\left(\r\\begin{array}{cccc}\r\\epsilon_{00,t} \\\\\r\\epsilon_{11,t} \\\\\r\\vdots \\\\\r\\epsilon_{JK,t} \\\\\r\\end{array}\r\\right)\r\\]\r状態空間モデルは観測可能なデータ（ex.経済統計）を用いて、観測不可能なデータ（ex.リスクプレミアムや限界消費性向等）を推定します。観測可能なデータと不可能なデータを関連付ける方程式を観測方程式、観測不可能なデータの挙動をモデル化した方程式を遷移方程式と呼びます。ここでは、1本目が観測方程式、2本目が遷移方程式となります。御覧の通り、観測方程式は通常のVARの形をしている一方、遷移方程式はAR(1)となっており、これによってパラメータ\\(B_{t}\\)は過去の値を引きずりながら\\(\\epsilon_{t}\\)によって確率的に変動します。\\(\\Phi\\)は自己回帰係数です。誤差項\\(u_{t}\\)と\\(\\epsilon_{t}\\)は平均０の正規分布に従い、その分散は\n\\[\r\\begin{eqnarray}\rVar(u_{t}) \u0026amp;=\u0026amp; \\sigma_{u}^{2} \\\\\rVar(\\epsilon_{t}) \u0026amp;=\u0026amp; \\sigma_{\\epsilon}^{2}R\r\\end{eqnarray}\r\\]\rで与えられます。ここで、\\(\\Phi,R,\\sigma_{u}^{2},\\sigma_{\\epsilon}^{2}\\)は既知であるとします。今、t-1期までのデータが入手可能であるとすると、そのデータをカルマンフィルタアルゴリズムで推定した\\(B_{t-1|t-1}\\)とその分散共分散行列\\(P_{t-1|t-1}\\)を用いて、t期の予想値を以下のように計算します。\n\\[\r\\begin{eqnarray}\rB_{t|t-1} \u0026amp;=\u0026amp; \\Phi B_{t-1|t-1} \\\\\rP_{t|t-1} \u0026amp;=\u0026amp; \\Phi P_{t|t-1} \\Phi\u0026#39; + \\sigma_{\\epsilon}^{2}R\r\\end{eqnarray}\r\\]\r観測可能な変数の予測値はこの値を用いて計算します。\n\\[\r\\hat{Y_{t}} = X_{t} B_{t|t-1}\r\\]\r次にt期の観測値が得られると次の更新方程式を用いて\\(B_{t|t}, P_{t|t}\\)を計算します。\n\\[\r\\begin{eqnarray}\rB_{t|t} \u0026amp;=\u0026amp; B_{t|t-1} + P_{t|t-1}X_{t}\u0026#39;(X_{t}P_{t|t-1}X_{t}\u0026#39; + \\sigma_{u}^{2})^{-1}(Y_{t}-X_{t}B_{t|t-1}) \\\\\rP_{t|t} \u0026amp;=\u0026amp; P_{t|t-1} + P_{t|t-1}X_{t}\u0026#39;(X_{t}P_{t|t-1}X_{t}\u0026#39; + \\sigma_{u}^{2})^{-1}X_{t}P_{t|t-1}\r\\end{eqnarray}\r\\]\r\\(\\sigma_{\\epsilon}^{2}R = 0\\)かつ\\(\\Phi = I\\)の場合は逐次最小二乗法に一致します。要は入手できるサンプル増えるたびにOLSをやり直していくことと同値だということです。こうして推計を行うのがBVARなのですが、カルマンフィルタは漸化式なので初期値\\(B_{0|0}, P_{0|0}\\)を決めてやる必要があります。BVARの２つ目の特徴は初期値の計算に混合推定法を用いているところであり、ここに前述した事前分布が関係してきます。\n\rカルマンフィルタの初期値をどのようにきめるか\r初期値\\(B_{0|0}, P_{0|0}\\)をどうやって計算するのかを考えた際にすぐ思いつく方法としては、カルマンフィルタのスタート地点tの前に、初期値推計期間をある程度用意し、\\(Y = XB + \\epsilon\\)で\\(B_{0|0}, P_{0|0}\\)を推定する方法があります。つまり、観測方程式をGLS推計し、そのパラメータを初期値とする方法です。その際のGLS推定量は\n\\[\r\\hat{B} = (X\u0026#39;V^{-1}X)^{-1}X\u0026#39;V^{-1}Y\r\\]\nです。これでもいいんですが、これではただ時変パラメータを推計しているだけで先ほど述べたover-parametrizationの問題にはアプローチできていません。そこで、ここではパラメータ\\(B\\)に対してなにか先験的な情報が得られているとしましょう。つまり、先述した「経済変数の挙動は基本はランダムウォークだけど他変数のラグに予測力向上に資するものがあればそれも取り入れるよ」という予想です。これを定式化してみましょう。つまり、パラメータ\\(B\\)に対して以下の制約式を課します。\n\\[\rr = RB + \\nu\r\\]\rここで、\\(E(\\nu) = 0,Var(\\nu) = V_{0}\\)です。また、\\(r\\)は\\(RB\\)の予想値であり、\\(V_{0}\\)はその予想値の周りでのばらつきを表しています。取っ付きにくいかもしれませんが、\\(r\\)は１階自己回帰係数に関わる部分は1、それ以外は0となるベクトルで、\\(R\\)は単位行列\\(I\\)だと思ってもらえばいいです。つまり、\n\\[\r\\begin{eqnarray}\r\\left(\r\\begin{array}{cccc}\r0 \\\\\r1 \\\\\r0 \\\\\r\\vdots \\\\\r1 \\\\\r\\vdots \\\\\r0 \\\\\r\\end{array}\r\\right)\r\u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r\\beta_{0,1}^{1} \\\\\r\\beta_{1,1}^{1} \\\\\r\\beta_{2,1}^{1} \\\\\r\\vdots \\\\\r\\beta_{j,1}^{j} \\\\\r\\vdots \\\\\r\\beta_{J,K}^{J} \\\\\r\\end{array}\r\\right)\r+\r\\left(\r\\begin{array}{cccc}\r\\nu_{0,1}^{1} \\\\\r\\nu_{1,1}^{1} \\\\\r\\nu_{2,1}^{1} \\\\\r\\vdots \\\\\r\\nu_{j,1}^{j} \\\\\r\\vdots \\\\\r\\nu_{J,K}^{J} \\\\\r\\end{array}\r\\right)\r\\end{eqnarray}\r\\]\rみたいな感じです。ここで\\(\\beta_{j,k}^{i}\\)はi番目の方程式のj番目の変数のk次ラグにかかるパラメータを表しています。混合推定では、正規分布に従う観測値とこの事前分布が独立であるという仮定の下で観測方程式\\(Y = XB + \\epsilon\\)と$ r = RB + $を以下のように組み合わせます。\n\\[\r\\left(\r\\begin{array}{cccc}\rY \\\\\rr \\end{array}\r\\right) = \\left(\r\\begin{array}{cccc}\rX \\\\\rR \\end{array}\r\\right)B + \\left(\r\\begin{array}{cccc}\r\\epsilon \\\\\r\\nu \\end{array}\r\\right)\r\\]\nここで、\n\\[\rE\\left(\r\\begin{array}{cccc}\r\\epsilon \\\\\r\\nu \\end{array}\r\\right) = 0 \\\\\rVar\\left(\r\\begin{array}{cccc}\r\\epsilon \\\\\r\\nu \\end{array}\r\\right) = \\left(\r\\begin{array}{cccc}\r\\sigma^{2}V \u0026amp; 0 \\\\\r0 \u0026amp; V_{0} \\\\\r\\end{array}\r\\right)\r\\]\nそして、このシステム体系をGLSで推計するのです。こうすることで、事前分布を考慮した初期値の推定を行うことができます。GLS推定量は以下のようになります。\n\\(\\displaystyle \\hat{B}_{M} = (\\frac{1}{\\sigma^{2}}X\u0026#39;V^{-1}X + R\u0026#39;V_{0}^{-1}R)^{-1}(\\frac{1}{\\sigma^{2}}X\u0026#39;V^{-1}y + R\u0026#39;V_{0}^{-1}r)\\)\nご覧になればわかるように、GLS推定量は上記2本の連立方程式(実際には行列なので何本もありますが)それぞれのGLS推定量を按分したような推定量になります。ここで、\\(\\sigma^{2}\\)は既知ではないので、いったんOLSで推計しその推定量を用います。問題は\\(V_{0}\\)の置き方です。\\(V_{0}\\)の各要素を小さくとれば、事前分布に整合的な推定量が得られます（つまり、ランダムウォーク）。逆に、大きくとれば通常のGLS推定量に近づいていきます。Doan, Litternam and Sims (1984)では以下のように分散を置いています。\n\\[\r\\begin{eqnarray}\r\\displaystyle Var(\\beta_{j,k}^{j}) \u0026amp;=\u0026amp; \\frac{\\pi_{5}・\\pi_{1}}{k・exp(\\pi_{4}w_{i}^{i})} \\\\\r\\displaystyle Var(\\beta_{j,k}^{i}) \u0026amp;=\u0026amp; \\frac{\\pi_{5}・\\pi_{2}・\\sigma_{i}^{2}}{k・exp(\\pi_{4}w_{j}^{i})・\\sigma_{j}^{2}} \\\\\rVar(c^{i}) \u0026amp;=\u0026amp; \\pi_{5}・\\pi_{3}・\\sigma_{i}^{2}\r\\end{eqnarray}\r\\]\nここで、\\(\\beta_{j,k}^{j}\\)は\\(i\\)番目の方程式の自己回帰パラメータ、\\(\\beta_{j,k}^{i}\\)は\\(i\\)番目方程式の他変数ラグ項にかかるパラメータ、\\(c^{i}\\)は定数項です。そして、\\(\\pi_{1},\\pi_{2},\\pi_{3},\\pi_{4},\\pi_{5}\\)はハイパーパラメータと呼ばれるもので、カルマンフィルタにかけるパラメータの初期値の事前分布の分布の広がりを決定するパラメータとなっています。これらは初期値の推定を行う前に値を指定する必要があります。各ハイパーパラメータの具体的な特徴は以下の通りです。\\(\\pi_{1}\\)は\\(\\beta_{j,k}^{j}\\)の分散にのみ出現することから自己回帰係数に影響を与えるパラメータとなっています。具体的には、\\(\\pi_{1}\\)が大きくなればなるほど自己回帰係数は事前分布から大きく離れた辺りを取りうることになります。\\(\\pi_{2}\\)は\\(\\beta_{j,k}^{i}\\)の分散にのみ出現することから他変数のラグ項に影響を与えるパラメータとなっています。こちらも値が大きくなればなるほど係数は事前分布から大きく離れた辺りを取りうることになります。\\(\\pi_{3}\\)は定数項の事前分布に影響を与えるパラメータでこちらも考え方は同じです。\\(\\pi_{4}\\)はラグ項の分散(つまり定数項以外)に影響を与えるパラメータで、値が大きくなるにつれ、初期値は事前分布に近づいていきます。最後に\\(\\pi_{5}\\)ですが、こちらは全体にかかるパラメータで、値が大きくなるにつれ、初期値は事前分布から遠ざかります。\n　上式には、ハイパーパラメータ以外にもパラメータや変数が存在します。\\(\\sigma_{i}, \\sigma_{j}\\)は\\(y_{j,t}, y_{i,t}\\)をそれぞれAR(m)でフィッティングをかけた時の残差の標準偏差の推定値です。変数間のスケーリングの違いを考慮するために、\\(\\sigma_{i}\\)を\\(\\sigma_{j}\\)で割ったものを使用しています。本来ならば、VARの残差の標準偏差を使用すべきなのですが、推定する前にわかるわけもないので、それぞれAR(m)で推定をかけ、その標準偏差を使用しています((ランダムウォークが先験情報なので整合性は取れているような気がします))。\\(w_{j}^{i}\\)は完全に恣意的なパラメータで、DLSではrelative weightsと呼ばれているものです。i番目の方程式のj番目の変数のラグ項にかかるパラメータが０であるかどうかについて、分析者の先験情報を反映するためのパラメータです。分散の式を見ればわかるように、relative weightが大きくなれば分散は小さくなり、推定値は事前分布に近づいていきます。DLSでは、ほとんどの変数は\\(w_{i}^{i}=0,w_{j}^{i}=1\\)でよいと主張されています。つまり、自己ラグにかかる事前分布の分散に関しては確信をもってランダムウォークであるといえる一方、他変数ラグについては予測力向上に役立つもののあることを考え、値を１と置いているのです。一方、為替レートや株価はランダムウォーク色が強いということから大きい値を使用しています。最後に、\\(Var(\\beta_{j,k}^{j})\\)と\\(Var(\\beta_{j,k}^{i})\\)には分母にkがついています。つまり、ラグ次数kが大きくなればなるほど、その係数は０に近づいていくことを先験的情報として仮定していることになります((このおかげでVARの次数をこちらで指定する必要がなくなります。適当に大きい次数を指定しておけば、必要のない次数の大きいパラメータに関しては事前と値が０になるので。))。\rこれらがいわゆるMinnesota Priorの正体です((実はこれに加えて自己回帰パラメータの総和が１、他変数のラグ項にかかるパラメータの総和が０となる制約を課すのですが、話が複雑になりすぎるので今回は割愛しました))。初期値が事前分布に近づけばBVARはランダムウォークに近づきますし、離れるとUVARに近づきます。現実はその間となるのですが、なにを評価尺度としてハイパーパラメータの値を決めるかというと、それは当てはまりの良さということになります。\n\rハイパーパラメータの決定方法とその評価尺度\r当てはまりの良さと言ってもいろいろありますが、DLSはその時点で観測可能なデータから予測できるk期先の予測値の当てはまりの良さを基準としています。DLSでは以下の予測誤差ベクトル\\(\\hat{\\epsilon}_{t+k|t}\\)のクロス積和を最小化することを目的関数として、ハイパーパラメータのチューニングを行っています。\n\\[\r\\hat{\\epsilon}_{t+k|t} = \\hat{Y}_{t+k|t}-Y_{t+k}\r\\]\rここで、\\(\\hat{Y}_{t+k|t}\\)はカルマンフィルタによるk期先の予測値です（kをいくつ先にすれば良いかは不明）。このクロス積は\\(\\hat{\\epsilon}_{t+k|t}\\hat{\\epsilon}\u0026#39;_{t+k|t}\\)であり、これをフィルタリングをかけるt=1期からサンプル期間であるt=T期まで計算していくので、最終的にはT個の予測誤差ベクトルを得ることになり、以下のようなこれらの総和を最小化します。\n\\[\r\\displaystyle \\sum_{t=1}^{T-k}\\hat{\\epsilon}_{t+k|t}\\hat{\\epsilon}\u0026#39;_{t+k|t}\r\\]\rより厳密にはこのクロス積和の対数値を最小化するハイパーパラメータの値をグリッドサーチやランダムサーチで探索していくことになります((ここは機械学習等で用いられているベイズ最適化を利用するとより高速に収束させることが可能かもです。))。\rとまあ、BVARの推定方法はこんな感じです。他のVARと違い、恣意的であり、また推定方法が機械学習に近い点が特徴ではないかと思います。そもそも、BVARは予測に特化したVARですから、他のVARとは別物と考える方が良いかもです。\n（追記　2019/4/29）\n\r\r3. Rでの実装\rここまでをRで実装したいと思います。まずは、varsパッケージに準備されているCanadaデータを使用して、通常のVARの推定をやってみます。これはカナダの1980～2000年の労働生産性、雇用、失業率、実質賃金をまとめたものになります。\nlibrary(vars)\rdata(Canada)\rCanada.var \u0026lt;- VAR(Canada,p=VARselect(Canada,lag.max = 4)$selection[1])\rsummary(Canada.var)\r## ## VAR Estimation Results:\r## ========================= ## Endogenous variables: e, prod, rw, U ## Deterministic variables: const ## Sample size: 81 ## Log Likelihood: -150.609 ## Roots of the characteristic polynomial:\r## 1.004 0.9283 0.9283 0.7437 0.7437 0.6043 0.6043 0.5355 0.5355 0.2258 0.2258 0.1607\r## Call:\r## VAR(y = Canada, p = VARselect(Canada, lag.max = 4)$selection[1])\r## ## ## Estimation results for equation e: ## ================================== ## e = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## e.l1 1.75274 0.15082 11.622 \u0026lt; 2e-16 ***\r## prod.l1 0.16962 0.06228 2.723 0.008204 ** ## rw.l1 -0.08260 0.05277 -1.565 0.122180 ## U.l1 0.09952 0.19747 0.504 0.615915 ## e.l2 -1.18385 0.23517 -5.034 3.75e-06 ***\r## prod.l2 -0.10574 0.09425 -1.122 0.265858 ## rw.l2 -0.02439 0.06957 -0.351 0.727032 ## U.l2 -0.05077 0.24534 -0.207 0.836667 ## e.l3 0.58725 0.16431 3.574 0.000652 ***\r## prod.l3 0.01054 0.06384 0.165 0.869371 ## rw.l3 0.03824 0.05365 0.713 0.478450 ## U.l3 0.34139 0.20530 1.663 0.100938 ## const -150.68737 61.00889 -2.470 0.016029 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## ## Residual standard error: 0.3399 on 68 degrees of freedom\r## Multiple R-Squared: 0.9988, Adjusted R-squared: 0.9985 ## F-statistic: 4554 on 12 and 68 DF, p-value: \u0026lt; 2.2e-16 ## ## ## Estimation results for equation prod: ## ===================================== ## prod = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## e.l1 -0.14880 0.28913 -0.515 0.6085 ## prod.l1 1.14799 0.11940 9.615 2.65e-14 ***\r## rw.l1 0.02359 0.10117 0.233 0.8163 ## U.l1 -0.65814 0.37857 -1.739 0.0866 . ## e.l2 -0.18165 0.45083 -0.403 0.6883 ## prod.l2 -0.19627 0.18069 -1.086 0.2812 ## rw.l2 -0.20337 0.13337 -1.525 0.1319 ## U.l2 0.82237 0.47034 1.748 0.0849 . ## e.l3 0.57495 0.31499 1.825 0.0723 . ## prod.l3 0.04415 0.12239 0.361 0.7194 ## rw.l3 0.09337 0.10285 0.908 0.3672 ## U.l3 0.40078 0.39357 1.018 0.3121 ## const -195.86985 116.95813 -1.675 0.0986 . ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## ## Residual standard error: 0.6515 on 68 degrees of freedom\r## Multiple R-Squared: 0.98, Adjusted R-squared: 0.9765 ## F-statistic: 277.5 on 12 and 68 DF, p-value: \u0026lt; 2.2e-16 ## ## ## Estimation results for equation rw: ## =================================== ## rw = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## e.l1 -4.716e-01 3.373e-01 -1.398 0.167 ## prod.l1 -6.500e-02 1.393e-01 -0.467 0.642 ## rw.l1 9.091e-01 1.180e-01 7.702 7.63e-11 ***\r## U.l1 -7.941e-04 4.417e-01 -0.002 0.999 ## e.l2 6.667e-01 5.260e-01 1.268 0.209 ## prod.l2 -2.164e-01 2.108e-01 -1.027 0.308 ## rw.l2 -1.457e-01 1.556e-01 -0.936 0.353 ## U.l2 -3.014e-01 5.487e-01 -0.549 0.585 ## e.l3 -1.289e-01 3.675e-01 -0.351 0.727 ## prod.l3 2.140e-01 1.428e-01 1.498 0.139 ## rw.l3 1.902e-01 1.200e-01 1.585 0.118 ## U.l3 1.506e-01 4.592e-01 0.328 0.744 ## const -1.167e+01 1.365e+02 -0.086 0.932 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## ## Residual standard error: 0.7601 on 68 degrees of freedom\r## Multiple R-Squared: 0.9989, Adjusted R-squared: 0.9987 ## F-statistic: 5239 on 12 and 68 DF, p-value: \u0026lt; 2.2e-16 ## ## ## Estimation results for equation U: ## ================================== ## U = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const ## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## e.l1 -0.61773 0.12508 -4.939 5.39e-06 ***\r## prod.l1 -0.09778 0.05165 -1.893 0.062614 . ## rw.l1 0.01455 0.04377 0.332 0.740601 ## U.l1 0.65976 0.16378 4.028 0.000144 ***\r## e.l2 0.51811 0.19504 2.656 0.009830 ** ## prod.l2 0.08799 0.07817 1.126 0.264279 ## rw.l2 0.06993 0.05770 1.212 0.229700 ## U.l2 -0.08099 0.20348 -0.398 0.691865 ## e.l3 -0.03006 0.13627 -0.221 0.826069 ## prod.l3 -0.01092 0.05295 -0.206 0.837180 ## rw.l3 -0.03909 0.04450 -0.879 0.382733 ## U.l3 0.06684 0.17027 0.393 0.695858 ## const 114.36732 50.59802 2.260 0.027008 * ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## ## Residual standard error: 0.2819 on 68 degrees of freedom\r## Multiple R-Squared: 0.9736, Adjusted R-squared: 0.969 ## F-statistic: 209.2 on 12 and 68 DF, p-value: \u0026lt; 2.2e-16 ## ## ## ## Covariance matrix of residuals:\r## e prod rw U\r## e 0.11550 -0.03161 -0.03681 -0.07034\r## prod -0.03161 0.42449 0.05589 0.01494\r## rw -0.03681 0.05589 0.57780 0.03660\r## U -0.07034 0.01494 0.03660 0.07945\r## ## Correlation matrix of residuals:\r## e prod rw U\r## e 1.0000 -0.14276 -0.1425 -0.73426\r## prod -0.1428 1.00000 0.1129 0.08136\r## rw -0.1425 0.11286 1.0000 0.17084\r## U -0.7343 0.08136 0.1708 1.00000\rplot(predict(Canada.var,n.ahead=20,ci=0.95))\rパッケージが整備されているので簡単に実行できました。上で見たようにやっていることは別々のOLSを4本推定しているだけです。\r次にBVARです。まず、カルマンフィルタと事前分布を定義します。カルマンフィルタは以前記事でご紹介したものと同じです。\nlibrary(seasonal)\rkalmanfiter \u0026lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){\r#-------------------------------------------------------------------\r# Implemention of Kalman filter\r# y - observed variable\r# I - the number of unobserved variable\r# t - parameter of endogenous variable in state equation\r# z - parameter of endogenous variable in observable equation\r# c - constant in state equaion\r# R - parameter of exogenous variable in state equation\r# Q - var-cov matrix of exogenous variable in state equation\r# d - constant in observable equaion\r# S - parameter of exogenous variable in observable equation\r# h - var-cov matrix of exogenous variable in observable equation\r# a_int - initial value of endogenous variable\r# sig_int - initial value of variance of endogenous variable\r#-------------------------------------------------------------------\rlibrary(MASS)\r# 1.Define Variable\rif (class(y)!=\u0026quot;matrix\u0026quot;){\ry \u0026lt;- as.matrix(y)\r}\rN \u0026lt;- NROW(y) # sample size\rL \u0026lt;- NCOL(y) # the number of observable variable a_pre \u0026lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable\ra_fil \u0026lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable\rsig_pre \u0026lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable\rsig_fil \u0026lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable\ry_pre \u0026lt;- array(0,dim = c(L,1,N)) # prediction of observed variable\rF_pre \u0026lt;- array(0,dim = c(L,L,N)) # auxiliary variable k \u0026lt;- array(0,dim = c(I,L,N)) # kalman gain\rif (any(is.na(a_int))){\ra_int \u0026lt;- matrix(0,nrow = I,ncol = 1)\r}\rif (any(is.na(sig_int))){\rsig_int \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(R))){\rR \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif (any(is.na(Q))){\rQ \u0026lt;- diag(1,nrow = I,ncol = I)\r}\rif(any(is.na(S))){\rS \u0026lt;- matrix(1,nrow = L,ncol = L)\r}\rif (any(is.na(h))){\rH \u0026lt;- array(0,dim = c(L,L,N))\rfor(i in 1:N){\rdiag(H[,,i]) = 1\r}\r}else if (class(h)!=\u0026quot;array\u0026quot;){\rH \u0026lt;- array(h,dim = c(NROW(h),NCOL(h),N))\r}\r# fill infinite if observed data is NA\rfor(i in 1:N){\rmiss \u0026lt;- is.na(y[i,])\rdiag(H[,,i])[miss] \u0026lt;- 1e+32\r}\ry[is.na(y)] \u0026lt;- 0\r# 2.Set Initial Value\ra_fil[,,1] \u0026lt;- a_int\rsig_fil[,,1] \u0026lt;- sig_int\r# 3.Implement Kalman filter\rfor (i in 1:N){\rif(class(z)==\u0026quot;array\u0026quot;){\rZ \u0026lt;- z[,,i]\r}else{\rZ \u0026lt;- z\r}\ra_pre[,,i] \u0026lt;- t%*%a_fil[,,i] + c\rsig_pre[,,i] \u0026lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)\ry_pre[,,i] \u0026lt;- Z%*%a_pre[,,i] + d\rF_pre[,,i] \u0026lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)\rk[,,i] \u0026lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])\ra_fil[,,i+1] \u0026lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])\rsig_fil[,,i+1] \u0026lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])\r}\r# 4.Aggregate results\rresult \u0026lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)\rnames(result) \u0026lt;- c(\u0026quot;state prediction\u0026quot;, \u0026quot;state filtered\u0026quot;, \u0026quot;state var prediction\u0026quot;, \u0026quot;state var filtered\u0026quot;, \u0026quot;observable prediction\u0026quot;, \u0026quot;kalman gain\u0026quot;,\r\u0026quot;parameter of state eq\u0026quot;, \u0026quot;parameter of observable eq\u0026quot;)\rreturn(result)\r}\r事前分布を計算する関数を定義します。まず、引数を計算しやすい形に変換しておきます。\n # set pi\rpi \u0026lt;- abs(rnorm(5)) # hyperparameter\rx \u0026lt;- Canada # dataset\rorder \u0026lt;- 4 # order\rlibrary(MASS)\r# 1. Process row data\rfor (i in 1:5){\reval(parse(text = paste0(\u0026quot;pi\u0026quot;,i,\u0026quot;=pi[[\u0026quot;,i,\u0026quot;]]\u0026quot;)))\r}\rif (class(x)!=\u0026quot;matrix\u0026quot;){\rx \u0026lt;- as.matrix(x)\r}\rdependent \u0026lt;- as.matrix(x[(order+1):NROW(x),1])\rfor (i in 2:NCOL(x)){\rdependent \u0026lt;- rbind(dependent,as.matrix(x[(order+1):NROW(x),i]))\r}\rexplanatory \u0026lt;- embed(cbind(1,x),order)[,-((order*NCOL(x)+order-NCOL(x)):(order*NCOL(x)+order))]\rexplanatory \u0026lt;- diag(1,nrow = NCOL(x),ncol = NCOL(x))%x%as.matrix(explanatory)\rnpara \u0026lt;- NCOL(explanatory) # the number of parameters/ ncol(x)*(ncol(x)*order + 1(const))\rdependentは非説明変数、explanatoryは説明変数です。ちょうど1式を再現した形になります。\n\\[\r\\left(\r\\begin{array}{cccc}\rY_{1} \\\\\rY_{2} \\\\\r\\vdots \\\\\rY_{J} \\\\\r\\end{array}\r\\right) = \\left(\r\\begin{array}{cccc}\rX_{1} \u0026amp; 0 \u0026amp; \\ldots \u0026amp; 0 \\\\\r0 \u0026amp; X_{2} \u0026amp; \\ldots \u0026amp; 0 \\\\\r\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\\r0 \u0026amp; 0 \u0026amp; \\ldots \u0026amp; X_{J}\r\\end{array}\r\\right) \\left(\r\\begin{array}{cccc}\rA_{1} \\\\\rA_{2} \\\\\r\\vdots \\\\\rA_{J} \\\\\r\\end{array}\r\\right) +\r\\left(\r\\begin{array}{cccc}\rU_{1} \\\\\rU_{2} \\\\\r\\vdots \\\\\rU_{J} \\\\\r\\end{array}\r\\right)\r\\]\n# 2. Make Prior matrixes\rr \u0026lt;- as.matrix(numeric(npara)) # a part of dependent variable in mixed estimation\riter1 \u0026lt;- numeric(NCOL(x))\rn \u0026lt;- 2\rfor (i in 1:length(iter1)){\riter1[i] \u0026lt;- n\rn \u0026lt;- n + order*NCOL(x) + 1\r}\rfor (i in iter1){\rr[i] \u0026lt;- 1\r}\rR \u0026lt;- diag(1,nrow = npara,ncol = npara) # a part of explanatory variables in mixed estimation\rV \u0026lt;- diag(1,nrow = NROW(explanatory),ncol = NROW(explanatory)) # coefficient matrix of var-cov matrix in mixed estimation\rV0 \u0026lt;- matrix(0,nrow = npara,ncol = npara) # Prior for var matrix\r# PRIOR FOR VAR OF CONSTANT\rsigi \u0026lt;- numeric(NCOL(x)) # var of AR(m)\rfor (i in 1:NCOL(x)){\rAR \u0026lt;- ar(x[,i],order.max = order)\rsigi[i] \u0026lt;- AR$var.pred\r}\rsigma_AR \u0026lt;- diag(sigi,nrow = NCOL(x),ncol = NCOL(x))\riter2 \u0026lt;- seq(1,npara,(NCOL(x)*order+1))\rn \u0026lt;- 1\rfor (i in iter2){\rV0[i,i] \u0026lt;- pi5*pi3*sigi[n]\rn \u0026lt;- n + 1\r}\r# PRIOR FOR VAR OF AUTOREGRESSIVE PARAMETER\rwi \u0026lt;- 0 # prior weight on s.d. of autoregressive parameter\rk \u0026lt;- 0 # decay parameter\riter3 \u0026lt;- numeric((NCOL(x)*order))\rn \u0026lt;- 2;iter3[1] \u0026lt;- n\rfor (i in 2:length(iter3)){\rif ((i-1)%%order == 0){\rn \u0026lt;- n + NCOL(x) + 2 # const\riter3[i] \u0026lt;- n }else{\rn \u0026lt;- n + NCOL(x)\riter3[i] \u0026lt;- n\r}\r}\rfor (i in iter3) {\rk \u0026lt;- k + 1\rV0[i,i] \u0026lt;- pi5*pi1/(k*exp(pi4*wi))\rif (k == order){\rk = 0\r}\r}\r# PRIOR FOR VAR OF DISTRIBUTED LAG PARAMETER\rwj \u0026lt;- 1 # prior weight on s.d. of distributed lag parameter\rk \u0026lt;- 1 # decay parameter\rcount \u0026lt;- 0\rn \u0026lt;- 1\rndis \u0026lt;- npara - NCOL(x) - length(iter3) # the number of distibuted lag in each equation\rl \u0026lt;- rep(1:NCOL(x),length=npara)\rtemp \u0026lt;- numeric(length(iter3))\rj \u0026lt;- 1;temp[1] \u0026lt;- j\rfor (i in 2:length(temp)){\rif ((i-1)%%order == 0){\rj \u0026lt;- j + NCOL(x) + 1 # const\rtemp[i] \u0026lt;- j }else{\rj \u0026lt;- j + NCOL(x)\rtemp[i] \u0026lt;- j\r}\r}\rl \u0026lt;- l[-c(temp)]; l \u0026lt;- l[1:(ndis)]\riter4 \u0026lt;- 1:npara; iter4 \u0026lt;- iter4[-c(iter2,iter3)]\rfor (i in iter4){\rcount \u0026lt;- count + 1\rV0[i,i] \u0026lt;- pi5*pi2*sigi[n]/(k*exp(pi4*wj)*sigi[l[count]])\rif(count%%(order-1) == 0){\rk \u0026lt;- k + 1\r}\rif (count%%(ndis/NCOL(x)) == 0){\rn = n + 1\rk = 1\r}\r}\r# 3. Estimate OLS\rbeta \u0026lt;- ginv((t(explanatory)%*%explanatory))%*%t(explanatory)%*%dependent\rsig \u0026lt;- as.numeric((NROW(explanatory)-NCOL(explanatory))^(-1)*\rt(dependent-explanatory%*%beta)%*%ginv(V)%*%\r(dependent-explanatory%*%beta))\r# 4. Implement Mixed Estimation for initial values\rbeta_M \u0026lt;- ginv((1/sig)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)%*%\r((sig)^(-1)*t(explanatory)%*%ginv(V)%*%dependent + t(R)%*%ginv(V0)%*%r)\rsig_M \u0026lt;- sig*ginv((sig)^(-1)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)\r# for kalman filter\robservable \u0026lt;- as.matrix(x[(order+1):NROW(x),]) # dependent variable\rcoefficient \u0026lt;- array(0,dim = c(nrow = NCOL(x),ncol = npara,(NROW(x)-order))) # explanatory variable\rfor(i in 1:(NROW(x)-order)){\rfor (k in 1:NCOL(x)){\rcoefficient[k,(1+(k-1)*npara/NCOL(x)):(k*npara/NCOL(x)),i] \u0026lt;- explanatory[i,1:(npara/NCOL(x))]\r}\r}\rresult \u0026lt;- list(observable, coefficient, beta_M, sig_M, sigma_AR, V0, order, npara)\rnames(result) \u0026lt;- c(\u0026quot;observable\u0026quot;, \u0026quot;coefficient\u0026quot;,\u0026quot;beta_M\u0026quot;, \u0026quot;sig_M\u0026quot;,\u0026quot;sigma_AR\u0026quot;, \u0026quot;V0\u0026quot;,\u0026quot;order\u0026quot;,\u0026quot;npara\u0026quot;)\rreturn(result)\r}\rMPrior \u0026lt;- function(x,order,pi){\r#-------------------------------------------------------------------\r# Make Minnesota Prior\r# x - dataset\r# order - max lag of autoregression\r# pi - a set of hyper-parameter of prior matrix\r#-------------------------------------------------------------------\rlibrary(MASS)\r# 1. Process row data\rfor (i in 1:5){\reval(parse(text = paste0(\u0026quot;pi\u0026quot;,i,\u0026quot;=pi[[\u0026quot;,i,\u0026quot;]]\u0026quot;)))\r}\rif (class(x)!=\u0026quot;matrix\u0026quot;){\rx \u0026lt;- as.matrix(x)\r}\rdependent \u0026lt;- as.matrix(x[1:(NROW(x)-order),1])\rfor (i in 2:NCOL(x)){\rdependent \u0026lt;- rbind(dependent,as.matrix(x[(order+1):NROW(x),i]))\r}\rexplanatory \u0026lt;- cbind(1,embed(x,order)[-1,])\rexplanatory \u0026lt;- diag(1,nrow = NCOL(x),ncol = NCOL(x))%x%as.matrix(explanatory)\rnpara \u0026lt;- NCOL(explanatory) # the number of parameters/ ncol(x)*(ncol(x)*order + 1(const))\r# 2. Make Prior matrixes\rr \u0026lt;- as.matrix(numeric(npara)) # a part of dependent variable in mixed estimation\riter1 \u0026lt;- numeric(NCOL(x))\rn \u0026lt;- 2\rfor (i in 1:length(iter1)){\riter1[i] \u0026lt;- n\rn \u0026lt;- n + order*NCOL(x) + 1\r}\rfor (i in iter1){\rr[i] \u0026lt;- 1\r}\rR \u0026lt;- diag(1,nrow = npara,ncol = npara) # a part of explanatory variables in mixed estimation\rV \u0026lt;- diag(1,nrow = NROW(explanatory),ncol = NROW(explanatory)) # coefficient matrix of var-cov matrix in mixed estimation\rV0 \u0026lt;- matrix(0,nrow = npara,ncol = npara) # Prior for var matrix\r# PRIOR FOR VAR OF CONSTANT\rsigi \u0026lt;- numeric(NCOL(x)) # var of AR(m)\rfor (i in 1:NCOL(x)){\rAR \u0026lt;- ar(x[,i],order.max = order)\rsigi[i] \u0026lt;- AR$var.pred\r}\rsigma_AR \u0026lt;- diag(sigi,nrow = NCOL(x),ncol = NCOL(x))\riter2 \u0026lt;- seq(1,npara,(NCOL(x)*order+1))\rn \u0026lt;- 1\rfor (i in iter2){\rV0[i,i] \u0026lt;- pi5*pi3*sigi[n]\rn \u0026lt;- n + 1\r}\r# PRIOR FOR VAR OF AUTOREGRESSIVE PARAMETER\rwi \u0026lt;- 0 # prior weight on s.d. of autoregressive parameter\rk \u0026lt;- 0 # decay parameter\riter3 \u0026lt;- numeric((NCOL(x)*order))\rn \u0026lt;- 2;iter3[1] \u0026lt;- n\rfor (i in 2:length(iter3)){\rif ((i-1)%%order == 0){\rn \u0026lt;- n + NCOL(x) + 2 # const\riter3[i] \u0026lt;- n }else{\rn \u0026lt;- n + NCOL(x)\riter3[i] \u0026lt;- n\r}\r}\rfor (i in iter3) {\rk \u0026lt;- k + 1\rV0[i,i] \u0026lt;- pi5*pi1/(k*exp(pi4*wi))\rif (k == order){\rk = 0\r}\r}\r# PRIOR FOR VAR OF DISTRIBUTED LAG PARAMETER\rwj \u0026lt;- 1 # prior weight on s.d. of distributed lag parameter\rk \u0026lt;- 1 # decay parameter\rcount \u0026lt;- 0\rn \u0026lt;- 1\rndis \u0026lt;- npara - NCOL(x) - length(iter3) # the number of distibuted lag in each equation\rl \u0026lt;- rep(1:NCOL(x),length=npara)\rtemp \u0026lt;- numeric(length(iter3))\rj \u0026lt;- 1;temp[1] \u0026lt;- j\rfor (i in 2:length(temp)){\rif ((i-1)%%order == 0){\rj \u0026lt;- j + NCOL(x) + 1 # const\rtemp[i] \u0026lt;- j }else{\rj \u0026lt;- j + NCOL(x)\rtemp[i] \u0026lt;- j\r}\r}\rl \u0026lt;- l[-c(temp)]; l \u0026lt;- l[1:(ndis)]\riter4 \u0026lt;- 1:npara; iter4 \u0026lt;- iter4[-c(iter2,iter3)]\rfor (i in iter4){\rcount \u0026lt;- count + 1\rV0[i,i] \u0026lt;- pi5*pi2*sigi[n]/(k*exp(pi4*wj)*sigi[l[count]])\rif(count%%(order-1) == 0){\rk \u0026lt;- k + 1\r}\rif (count%%(ndis/NCOL(x)) == 0){\rn = n + 1\rk = 1\r}\r}\r# 3. Estimate OLS\rbeta \u0026lt;- ginv((t(explanatory)%*%explanatory))%*%t(explanatory)%*%dependent\rsig \u0026lt;- as.numeric((NROW(explanatory)-NCOL(explanatory))^(-1)*\rt(dependent-explanatory%*%beta)%*%ginv(V)%*%\r(dependent-explanatory%*%beta))\r# 4. Implement Mixed Estimation for initial values\rbeta_M \u0026lt;- ginv((1/sig)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)%*%\r((sig)^(-1)*t(explanatory)%*%ginv(V)%*%dependent + t(R)%*%ginv(V0)%*%r)\rsig_M \u0026lt;- sig*ginv((sig)^(-1)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)\r# for kalman filter\robservable \u0026lt;- as.matrix(x[(order+1):NROW(x),]) # dependent variable\rcoefficient \u0026lt;- array(0,dim = c(nrow = NCOL(x),ncol = npara,(NROW(x)-order))) # explanatory variable\rfor(i in 1:(NROW(x)-order)){\rfor (k in 1:NCOL(x)){\rcoefficient[k,(1+(k-1)*npara/NCOL(x)):(k*npara/NCOL(x)),i] \u0026lt;- explanatory[i,1:(npara/NCOL(x))]\r}\r}\rresult \u0026lt;- list(observable, coefficient, beta_M, sig_M, sigma_AR, V0, order, npara)\rnames(result) \u0026lt;- c(\u0026quot;observable\u0026quot;, \u0026quot;coefficient\u0026quot;,\u0026quot;beta_M\u0026quot;, \u0026quot;sig_M\u0026quot;,\u0026quot;sigma_AR\u0026quot;, \u0026quot;V0\u0026quot;,\u0026quot;order\u0026quot;,\u0026quot;npara\u0026quot;)\rreturn(result)\r}\rBVAR \u0026lt;- function(m){\r#-------------------------------------------------------------------\r# Estimation of Bayesian Vector Auto-Regression\r# m - dataset\r#-------------------------------------------------------------------\r# 1. Process data\robservable \u0026lt;- m$observable\rcoefficient \u0026lt;- m$coefficient\rbeta_M \u0026lt;- m$beta_M\rsig_M \u0026lt;- m$sig_M\rsigma_AR \u0026lt;- m$sigma_AR\rV0 \u0026lt;- m$V0\rorder \u0026lt;- m$order\rnpara \u0026lt;- m$npara\r# 2. Run Kalmanfilter\rresults \u0026lt;- kalmanfiter(observable,npara,diag(1,nrow = npara,ncol = npara),\rcoefficient,0,NA,V0,0,NA,sigma_AR,beta_M,sig_M)\rreturn(results)\r}\rm \u0026lt;- MPrior(Canada,3,abs(rnorm(5)))\rresults \u0026lt;- BVAR(m)\rBVAR_Prediction \u0026lt;- function(results,horizon){\rstate_pred \u0026lt;- results$`state prediction`\rstate_fil \u0026lt;- results$`state filtered`\robs_pred \u0026lt;- results$`observable prediction`\rA \u0026lt;- results$`parameter of state eq`\rnparaall \u0026lt;- dim(state_pred)[1]\rnum \u0026lt;- dim(obs_pred)[1]\rnpara \u0026lt;- order*num+1\rorder \u0026lt;- (nparaall-num)/(num^2)\rsamplesize \u0026lt;- dim(state_pred)[3]\ry \u0026lt;- matrix(0,horizon,num)\rB \u0026lt;- matrix(0,horizon,nparaall)\rx \u0026lt;- results$`parameter of observable eq`[1,1:npara,samplesize]\rB[1,] \u0026lt;- state_fil[,1,samplesize]\rfor (i in 1:(horizon-1)) {\rB[i+1,] \u0026lt;- A%*%B[i,]\ry[i+1,] \u0026lt;- B[i+1,]%*%(diag(1,num)%x%x)\rx \u0026lt;- c(1,y[i+1,],x[-c(1,seq(from=(npara-order),to=npara))])\r}\rresult \u0026lt;- list(B,y)\rnames(result) \u0026lt;- c(\u0026quot;B\u0026quot;,\u0026quot;y\u0026quot;)\rreturn(result)\r}\rpre \u0026lt;- BVAR_Prediction(results,20)\rBVAR_OPT \u0026lt;- function(x){\rint_pi \u0026lt;- rnorm(5)\r}\r\r\rSims, Christopher A, 1980. “Macroeconomics and Reality,” Econometrica, Econometric Society, vol. 48(1), pages 1-48, January.↩︎\n\r\r\r","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1556496000,"objectID":"20dd4f483681dd9058ecc0bf2b0fcdd8","permalink":"/post/post5/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/post/post5/","section":"post","summary":"Bayesian VAR（BVAR）と呼ばれる、VARの仲でも予測精度の向上を目材して開発されたモデルを勉強してみました。","tags":["BVAR"],"title":"BVARについて","type":"post"},{"authors":null,"categories":["マクロ経済学"],"content":"\r\r\r\r1. Gianonne et. al. (2008)版マルチファクターモデル\r2. Rで実装する\r\r\rおはこんばんにちは。\r前回、統計ダッシュボードからAPI接続で統計データを落とすという記事を投稿しました。\r今回はそのデータを、Gianonne et. al. (2008)のマルチファクターモデルにかけ、四半期GDPの予測を行いたいと思います。\n1. Gianonne et. al. (2008)版マルチファクターモデル\r元論文\n前回の投稿でも書きましたが、この論文はGiannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標を2つのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しています。\rまず、このモデルについてご紹介します。このモデルでは2段階推計を行います。まず主成分分析により経済統計を統計間の相関が0となるファクターへ変換します（参考）。そして、その後の状態空間モデルでの推計で必要になるパラメータをOLS推計し、そのパラメータを使用してカルマンフィルタ＆カルマンスムーザーを回し、ファクターを推計しています。では、具体的な説明に移ります。\r統計データを\\(x_{i,t|v_j}\\)と定義します。ここで、\\(i=1,...,n\\)は経済統計を表し（つまり\\(n\\)が全統計数）、\\(t=1,...,T_{iv_j}\\)は統計\\(i\\)のサンプル期間の時点を表しています（つまり、\\(T_{iv_j}\\)は統計\\(i\\)のその時点での最新データ日付を表す）。また、\\(v_j\\)はある時点\\(j\\)（2005年など）で得られる情報集合（vintage）を表しています。統計データ\\(x_{i,t|v_j}\\)は以下のようにファクター\\(f_{r,t}\\)の線形結合で表すことができます（ここで\\(r\\)はファクターの数を表す）。\n\\[\rx_{i,t|v\\_j} = \\mu_i + \\lambda_{i1}f_{1,t} + ... + \\lambda_{ir}f_{r,t} + \\xi_{i,t|v_j} \\tag{1}\r\\]\n\\(\\mu\\_i\\)は定数項、\\(\\lambda\\_{ir}\\)はファクターローディング、\\(\\xi\\_{i,t|v\\_j}\\)はホワイトノイズの誤差項を表しています。これを行列形式で書くと以下のようになります。\n\\[\rx_{t|v_j} = \\mu + \\Lambda F_t + \\xi_{t|v_j} = \\mu + \\chi_t + \\xi_{t|v_j} \\tag{2}\r\\]\nここで、\\(x_{t|v_j} = (x_{1,t|v_j}, ..., x_{n,t|v_j} )^{\\mathrm{T}}\\)、\\(\\xi_{t|v_j}=(\\xi_{1,t|v_j}, ..., \\xi_{n,t|v_j})^{\\mathrm{T}}\\)、\\(F_t = (f_{1,t}, ..., f_{r,t})^{\\mathrm{T}}\\)であり、\\(\\Lambda\\)は各要素が$ _{ij}\\(の\\)nr\\(行列のファクターローディングを表しています。また、\\)t = F_t\\(です。よって、ファクター\\) F_t\\(を推定するためには、データ\\)x{i,t|v_j}$を以下のように基準化したうえで、分散共分散行列を計算し、その固有値問題を解けばよいという事になります。\n\\[\r\\displaystyle z_{it} = \\frac{1}{\\hat{\\sigma}_i}(x_{it} - \\hat{\\mu}_{it}) \\tag{3}\r\\]\nここで、\\(\\displaystyle \\hat{\\mu}_{it} = 1/T \\sum_{t=1}^T x_{it}\\)であり、\\(\\hat{\\sigma}_i = \\sqrt{1/T \\sum_{t=1}^T (x_{it}-\\hat{\\mu_{it}})^2}\\)です（ここで\\(T\\)はサンプル期間）。分散共分散行列\\(S\\)を以下のように定義します。\n\\[\r\\displaystyle S = \\frac{1}{T} \\sum_{t=1}^T z_t z_t^{\\mathrm{T}} \\tag{4}\r\\]\r次に、\\(S\\)のうち、固有値を大きい順に\\(r\\)個取り出し、それを要素にした$ r r\\(対角行列を\\) D\\(、それに対応する固有ベクトルを\\)n r\\(行列にしたものを\\) V\\(と定義します。ファクター\\) _t$は以下のように推計できます。\n\\[\r\\tilde{F}_t = V^{\\mathrm{T}} z_t \\tag{5}\r\\]\rファクターローディング\\(\\Lambda\\)と誤差項の共分散行列\\(\\Psi = \\mathop{\\mathbb{E}} [\\xi_t\\xi^{\\mathrm{T}}_t]\\)は\\(\\tilde{F}_t\\)を\\(z_t\\)に回帰することで推計します。\n\\[\r\\displaystyle \\hat{\\Lambda} = \\sum_{t=1}^T z_t \\tilde{F}^{\\mathrm{T}}_t (\\sum_{t=1}^T\\tilde{F}_t\\tilde{F}^{\\mathrm{T}}_t)^{-1} = V \\tag{6}\r\\]\n\\[\r\\hat{\\Psi} = diag(S - VDV) \\tag{7}\r\\]\n注意して頂きたいのは、ここで推計した\\(\\tilde{F}_t\\)は、以下の状態空間モデルでの推計に必要なパラメータを計算するための一時的な推計値であるという事です（２段階推計の１段階目という事）。\n\\[\rx_{t|v_j} = \\mu + \\Lambda F\\_t + \\xi_{t|v_j} = \\mu + \\chi_t + \\xi_{t|v_j} \\tag{2}\r\\]\n\\[\rF\\_t = AF\\_{t-1} + u\\_t \\tag{8}\r\\]\rここで、\\(u_t\\)は平均0、分散\\(H\\)のホワイトノイズです。再掲している(2)式が観測方程式、(8)式が遷移方程式となっています。推定すべきパラメータは\\(\\Lambda\\)、\\(\\Psi\\)以外に\\(A\\)と\\(H\\)があります（\\(\\mu=0\\)としています）。\\(A\\)は主成分分析により計算した\\(\\tilde{F}_t\\)をVAR(1)にかけることで推定します。\n\\[\r\\hat{A} = \\sum_{t=2}^T\\tilde{F}_t\\tilde{F}_{t-1}^{\\mathrm{T}} (\\sum_{t=2}^T\\tilde{F}_{t-1}\\tilde{F}_{t-1}^{\\mathrm{T}})^{-1} \\tag{9}\r\\]\r\\(H\\)は今推計したVAR(1)の誤差項の共分散行列から計算します。これで必要なパラメータの推定が終わりました。次にカルマンフィルタを回します。カルマンフィルタに関してはこちらを参考にしてください。わかりやすいです。これで最終的に\\(\\hat{F}_{t|v_j}\\)の推計ができるわけです。\rGDPがこれらのファクターで説明可能であり（つまり固有の変動がない）、GDPと月次経済指標がjointly normalであれば以下のような単純なOLS推計でGDPを予測することができます。もちろん月次経済指標の方が早く公表されるので、内生性の問題はないと考えられます。\n\\[\r\\hat{y}_{3k|v_j} = \\alpha + \\beta^{\\mathrm{T}} \\hat{F}_{3k|v_j} \\tag{10}\r\\]\nここで、\\(3k\\)は四半期の最終月を示しています（3月、6月など）\\(\\hat{y}_{3k|v_j}\\)は\\(j\\)時点で得られる情報集合\\(v_j\\)での四半期GDPを表しており、\\(\\hat{F}_{3k|v_j}\\)はその時点で推定したファクターを表しています（四半期最終月の値だけを使用している点に注意）。これで推計方法の説明は終わりです。\n\r2. Rで実装する\rでは実装します。前回記事で得られたデータ（dataset）が読み込まれている状態からスタートします。まず、主成分分析でファクターを計算します。なお、前回の記事で3ファクターの累積寄与度が80%を超えたため、今回もファクター数は3にしています。\n#------------------------\r# Giannone et. al. 2008 #------------------------\rlibrary(xts)\rlibrary(MASS)\rlibrary(tidyverse)\r# 主成分分析でファクターを計算\rf \u0026lt;- 3 # ファクター数を定義\ra \u0026lt;- which(dataset1$publication == \u0026quot;2012-04-01\u0026quot;) # サンプル開始期間を2012年に設定。\rdataset2 \u0026lt;- dataset1[a:nrow(dataset1),]\rrownames(dataset2) \u0026lt;- dataset2$publication\rdataset2 \u0026lt;- dataset2[,-2]\rz \u0026lt;- scale(dataset2) # zは基準化されたサンプルデータ\rfor (i in 1:nrow(z)){\reval(parse(text = paste(\u0026quot;S_i \u0026lt;- z[i,]%*%t(z[i,])\u0026quot;,sep = \u0026quot;\u0026quot;)))\rif (i==1){\rS \u0026lt;- S_i\r}else{\rS \u0026lt;- S + S_i\r}\r}\rS \u0026lt;- (1/nrow(z))*S # 分散共分散行列を計算 (4)式\rgamma \u0026lt;- eigen(S) D \u0026lt;- diag(gamma$values[1:f])\rV \u0026lt;- gamma$vectors[,1:f]\rF_t \u0026lt;- matrix(0,nrow(z),f)\rfor (i in 1:nrow(z)){\reval(parse(text = paste(\u0026quot;F_t[\u0026quot;,i,\u0026quot;,]\u0026lt;- z[\u0026quot;,i,\u0026quot;,]%*%V\u0026quot;,sep = \u0026quot;\u0026quot;))) # (5)式を実行\r}\rF_t.xts \u0026lt;- xts(F_t,order.by = as.Date(row.names(z)))\rplot.zoo(F_t.xts,col = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;yellow\u0026quot;,\u0026quot;purple\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 時系列プロット\rlambda_hat \u0026lt;- V\rpsi \u0026lt;- diag(S-V%*%D%*%t(V)) # (7)式\rR \u0026lt;- diag(diag(cov(z-z%*%V%*%t(V)))) \r推計したファクター\\(\\tilde{F}\\_t\\)の時系列プロットは以下のようになり、前回princomp関数で計算したファクターと完全一致します（じゃあprincompでいいやんと思われるかもしれませんが実装しないと勉強になりませんので）。\n次に、VAR(1)を推計し、パラメータを取り出します。\n# VAR(1)モデルを推計\ra \u0026lt;- matrix(0,f,f)\rb \u0026lt;- matrix(0,f,f)\rfor(t in 2:nrow(z)){\ra \u0026lt;- a + F_t[t,]%*%t(F_t[t-1,])\rb \u0026lt;- b + F_t[t-1,]%*%t(F_t[t-1,])\r}\rb_inv \u0026lt;- solve(b)\rA_hat \u0026lt;- a%*%b_inv # (9)式\re \u0026lt;- numeric(f)\rfor (t in 2:nrow(F_t)){\re \u0026lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat\r}\rH \u0026lt;- t(e)%*%e\rQ \u0026lt;- diag(1,f,f)\rQ[1:f,1:f] \u0026lt;- H\rVAR(1)に関してもvar関数とパラメータの数値が一致することを確認済みです。いよいよカルマンフィルタを実行します。\n# カルマンフィルタを実行\rRR \u0026lt;- array(0,dim = c(ncol(z),ncol(z),nrow(z))) # RRは観測値の分散行列（相関はないと仮定）\rfor(i in 1:nrow(z)){\rmiss \u0026lt;- is.na(z[i,])\rR_temp \u0026lt;- diag(R)\rR_temp[miss] \u0026lt;- 1e+32 # 欠損値の分散は無限大にする\rRR[,,i] \u0026lt;- diag(R_temp)\r}\rzz \u0026lt;- z; zz[is.na(z)] \u0026lt;- 0 # 欠損値（NA）に0を代入（計算結果にはほとんど影響しない）。\ra_t \u0026lt;- matrix(0,nrow(zz),f) # a_tは状態変数の予測値\ra_tt \u0026lt;- matrix(0,nrow(zz),f) # a_ttは状態変数の更新後の値\ra_tt[1,] \u0026lt;- F_t[1,] # 状態変数の初期値には主成分分析で推計したファクターを使用\rsigma_t \u0026lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の予測値\rsigma_tt \u0026lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の更新値\rp \u0026lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))\rsigma_tt[,,1] \u0026lt;- matrix(p,3,3) # 状態変数の分散の初期値はVAR(1)の推計値から計算\ry_t \u0026lt;- matrix(0,nrow(zz),ncol(zz)) # y_tは観測値の予測値\rK_t \u0026lt;- array(0,dim = c(f,ncol(zz),nrow(zz))) # K_tはカルマンゲイン\rdata.m \u0026lt;- as.matrix(dataset2)\r# カルマンフィルタを実行\rfor (t in 2:nrow(zz)){\ra_t[t,] \u0026lt;- A_hat%*%a_tt[t-1,]\rsigma_t[,,t] \u0026lt;- A_hat%*%sigma_tt[,,t-1]%*%t(A_hat) + Q\ry_t[t,] \u0026lt;- as.vector(V%*%a_t[t,])\rS_t \u0026lt;- V%*%sigma_tt[,,t-1]%*%t(V)+RR[,,t]\rGG \u0026lt;- t(V)%*%diag(1/diag(RR[,,t]))%*%V\rSinv \u0026lt;- diag(1/diag(RR[,,t])) - diag(1/diag(RR[,,t]))%*%V%*%ginv(diag(nrow(A_hat))+sigma_t[,,t]%*%GG)%*%sigma_t[,,t]%*%t(V)%*%diag(1/diag(RR[,,t]))\rK_t[,,t] \u0026lt;- sigma_t[,,t]%*%t(V)%*%Sinv\ra_tt[t,] \u0026lt;- a_t[t,] + K_t[,,t]%*%(zz[t,]-y_t[t,])\rsigma_tt[,,t] \u0026lt;- sigma_t[,,t] - K_t[,,t]%*%V%*%sigma_tt[,,t-1]%*%t(V)%*%t(K_t[,,t])\r}\rF.xts \u0026lt;- xts(a_tt,order.by = as.Date(rownames(data.m)))\rplot.zoo(F.xts, col = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;yellow\u0026quot;,\u0026quot;purple\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 得られた推計値を時系列プロット\rカルマンフィルタにより推計したファクターの時系列プロットが以下です。遷移方程式がAR(1)だったからかかなり平準化された値となっています。\nでは、この得られたファクターをOLSにかけます。\n# 得られたファクターとGDPをOLSにかける\rF_q \u0026lt;- as.data.frame(a_tt[seq(3,nrow(a_tt),3),]) # 四半期の終わり月の値だけを引っ張ってくる colnames(F_q) \u0026lt;- c(\u0026quot;factor1\u0026quot;,\u0026quot;factor2\u0026quot;,\u0026quot;factor3\u0026quot;)\rcolnames(GDP) \u0026lt;- c(\u0026quot;publication\u0026quot;,\u0026quot;GDP\u0026quot;)\rt \u0026lt;- which(GDP$publication==\u0026quot;2012-04-01\u0026quot;)\rt2 \u0026lt;- which(GDP$publication==\u0026quot;2015-01-01\u0026quot;) # 2012-2q~2015-1qまでのデータが学習データ、それ以降がテストデータ\rGDP_q \u0026lt;- GDP[t:nrow(GDP),]\rdataset.q \u0026lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),])\rtest \u0026lt;- lm(GDP~factor1 + factor2 + factor3,data=dataset.q)\rsummary(test)\r## ## Call:\r## lm(formula = GDP ~ factor1 + factor2 + factor3, data = dataset.q)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1350.34 -361.67 -31.63 375.61 1105.07 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 125729.4 4134.2 30.412 1.07e-08 ***\r## factor1 -199.0 1651.3 -0.121 0.907 ## factor2 -1699.7 960.2 -1.770 0.120 ## factor3 -2097.6 3882.5 -0.540 0.606 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## Residual standard error: 897.5 on 7 degrees of freedom\r## Multiple R-squared: 0.783, Adjusted R-squared: 0.69 ## F-statistic: 8.419 on 3 and 7 DF, p-value: 0.0101\rout_of_sample \u0026lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成\rtest.pred \u0026lt;- predict(test, out_of_sample, interval=\u0026quot;prediction\u0026quot;)\rpred.GDP.xts \u0026lt;- xts(cbind(test.pred[,1],out_of_sample$GDP),order.by = out_of_sample$publication)\rplot.zoo(pred.GDP.xts,col = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 予測値と実績値を時系列プロット\rOLSの推計結果はfactor1（赤）とfactor2（青）が有意との結果。前回の投稿でも言及したように、factor1（赤）はリスクセンチメントを表していそうなので、係数の符号が負であることは頷ける。ただし、factor2（青）も符号が負なのではなぜなのか…。このファクターは生産年齢人口など経済の潜在能力を表していると思っていたのに。かなり謎。まあとりあえず予測に移りましょう。このモデルを使用したGDPの予測値と実績値の推移はいかのようになりました。直近の精度は悪くない？\nというか、これ完全に単位根の問題を無視してOLSしてしまっているな。ファクターもGDPも完全に単位根を持つけど念のため単位根検定をかけてみます。\nlibrary(tseries)\radf.test(F_q$factor1)\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor1\r## Dickey-Fuller = -2.8191, Lag order = 2, p-value = 0.2603\r## alternative hypothesis: stationary\radf.test(F_q$factor2)\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor2\r## Dickey-Fuller = -2.6749, Lag order = 2, p-value = 0.3153\r## alternative hypothesis: stationary\radf.test(F_q$factor3)\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$factor3\r## Dickey-Fuller = -2.8928, Lag order = 2, p-value = 0.2323\r## alternative hypothesis: stationary\radf.test(GDP_q$GDP)\r## ## Augmented Dickey-Fuller Test\r## ## data: GDP_q$GDP\r## Dickey-Fuller = 1.5034, Lag order = 3, p-value = 0.99\r## alternative hypothesis: stationary\rはい。全部単位根もってました…。階差をとったのち、単位根検定を行います。\nGDP_q \u0026lt;- GDP_q %\u0026gt;% mutate(growth.rate=(GDP/lag(GDP)-1)*100)\rF_q \u0026lt;- F_q %\u0026gt;% mutate(f1.growth.rate=(factor1/lag(factor1)-1)*100,\rf2.growth.rate=(factor2/lag(factor2)-1)*100,\rf3.growth.rate=(factor3/lag(factor3)-1)*100)\radf.test(GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)])\r## ## Augmented Dickey-Fuller Test\r## ## data: GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)]\r## Dickey-Fuller = -0.31545, Lag order = 3, p-value = 0.9838\r## alternative hypothesis: stationary\radf.test(F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)])\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)]\r## Dickey-Fuller = -2.7762, Lag order = 2, p-value = 0.2767\r## alternative hypothesis: stationary\radf.test(F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)])\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)]\r## Dickey-Fuller = -2.6156, Lag order = 2, p-value = 0.3379\r## alternative hypothesis: stationary\radf.test(F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)])\r## ## Augmented Dickey-Fuller Test\r## ## data: F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)]\r## Dickey-Fuller = -2.9893, Lag order = 2, p-value = 0.1955\r## alternative hypothesis: stationary\rfactor1だけは5%有意水準で帰無仮説を棄却できない…。困りました。有意水準を10%ということにして、とりあえず階差でOLSしてみます。\ndataset.q \u0026lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),])\rcolnames(dataset.q) \u0026lt;- c(\u0026quot;publication\u0026quot;,\u0026quot;GDP\u0026quot;,\u0026quot;growth.rate\u0026quot;,\u0026quot;factor1\u0026quot;,\u0026quot;factor2\u0026quot;,\u0026quot;factor3\u0026quot;,\u0026quot;f1.growth.rate\u0026quot;,\u0026quot;f2.growth.rate\u0026quot;,\u0026quot;f3.growth.rate\u0026quot;)\rtest1 \u0026lt;- lm(growth.rate~f1.growth.rate + f2.growth.rate + f3.growth.rate,data=dataset.q)\rsummary(test1)\r## ## Call:\r## lm(formula = growth.rate ~ f1.growth.rate + f2.growth.rate + ## f3.growth.rate, data = dataset.q)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -1.6940 -0.2411 0.2041 0.4274 1.0904 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|)\r## (Intercept) 8.978e-02 4.588e-01 0.196 0.851\r## f1.growth.rate -6.353e-03 1.375e-02 -0.462 0.660\r## f2.growth.rate 6.155e-04 6.026e-03 0.102 0.922\r## f3.growth.rate -9.249e-05 5.152e-04 -0.180 0.863\r## ## Residual standard error: 0.956 on 6 degrees of freedom\r## (1 observation deleted due to missingness)\r## Multiple R-squared: 0.04055, Adjusted R-squared: -0.4392 ## F-statistic: 0.08452 on 3 and 6 DF, p-value: 0.966\r推計結果がわるくなりました…。予測値を計算し、実績値とプロットしてみます。\nout_of_sample1 \u0026lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成\rtest1.pred \u0026lt;- predict(test1, out_of_sample1, interval=\u0026quot;prediction\u0026quot;)\rpred1.GDP.xts \u0026lt;- xts(cbind(test1.pred[,1],out_of_sample1$growth.rate),order.by = out_of_sample1$publication)\rplot.zoo(pred1.GDP.xts,col = c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 予測値と実績値を時系列プロット\rん～、これはやり直しですね。今日はここまでで勘弁してください…。\n\r","date":1531699200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1531699200,"objectID":"fbb0ce36043730cfcf2788a681f8758a","permalink":"/post/post6/","publishdate":"2018-07-16T00:00:00Z","relpermalink":"/post/post6/","section":"post","summary":"前回集めた経済データをGiannone et al (2008)のマルチファクターモデルで推定し、四半期GDPを予測したいと思います。","tags":["R","カルマンフィルタ"],"title":"Gianonne et. al. (2008)のマルチファクターモデルで四半期GDPを予想してみた","type":"post"},{"authors":null,"categories":["マクロ経済学","Webスクレイピング"],"content":"\r\r\r\r1. 先行研究と具体的にやりたいこと\r2. 統計ダッシュボードからのデータの収集\r3. 得られたデータを主成分分析にかけてみる\r\r\rおはこんばんちわ。\n最近、競馬ばっかりやってましたが、そろそろ本業のマクロの方もやらないとなということで今回は日次GDP推計に使用するデータを総務省が公開している統計ダッシュボードから取ってきました。\rそもそも、前の記事では四半期GDP速報の精度が低いことをモチベーションに高頻度データを用いてより精度の高い予測値をはじき出すモデルを作れないかというテーマで研究を進めていました。しかし、先行研究を進めていくうちに、どうやら大規模な経済指標を利用することで日次で四半期GDPの予測値を計算することが可能であることが判明しました。しかも、精度も良い(米国ですが)ということで、なんとかこの方向で研究を進めていけないかということになりました。\n1. 先行研究と具体的にやりたいこと\r先行研究\nGiannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標をいくつかのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しました。なお、ファクターの計算にはカルマンスムージングを用いています(詳しい推計方法は論文\u0026amp;Technical Appendixを参照)。理論的な定式化は無いのですが、なかなか当たります。そもそも私がこの研究に興味を持ったのは、以下の本を立ち読みした際に参考文献として出てきたからで、いよいよ運用機関などでも使用され始めるのかと思い、やっておこうと思った次第です。\n実践 金融データサイエンス 隠れた構造をあぶり出す6つのアプローチ\rとりあえずはGiannoneの日本版をやろうかなと思っています。実はこの後に、ファクターモデルとDSGEを組み合わせたモデルがありましてそこまで発展させたいなーなんて思っておりますが。とにかく、ファクターを計算するための経済統計が必要ですので、今回はそれを集めてきたというのがこの記事の趣旨です。\n\r2. 統計ダッシュボードからのデータの収集\r政府や日銀が公表しているデータの一部は統計ダッシュボードから落とすことができます。これは総務省統計局が提供しているもので、これまで利用しにくかった経済統計をより身近に使用してもらおうというのが一応のコンセプトとなっています。似たものに総務省統計局が提供しているestatがありますが、日銀の公表データがなかったり、メールアドレスの登録が必要だったりと非常に使い勝手が悪いです(個人的感想)。ただ、estatにはestatapiというRパッケージがあり、データを整形するのは比較的容易であると言えます。今回、統計ダッシュボードを選択した理由はそうは言っても日銀のデータがないのはダメだろうという理由で、データの整形に関しては関数を組みました。\rそもそも統計ダッシュボードは経済統計をグラフなどで見て楽しむ？ものですが、私のような研究をしたい者を対象にAPIを提供してくれています。取得できるデータは大きく分けて6つあります。\nやり方は簡単で、ベースのurlと欲しい統計のIDをGET関数で渡すことによって、データを取得することができます。公式にも以下のように書かれています。\n\r基本的な使い方としては、まず①「統計メタ情報（系列）取得」で取得したいデータの[系列コード]を検索し、 その後⑥「統計データ取得」で[系列コード]を検索条件に指定し、その系列の情報を取得します。\r（②③④⑤は補助的な情報として独立して取得できるようにしています。データのみ必要な場合は当該機能は不要です。）\r具体的な使い方は、以下の「WebAPIの詳細仕様」に記載する[ベースURL]に検索条件となる[パラメータ]を“\u0026amp;”で連結し、HTTPリクエスト（GET）を送信することで目的のデータを取得できます。\r各パラメータは「パラメータ名=値」のように名称と値を’=‘で結合し、複数のパラメータを指定する場合は「パラメータ名=値\u0026amp;パラメータ名=値\u0026amp;…」のようにそれぞれのパラメータ指定を’\u0026amp;’で結合してください。\rまた、パラメータ値は必ずURLエンコード(文字コードUTF-8)してから結合してください。\n\r今回も以下の文献を参考にデータを取ってきたいと思います。\nRによるスクレイピング入門\rまず、最初にこのAPIからデータを取得し、得られた結果を分析しやすいように整形する関数を定義したいと思います。\nlibrary(httr)\rlibrary(estatapi)\rlibrary(dplyr)\rlibrary(XML)\rlibrary(stringr)\rlibrary(xts)\rlibrary(GGally)\rlibrary(ggplot2)\rlibrary(seasonal)\rlibrary(dlm)\rlibrary(vars)\rlibrary(MASS)\r# 関数を定義\rget_dashboard \u0026lt;- function(ID){\rbase_url \u0026lt;- \u0026quot;https://dashboard.e-stat.go.jp/api/1.0/JsonStat/getData?\u0026quot;\rres \u0026lt;- GET(\rurl = base_url,\rquery = list(\rIndicatorCode=ID\r)\r)\rresult \u0026lt;- content(res)\rx \u0026lt;- result$link$item[[1]]$value\rx \u0026lt;- t(do.call(\u0026quot;data.frame\u0026quot;,x))\rdate_x \u0026lt;- result$link$item[[1]]$dimension$Time$category$label\rdate_x \u0026lt;- t(do.call(\u0026quot;data.frame\u0026quot;,date_x))\rdate_x \u0026lt;- str_replace_all(date_x, pattern=\u0026quot;年\u0026quot;, replacement=\u0026quot;/\u0026quot;)\rdate_x \u0026lt;- str_replace_all(date_x, pattern=\u0026quot;月\u0026quot;, replacement=\u0026quot;\u0026quot;)\rdate_x \u0026lt;- as.Date(gsub(\u0026quot;([0-9]+)/([0-9]+)\u0026quot;, \u0026quot;\\\\1/\\\\2/1\u0026quot;, date_x))\rdate_x \u0026lt;- as.Date(date_x, format = \u0026quot;%m/%d/%Y\u0026quot;)\rdate_x \u0026lt;- as.numeric(date_x)\rdate_x \u0026lt;- as.Date(date_x, origin=\u0026quot;1970-01-01\u0026quot;)\r#x \u0026lt;- cbind(x,date_x)\rx \u0026lt;- data.frame(x)\rx[,1] \u0026lt;- as.character(x[,1])%\u0026gt;%as.numeric(x[,1])\rcolnames(x) \u0026lt;- c(result$link$item[[1]]$label)\rx \u0026lt;- x %\u0026gt;% mutate(\u0026quot;publication\u0026quot; = date_x)\rreturn(x)\r}\rまずベースのurlを定義しています。今回はデータが欲しいので⑥統計データのベースurlを使用します（\r参考）。次にベースurlと統計ID（IndicatorCode）をGET関数で渡し、結果を取得しています。統計IDについてはエクセルファイルで公開されています。得られた結果の中身（リスト形式）をresultに格納し、リストの深層にある原数値データ（value）をxに格納します。原数値データもリスト形式なので、それをdo.callでデータフレームに変換しています。次に、データ日付を取得します。resultの中を深くたどるとTime→category→labelというデータがあり、そこに日付データが保存されているので、それをdate_xに格納し、同じようにデータフレームへ変換します。データの仕様上、日付は「yyyy年mm月」になっていますが、これだとRは日付データとして読み取ってくれないので、str_replace_all等で変換したのち、Date型に変換しています。列名にデータ名（result→link→item[[1]]→label）をつけ、データ日付をxに追加したら完成です。\rそのほか、data_connectという関数も定義しています。これはデータ系列によれば、たとえば推計方法の変更などで1980年～2005年の系列と2003年～2018年までの系列の2系列があるようなデータも存在し、この2系列を接続するための関数です。これは単純に接続しているだけなので、説明は省略します。\ndata_connect \u0026lt;- function(x){\ra \u0026lt;- min(which(x[,ncol(x)] != \u0026quot;NA\u0026quot;))\rb \u0026lt;- x[a,ncol(x)]/x[a,1]\rc \u0026lt;- x[1:a-1,1]*b\rreturn(c)\r}\rでは、実際にデータを取得していきます。今回取得するデータは月次データとなっています。これは統計dashboardが月次以下のデータがとれないからです。なので、例えば日経平均などは月末の終値を引っ張っています。ただし、GDPは四半期データとなっています。さきほど定義したget_dashboardの使用方法は簡単で、引数に統計ダッシュボードで公開されている統計IDを入力するだけでデータが取れます。今回使用するデータを以下の表にまとめました。\n# データを取得\rNikkei \u0026lt;- get_dashboard(\u0026quot;0702020501000010010\u0026quot;)\rcallrate \u0026lt;- get_dashboard(\u0026quot;0702020300000010010\u0026quot;)\rTOPIX \u0026lt;- get_dashboard(\u0026quot;0702020590000090010\u0026quot;)\rkikai \u0026lt;- get_dashboard(\u0026quot;0701030000000010010\u0026quot;)\rkigyo.bukka \u0026lt;- get_dashboard(\u0026quot;0703040300000090010\u0026quot;)\rmoney.stock1 \u0026lt;- get_dashboard(\u0026quot;0702010201000010030\u0026quot;)\rmoney.stock2 \u0026lt;- get_dashboard(\u0026quot;0702010202000010030\u0026quot;)\rmoney.stock \u0026lt;- dplyr::full_join(money.stock1,money.stock2,by=\u0026quot;publication\u0026quot;)\rc \u0026lt;- data_connect(money.stock)\ra \u0026lt;- min(which(money.stock[,ncol(money.stock)] != \u0026quot;NA\u0026quot;))\rmoney.stock[1:a-1,ncol(money.stock)] \u0026lt;- c\rmoney.stock \u0026lt;- money.stock[,c(2,3)]\rcpi \u0026lt;- get_dashboard(\u0026quot;0703010401010090010\u0026quot;)\rexport.price \u0026lt;- get_dashboard(\u0026quot;0703050301000090010\u0026quot;)\rimport.price \u0026lt;- get_dashboard(\u0026quot;0703060301000090010\u0026quot;)\rimport.price$`輸出物価指数（総平均）（円ベース）2015年基準` \u0026lt;- NULL\rpublic.expenditure1 \u0026lt;- get_dashboard(\u0026quot;0802020200000010010\u0026quot;)\rpublic.expenditure2 \u0026lt;- get_dashboard(\u0026quot;0802020201000010010\u0026quot;)\rpublic.expenditure \u0026lt;- dplyr::full_join(public.expenditure1,public.expenditure2,by=\u0026quot;publication\u0026quot;)\rc \u0026lt;- data_connect(public.expenditure)\ra \u0026lt;- min(which(public.expenditure[,ncol(public.expenditure)] != \u0026quot;NA\u0026quot;))\rpublic.expenditure[1:a-1,ncol(public.expenditure)] \u0026lt;- c\rpublic.expenditure \u0026lt;- public.expenditure[,c(2,3)]\rexport.service \u0026lt;- get_dashboard(\u0026quot;1601010101000010010\u0026quot;)\rworking.population \u0026lt;- get_dashboard(\u0026quot;0201010010000010020\u0026quot;)\ryukoukyuujinn \u0026lt;- get_dashboard(\u0026quot;0301020001000010010\u0026quot;)\rhours_worked \u0026lt;- get_dashboard(\u0026quot;0302010000000010000\u0026quot;)\rnominal.wage \u0026lt;- get_dashboard(\u0026quot;0302020000000010000\u0026quot;) iip \u0026lt;- get_dashboard(\u0026quot;0502070101000090010\u0026quot;)\rshukka.shisu \u0026lt;- get_dashboard(\u0026quot;0502070102000090010\u0026quot;)\rzaiko.shisu \u0026lt;- get_dashboard(\u0026quot;0502070103000090010\u0026quot;)\rsanji.sangyo \u0026lt;- get_dashboard(\u0026quot;0603100100000090010\u0026quot;)\rretail.sells \u0026lt;- get_dashboard(\u0026quot;0601010201010010000\u0026quot;)\rGDP1 \u0026lt;- get_dashboard(\u0026quot;0705020101000010000\u0026quot;)\rGDP2 \u0026lt;- get_dashboard(\u0026quot;0705020301000010000\u0026quot;)\rGDP \u0026lt;- dplyr::full_join(GDP1,GDP2,by=\u0026quot;publication\u0026quot;)\rc \u0026lt;- data_connect(GDP)\ra \u0026lt;- min(which(GDP[,ncol(GDP)] != \u0026quot;NA\u0026quot;))\rGDP[1:a-1,ncol(GDP)] \u0026lt;- c\rGDP \u0026lt;- GDP[,c(2,3)]\ryen \u0026lt;- get_dashboard(\u0026quot;0702020401000010010\u0026quot;)\rhousehold.consumption \u0026lt;- get_dashboard(\u0026quot;0704010101000010001\u0026quot;)\rJGB10y \u0026lt;- get_dashboard(\u0026quot;0702020300000010020\u0026quot;)\r今取得したデータは原数値系列のデータが多いので、それらは季節調整をかけます。なぜ季節調整済みのデータを取得しないのかというとそれらのデータは何故か極端にサンプル期間が短くなってしまうからです。ここらへんは使い勝手が悪いです。\n# 季節調整をかける\rSys.setenv(X13_PATH = \u0026quot;C:\\\\Program Files\\\\WinX13\\\\x13as\u0026quot;)\rcheckX13()\rseasoning \u0026lt;- function(data,i,start.y,start.m){\rtimeseries \u0026lt;- ts(data[,i],frequency = 12,start=c(start.y,start.m))\rm \u0026lt;- seas(timeseries)\rsummary(m$data)\rreturn(m$series$s11)\r}\rk \u0026lt;- seasoning(kikai,1,2005,4)\rkikai$`機械受注額（船舶・電力を除く民需）` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(kigyo.bukka,1,1960,1)\rkigyo.bukka$`国内企業物価指数（総平均）2015年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(cpi,1,1970,1)\rcpi$`消費者物価指数（生鮮食品を除く総合）2015年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(export.price,1,1960,1)\rexport.price$`輸出物価指数（総平均）（円ベース）2015年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(import.price,1,1960,1)\rimport.price$`輸入物価指数（総平均）（円ベース）2015年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(public.expenditure,2,2004,4)\rpublic.expenditure$公共工事受注額 \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(export.service,1,1996,1)\rexport.service$`貿易・サービス収支` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(yukoukyuujinn,1,1963,1)\ryukoukyuujinn$有効求人倍率 \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(hours_worked,1,1990,1)\rhours_worked$総実労働時間 \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(nominal.wage,1,1990,1)\rnominal.wage$現金給与総額 \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(iip,1,1978,1)\riip$`鉱工業生産指数　2010年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(shukka.shisu,1,1990,1)\rshukka.shisu$`鉱工業出荷指数　2010年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(zaiko.shisu,1,1990,1)\rzaiko.shisu$`鉱工業在庫指数　2010年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(sanji.sangyo,1,1988,1)\rsanji.sangyo$`第３次産業活動指数　2010年基準` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(retail.sells,1,1980,1)\rretail.sells$`小売業販売額（名目）` \u0026lt;- as.numeric(k)\rk \u0026lt;- seasoning(household.consumption,1,2010,1)\rhousehold.consumption$`二人以上の世帯　消費支出（除く住居等）` \u0026lt;- as.numeric(k)\rGDP.ts \u0026lt;- ts(GDP[,2],frequency = 4,start=c(1980,1))\rm \u0026lt;- seas(GDP.ts)\rGDP$`国内総生産（支出側）（実質）2011年基準` \u0026lt;- m$series$s11\rここでは詳しく季節調整のかけ方は説明しません。x13arimaを使用しています。上述のコードを回す際はx13arimaがインストールされている必要があります。以下の記事を参考にしてください。\n[http://sinhrks.hatenablog.com/entry/2014/11/09/003632]\nでは、データ日付を基準に落としてきたデータを結合し、データセットを作成します。\n# データセットに結合\rdataset \u0026lt;- dplyr::full_join(kigyo.bukka,callrate,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,kikai,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,Nikkei,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,money.stock,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,cpi,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,export.price,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,import.price,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,public.expenditure,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,export.service,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,working.population,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,yukoukyuujinn,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,hours_worked,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,nominal.wage,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,iip,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,shukka.shisu,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,zaiko.shisu,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,sanji.sangyo,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,retail.sells,by=\u0026quot;publication\u0026quot;)\rdataset \u0026lt;- dplyr::full_join(dataset,yen,by=\u0026quot;publication\u0026quot;)\rcolnames(dataset) \u0026lt;- c(\u0026quot;DCGPI\u0026quot;,\u0026quot;publication\u0026quot;,\u0026quot;callrate\u0026quot;,\u0026quot;Machinery_Orders\u0026quot;,\r\u0026quot;Nikkei225\u0026quot;,\u0026quot;money_stock\u0026quot;,\u0026quot;CPI\u0026quot;,\u0026quot;export_price\u0026quot;,\r\u0026quot;import_price\u0026quot;,\u0026quot;public_works_order\u0026quot;,\r\u0026quot;trade_service\u0026quot;,\u0026quot;working_population\u0026quot;,\r\u0026quot;active_opening_ratio\u0026quot;,\u0026quot;hours_worked\u0026quot;,\r\u0026quot;wage\u0026quot;,\u0026quot;iip_production\u0026quot;,\u0026quot;iip_shipment\u0026quot;,\u0026quot;iip_inventory\u0026quot;,\r\u0026quot;ITIA\u0026quot;,\u0026quot;retail_sales\u0026quot;,\u0026quot;yen\u0026quot;)\r最後に列名をつけています。datasetはそれぞれのデータの公表開始時期が異なるために大量のNAを含むデータフレームとなっているので、NAを削除するために最もデータの開始時期が遅い機械受注統計に合わせてデータセットを再構築します。\na \u0026lt;- min(which(dataset$Machinery_Orders != \u0026quot;NA\u0026quot;))\rdataset1 \u0026lt;- dataset[a:nrow(dataset),]\rdataset1 \u0026lt;- na.omit(dataset1)\rrownames(dataset1) \u0026lt;- dataset1$publication\rdataset1 \u0026lt;- dataset1[,-2]\rdataset1.xts \u0026lt;- xts(dataset1,order.by = as.Date(rownames(dataset1)))\rこれでとりあえずデータの収集は終わりました。\n\r3. 得られたデータを主成分分析にかけてみる\r本格的な分析はまた今後にしたいのですが、データを集めるだけでは面白くないので、Gianonneらのように主成分分析を行いたいと思います。主成分分析をこれまでに学んだことのない方は以下を参考にしてください。個人的にはわかりやすいと思っています。\n主成分分析(Principal Component Analysis, PCA)～データセットの見える化・可視化といったらまずはこれ！～\nでは主成分分析を実行してみます。Rではprincomp関数を使用することで非常に簡単に主成分分析を行うことができます。\n# 主成分分析を実行\rfactor.pca \u0026lt;- princomp(~.,cor = TRUE,data = dataset1) # cor = TRUEでデータの基準化を自動で行ってくれる。\rsummary(factor.pca)\r## Importance of components:\r## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\r## Standard deviation 3.2538097 1.8606047 1.5142917 1.05061250 0.88155352\r## Proportion of Variance 0.5293639 0.1730925 0.1146540 0.05518933 0.03885683\r## Cumulative Proportion 0.5293639 0.7024563 0.8171103 0.87229965 0.91115648\r## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10\r## Standard deviation 0.7504599 0.63476742 0.48794520 0.413374289 0.358909911\r## Proportion of Variance 0.0281595 0.02014648 0.01190453 0.008543915 0.006440816\r## Cumulative Proportion 0.9393160 0.95946246 0.97136699 0.979910904 0.986351721\r## Comp.11 Comp.12 Comp.13 Comp.14\r## Standard deviation 0.296125594 0.254294037 0.233119328 0.1394055697\r## Proportion of Variance 0.004384518 0.003233273 0.002717231 0.0009716956\r## Cumulative Proportion 0.990736239 0.993969512 0.996686743 0.9976584386\r## Comp.15 Comp.16 Comp.17 Comp.18\r## Standard deviation 0.1356026290 0.1104356585 0.0861468897 0.070612034\r## Proportion of Variance 0.0009194036 0.0006098017 0.0003710643 0.000249303\r## Cumulative Proportion 0.9985778423 0.9991876440 0.9995587083 0.999808011\r## Comp.19 Comp.20\r## Standard deviation 0.053565104 3.115371e-02\r## Proportion of Variance 0.000143461 4.852767e-05\r## Cumulative Proportion 0.999951472 1.000000e+00\rscreeplot(factor.pca)\rpc \u0026lt;- predict(factor.pca,dataset1)[,1:3] # 主成分を計算\rpc.xts \u0026lt;- xts(pc,order.by = as.Date(rownames(dataset1)))\rplot.zoo(pc.xts,col=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;,\u0026quot;green\u0026quot;,\u0026quot;purple\u0026quot;,\u0026quot;yellow\u0026quot;),plot.type = \u0026quot;single\u0026quot;) # 主成分を時系列プロット\r第3主成分まででデータの約80％が説明できる結果を得たので、第3主成分までのプロットをお見せします。第1主成分（赤）はリーマンショックや東日本大震災、消費税増税のあたりで急上昇しています。ゆえに経済全体のリスクセンチメントを表しているのではないかと思っています。第2主成分（青）と第3主成分（緑）はリーマンショックのあたりで大きく落ち込んでいることは共通していますが2015年～現在の動きが大きく異なっています。また、第2主成分（青）はサンプル期間を通して過去トレンドを持つことから日本経済の潜在能力のようなものを表しているのではないでしょうか（そうするとリーマンショックまで上昇傾向にあることが疑問なのですが）。第3主成分（緑）はいまだ解読不能です（物価＆為替動向を表しているのではないかと思っています）。とりあえず今日はこれまで。次回はGianonne et. al.(2008)の日本版の再現を行いたいと思います。\n\r","date":1531526400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1531526400,"objectID":"4d722651b532773776680a7d0533fd3f","permalink":"/post/post7/","publishdate":"2018-07-14T00:00:00Z","relpermalink":"/post/post7/","section":"post","summary":"Rでデータ集めをします。データ分析はデータ集めと前処理が7割を占めるといわれる中、データ集めを自動化すべくウェブスクレイピングを行いました。","tags":["R","API"],"title":"日次GDP推計に使用する経済統計を統計ダッシュボードから集めてみた","type":"post"},{"authors":null,"categories":["競馬","Webスクレイピング"],"content":"\r\r\r\r1. Rvestとは\r2. レース結果をスクレイピングしてみる\r\r\rみなさん、おはこんばんにちは。\n競馬のレース結果を的中させるモデルを作ろうということで研究をはじめましたが、まずはデータを自分で取ってくるところからやろうとおもいます。どこからデータを取ってくるのかという点が重要になるわけですが、データ先としてはdatascisotistさんがまとめられた非常にわかりやすい記事があります。どこからデータが取れるのかというと大きく分けて二つで、①JRA提供のJRA-VANや電子競馬新聞でおなじみの？JRJDといったデータベース、②netkeiba、yahoo競馬とといった競馬情報サイト、となっています。②の場合は自分でコードを書き、クローリングを行う必要があります。今回は②を選択し、yahoo競馬のデータをクローリングで落としてきたいと思います。Rでクローリングを行うパッケージとしては、rvest, httr, XMLがありますが、今回は1番簡単に使えるrvestを用います。yahoo競馬では以下のように各レース結果が表にまとめられています（5月の日本ダービーの結果）。\nyahoo競馬\n各馬のざっくりとした特徴やレース結果（通過順位等含む）、オッズが掲載されています。とりあえず、このぐらい情報があれば良いのではないかと思います（オッズの情報はもう少し欲しいのですが）。ただ、今後は少しずつ必要になった情報を拡充していこうとも思っています。1986年までのレース結果が格納されており、全データ数は50万件を超えるのではないかと思っています。ただ、単勝オッズが利用できるのは1994年からのようなので今回は1994年から直近までのデータを落としてきます。今回のゴールは、このデータをcsvファイル or SQLに格納することです。\n1. Rvestとは\rRvestとは、webスクレイピングパッケージの一種でdplyrでおなじみのHadley Wickhamさんによって作成されたパッケージです。たった数行でwebスクレイピングができる優れものとなっており、操作が非常に簡単であるのが特徴です。今回は以下の本を参考にしました。\nRによるスクレイピング入門\rそもそも、htmlも大学1年生にやった程度でほとんど忘れていたのですが、この本はそこも非常にわかりやすく解説されており、非常に実践的な本だと思います。\n\r2. レース結果をスクレイピングしてみる\r実際にyahoo競馬からデータを落としてみたいと思います。コードは以下のようになっています。ご留意頂きたいのはこのコードをそのまま使用してスクレイピングを行うことはご遠慮いただきたいという事です。webスクレイピングは高速でサイトにアクセスするため、サイトへの負荷が大きくなる可能性があります。スクレイピングを行う際は、時間を空けるコーディングするなどその点に留意をして行ってください（最悪訴えられる可能性がありますが、こちらは一切の責任を取りません）。\n# rvestによる競馬データのwebスクレイピング\r#install.packages(\u0026quot;rvest\u0026quot;)\r#if (!require(\u0026quot;pacman\u0026quot;)) install.packages(\u0026quot;pacman\u0026quot;)\rpacman::p_load(qdapRegex)\rlibrary(rvest)\rlibrary(stringr)\rlibrary(dplyr)\r使用するパッケージはqdapRegex、rvest、stringr、dplyrです。qdapRegexはカッコ内の文字を取り出すために使用しています。\nkeiba.yahoo \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp/schedule/list/2016/?month=\u0026quot;,k))\rrace_url \u0026lt;- keiba.yahoo %\u0026gt;%\rhtml_nodes(\u0026quot;a\u0026quot;) %\u0026gt;%\rhtml_attr(\u0026quot;href\u0026quot;) # 全urlを取得\r# レース結果のをurlを取得\rrace_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026quot;result\u0026quot;)==1] # 「result」が含まれるurlを抽出\rまず、read_htmlでyahoo競馬のレース結果一覧のhtml構造を引っ張ってきます（リンクは2016年1月の全レース）。ここで、kと出ているのは月を表し、k=1であれば2016年1月のレース結果を引っ張ってくるということです。keiba.yahooを覗いてみると以下のようにそのページ全体のhtml構造が格納されているのが分かります。\nkeiba.yahoo\r## $node\r## \u0026lt;pointer: (nil)\u0026gt;\r## ## $doc\r## \u0026lt;pointer: (nil)\u0026gt;\r## ## attr(,\u0026quot;class\u0026quot;)\r## [1] \u0026quot;xml_document\u0026quot; \u0026quot;xml_node\u0026quot;\rrace_urlにはyahoo.keibaのうちの2016年k月にあった全レース結果のリンクを格納しています。html_nodeとはhtml構造のうちどの要素を引っ張るかを指定し、それを引っ張る関数で、簡単に言えばほしいデータの住所を入力する関数であると認識しています（おそらく正しくない）。ここではa要素を引っ張ることにしています。注意すべきことは、html_nodeは欲しい情報をhtml形式で引っ張ることです。なので、テキストデータとしてリンクを保存するためにはhtml_attrを使用する必要があります。html_attrの引数として、リンク属性を表すhrefを渡しています。これでレース結果のurlが取れたと思いきや、実はこれでは他のリンクもとってしまっています。一番わかりやすいのが広告のリンクです。こういったリンクは除外する必要があります。レース結果のurlには“result”が含まれているので、この文字が入っている要素だけを抽出したのが一番最後のコードです。\nfor (i in 1:length(race_url)){\rrace1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,race_url[i])) # レース結果のurlを取得\r# レース結果をスクレイピング\rrace_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_table()\rrace_result \u0026lt;- do.call(\u0026quot;data.frame\u0026quot;,race_result) # リストをデータフレームに変更\rcolnames(race_result) \u0026lt;- c(\u0026quot;order\u0026quot;,\u0026quot;frame_number\u0026quot;,\u0026quot;horse_number\u0026quot;,\u0026quot;horse_name/age\u0026quot;,\u0026quot;time/margin\u0026quot;,\u0026quot;passing_rank/last_3F\u0026quot;,\u0026quot;jockey/weight\u0026quot;,\u0026quot;popularity/odds\u0026quot;,\u0026quot;trainer\u0026quot;) #　列名変更\rさて、いよいよレース結果のスクレイピングを行います。さきほど取得したリンク先のhtml構造を一つ一つ取得し、その中で必要なテキスト情報を引っ張るという作業をRに実行させます（なのでループを使う）。race_1にはあるレース結果ページのhtml構造が格納されおり、race_resultにはその結果が入っています。html_nodesの引数に入っているxpathですが、これはXLMフォーマットのドキュメントから効率的に要素を抜き出す言語です。先ほど説明した住所のようなものと思っていただければ良いと思います。その横に書いてある//table[@id = 'raceScore']が住所です。これはwebブラウザから簡単に探すことができます。Firefoxの説明になりますが、ほかのブラウザでも同じような機能があると思います。スクレイプしたい画面でCtrl+Shift+Cを押すと下のような画面が表示されます。\nこのインスペクターの横のマークをクリックすると、カーソルで指した部分のhtml構造（住所）が表示されます。この場合だと、レース結果はtable属性のidがraceScoreの場所に格納されていることが分かります。なので、上のコードではxpath=のところにそれを記述しているのです。そして、レース結果は表（table）形式でドキュメント化されているので、html_tableでごっそりとスクレイプしました。基本的にリスト形式で返されるので、それをデータフレームに変換し、適当に列名をつけています。\n# 通過順位と上り3Fのタイム\rrace_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- race_result[-6]\r# タイムと着差\rrace_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026quot;)))\rrace_result \u0026lt;- race_result[-5]\r# 馬名、馬齢、馬体重\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;[ァ-ヴー・]+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\d{3}\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026quot;)))\rrace_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\r# ジョッキー\rrace_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026quot;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026quot;)))\rrace_result \u0026lt;- race_result[-4]\r# オッズと人気\rrace_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\(.+\\\\)\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\d+[^(\\\\d+.\\\\d)]\u0026quot;)))\rrace_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\rここまででデータは取得できたわけなのですが、そのデータは綺麗なものにはなっていません。 上のコードでは、その整形作業を行っています。現在、取得したデータは以下のようになっています。\nhead(race_result)\r{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"order\"],\"name\":[1],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"frame_number\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"horse_number\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"trainer\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"passing_rank\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"last_3F\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"time\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"margin\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_name\"],\"name\":[9],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_age\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight\"],\"name\":[11],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight_change\"],\"name\":[12],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"jockey\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"odds\"],\"name\":[14],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"popularity\"],\"name\":[15],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_date\"],\"name\":[16],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_name\"],\"name\":[17],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"2\",\"3\":\"2\",\"4\":\"河野 通文\",\"5\":\"06-07-06-06\",\"6\":\"35.1\",\"7\":\"3.19.7\",\"8\":\"character(0)\",\"9\":\"センゴクシルバー\",\"10\":\"牡6\",\"11\":\"478\",\"12\":\"+2\",\"13\":\"田中 勝春\",\"14\":\"2.3\",\"15\":\"1\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"1\"},{\"1\":\"2\",\"2\":\"6\",\"3\":\"9\",\"4\":\"武 邦彦\",\"5\":\"06-05-04-04\",\"6\":\"35.7\",\"7\":\"3.19.9\",\"8\":\"1 1/4馬身\",\"9\":\"ジャムシード\",\"10\":\"牡6\",\"11\":\"480\",\"12\":\"0\",\"13\":\"柴田 政人\",\"14\":\"14\",\"15\":\"7\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"2\"},{\"1\":\"3\",\"2\":\"7\",\"3\":\"10\",\"4\":\"白井 寿昭\",\"5\":\"08-07-06-06\",\"6\":\"35.7\",\"7\":\"3.20.1\",\"8\":\"1 1/2馬身\",\"9\":\"ホクセツギンガ\",\"10\":\"牡6\",\"11\":\"500\",\"12\":\"+4\",\"13\":\"小屋敷 昭\",\"14\":\"7.5\",\"15\":\"3\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"3\"},{\"1\":\"4\",\"2\":\"7\",\"3\":\"11\",\"4\":\"矢野 進\",\"5\":\"03-03-03-02\",\"6\":\"36.3\",\"7\":\"3.20.4\",\"8\":\"1 3/4馬身\",\"9\":\"サマーワイン\",\"10\":\"牝5\",\"11\":\"422\",\"12\":\"-4\",\"13\":\"木幡 初広\",\"14\":\"32.6\",\"15\":\"9\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"1\",\"3\":\"1\",\"4\":\"秋山 史郎\",\"5\":\"12-12-12-12\",\"6\":\"35.6\",\"7\":\"3.20.5\",\"8\":\"1/2馬身\",\"9\":\"ダイワジェームス\",\"10\":\"牡6\",\"11\":\"472\",\"12\":\"+6\",\"13\":\"大塚 栄三郎\",\"14\":\"5.9\",\"15\":\"2\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"5\"},{\"1\":\"6\",\"2\":\"5\",\"3\":\"7\",\"4\":\"須貝 彦三\",\"5\":\"11-10-06-06\",\"6\":\"36.1\",\"7\":\"3.20.5\",\"8\":\"ハナ\",\"9\":\"シゲノランボー\",\"10\":\"牡7\",\"11\":\"438\",\"12\":\"-6\",\"13\":\"須貝 尚介\",\"14\":\"8.7\",\"15\":\"4\",\"16\":\"1994年1月31日\",\"17\":\"第44回ダイヤモンドステークス（GIII）\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\rご覧のように、\\nが入っていたり、通過順位と上り3ハロンのタイムが一つのセルに入っていたりとこのままでは分析ができません。不要なものを取り除いたり、データを二つに分割する作業が必要になります。今回の記事ではこの部分について詳しくは説明しません。この部分は正規表現を駆使する必要がありますが、私自身全く詳しくないからです。今回も手探りでやりました。\n# レース情報\rrace_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026quot;\\\\d+年\\\\d+月\\\\d+日\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;)))\r# ファイル格納\rif (k ==1 \u0026amp;\u0026amp; i == 1){\rdataset \u0026lt;- race_result\r} else {\rdataset \u0026lt;- rbind(dataset,race_result)\r}# if文の終わり\r} # iループの終わり\rwrite.csv(race_result,\u0026quot;race_result.csv\u0026quot;)\r最後に、レース日時とレース名を抜き出し、データを一時的に格納するコードとcsvファイルに書き出すコードを書いて終了です。完成データセットは以下のような状態になっています。\nhead(dataset)\r{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"order\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"frame_number\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"horse_number\"],\"name\":[3],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"trainer\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"passing_rank\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"last_3F\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"time\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"margin\"],\"name\":[8],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_name\"],\"name\":[9],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_age\"],\"name\":[10],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight\"],\"name\":[11],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"horse_weight_change\"],\"name\":[12],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"jockey\"],\"name\":[13],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"odds\"],\"name\":[14],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"popularity\"],\"name\":[15],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_date\"],\"name\":[16],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"race_name\"],\"name\":[17],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"1\",\"2\":\"8\",\"3\":\"11\",\"4\":\"森安 弘昭\",\"5\":\"01-01-01-01\",\"6\":\"35.7\",\"7\":\"2.00.7\",\"8\":\"character(0)\",\"9\":\"ヒダカハヤト\",\"10\":\"牡8\",\"11\":\"488\",\"12\":\"-2\",\"13\":\"大塚 栄三郎\",\"14\":\"29\",\"15\":\"10\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"1\"},{\"1\":\"2\",\"2\":\"5\",\"3\":\"6\",\"4\":\"矢野 進\",\"5\":\"06-06-04-03\",\"6\":\"35.3\",\"7\":\"2.00.8\",\"8\":\"3/4馬身\",\"9\":\"ステージチャンプ\",\"10\":\"牡5\",\"11\":\"462\",\"12\":\"+14\",\"13\":\"岡部 幸雄\",\"14\":\"3.2\",\"15\":\"1\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"2\"},{\"1\":\"3\",\"2\":\"4\",\"3\":\"4\",\"4\":\"新関 力\",\"5\":\"03-03-02-02\",\"6\":\"35.8\",\"7\":\"2.01.1\",\"8\":\"1 3/4馬身\",\"9\":\"マキノトウショウ\",\"10\":\"牡5\",\"11\":\"502\",\"12\":\"+6\",\"13\":\"的場 均\",\"14\":\"6.9\",\"15\":\"4\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"3\"},{\"1\":\"4\",\"2\":\"8\",\"3\":\"12\",\"4\":\"大和田 稔\",\"5\":\"02-02-03-03\",\"6\":\"35.7\",\"7\":\"2.01.1\",\"8\":\"ハナ\",\"9\":\"ペガサス\",\"10\":\"牡5\",\"11\":\"464\",\"12\":\"+4\",\"13\":\"安田 富男\",\"14\":\"5.7\",\"15\":\"2\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"4\"},{\"1\":\"5\",\"2\":\"7\",\"3\":\"9\",\"4\":\"田中 和夫\",\"5\":\"07-07-07-05\",\"6\":\"35.8\",\"7\":\"2.01.6\",\"8\":\"3馬身\",\"9\":\"シャマードシンボリ\",\"10\":\"牡7\",\"11\":\"520\",\"12\":\"+4\",\"13\":\"田中 剛\",\"14\":\"29.6\",\"15\":\"12\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"5\"},{\"1\":\"6\",\"2\":\"3\",\"3\":\"3\",\"4\":\"中島 敏文\",\"5\":\"09-10-10-09\",\"6\":\"35.5\",\"7\":\"2.01.7\",\"8\":\"3/4馬身\",\"9\":\"モンタミール\",\"10\":\"牡7\",\"11\":\"474\",\"12\":\"+4\",\"13\":\"蓑田 早人\",\"14\":\"10.4\",\"15\":\"6\",\"16\":\"1994年1月5日\",\"17\":\"第43回日刊スポーツ賞金杯（GIII）\",\"_rn_\":\"6\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\r\r以上です。次回はこのデータセットを使用して、分析を行っていきます。次回までには1994年からのデータを全てスクレイピングしてきます。\n【追記（2018/6/10）】\n上述したスクリプトを用いて、スクレイピングを行ったところエラーが出ました。どうやらレース結果の中には強風などで中止になったものも含まれているらしく、そこでエラーが出る様子（race_resultがcharacter(0)になってしまう）。なので、この部分を修正したスクリプトを以下で公開しておきます。こちらは私の PC環境では正常に作動しています。\n# rvestによる競馬データのwebスクレイピング\r#install.packages(\u0026quot;rvest\u0026quot;)\r#if (!require(\u0026quot;pacman\u0026quot;)) install.packages(\u0026quot;pacman\u0026quot;)\rinstall.packages(\u0026quot;beepr\u0026quot;)\rpacman::p_load(qdapRegex)\rlibrary(rvest)\rlibrary(stringr)\rlibrary(dplyr)\rlibrary(beepr)\r# pathの設定\rsetwd(\u0026quot;C:/Users/assiy/Dropbox/競馬統計解析\u0026quot;)\rfor(year in 1994:2018){\r# yahoo競馬のレース結果一覧ページの取得\rfor (k in 1:12){\rkeiba.yahoo \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp/schedule/list/\u0026quot;, year,\u0026quot;/?month=\u0026quot;,k))\rrace_url \u0026lt;- keiba.yahoo %\u0026gt;%\rhtml_nodes(\u0026quot;a\u0026quot;) %\u0026gt;%\rhtml_attr(\u0026quot;href\u0026quot;) # 全urlを取得\r# レース結果のをurlを取得\rrace_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026quot;result\u0026quot;)==1] # 「result」が含まれるurlを抽出\rfor (i in 1:length(race_url)){\rSys.sleep(10)\rprint(str_c(\u0026quot;現在、\u0026quot;, year, \u0026quot;年\u0026quot;, k, \u0026quot;月\u0026quot;, i,\u0026quot;番目のレースの保存中です\u0026quot;))\rrace1 \u0026lt;- read_html(str_c(\u0026quot;https://keiba.yahoo.co.jp\u0026quot;,race_url[i])) # レース結果のurlを取得\r# レースが中止でなければ処理を実行\rif (identical(race1 %\u0026gt;%\rhtml_nodes(xpath = \u0026quot;//div[@class = \u0026#39;resultAtt mgnBL fntSS\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text(),character(0)) == TRUE){\r# レース結果をスクレイピング\rrace_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_table()\rrace_result \u0026lt;- do.call(\u0026quot;data.frame\u0026quot;,race_result) # リストをデータフレームに変更\rcolnames(race_result) \u0026lt;- c(\u0026quot;order\u0026quot;,\u0026quot;frame_number\u0026quot;,\u0026quot;horse_number\u0026quot;,\u0026quot;horse_name/age\u0026quot;,\u0026quot;time/margin\u0026quot;,\u0026quot;passing_rank/last_3F\u0026quot;,\u0026quot;jockey/weight\u0026quot;,\u0026quot;popularity/odds\u0026quot;,\u0026quot;trainer\u0026quot;) #　列名変更\r# 通過順位と上り3Fのタイム\rrace_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026quot;\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- race_result[-6]\r# タイムと着差\rrace_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026quot;)))\rrace_result \u0026lt;- race_result[-5]\r# 馬名、馬齢、馬体重\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;[ァ-ヴー・]+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\d{3}\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026quot;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026quot;)))\rrace_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\r# ジョッキー\rrace_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026quot;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026quot;)))\rrace_result \u0026lt;- race_result[-4]\r# オッズと人気\rrace_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\(.+\\\\)\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026quot;\\\\d+[^(\\\\d+.\\\\d)]\u0026quot;)))\rrace_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026quot;\u0026quot;)\rrace_result \u0026lt;- race_result[-4]\r# レース情報\rrace_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026quot;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026quot;) %\u0026gt;%\rhtml_text()\rrace_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026quot;\\\\d+年\\\\d+月\\\\d+日\u0026quot;)))\rrace_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026quot;\\\\s\u0026quot;,\u0026quot;\u0026quot;)))\r## ファイル貯めるのかく\rif (k == 1 \u0026amp;\u0026amp; i == 1 \u0026amp;\u0026amp; year == 1994){\rdataset \u0026lt;- race_result\r} else {\rdataset \u0026lt;- rbind(dataset,race_result)\r} # if文2の終わり\r} # if文1の終わり\r} # iループの終わり\r} # kループの終わり\rbeep()\r} # yearループの終わり\rwrite.csv(dataset,\u0026quot;race_result.csv\u0026quot;, row.names = FALSE)\rこれを回すのに16時間かかりました（笑）データ数は想定していたよりは少なく、97939になりました。\n\r","date":1526688000,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1528588800,"objectID":"e816798bf62f83b8a57cccf9397181f5","permalink":"/post/post9/","publishdate":"2018-05-19T00:00:00Z","relpermalink":"/post/post9/","section":"post","summary":"今、競馬×データサイエンスが熱いです。ウマナリティクスなるものがあり、これまでのレース結果からなんらかのモデルを作成し、順位予想や回収率を高める馬券購入方法を考えようとする人が一定数いるようです。今回は競馬をデータ解析するためのデータを取得します。rvestを用いて、ごりごりにクローリングを行いました。","tags":["前処理","R"],"title":"rvestでyahoo競馬にある過去のレース結果をクローリングしてみた","type":"post"},{"authors":null,"categories":[],"content":"\r\r\rどうもはじめまして。\r東京にある資産運用会社に勤める新卒1年目の葦原彩人と申します。\n簡単に自己紹介をしたいと思います。\r大学院卒の24歳です。\r専攻はマクロ経済学で特にDSGEモデル、状態空間モデル、カルマンフィルタ、ベイズ推定、MCMCなんかをやっていました。\r指導教官の研究サポートとして自然言語処理をやったこともあります。\rもともと学者志望でしたが金銭的な問題で就職することになりました。\r今は営業サポートの下働きをしています。\nこのブログは研究への興味関心を捨てることができない私が趣味として研究を進めていく際の備忘録という位置付けになるのだと思います。\n現在進めている研究は以下の２つです。\n馬券版ファクターモデル\r四半期GDP予測モデル\r\r1は、馬券市場が株式市場よりも投機的に魅力のある市場であるという観点から、回収率100%超えを目指す馬券ポートフォリオを構築するモデルを作成できないかというものです。株式にはファーマフレンチのようなファクターモデルがありますが、それを応用して馬券版ファクターモデルなるものを作れないかと思っています。\n2は最近の四半期GDP速報の精度が低いという問題意識から、精度の高い予測モデルを新たに構築できないかというものです。特にこれまでのようなマクロ経済理論に基づいたモデルではなく、機械学習を用い、通常マクロ経済学の研究で使用しないようなデータを織り込んだモデルを作成する方向で研究を進めていきたいと思っています。\nまだ、研究は始めたばかりなのですが、\r興味を持ってもらえるように頑張りたいと思います…\rでは、最初はこの辺で。\nよろしくお願いします。\n","date":1526601600,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1526601600,"objectID":"ceb64c07c657dc4dad3f41bf7df6c579","permalink":"/post/post4/","publishdate":"2018-05-18T00:00:00Z","relpermalink":"/post/post4/","section":"post","summary":"どうもはじめまして。このブログを始めるに当たってまずは自己紹介をしたいと思います。","tags":[],"title":"はじめまして","type":"post"},{"authors":["Ayato Ashihara"],"categories":null,"content":" Click the Cite button above to get Bibtex and Cite ME !!   ","date":1522454400,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1522454400,"objectID":"8eb0b83c007669e1fcaec626e70937a6","permalink":"/publication/%E7%B4%80%E8%A6%812019/","publishdate":"2018-03-31T00:00:00Z","relpermalink":"/publication/%E7%B4%80%E8%A6%812019/","section":"publication","summary":"Japan  has  experienced  a  long-lasting  stagnation  since  the  early  1990s.  According  to  the  Reference Dates of Business Cycle, the Cabinet Office of Japan, there are four recession periods between 1987 and 2010,  and  three  of  them  are  considered  to  be  financially-related.  This  implies  that  the  stagnation  wastriggered by financial factors. Nonetheless, many studies using Dynamic Stochastic General Equilibrium models claim that a decline in Total Factor Productivity is the main driver of the stagnation. To resolve this contradiction,  this study estimates the Japanese economy by a New-Keynesian  DSGE  model augmented with  financial  friction  used  in  Christiano,  Motto  and  Rostagno  (2014),  where “risk  shock”  is newly incorporated into the model that refers to uncertainty in the financial market. According to our estimationresults, the estimated risk shock can explain the overall fluctuations of GDP and investment, and thus it isconsidered to be the main driver of the stagnation. We also find that it is highly correlated with the Business condition  Diffusion  Index,  the  Financial  Position  Diffusion  Index  and  the  Lending  Attitude  Index  of  Financial Institutions in Tankan released by the Bank of Japan. Therefore, we conclude that the estimated risk  shock  can  be  interpreted  as  the  firms’   distrust  toward  their  business  conditions,  and  it  delayed  theirinvestment decisions, then causing the prolonged economic contraction.","tags":"","title":"Does Financial Risk Explain Japan’s Great Stagnation?","type":"publication"},{"authors":["Ayato Ashihara"],"categories":null,"content":" Click the Cite button above to get Bibtex and Cite ME !!   ","date":1488844800,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1488844800,"objectID":"e1ffa3282e6f7fc6edad66690c0b404c","permalink":"/publication/ael2018/","publishdate":"2017-03-07T00:00:00Z","relpermalink":"/publication/ael2018/","section":"publication","summary":"Using Japanese financial data that provide enough observations under the good and bad regimes of financial conditions, we find that fiscal multipliers are smaller in the bad regime than in the good regime.","tags":[""],"title":"Is fiscal expansion more effective in a financial crisis?","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"ja","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]