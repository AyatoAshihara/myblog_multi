<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | 東京の資産運用会社で働く社会人が研究に没頭するブログ</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>ja</language><lastBuildDate>Thu, 10 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Rcppでデータハンドリングを高速に行う(Tickデータの処理を事例に)</title>
      <link>/post/post21/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/post21/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#やりたいこと&#34;&gt;0. やりたいこと&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#データの読み込み&#34;&gt;1. データの読み込み&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#前処理&#34;&gt;2. 前処理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#jump統計量の計算&#34;&gt;3. Jump統計量の計算&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ggplot2を用いた可視化&#34;&gt;4. ggplot2を用いた可視化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#まとめ&#34;&gt;5. まとめ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;やりたいこと&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;0. やりたいこと&lt;/h2&gt;
&lt;p&gt;今回お見せするのは前述の通り、為替のTickデータを使った前処理(と解析)になります。主眼を&lt;code&gt;Rcpp&lt;/code&gt;を用いた効率化に置いていますので詳しくは踏み入りませんが、やりたいことをざっくりと先に示しておきます。&lt;br /&gt;
やりたいのは、JPY/USDレートの5分刻みリターンから&lt;em&gt;Jump&lt;/em&gt;を検知することです。ここでのJumpとはそれまでと比べて為替レートがガクッと上昇(下落)した点です。日中為替レートは小刻みに動きますが、なにかイベントがあると大きく上昇(下落)します。どんなイベントがJumpを引き起こすのかは非常に興味深い点です。これを検証するにはまずJumpを検知する必要があるのです。 参考とするのは以下の論文です。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/rfs/article-abstract/21/6/2535/1574138?redirectedFrom=fulltext&#34;&gt;Suzanne S. Lee &amp;amp; Per A. Mykland, 2008. “Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,” Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Citationが204もある非常に評価されている論文です。推定方法を掻い摘んで説明します。まず、連続複利リターンを&lt;span class=&#34;math inline&#34;&gt;\(d\log S(t)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt;とします。ここで、&lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;時点での資産価格です。市場にJumpがない場合、&lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;は以下の確率過程に従うと仮定します。 &lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) \tag{1}
\]&lt;/span&gt; ここで、&lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt;は標準ブラウン運動、&lt;span class=&#34;math inline&#34;&gt;\(\mu(t)\)&lt;/span&gt;はドリフト項、&lt;span class=&#34;math inline&#34;&gt;\(\sigma(t)\)&lt;/span&gt;はスポットボラティリティです。また、Jumpがあるとき、&lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;は &lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) + Y(t)dJ(t) \tag{2}
\]&lt;/span&gt; に従うと仮定します。ここで、&lt;span class=&#34;math inline&#34;&gt;\(J(t)\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt;とは独立したカウント過程です。&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y(t)\)&lt;/span&gt;はジャンプのサイズを表現しており、予測可能な過程であるとします。&lt;/p&gt;
&lt;p&gt;次に、&lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;の対数リターンを考えます。それはつまり&lt;span class=&#34;math inline&#34;&gt;\(\log S(t_i)/S(t_{i-1})\)&lt;/span&gt;ですが、これは正規分布&lt;span class=&#34;math inline&#34;&gt;\(N(0,\sigma(t_i))\)&lt;/span&gt;に従います。&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(t_{i-1}\)&lt;/span&gt;から&lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt;にJumpがあった際の&lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;時点の統計量&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt;を以下で定義します。&lt;/p&gt;
&lt;p&gt;こうして計算されたJump統計量&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt;をどのように統計的検定に用いてJumpを検出するかに話を移しましょう。これは確率変数である&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt;の最大値(こちらも確率変数)を考え、その分布から大きく逸脱した値を取った場合(95%点とか)、そのリターンをJumpとします。&lt;br /&gt;
期間&lt;span class=&#34;math inline&#34;&gt;\([t_{i-1},t_{i}]\)&lt;/span&gt;にJumpがないとした場合、この期間の長さ&lt;span class=&#34;math inline&#34;&gt;\(\Delta=t_{i}-t_{i-1}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;に近づけると、つまり&lt;span class=&#34;math inline&#34;&gt;\(\Delta\rightarrow0\)&lt;/span&gt;とすると、標準正規変数の絶対値の標本最大値は、ガンベル分布に収束します。皆さん大好き極値統計ですね。よって、Jumpは以下の条件が満たされた際に帰無仮説が棄却され、検出することができます。 &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} &amp;gt; G^{-1}(1-\alpha)S_{n} + C_{n} \tag{5}
\]&lt;/span&gt; ここで、&lt;span class=&#34;math inline&#34;&gt;\(G^{-1}(1-\alpha)\)&lt;/span&gt;は標準ガンベル分布の&lt;span class=&#34;math inline&#34;&gt;\((1-\alpha)\)&lt;/span&gt;分位関数です。&lt;span class=&#34;math inline&#34;&gt;\(\alpha=10%\)&lt;/span&gt;だと2.25になります。また、 &lt;span class=&#34;math display&#34;&gt;\[
S_{n} = \frac{1}{c(2\log n)^{0.5}} \\
C_{n} = \frac{(2\log n)^{0.5}}{c}-\frac{\log \pi+\log(\log n)}{2c(2\log n)^{0.5}}
\]&lt;/span&gt; です(導出はしませんが、1式と2式を使って証明できます)。ここで、&lt;span class=&#34;math inline&#34;&gt;\(c=(2/\pi)^{0.5}\)&lt;/span&gt;で、&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;は推定に使用する総サンプルサイズです。 最終的に、&lt;span class=&#34;math inline&#34;&gt;\(Jump_{t_i}\)&lt;/span&gt;は &lt;span class=&#34;math display&#34;&gt;\[
Jump_{t_i} = \log\frac{S(t_i)}{S(t_{i-1})}×I(\mathcal{L(i)} - G^{-1}(1-\alpha)S_{n} + C_{n})\tag{6}
\]&lt;/span&gt; で求められることになります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(I(・)\)&lt;/span&gt;は中身が0より大きいと1、それ以外は0を返すIndicator関数です。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;データの読み込み&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. データの読み込み&lt;/h2&gt;
&lt;p&gt;では、推定方法がわかったのでまずTickデータの読み込みをしましょう。データは&lt;code&gt;QuantDataManager&lt;/code&gt;からcsvを取得し、それを作業ディレクトリに保存しています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)

# Tick dataの読み込み
strPath &amp;lt;- r&amp;quot;(C:\Users\hogehoge\JPYUSD_Tick_2011.csv)&amp;quot;
JPYUSD &amp;lt;- readr::read_csv(strPath)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;関係ないんですが、最近Rを4.0.2へ上げました。4.0以上では&lt;code&gt;Python&lt;/code&gt;でできた文字列のEscapeができるとうことで今までのストレスが解消されてかなりうれしいです。
データは以下のような感じで、日付の他にBid値、Ask値と取引量が格納されています。なお、ここでは2011年のTickを使用しています。東日本大震災の時のドル円を対象とするためです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     DateTime                        Bid             Ask            Volume     
##  Min.   :2011-01-03 07:00:00   Min.   :75.57   Min.   :75.58   Min.   : 1.00  
##  1st Qu.:2011-03-30 15:09:23   1st Qu.:77.43   1st Qu.:77.44   1st Qu.: 2.00  
##  Median :2011-06-15 14:00:09   Median :80.40   Median :80.42   Median : 2.00  
##  Mean   :2011-06-22 05:43:11   Mean   :79.91   Mean   :79.92   Mean   : 2.55  
##  3rd Qu.:2011-09-09 13:54:51   3rd Qu.:81.93   3rd Qu.:81.94   3rd Qu.: 3.00  
##  Max.   :2011-12-30 06:59:59   Max.   :85.52   Max.   :85.54   Max.   :90.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ちなみに、&lt;code&gt;DateTime&lt;/code&gt;はUTC基準で日本時間だと2011/1/3 07:00:00から2011-12-30 06:59::59(米国時間2011-12-30 16:59:59)までを含んでいます。サンプルサイズは約1200万件です。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NROW(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11946621&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;前処理&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. 前処理&lt;/h2&gt;
&lt;p&gt;では次にBidとAskから仲値を計算し、後でリターンを算出するために対数を取っておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# AskとBidの仲値を計算し、対数化(対数リターン算出用)
JPYUSD &amp;lt;- JPYUSD %&amp;gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %&amp;gt;% 
                     dplyr::mutate(logMid = log(Mid))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;現状不規則に並んでいる取引データを5min刻みのリターンに整形します。やり方は、
1. 1年間を5min毎に刻んだ&lt;code&gt;POSIXct&lt;/code&gt;ベクトルを作る。
2. 1.を引数として渡すと、その5minのWindowのうち、最初と最後のサンプルから対数リターンを順々に計算する関数を作成する。
3. 実行。
という計画です。まず、1.のベクトルを作成します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5min刻みでのリターンを算出するためのPOSIXベクトルを作成(288×日数)
start &amp;lt;- as.POSIXct(&amp;quot;2011-01-02 22:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
end &amp;lt;- as.POSIXct(&amp;quot;2011-12-31 21:55:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
from &amp;lt;- seq(from=start,to=end,by=5*60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、2.に移ろうということなんですが、データが1200万件もあると&lt;code&gt;R&lt;/code&gt;で&lt;code&gt;purrr::map&lt;/code&gt;とか&lt;code&gt;apply&lt;/code&gt;属を使用したとしても、関数呼び出しに時間がかかって結構非効率だったりします。。。&lt;code&gt;sapply&lt;/code&gt;でやってみましたがなかなか処理が完了せず、強制終了しました。こういうときには、&lt;code&gt;Rccp&lt;/code&gt;が便利です。&lt;code&gt;R&lt;/code&gt;はグラフや統計処理のための非常に便利な関数が多数ありますが、ユーザーで定義した関数の呼び出しを含む、大量の繰り返し処理を苦手とします(スクリプト言語なのでコンパイル言語よりはという意味です)。なので、繰り返し処理の部分だけ、&lt;code&gt;C++&lt;/code&gt;で書いてしまって、それを&lt;code&gt;Rcpp&lt;/code&gt;をつかって&lt;code&gt;R&lt;/code&gt;の関数としてコンパイルし、実行。結果の集計や可視化、執筆は&lt;code&gt;R&lt;/code&gt;で行うというフローが非常に効率的です。
また、&lt;code&gt;Rccp&lt;/code&gt;は&lt;code&gt;R&lt;/code&gt;に似た違和感の少ない記述方法で&lt;code&gt;C++&lt;/code&gt;を記述するのを助けてくれます。詳しいことは以下を見れば問題ないと思います。かなりまとまっていて控えめに言って神です。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://teuder.github.io/rcpp4everyone_ja/&#34;&gt;みんなのRcpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では、2.にあたるコードを書いていきます。コーディングに当たってはネット上の記事を参考にしました。&lt;code&gt;C++&lt;/code&gt;は&lt;code&gt;R&lt;/code&gt;よりも歴史があるし、使用者も多いので知りたい情報はすぐ見つけられます。&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;algorithm&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
DataFrame Rolling_r_cpp(
    DataFrame input,               //（計測時刻time, 計測値data）のデータフレーム
    newDatetimeVector from,        //計算するタイミングの始点ベクトル
    double time_window = 5*60)  //計算するwindow幅（秒）
{ 
  
  // 計測時刻と計測値をベクトルとして取り出す
  newDatetimeVector time = input[&amp;quot;DateTime&amp;quot;]; // 今回は time は昇順にソートされているのが前提です。
  NumericVector     data = input[&amp;quot;logMid&amp;quot;];
  
  // 計算するタイミングの終点ベクトル
  newDatetimeVector to = from + time_window;
  
  // 計算する数
  R_xlen_t N = from.length();
  
  // 格納するベクトル
  NumericVector value(N);
  
  // ベクトル要素の位置をあらわすオブジェクト
  newDatetimeVector::iterator begin = time.begin();
  newDatetimeVector::iterator end   = time.end();
  newDatetimeVector::iterator p1    = begin;
  newDatetimeVector::iterator p2    = begin;
  
  // window i についてループ
  for(R_xlen_t i = 0; i &amp;lt; N; ++i){
    // Rcout &amp;lt;&amp;lt; &amp;quot;i=&amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    double f = from[i];         //windowの始点の時刻
    double t = f + time_window; //windowの終点の時刻
    
    // windowの終点が最初の計測時刻以前の時はNA、または
    // windowの始点が最後の計測時刻のより後の時はNA
    if(t &amp;lt;= *begin || f &amp;gt; *(end-1)){ 
      value[i]  = NA_REAL;
      continue;//次のループへ
    }
    
    // ベクトル time の位置 p1 以降の要素xから
    // 時刻がwindowの始点f「以降」である「最初の要素」の位置を p1 とする
    p1 = std::find_if(p1, end, [&amp;amp;f](double x){return f&amp;lt;=x;});
    // p1 = std::lower_bound(p1, end, f); //上と同義
    
    // ベクトル time の位置 p1 以降の要素xから
    // 時刻がwindowの終点t「より前」である「最後の要素」の位置を p2 とする
    // （下では、時刻がwindowの終点t「以降」である「最初の要素」の１つ前の位置、にすることで実現している’）
    p2 = std::find_if(p1, end, [&amp;amp;t](double x){return t&amp;lt;=x;}) - 1 ;
    // p2 = std::lower_bound(p1, end, t) - 1 ;//上と同義
    
    // 要素の位置p1,p2を、要素番号i1, i2に変換する
    R_xlen_t i1 = p1 - begin;
    R_xlen_t i2 = p2 - begin; 
    
    
    // 要素番号の確認
    // C++は要素番号が0から始まるのでRに合わせるために1を足している
    // Rcout &amp;lt;&amp;lt; &amp;quot;i1 = &amp;quot; &amp;lt;&amp;lt; i1+1 &amp;lt;&amp;lt; &amp;quot; i2 = &amp;quot; &amp;lt;&amp;lt; i2+1 &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    
    // 該当する範囲のデータについて計算する
    if(i1&amp;gt;i2) {
      value[i] = NA_REAL; // window内にデータがない場合
    } else { 
      value[i] = data[i2] - data[i1];
    }
    // ↑を変更することで様々なwindow関数を作成できる
    
  }
  
  // 計算した時間と、値をデータフレームとして出力する
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, from),
      Named(&amp;quot;r&amp;quot;, value*100));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Rcpp::sourceCpp&lt;/code&gt;でコンパイルしたら、以下のように&lt;code&gt;R&lt;/code&gt;の関数として実行します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- Rolling_r_cpp(JPYUSD,from))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       0.03       0.00       0.03&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;はい。1200万件のデータの処理に1秒かかりません。便利ー。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r         
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014  
##  Median :2011-07-03 09:57:30   Median : 0.000  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880  
##                                NA&amp;#39;s   :29977&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;問題なく、リターンが計算されています。では、&lt;code&gt;Realized Bipower Variation&lt;/code&gt;の計算に移りましょう。5min刻みの場合はWindowの長さは270が推奨でしたが、そこも引数として柔軟を持たせた作りにします。また、&lt;code&gt;NA&lt;/code&gt;の処理についても丁寧に行います。&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;cmath&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
float rbv_cpp(
    NumericVector x, // rbvを計算するリターンベクトル
    bool na_rm = true) // xにNAが含まれている場合、取り除いて計算するか
{
  
  // 計算回数を取得
  R_xlen_t N = x.length();
  
  // 計算結果を入れる変数を定義
  float out = 0;

  // xの欠損有無を確認
  LogicalVector lg_NA = is_na(x);
  
  // xにNAが存在した場合、そのNAを除いて計算するかどうか
  if(any(lg_NA).is_true() and na_rm==FALSE){
    out = NA_REAL; // NAを計算結果として出力
  } else {
    
    // NAを除く場合
    if (any(lg_NA).is_true() and na_rm==TRUE){
      x[is_na(x)==TRUE] = 0.00; // NAに0を埋め、実質的に計算から除外する
    }
    
    // rbvの分子(総和)を計算
    for(R_xlen_t i = 1; i &amp;lt; N; ++i){
      out = out + std::abs(x[i])*std::abs(x[i-1]);
    }
    
    // 平均値を計算し、ルートをとる
    long denomi; //分母
    if(N-sum(lg_NA)-2&amp;gt;0){
      denomi = N-sum(lg_NA)-2;
    } else {
      denomi = 1;
    }
    out = out/denomi;
    out = std::sqrt(out);
  }
  
  return out;
}

// [[Rcpp::export]]
DataFrame Rolling_rbv_cpp(
    DataFrame input, //（計測時刻time, 計測値data）のデータフレーム
    int K = 270, // 計算するRolling Window幅
    bool na_pad = false, // Window幅が足りないときにNAを返すか
    bool na_remove = false // Window幅の中にNAが存在した場合、除いて計算を行うか
){
  // リターンベクトルとサンプル数を取り出す
  NumericVector data = input[&amp;quot;r&amp;quot;];
  R_xlen_t T = data.length();
  
  // 計算結果を格納するベクトルを準備
  NumericVector value(T);
  
  // Windows幅毎にRBVを計算し、格納する
  if(na_pad==TRUE){
    value[0] = NA_REAL; // NAを返す
    value[1] = NA_REAL; // NAを返す
    value[2] = NA_REAL; // NAを返す
  } else {
    value[0] = 0; // 0を返す
    value[1] = 0; // 0を返す
    value[2] = 0; // NAを返す
  }
  
  for(R_xlen_t t = 3; t &amp;lt; T; ++t){
    // Windows幅が足りるかどうかで処理を分岐
    if (t-K&amp;gt;=0){
      value[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // 通常計算を実行
    } else if(na_pad==FALSE) {
      value[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Kに満たない不完全なWidnows幅で計算を実行
    } else {
      value[t] = NA_REAL; // NAを返す
    }
  }
  
  // 計算した時間と値をデータフレームとして出力する
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, input[&amp;quot;from&amp;quot;]),
      Named(&amp;quot;r&amp;quot;, data),
      Named(&amp;quot;rbv&amp;quot;,value));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、これもコンパイルし、&lt;code&gt;R&lt;/code&gt;で実行します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- results %&amp;gt;% Rolling_rbv_cpp(na_remove = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       0.24       0.08       0.31&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;こちらも一瞬ですね。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;jump統計量の計算&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Jump統計量の計算&lt;/h2&gt;
&lt;p&gt;では、次に今計算したリターンと標準偏差から統計量&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_{t_i}\)&lt;/span&gt;を計算しましょう。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 対数リターンの絶対値を標準化=Jump統計量
results &amp;lt;- results %&amp;gt;% dplyr::mutate(J=ifelse(rbv&amp;gt;0,abs(r)/rbv,NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;今こんな感じです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、Jump検定に移りましょう。まず、必要な関数を定義しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Jump検定を計算するための定数&amp;amp;関数を準備
c &amp;lt;- (2/pi)^0.5
Cn &amp;lt;- function(n){
  return((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5))
}
Sn &amp;lt;- function(n){
  1/(c*(2*log(n))^0.5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では検定を行います。棄却されたサンプルは1、それ以外は0を返します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Jump検定(10%)を実行(返り値はlogical)
N &amp;lt;- NROW(results$J)
results &amp;lt;- results %&amp;gt;% dplyr::mutate(Jump = J &amp;gt; 2.25*Sn(N) + Cn(N))
summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423  
##     Jump        
##  Mode :logical  
##  FALSE:59864    
##  TRUE :257      
##  NA&amp;#39;s :44423    
##                 
##                 
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ggplot2を用いた可視化&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. ggplot2を用いた可視化&lt;/h2&gt;
&lt;p&gt;数値が計算できましたので可視化しましょう。2011/03/11の日中のJPY/USDの5min刻み対数リターンの推移とJumpを重ねてPlotします。ちなみに横軸は日本時間に修正しています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 2011/03/11の東日本大震災発生時のJumpについてPlot
results %&amp;gt;% 
  dplyr::filter(from &amp;gt;= as.POSIXct(&amp;quot;2011-03-11 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;),from &amp;lt; as.POSIXct(&amp;quot;2011-03-12 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)) %&amp;gt;% 
  ggplot2::ggplot(ggplot2::aes(x=from,y=r)) +
  ggplot2::geom_path(linetype=3) +
  ggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=&amp;quot;red&amp;quot;)) +
  ggplot2::scale_x_datetime(date_breaks = &amp;quot;2 hours&amp;quot;, labels = scales::date_format(format=&amp;quot;%H:%M&amp;quot;,tz=&amp;quot;Asia/Tokyo&amp;quot;)) +
  ggplot2::ggtitle(&amp;quot;JPY/USD Jumps within Tohoku earthquake on 2011-3-11&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 36 row(s) containing missing values (geom_path).

## Warning: Removed 36 row(s) containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ここまで執筆するのに結構時間使っていて、今23:37なんで深い考察は控えますが、震災が発生したのが14:46:18ですから市場は震災直後即座に円安に反応したことが分かります。その後なぜか円高方向へ進み19:00にはピークになっています。安全資産の円とか言われますが、この時ばかりは不確実性の高まりからして安全じゃないだろと思いますが。。。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;まとめ&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. まとめ&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Rcpp&lt;/code&gt;を使った&lt;code&gt;R&lt;/code&gt;分析の効率化について紹介しました。&lt;code&gt;C++&lt;/code&gt;は愚直にコードを書いてもRより格段に処理が早いのでコーディングミスしにくい印象です。学術的な実装をやるときは内容が複雑になるのでこれはありがたいです。また、コンパイルエラーが起こってもRStudioを使っていればどこでコンパイルエラーが起こっているか手がかりをくれますのでその点でもストレスはないのでお勧めです。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;非負、整数、非減少の値を持つ確率過程のこと。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;平均はドリフト項の形状により必ずしも0にはなりませんが、今ドリフト項は十分小さい値を想定しているのでこの書き方にさせてください。論文ではより厳密に定義しています。 &lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} \equiv \frac{|\log S(t_i)/S(t_{i-1})|}{\hat{\sigma}_{t_i}} \tag{3}
\]&lt;/span&gt; 対数リターンの絶対値を単純に標準化したものですが、標準偏差の推定量には以下で定義される“Realized Bipower Variation”を使用しています。 &lt;span class=&#34;math display&#34;&gt;\[
\hat{\sigma}_{t_i} = \frac{1}{K-2}\sum_{j=i-K+2}^{i-2}|\log S(t_j)/\log S(t_{j-1})||\log S(t_{j-1})/\log S(t_{j-2})| \tag{4}
\]&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;はWindowに含まれるサンプルサイズの数です。仮に5min刻みリターンを用い、2020/9/10 10:00にJumpが発生した場合、&lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt;としている場合は前日2020/9/9 11:30から2020/9/11 09:55までのサンプルを用いて計算することになります。やっていることは、リターンの絶対値をかけたものを足し合わせるということですが、これでJumpが生じた次の瞬間(つまり&lt;span class=&#34;math inline&#34;&gt;\(t_{i+1}\)&lt;/span&gt;とか）の推定値がJumpに影響されにくいようです。ちなみに&lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt;は5min刻みリターンの場合の推奨値と別の文献で紹介されています。&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>そのバックテスト本当に再現性ありますか？</title>
      <link>/post/post19/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/post19/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#今回のテーマバックテストとは&#34;&gt;1. 今回のテーマ「バックテスト」とは？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#バックテストはオーバーフィットする&#34;&gt;2. バックテストはオーバーフィットする&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#シャープレシオが従う分布とは&#34;&gt;3. シャープレシオが従う分布とは&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-minimum-backtest-lengthを導出してみる&#34;&gt;4. &lt;code&gt;the minimum backtest length&lt;/code&gt;を導出してみる&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#終わりに&#34;&gt;4. 終わりに&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;今回のテーマバックテストとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. 今回のテーマ「バックテスト」とは？&lt;/h2&gt;
&lt;p&gt;バックテストは、アルゴリズムによる投資戦略のヒストリカルシミュレーションです。バックテストは、立案した投資戦略がある期間にわたって実行されていた場合に発生したであろう利益と損失をアルゴリズムを用いて計算します。その際、シャープレシオやインフォメーションレシオなどの投資戦略のパフォーマンスを評価する一般的な統計量が使用されています。投資家は通常、これらのバックテストの統計量を調査し、最高のパフォーマンスを発揮する投資(運用)戦略に資産配分を決定するため、資産運用会社は良好なパフォーマンスを血のにじむような回数のバックテストを試行錯誤し、資料を作ってプレゼンしたりするわけです。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://stat.ameba.jp/user_images/20190212/22/nash210/51/5f/j/o0705061514355131242.jpg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;3倍3分法のバックテスト&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;投資家の立場に立つなら、バックテストされた投資戦略のパフォーマンスについては、インサンプル(IS)とアウトオブサンプル(OOS)を区別することが重要です。ISのパフォーマンスは、投資戦略の設計に使用したサンプル（機械学習の文献では「学習期間」や「訓練セット」と呼ばれる物です）でシミュレートしたものです。一方、OOSパフォーマンスは、投資戦略の設計に使用されなかったサンプル（別名「テストセット」）でシミュレーションされたものです。バックテストは、そのパフォーマンスを持ってその投資戦略の有効性を占う物ですので、ISのパフォーマンスがOOSのパフォーマンスと一致している場合に再現性が担保され、現実的であるということができます。ただ、アウトサンプルの結果はこれからの結果であるので、バックテストを受け取った時点でそのバックテストが信頼に足るものか判断することは難しいです。hold-out法などで、以下のように学習データとテストデータを分け、OOSでのテストを行っているものもありますが、OOSの結果をフィードバックして戦略の改善ができる以上、純粋なアウトサンプルとは呼べません。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.triton.biz/blog1/wp-content/uploads/2018/04/pic001.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ですので、ファンドマネージャーから良い結果のバックテストを受け取った場合、そのシミュレーションがどれだけ現実的であるかをなんとかして評価することが非常に重要となります。また、ファンドマネージャーも自身のバックテスト結果が持つ不確実性を理解しておくことが重要です。今回はバックテストのシミュレーションの現実性をどのようにして評価するのか、再現性のあるバックテストを行うためには何に注意すれば良いのかを調べてみたいと思います。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;バックテストはオーバーフィットする&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. バックテストはオーバーフィットする&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2308659&#34;&gt;Bailey, Borwein, López de Prado and Zhu(2015)&lt;/a&gt;は、どのような金融時系列でも、バックテストのシミュレーションをオーバーフィット(過学習)させることが(比較的)簡単にできると主張しています。ここで、オーバーフィットとは、機械学習の概念であり，モデルが一般的な構造よりも特定の観察データ(ISデータ)にフォーカスしてしまう状況を表します。&lt;/p&gt;
&lt;p&gt;Bailey et. al.(2015)では、この主張の一例として株式戦略のバックテスト結果が芳しくない状況が挙げられています。バックテストではその名の通り過去データを使用しているので、具体的に損失が発生している銘柄を特定することが可能で、その銘柄の推奨を削除するためにいくつかのパラメータを追加し、取引システムを設計することで、パフォーマンスを向上させることができるというわけです（「データ・スヌーピング」として知られているテクニック）。数回シミュレーションを繰り返えせば、特定のサンプルに存在するが、母集団の中では稀であるかもしれない特徴から利益を得る「最適なパラメータ」を導くことができます。&lt;/p&gt;
&lt;p&gt;機械学習の文献では、オーバーフィッティングの問題を対処するための膨大な研究の蓄積があります。ですが、Bailey et. al.(2015)は、機械学習の文脈で提案されている手法は一般的に複数の投資問題には適用できないと主張します。その理由は以下4点のようです。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;機械学習でオーバーフィッティングを防ぐ手法は、予測の説明力や質を評価するために、その事象が定義される領域において明示的な点推定と信頼区間を必要としますが、このような明確な予測を行う投資戦略はほとんどないため。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例えば、「E-mini S&amp;amp;P500は、金曜日の終値で1標準偏差5ポイントで1,600前後になると予測されています」とはあまり言われず、むしろ「買い」または「強い買い」といった定性的な推奨が提供されることが一般的です。しかも、この予想は予測の有効期限も明示されず、なにか予期せぬ事象が発生した際に変更がなされます。一方、定量予測では金曜日の終値と明記されています。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;仮に特定の投資戦略が予測式に依存していたとしても、投資戦略の他の構成要素がオーバーフィットされている可能性がある。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言い換えれば、単に予測式を調整する以外にも、投資戦略をオーバーフィットさせる方法はたくさんあるということです。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;回帰のオーバーフィットの方法はパラメトリックであり、金融の場合観察不可能なデータに関する多くの仮定を含むため。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;いくつかの手法は試行回数をコントロールしていないため。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bailey et. al.(2015)では、バックテストのパフォーマンスが比較的低い投資戦略を特定するためには、&lt;strong&gt;比較的少ない試行回数&lt;/strong&gt;が必要であることを示しています。ここでの試行回数とは試行錯誤の回数だと思ってください。また、試行回数に応じて必要とされるバックテストの期間である&lt;code&gt;the minimum backtest length&lt;/code&gt;（MinBTL）を計算しています。この論文では、パフォーマンスを評価するために常にシャープレシオが使用されていますが、他のパフォーマンス指標にも応用できるそうです。その内容を見てみましょう。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;シャープレシオが従う分布とは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. シャープレシオが従う分布とは&lt;/h2&gt;
&lt;p&gt;MinBTLを導出するために、まずシャープレシオの(漸近)分布を導出します。そもそも、投資戦略の設計は、通常、特定のパターンが金融変数の将来値を予測するのに役立つかもしれないという事前知識または信念から始まります。例えば、さまざまな満期の債券の間にリードラグ効果を認識している場合は、イールドカーブが上昇した場合に均衡値への回帰に賭ける戦略を設計することができます。このモデルは、cointegration equation、ベクトル誤差補正モデル、確率微分方程式のシステムなどの形をとることが考えられます。&lt;/p&gt;
&lt;p&gt;このようなモデル構成（または試行）の数は膨大であり、ファンドマネージャーは当然、戦略のパフォーマンスを最大化するものを選択したいと考え、そのためにヒストリカルシミュレーション（バックテスト）を行います(前述)。バックテストでは、最適なサンプルサイズ、シグナルの更新頻度、リスクサイジング、ストップロス、最大保有期間などなどを他の変数との兼ね合いの中で評価します。&lt;/p&gt;
&lt;p&gt;この論文中でパフォーマンス評価の尺度として使用されるシャープレシオは、過去のリターンのサンプルに基づいて、戦略のパフォーマンスを評価する統計量で、BMに対する平均超過リターン/標準偏差(リスク)として定義されます。通常には、「リスク1標準偏差に対するリターン」と解釈され、資産クラスにもよりますが1を上回っていると非常に良い戦略であると見なせます。以下では、ある戦略の超過リターン&lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt;がi.i.d.の確率変数であり、正規分布に従うと仮定します。つまり、&lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt;の分布は&lt;span class=&#34;math inline&#34;&gt;\(r_s(t\neq s)\)&lt;/span&gt;と独立であることを仮定しています。あまり現実的な仮定ではありませんが。。。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_t \sim \mathcal{N}(\mu,\sigma^2)
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt;は平均&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;、分散&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;の正規分布を表しています。今、時点t~t-q+1の超過リターン&lt;span class=&#34;math inline&#34;&gt;\(r_{t}(q)\)&lt;/span&gt;を&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_{t}(q) \equiv r_{t} + r_{t-1} + ... + r_{t-q+1}
\]&lt;/span&gt;
と定義すると(複利部分を無視してます)、年率化されたシャープレシオは&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
SR(q) &amp;amp;=&amp;amp; \frac{E[r_{t}(q)]}{\sqrt{Var(r_{t}(q))}}\\
&amp;amp;=&amp;amp; \frac{q\mu}{\sqrt{q}\sigma}\\
&amp;amp;=&amp;amp; \frac{\mu}{\sigma}\sqrt{q}
\end{eqnarray}
\]&lt;/span&gt;
と表すことができます。ここで、&lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;は年毎のリターンの数(頻度)です。例えば、日次リターンの場合&lt;span class=&#34;math inline&#34;&gt;\(q=365\)&lt;/span&gt;となります(閏年を除く)。
&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;は一般に未知ですので、&lt;span class=&#34;math inline&#34;&gt;\(SR\)&lt;/span&gt;の真値を知ることはできません。なので、&lt;span class=&#34;math inline&#34;&gt;\(R_t\)&lt;/span&gt;を標本リターン、リスクフリーレート&lt;span class=&#34;math inline&#34;&gt;\(R^f\)&lt;/span&gt;(定数)とすると、標本平均&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=1/T\sum_{t=1}^T R_{t}-R^f\)&lt;/span&gt;と標本標準偏差&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}=\sqrt{1/T\sum_{t=1}^{T}(R_{t}-\hat{\mu})}\)&lt;/span&gt;を用いてシャープレシオの推定値を計算することになります(&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;はバックテストを行うサンプルサイズ)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q) = \frac{\hat{\mu}}{\hat{\sigma}}\sqrt{q}
\]&lt;/span&gt;
必然的な結果として、&lt;span class=&#34;math inline&#34;&gt;\(SR\)&lt;/span&gt;の計算はかなりの推定誤差が伴う可能性が高くなります。では、本節の本題、&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}\)&lt;/span&gt;の漸近分布を導出してみましょう。まず、&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt;の漸近分布はi.i.d.と&lt;span class=&#34;math inline&#34;&gt;\(\mu, \sigma\)&lt;/span&gt;が有限な値をとることから中心極限定理を適用することにより、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{T}\hat{\mu}\sim^{a}\mathcal{N}(\mu,\sigma^2), \\
\sqrt{T}\hat{\sigma}^2\sim^a\mathcal{N}(\sigma^2,2\sigma^4)
\]&lt;/span&gt;
となります。シャープレシオはこの&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt;から計算される確率変数であるので、この関数を&lt;span class=&#34;math inline&#34;&gt;\(g(\hat{{\boldsymbol \theta}})\)&lt;/span&gt;と表しましょう。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}=(\hat{\mu},\hat{\sigma}^2)&amp;#39;\)&lt;/span&gt;です。今、i.i.d.であるので&lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}\)&lt;/span&gt;は互いに独立となり、上記の議論から漸近同時分布は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{T}\hat{{\boldsymbol \theta}} \sim^a \mathcal{N}({\boldsymbol \theta},{\boldsymbol V_{\boldsymbol \theta}})
\]&lt;/span&gt;
と書けます。ここで、&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol V_{\boldsymbol \theta}}\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
{\boldsymbol V_{\boldsymbol \theta}} = \left( 
    \begin{array}{cccc}
      \sigma^2 &amp;amp; 0\\
      0 &amp;amp; 2\sigma^4\\
    \end{array}
  \right)
\]&lt;/span&gt;
です。シャープレシオの推定値は今&lt;span class=&#34;math inline&#34;&gt;\(g(\hat{{\boldsymbol \theta}})\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}\)&lt;/span&gt;だけの関数になっていますのでデルタ法より、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR} = g(\hat{{\boldsymbol \theta}}) \sim^a \mathcal{N}(g({\boldsymbol \theta}),\boldsymbol V_g)
\]&lt;/span&gt;
と漸近的に正規分布に従います。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol V_g\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol V_g=\frac{\partial g}{\partial{\boldsymbol \theta}}{\boldsymbol V_{\boldsymbol \theta}}\frac{\partial g}{\partial{\boldsymbol \theta}&amp;#39;}
\]&lt;/span&gt;
です。&lt;span class=&#34;math inline&#34;&gt;\(g({\boldsymbol \theta})=\mu/\sigma\)&lt;/span&gt;なので、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial g}{\partial{\boldsymbol \theta}&amp;#39;} = \left[ 
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma^2}\\
    \end{array}
  \right]
  = \left[ 
    \begin{array}{cccc}
      \frac{1}{\sigma}\\
      -\frac{\mu}{2\sigma^3}\\
    \end{array}
  \right]
\]&lt;/span&gt;
よって、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\boldsymbol V_g &amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}, \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right)
  \left( 
    \begin{array}{cccc}
      \sigma^2 &amp;amp; 0\\
      0 &amp;amp; 2\sigma^4\\
    \end{array}
  \right)
  \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right) \\
  &amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\sigma^2, \frac{\partial g}{\partial \sigma}2\sigma^4\\
    \end{array}
  \right)
    \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right) \\
  &amp;amp;=&amp;amp; (\frac{\partial g}{\partial \mu})^2\sigma^2 + (\frac{\partial g}{\partial \sigma})^2\sigma^4 \\
  &amp;amp;=&amp;amp; 1 + \frac{\mu^2}{2\sigma^2} \\
  &amp;amp;=&amp;amp; 1 + \frac{1}{2}SR^2
\end{eqnarray}
\]&lt;/span&gt;
と導出することができます。シャープレシオの絶対値が大きくなるほど指数的に分散が大きくなる傾向があるので良いパフォーマンスを見た時には注意が必要かもしれません。年率化されたシャープレシオの推定値&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}(q)\)&lt;/span&gt;が従う分布はここから&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q)\sim^a \mathcal{N}(\sqrt{q}SR,\frac{V(q)}{T}) \\
V(q) = q{\boldsymbol V}_g = q(1 + \frac{1}{2}SR^2)
\]&lt;/span&gt;
となります。今、&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;をバックテストを行う年数とすると&lt;span class=&#34;math inline&#34;&gt;\(T=yq\)&lt;/span&gt;と書け、これを用いて上式を以下のように書き換えることができます(日次リターンで3年計測の場合、サンプルサイズ&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(T=3×365=1095\)&lt;/span&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q)\sim^a \mathcal{N}(\sqrt{q}SR,\frac{1+\frac{1}{2}SR^2}{y}) \tag{1}
\]&lt;/span&gt;
頻度&lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;はシャープレシオの平均には影響しますが分散には影響を及ぼしません。これでシャープレシオの推定値の漸近分布を導出することができました。さて、これを使ってなにをしたかったのかということですが、私たちは今バックテストの信頼性について考えていたのでした。つまり、FMが新商品を開発するために頭をひねって考え出した&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;個の投資戦略案のバックテストをした際に、それらのシャープレシオの真値がどれも0であるにも関わらず、非常に高い(良い)値が出る確率はいかほどなのかということです。Bailey et. al.(2015)では以下のように記述されていました。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How high is the expected maximum Sharpe ratio IS among a set of strategy configurations where the true Sharpe ratio is zero?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;また、期待最大シャープレシオの値を小さくするためには、いったいどれほどの期間バックテストをすべきなのかも知りたいわけです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-minimum-backtest-lengthを導出してみる&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. &lt;code&gt;the minimum backtest length&lt;/code&gt;を導出してみる&lt;/h2&gt;
&lt;p&gt;今考えている状況は、&lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt;で&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;を簡単化のために1年とすると(1)式より&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}(q)\)&lt;/span&gt;は標準正規分布&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(0,1)\)&lt;/span&gt;に従います。さて、今から私たちは&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}_n(n=1,2,...N)\)&lt;/span&gt;の最大値&lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]_N\)&lt;/span&gt;の期待値について考えていくのですが、勘の良い人ならお気づきの通り、議論は極値統計の文脈に入っていくことになります。&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}_n\sim\mathcal{N}(0,1)\)&lt;/span&gt;はi.i.d.なので、その最大統計量の極値分布はFisher-Tippett-Gnedenko定理よりガンベル分布になります(証明追えてないです、ごめんなさい)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\lim_{N\rightarrow\infty}prob[\frac{\max[\hat{SR}]_N-\alpha}{\beta}\leq x] = G(x) = e^{-e^{-x}}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(\alpha=Z(x)^{-1}[1-1/N], \beta=Z(x)^{-1}[1-1/Ne^{-1}]-\alpha\)&lt;/span&gt;で、&lt;span class=&#34;math inline&#34;&gt;\(Z(x)\)&lt;/span&gt;は標準正規分布の累積分布関数を表しています。ガンベル分布のモーメント母関数&lt;span class=&#34;math inline&#34;&gt;\(M_x(t)\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
M_x(t) &amp;amp;=&amp;amp; E[e^{tx}] = \int_{-\infty}^\infty e^{tx}e^{-x}e^{-e^{-x}}dx \\
\end{eqnarray}
\]&lt;/span&gt;
と書け、&lt;span class=&#34;math inline&#34;&gt;\(x=-\log(y)\)&lt;/span&gt;と変数変換すると&lt;span class=&#34;math inline&#34;&gt;\(dx/dy=-1/y=-(e^{-x})^{-1}\)&lt;/span&gt;なので、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
M_x(t) &amp;amp;=&amp;amp; \int_{\infty}^0-e^{-t\log(y)}e^{-y}dy \\
&amp;amp;=&amp;amp; \int_{0}^\infty y^{-t}e^{-y}dy \\
&amp;amp;=&amp;amp; \Gamma(1-t)
\end{eqnarray}
\]&lt;/span&gt;
となります。&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x)\)&lt;/span&gt;はガンマ関数です。ここから、標準化された最大統計量の期待値(平均)は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\lim_{N\rightarrow\infty} E[\frac{\max[\hat{SR}]_N-\alpha}{\beta}] &amp;amp;=&amp;amp; M_x&amp;#39;(t)|_{t=0} \\
&amp;amp;=&amp;amp; (-1)\Gamma&amp;#39;(1) \\
&amp;amp;=&amp;amp; (-1)(-\gamma) = \gamma
\end{eqnarray}
\]&lt;/span&gt;
となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\gamma\approx0.5772156649...\)&lt;/span&gt;はEuler-Mascheroni定数です。よって、&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;が大きいとき、i.i.d.の標準正規分布の最大統計量の期待値は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[\max[\hat{SR}]] \approx \alpha + \gamma\beta = (1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}] \tag{2}
\]&lt;/span&gt;
と近似できます(&lt;span class=&#34;math inline&#34;&gt;\(N&amp;gt;1\)&lt;/span&gt;)。これがBailey et. al.(2015)のProposition 1.になります。&lt;span class=&#34;math inline&#34;&gt;\(E[\max[\hat{SR}]]\)&lt;/span&gt;を戦略数(試行錯誤数)&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;の関数としてプロットしたのが以下になります。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ExMaxSR = function(N){
  gamma_ct = -digamma(1)
  Z = qnorm(0.99)
  return((1-gamma_ct)*Z*(1-1/N) + gamma_ct*Z*(1-1/N*exp(1)^{-1}))
}
N = list(0:100)
result = purrr::map(N,ExMaxSR)
ggplot2::ggplot(data.frame(ExpMaxSR = unlist(result),N = unlist(N)),aes(x=N,y=ExpMaxSR)) +
  geom_line(size=1) + ylim(0,3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;小さい&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;に対して急激に&lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]\)&lt;/span&gt;の期待値が上昇していることがわかると思います。&lt;span class=&#34;math inline&#34;&gt;\(N=10\)&lt;/span&gt;の時、&lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]=1.54\)&lt;/span&gt;となっており、全ての戦略のシャープレシオの真値が0にも拘わらず、少なくとも1つは見かけ上かなり良いパフォーマンスの戦略が見つかることが期待されます。金融ではhold-out法でのバックテストはしばしば使用されるかと思いますが、この方法は試行(錯誤)回数を考慮に入れていないため、&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;が大きいときには信頼に足る結果を返してくれないわけです。バックテストの結果を向上させるため、闇雲にあれやこれやとシミュレーションを行うことは非常に危険だと思いませんか？最終的にプレゼン資料に上がってくるのは&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;個の戦略のうち、最もパフォーマンスが良いもののみですから、今回の例のように10個戦略を考えただけでもどれかはシャープレシオが1.87付近に分布しているわけです。試行錯誤数なんてもちろん資料には記載しませんから、非常にミスリーディングなわけです。こういった資料を評価する際にはまず偽陽性を疑ってかかった方がいいかもしれません。&lt;/p&gt;
&lt;p&gt;では、どうすれば良いのかという話ですが、Bailey et. al.(2015)では、Minimum Backtest Lengthを計算しています。要は試行(錯誤)数&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;を増やすにつれて、バックテストの年数&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;も伸ばしていけよと戒めているわけです。&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;とMinimum Backtest Lengthの関係性を示していきましょう。先ほどと同じく&lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt;を仮定しますが、&lt;span class=&#34;math inline&#34;&gt;\(y\neq 1\)&lt;/span&gt;であるケースを考えます。年率化シャープレシオの最大統計量の期待値は(2)式より、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[\max[\hat{SR}(q)]_N] \approx y^{-1/2}((1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}])
\]&lt;/span&gt;
となります。これを&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;に対して解いてやることでMinBTLが求まります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MinBTL \approx (\frac{(1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}]}{\bar{E[\max[\hat{SR}(q)]_N]}})^2
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(\bar{E[\max[\hat{SR}(q)]_N]}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(E[\max[\hat{SR}(q)]_N]\)&lt;/span&gt;の上限値で、シャープレシオの真値が0である&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;戦略でシャープレシオの最大統計量が取りうる値を抑えます。その際に、必要なバックテスト年数&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;がMinBTLとして導出されるのです。&lt;span class=&#34;math inline&#34;&gt;\(\bar{E[\max[\hat{SR}(q)]_N]}=1\)&lt;/span&gt;として、MinBTLを&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;の関数としてプロットしたものが以下です。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MinBTL &amp;lt;- function(N,MaxSR){
  return((ExMaxSR(N)/MaxSR)^2)
}
N = list(1:100)
result = purrr::map2(N,1,MinBTL)
ggplot2::ggplot(data.frame(MinBTL = unlist(result),N = unlist(N)),aes(x=N,y=MinBTL)) +
  geom_line(size=1) + ylim(0,6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simSR &amp;lt;- function(T1){
    r = rnorm(T1)
    return(mean(r)/sd(r))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;仮にバックテスト年数が3年以内しかできない場合は試行(錯誤)回数&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;はほぼ1回に抑えないといけないことになります。3年以内の場合は一発で当ててねという厳しめの制約です。注意しないといけないのは、MinBTLの範囲内でバックテストを行っていたとしてもオーバーフィットすることは考えられるということです。つまり、MinBTLは必要条件であって十分条件でないというわけです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;終わりに&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. 終わりに&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257497&#34;&gt;López de Prado(2018)&lt;/a&gt;では、オーバーフィッティングを防ぐ汎用的な手段として以下が挙げられています。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Develop models for entire asset classes or investment universes, rather than for specific securities. Investors diversify, hence they do not make mistake X only on security Y. If you find mistake X only on security Y, no matter how apparently profitable, it is likely a false discovery.
(拙訳：特定の有価証券ではなく、アセットクラス全体またはユニバース全体のモデルを開発すること。投資家はリスクを分散させているので、彼らはある証券Yだけに対してミスXをすることはありません。あなたが証券YだけにミスXを見つけた場合は、それがどんなに明らかに有益であっても、誤発見である可能性が高い。)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apply bagging as a means to both prevent overfitting and reduce the variance of the forecasting error. If bagging deteriorates the performance of a strategy, it was likely overfit to a small number of observations or outliers.
(拙訳：オーバーフィットを防ぎ、予測誤差の分散を減らすための手段として、バギングを適用すること。バギングが戦略のパフォーマンスを悪化させる場合、それは少数の観測値または外れ値にオーバーフィットした可能性が高い。)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Do not backtest until all your research is complete.&lt;/strong&gt;
(拙訳：&lt;strong&gt;すべてのリサーチが完了するまでバックテストをしないこと。&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Record every backtest conducted on a dataset so that the probability of backtest overfitting may be estimated on the final selected result (see &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2326253&#34;&gt;Bailey, Borwein, López de Prado and Zhu(2017)&lt;/a&gt;), and the Sharpe ratio may be properly deflated by the number of trials carried out (&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2465675&#34;&gt;Bailey and López de Prado(2014.b)&lt;/a&gt;).
(拙訳：研究者が最終的に選択したバックテスト結果がオーバーフィットしている確率を推定できるように、単一の(同じ)データセットで実施されたバックテストをすべて記録すること（Bailey, Borwein, López de Prado and Zhu [2017]）、また、実施された試行数によってシャープレシオを適切にデフレーションできるようにすること（Bailey and López de Prado [2014]）。)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simulate scenarios rather than history. A standard backtest is a historical simulation, which can be easily overfit. History is just the random path that was realized, and it could have been entirely different. Your strategy should be profitable under a wide range of scenarios, not just the anecdotal historical path. It is harder to overfit the outcome of thousands of “what if” scenarios.
(拙訳：ヒストリカルではなくシナリオをシミュレーションすること。標準的なバックテストはヒストリカルシミュレーションであり、オーバーフィットしやすい。歴史(これまでの実績)はランダムなパスの実現値に過ぎず、全く違ったものになっていた可能性があります。あなたの戦略は、逸話的なヒストリカルパスではなく、様々なシナリオの下で利益を得ることができるものであるべきです。何千もの「もしも」のシナリオ結果をオーバーフィットさせるのは(ヒストリカルシミュレーションで過学習するよりも)より難しいことです。)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do not research under the influence of a backtest. If the backtest fails to identify a profitable strategy,
start from scratch. Resist the temptation of reusing those results.
(拙訳：バックテストのフィードバックを受けてリサーチしないこと。バックテストが有益な戦略を見つけ出すことに失敗した場合は、ゼロからリサーチを再始動してください。それらの結果を再利用する誘惑に抗ってください。)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;3と6は本日の論文と関係のある文脈だと思います。この分野は他にも研究の蓄積があるので、業務でバックテストを行うという人は運用手法の勉強もいいですが、そもそものお作法としてバックテストの正しい運用方法について学ぶことをお勧めします。&lt;br /&gt;
さて、いつもとは違う観点で、少しメタ的なトピックに取り組んでみました。自分自身仕事柄バックテスト結果などを見ることも多いですし、このブログでもしばしばhold-out法でのバックテストをしています。得られた結果の不確実性を理解して、評価できるよう今後もこのトピックの研究を追っていきたいと思います。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CNNを使って馬体写真から順位予想してみた</title>
      <link>/post/post18/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/post18/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#データ収集のためのクローリング&#34;&gt;1. データ収集のためのクローリング&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#データをどこから取得するか&#34;&gt;データをどこから取得するか&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#seleniumでクローリングを実行する&#34;&gt;seleniumでクローリングを実行する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#kerasを用いてcnnを学習させる&#34;&gt;2. &lt;code&gt;Keras&lt;/code&gt;を用いてCNNを学習させる&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#kerasとは&#34;&gt;Kerasとは？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cnnとは&#34;&gt;CNNとは？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#コーディング&#34;&gt;コーディング&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#不均衡データ調整のためのアンダーサンプリング&#34;&gt;不均衡データ調整のためのアンダーサンプリング&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#shap値を用いた結果解釈&#34;&gt;3. Shap値を用いた結果解釈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#最後に&#34;&gt;4. 最後に&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。今回は競馬予想についての記事を書きたいと思います。前回、&lt;code&gt;LightGBM&lt;/code&gt;を用いてyahoo競馬から取得したレース結果データ(テーブルデータ)を用いて、競馬順位予想モデルを作成しました。前回は構造データを用いましたが、このご時世ですからこんな分析は誰にでもできるわけです。時代は非構造データ、というわけで今回は馬体画像から特徴量を抽出し、順位予想を行う畳み込みニューラルネットワーク(&lt;code&gt;Convolutional Neural Network&lt;/code&gt;, CNN)を作成してみました。画像解析は&lt;code&gt;Earth Engine&lt;/code&gt;を用いた衛星画像の解析に続いて2回目、深層学習はこのブログでは初めてと言うことになります。なお、Pythonを使用しています。&lt;/p&gt;
&lt;div id=&#34;データ収集のためのクローリング&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. データ収集のためのクローリング&lt;/h2&gt;
&lt;p&gt;まず、馬体画像をネットから収集することから始めます。1番良いのはレース当日のパドックの写真を使用することでしょう。ただ、パドックの写真をまとまった形で掲載してくれているサイトは調べた限りは存在しませんでした。もしかしたら、Youtubeに競馬ファンの方がパドック動画を上げていらっしゃるかも知れませんので、それを画像に切り抜いて使う or 動画としてCNN→RNNの&lt;code&gt;Encoder-Decoder&lt;/code&gt;モデルに適用すると面白いかもしれません。しかし、そこまでの能力は今の自分にはありません。&lt;/p&gt;
&lt;div id=&#34;データをどこから取得するか&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;データをどこから取得するか&lt;/h3&gt;
&lt;p&gt;そこで、今回は&lt;a href=&#34;https://www.daily.co.jp/horse/horsecheck/photo/&#34;&gt;デイリーのWebサイト&lt;/a&gt;からデータを取得しています。ここには直近1年間?のG1レースに出馬する競走馬のレース前の馬体写真が掲載されています。実際のレース場へ行けない馬券師さんたちはこの写真を見て馬の状態を分析していると思われます。&lt;br /&gt;
なお、このサイトには出馬全頭の馬体写真が掲載されているわけではありません。また、G1の限られたレースのみですので、そもそも全ての馬が仕上がっている可能性もあり、差がつかないことも十分予想されます。ただ、手っ取り早くやってみることを優先し、今回はこのデータを使用することにしたいと思います。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;seleniumでクローリングを実行する&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;seleniumでクローリングを実行する&lt;/h3&gt;
&lt;p&gt;クローリングにはseleniumを使用します。今回はCNNがメインなのでWebクローリングについては説明しません。使用したコードは以下です。&lt;br /&gt;
【注意】以下のコードを使用される場合は自己責任でお願いします。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.action_chains import ActionChains
from time import sleep
from urllib import request
import random

# seleniumのオプション設定（おまじない）
options = Options()
options.add_argument(&amp;#39;--disable-gpu&amp;#39;);
options.add_argument(&amp;#39;--disable-extensions&amp;#39;);
options.add_argument(&amp;#39;--proxy-server=&amp;quot;direct://&amp;quot;&amp;#39;);
options.add_argument(&amp;#39;--proxy-bypass-list=*&amp;#39;);
options.add_argument(&amp;#39;--start-maximized&amp;#39;);

# driver指定
DRIVER_PATH = r&amp;#39;C:/Users/aashi/Desktop/chromedriver_win32/chromedriver.exe&amp;#39;
driver = webdriver.Chrome(executable_path=DRIVER_PATH, chrome_options=options)

# urlを渡し、サイトへアクセス
url = &amp;#39;https://www.daily.co.jp/horse/horsecheck/photo/&amp;#39;
driver.get(url)
driver.implicitly_wait(15) # オブジェクトのロード待ちの最大時間でこれを越えるとエラー
sleep(5) # webページの遷移を行うので1秒sleep

# 各レース毎に画像データ保存
selector0 = &amp;quot;body &amp;gt; div &amp;gt; main &amp;gt; div &amp;gt; div.primaryContents &amp;gt; article &amp;gt; div &amp;gt; section &amp;gt; a&amp;quot;
elements = driver.find_elements_by_css_selector(selector0)
for i in range(0,len(elements)):
  elements = driver.find_elements_by_css_selector(selector0)
  element = elements[i]
  element.click()
  sleep(5) # webページの遷移を行うので5秒sleep

  target = driver.find_element_by_link_text(&amp;#39;Ｇ１馬体診断写真集のTOP&amp;#39;)
  actions = ActionChains(driver)
  actions.move_to_element(target)
  actions.perform()
  sleep(5) # webページの遷移を行うので5秒sleep
  selector = &amp;quot;body &amp;gt; div.wrapper.horse.is-fixedHeader.is-fixedAnimation &amp;gt; main &amp;gt; div &amp;gt; div.primaryContents &amp;gt; article &amp;gt; article &amp;gt; div.photoDetail-wrapper &amp;gt; section &amp;gt; div &amp;gt; figure&amp;quot;
  figures = driver.find_elements_by_css_selector(selector)
  download_dir = r&amp;#39;C:\Users\aashi\umanalytics\photo\image&amp;#39;
  selector = &amp;quot;body &amp;gt; div &amp;gt; main &amp;gt; div &amp;gt; div.primaryContents &amp;gt; article &amp;gt; article &amp;gt; div.photoDetail-wrapper &amp;gt; section &amp;gt; h1&amp;quot;
  race_name = driver.find_element_by_css_selector(selector).text
  for figure in figures:
    img_name = figure.find_element_by_tag_name(&amp;#39;figcaption&amp;#39;).text
    horse_src = figure.find_element_by_tag_name(&amp;#39;img&amp;#39;).get_attribute(&amp;quot;src&amp;quot;)    
    save_name = download_dir + &amp;#39;/&amp;#39; + race_name + &amp;#39;_&amp;#39; + img_name + &amp;#39;.jpg&amp;#39;
    request.urlretrieve(horse_src,save_name)
  driver.back()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;保存した画像を実際のレース結果と突合し、手作業で上位3位以内グループとそれ以外のグループに分けました。以下のような感じで画像が保存されています。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;horse_photo.PNG&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;保存された馬体画像&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;これで元データの収集が完了しました。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kerasを用いてcnnを学習させる&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. &lt;code&gt;Keras&lt;/code&gt;を用いてCNNを学習させる&lt;/h2&gt;
&lt;div id=&#34;kerasとは&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Kerasとは？&lt;/h3&gt;
&lt;p&gt;さて、次に&lt;code&gt;Keras&lt;/code&gt;を使ってCNNを学習させましょう。まず、&lt;code&gt;Keras&lt;/code&gt;とは&lt;code&gt;Tensorflow&lt;/code&gt;や&lt;code&gt;Theano&lt;/code&gt;上で動く&lt;code&gt;Neural Network&lt;/code&gt;ライブラリの1つです。&lt;code&gt;Keras&lt;/code&gt;は比較的短いコードでモデルを組むことができ、また学習アルゴリズムが多いことが特徴のようです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cnnとは&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;CNNとは？&lt;/h3&gt;
&lt;p&gt;CNNは画像解析を行う際によく使用される&lt;code&gt;(Deep) Neural Network&lt;/code&gt;の1種で、その名の通り&lt;code&gt;Convolution&lt;/code&gt;(畳み込み)を追加した物となっています。畳み込みとは以下のような処理のことを言います。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://cdn-ak.f.st-hatena.com/images/fotolife/t/tdualdir/20180501/20180501211957.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;畳み込み層の処理&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;ここのインプットとは画像データのことです。画像解析では画像を数値として認識し、解析を行います。コンピュータ上の画像は&lt;code&gt;RGB&lt;/code&gt;値という、赤(Red)、緑(Green)、青(Blue)の3色の0~255までの数値の強弱で表現されています。赤255、緑0、青0といった形で3層のベクトルになっており、この場合完全な赤が表現されます。上図の場合、a,b,cなどが各ピクセルの&lt;code&gt;RGB&lt;/code&gt;値のいずれかを表していると考えることができます。畳み込みはこの&lt;code&gt;RGB&lt;/code&gt;値をカーネルと呼ばれる行列との内積をとることで画像の特徴量を計算します。畳み込み層の意味は以下の動画がわかりやすいです。&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/vU-JfZNBdYU&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;カーネルを上手くその画像の特徴的な部分を取得できるように学習することで、画像の識別が可能になります。畳み込み層はCNNの最重要部分だと思います。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://th.bing.com/th/id/OIP.F2Ik_XFzmu5jZF-byiAKQQHaCg?w=342&amp;amp;h=118&amp;amp;c=7&amp;amp;o=5&amp;amp;dpr=1.25&amp;amp;pid=1.7&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;CNNの全体像&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;上図のようにCNNは畳み込み意外にももちろん入力層や出力層など通常の&lt;code&gt;Neural Network&lt;/code&gt;と同じ層も持っています。なお、&lt;code&gt;MaxPooling&lt;/code&gt;層について知りたい人は以下の動画を参照されてください。&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/MLixg9K6oeU&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;深層学習の学習方法については最急下降法(勾配法)がオーソドックスなものとして知られていますが、&lt;code&gt;Adam&lt;/code&gt;など色々な拡張アルゴリズムが提案されています。基本的には、&lt;code&gt;Adam&lt;/code&gt;は&lt;code&gt;momentum&lt;/code&gt;を使用することが多いでしょうか。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;コーディング&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;コーディング&lt;/h3&gt;
&lt;p&gt;では、実際にコーディングしていきます。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.convolutional import MaxPooling2D
from keras.layers import Activation, Conv2D, Flatten, Dense,Dropout
from sklearn.model_selection import train_test_split
from keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop, Nadam
from PIL import Image
import numpy as np
import glob
import matplotlib.pyplot as plt
import time
import os&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;まず最初に収集してきた画像データを数値データに変換し学習データを作成します。
ディレクトリ構造は以下のようになっており、上位画像とその他画像が別ディレクトリに保存されています。各ディレクトリから画像を読み込む際に、上位画像には1、その他には0というカテゴリ変数を与えます。&lt;/p&gt;
&lt;p&gt;馬体写真&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上位&lt;/li&gt;
&lt;li&gt;その他&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#フォルダを指定
folders = os.listdir(r&amp;quot;C:\Users\aashi\umanalytics\photo\image&amp;quot;)
#画総数を指定(今回は50×50×3)。
image_size = 300
dense_size = len(folders)

X = []
Y = []

#それぞれのフォルダから画像を読み込み、Image関数を使用してRGB値ベクトル(numpy array)へ変換
for i, folder in enumerate(folders):
  files = glob.glob(&amp;quot;C:/Users/aashi/umanalytics/photo/image/&amp;quot; + folder + &amp;quot;/*.jpg&amp;quot;)
  index = i
  for k, file in enumerate(files):
    image = Image.open(file)
    image = image.convert(&amp;quot;L&amp;quot;).convert(&amp;quot;RGB&amp;quot;)
    image = image.resize((image_size, image_size)) #画素数を落としている
 
    data = np.asarray(image)
    X.append(data)
    Y.append(index)

X = np.array(X)
Y = np.array(Y)
X = X.astype(&amp;#39;float32&amp;#39;)
X = X / 255.0 # 0~1へ変換
X.shape
Y = np_utils.to_categorical(Y, dense_size)

#訓練データとテストデータへ変換
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;訓練データとテストデータの分割ができました。今考えているのは「上位」と「その他」の2値分類となっていますが、「上位」を3位以内と定義したので不均衡なデータとなっています(その他データが上位データの5倍くらい)。こういった場合、そのままのデータで学習をするとサンプルサイズが多い方のラベル(この場合「その他」)を予測しやすくなり、バイアスのあるモデルとなります。よって、学習データは2クラスそれぞれが同じサンプルサイズとなるよう調整してやる必要があります。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;index_zero = np.random.choice(np.array(np.where(y_train[:,1]==0))[0,],np.count_nonzero(y_train[:,1]==1),replace=False)
index_one = np.array(np.where(y_train[:,1]==1))[0]
y_resampled = y_train[np.hstack((index_one,index_zero))]
X_resampled = X_train[np.hstack((index_one,index_zero))]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;学習データにはこの&lt;code&gt;y_resampled&lt;/code&gt;と&lt;code&gt;X_resampled&lt;/code&gt;を使用します。次に、CNNを構築していきます。&lt;code&gt;Keras&lt;/code&gt;では、&lt;code&gt;sequential model&lt;/code&gt;を指定し、&lt;code&gt;add&lt;/code&gt;メソッドで層を追加して行くことでモデルを定義します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = Sequential()
model.add(Conv2D(32, (3, 3), padding=&amp;#39;same&amp;#39;,input_shape=X_train.shape[1:]))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(Conv2D(32, (3, 3)))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), padding=&amp;#39;same&amp;#39;))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(Conv2D(64, (3, 3)))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(Dropout(0.5))
model.add(Dense(dense_size))
model.add(Activation(&amp;#39;softmax&amp;#39;))

model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model: &amp;quot;sequential&amp;quot;
## _________________________________________________________________
## Layer (type)                 Output Shape              Param #   
## =================================================================
## conv2d (Conv2D)              (None, 300, 300, 32)      896       
## _________________________________________________________________
## activation (Activation)      (None, 300, 300, 32)      0         
## _________________________________________________________________
## conv2d_1 (Conv2D)            (None, 298, 298, 32)      9248      
## _________________________________________________________________
## activation_1 (Activation)    (None, 298, 298, 32)      0         
## _________________________________________________________________
## max_pooling2d (MaxPooling2D) (None, 149, 149, 32)      0         
## _________________________________________________________________
## dropout (Dropout)            (None, 149, 149, 32)      0         
## _________________________________________________________________
## conv2d_2 (Conv2D)            (None, 149, 149, 64)      18496     
## _________________________________________________________________
## activation_2 (Activation)    (None, 149, 149, 64)      0         
## _________________________________________________________________
## conv2d_3 (Conv2D)            (None, 147, 147, 64)      36928     
## _________________________________________________________________
## activation_3 (Activation)    (None, 147, 147, 64)      0         
## _________________________________________________________________
## max_pooling2d_1 (MaxPooling2 (None, 73, 73, 64)        0         
## _________________________________________________________________
## dropout_1 (Dropout)          (None, 73, 73, 64)        0         
## _________________________________________________________________
## flatten (Flatten)            (None, 341056)            0         
## _________________________________________________________________
## dense (Dense)                (None, 512)               174621184 
## _________________________________________________________________
## activation_4 (Activation)    (None, 512)               0         
## _________________________________________________________________
## dropout_2 (Dropout)          (None, 512)               0         
## _________________________________________________________________
## dense_1 (Dense)              (None, 2)                 1026      
## _________________________________________________________________
## activation_5 (Activation)    (None, 2)                 0         
## =================================================================
## Total params: 174,687,778
## Trainable params: 174,687,778
## Non-trainable params: 0
## _________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、学習パートに入ります。アルゴリズムには&lt;code&gt;Adadelta&lt;/code&gt;を使用します。よくわかってないんですけどね。。。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;optimizers =&amp;quot;Adadelta&amp;quot;
results = {}
epochs = 50
model.compile(loss=&amp;#39;categorical_crossentropy&amp;#39;, optimizer=optimizers, metrics=[&amp;#39;accuracy&amp;#39;])
results = model.fit(X_resampled, y_resampled, validation_split=0.2, epochs=epochs)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;不均衡データ調整のためのアンダーサンプリング&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;不均衡データ調整のためのアンダーサンプリング&lt;/h3&gt;
&lt;p&gt;ここから、Testデータで2値分類を行うのですが、学習データをアンダーサンプリングしているので、予測確率を計算する際にアンダーサンプリングを行ったサンプル選択バイアスが生じてしまいます。論文は&lt;a href=&#34;https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf&#34;&gt;こちら&lt;/a&gt;。
よって、補正が必要になるのですがこの部分の定式化をここでしておきたいと思います。現在行っている2値分類問題を説明千数&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;から2値を取る目的変数&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;を予測する問題と表現することにします。データセット&lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;は正例が負例よりもかなり少なく、負例のサンプルサイズを正例に合わせたデータセットを&lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt;とします。ここで、&lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;のサンプル組が&lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt;にも含まれる場合に1を取り、含まれない場合に0をとるカテゴリ変数&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;を定義します。
データセット&lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;を用いて構築したモデルに説明変数&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;を与えた時、正例と予測する条件付き確率は&lt;span class=&#34;math inline&#34;&gt;\(P(y=1|x)\)&lt;/span&gt;で表すことができます。一方、&lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt;を用いて構築したモデルで正例を予測する条件付き確率はベイズの定理とカテゴリ変数&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;を用いて、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(y=1|x,s=1) = \frac{P(s=1|y=1)P(y=1|x)}{P(s=1|y=1)P(y=1|x) + P(s=1|y=0)P(y=0|x)}
\]&lt;/span&gt;
と書けます。&lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt;は負例のサンプルサイズを正例に合わせているため、&lt;span class=&#34;math inline&#34;&gt;\(P(s=1,y=1)=1\)&lt;/span&gt;であるので上式は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(y=1|x,s=1) = \frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)P(y=0|x)}
= \frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)(1-P(y=1|x))}
\]&lt;/span&gt;
と書き換えることができます。&lt;span class=&#34;math inline&#34;&gt;\(P(s=1|y=0)\neq0\)&lt;/span&gt;であることは&lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt;の定義より自明です(0だと正例しかない不均衡データになる)。よって、&lt;span class=&#34;math inline&#34;&gt;\(P(y=0,x)\neq0\)&lt;/span&gt;である限り、アンダーサンプリングのモデルが正例とはじき出す確率は元のデータセットが出す確率に対して正のバイアスがあることがわかります。求めたいのはバイアスのない&lt;span class=&#34;math inline&#34;&gt;\(P(y=1|x)\)&lt;/span&gt;なので&lt;span class=&#34;math inline&#34;&gt;\(P=P(y=1|x),P_s=P(y|x,s=1),\beta=P(s=1,y=0)\)&lt;/span&gt;とすると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P = \frac{\beta P_s}{\beta P_s-P_s+1}
\]&lt;/span&gt;
とかけ、この関係式を用いてバイアスを補正することができます。
今確認したことを関数として定義しましょう。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def calibration(y_proba, beta):
    return y_proba / (y_proba + (1 - y_proba) / beta)

sampling_rate = sum(y_train[:,1]) / sum(1-y_train[:,1])
y_proba_calib = calibration(model.predict(X_test), sampling_rate)
y_pred = np_utils.to_categorical(np.argmax(y_proba_calib,axis=1), dense_size)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
score = accuracy_score(y_test, y_pred)
print(&amp;#39;Test accuracy:&amp;#39;, score)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Test accuracy: 0.288135593220339&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;まったく良くない結果です。&lt;code&gt;ConfusionMatrix&lt;/code&gt;を出してみたところどうやらうまくいっていないことがわかりました。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ConfusionMatrixDisplay(confusion_matrix(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))).plot()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000004939E748&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不均衡データのバイアス修正はしたんですが、それでもなお負値を予測しやすいモデルとなっています。これでは使えないですね。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;shap値を用いた結果解釈&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Shap値を用いた結果解釈&lt;/h2&gt;
&lt;p&gt;今学習したモデルの&lt;code&gt;shap&lt;/code&gt;値を考え、結果の解釈をしたいと思います。&lt;code&gt;shap&lt;/code&gt;値については時間があれば、説明を追記したいと思います。簡単に言えば、CNNが画像のどの部分に特徴を捉え、馬が上位に入るかを予想したかを可視化で捉えることができます。この馬の解析をすることにします。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.imshow(X_test[0])
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shap
background = X_train[np.random.choice(X_train.shape[0],100,replace=False)]

e = shap.GradientExplainer(model,background)

shap_values = e.shap_values(X_test[[0]])
shap.image_plot(shap_values[1],X_test[[0]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;非常に微妙ですが、足や臀部などを評価しているようにみえます。ですが、背景に反応しているようにも見えるので馬体のみ取り出すトリミングをやる必要がありますね。これは物体検知のモデルを構築する必要がありそうです。また、今度の機会に考えます。
各層において画像のどの側面を捉えているかを可視化してみたいと思います。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from keras import models

layer_outputs = [layer.output for layer in model.layers[:8]]
layer_names = []
for layer in model.layers[:8]:
    layer_names.append(layer.name)
images_per_row = 16

activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
activations = activation_model.predict(X_train[[0]])

for layer_name, layer_activation in zip(layer_names, activations):
    n_features = layer_activation.shape[-1]

    size = layer_activation.shape[1]

    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))

    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0,
                                             :, :,
                                             col * images_per_row + row]
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype(&amp;#39;uint8&amp;#39;)
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, cmap=&amp;#39;viridis&amp;#39;)
    plt.show()
    plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-3.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-4.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-5.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-6.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-7.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-8.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-9.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;自分が未熟なこともあり、解釈が難しいですね。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;最後に&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. 最後に&lt;/h2&gt;
&lt;p&gt;正直まったく上手くいっていません。やはり馬体から順位予測をするのは難しいのでしょうか。ほかの変数と掛け合わせると結果が変わったりするのでしょうか。今のままだとよい特徴量を抽出することができていないように思います。
Youtubeからパドック動画を取得して、&lt;code&gt;Encoder-Decoder&lt;/code&gt;モデルで解析するところまでやらないとうまくいかないんですかね。自分の実力が十分上がれば是非やってみたいと思います(いつになることやら)。それまでには、PCのスペックを上げないといけません。定額給付金を使うかな。。。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>WindowsにNEologd辞書をインストールして、RMeCabを実行する方法</title>
      <link>/post/post17/</link>
      <pubDate>Sun, 19 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/post17/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mecabrmecabとは&#34;&gt;1. Mecab(RMeCab)とは？&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#固有名詞に弱いデフォルトのmecab&#34;&gt;固有名詞に弱いデフォルトのMeCab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#新語に強いneologd辞書&#34;&gt;新語に強いNEologd辞書&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#インストール手順&#34;&gt;2. インストール手順&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a.-windows-subsystem-for-linuxwslのインストール&#34;&gt;A. &lt;code&gt;Windows Subsystem for Linux&lt;/code&gt;(WSL)のインストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#b.-ubuntu-linuxのインストール&#34;&gt;B. Ubuntu Linuxのインストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#c.-ubuntu-linuxにmecabをインストール&#34;&gt;C. Ubuntu LinuxにMeCabをインストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#d.-ubuntu-for-linuxにneologd辞書をインストール&#34;&gt;D. Ubuntu for Linuxに&lt;code&gt;NEologd&lt;/code&gt;辞書をインストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#e.-ubuntu-for-linuxからwindowsへ辞書ファイルをコピー&#34;&gt;E. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#f.-windows内で辞書ファイルのコンパイルshift-jisを行う&#34;&gt;F. Windows内で辞書ファイルのコンパイル(&lt;code&gt;SHIFT-JIS&lt;/code&gt;)を行う。&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#まとめ&#34;&gt;3. まとめ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;皆さんおはこんばんにちは。
最近在宅勤務で運動不足ですが、平日休日問わず研究活動をしています。
学術誌投稿を目指したBlog記事には書けない内容なので、更新はストップしてしまっていますがちゃんと活動はしています。&lt;/p&gt;
&lt;p&gt;今回はその研究の中で利用しているMeCabにまつわるTipsのご紹介です。MeCabというと形態素解析ソフトということはこのブログを読まれている方はお分かりかと思いますが、その辞書にNEologd辞書を使用できるようにしてみたというのが内容です。MeCabってなに？という方もいらっしゃるかもしれませんので、かなり簡単にご紹介します。&lt;/p&gt;
&lt;div id=&#34;mecabrmecabとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Mecab(RMeCab)とは？&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;MeCab&lt;/code&gt;は日本語テキストマイニングで使用される形態素解析ソフトです。英語などの言語とは異なり、日本語は単語毎にスペースを置かないため、テキストマイニングを行う際、そのままでは文章を単語単位に区切って集計するといったことができません。例えば、「これはペンです。」という文章はそのままでは「これはペンです。」という&lt;em&gt;1つ&lt;/em&gt;の単語として認識されてしまいます。ですが、単語の頻度分析を行う際などは、「これ/は/ペン/です/。」という風に文章を単語レベルにまで分割し、「ペン」という特徴量を取得したいわけです。それができるのが&lt;code&gt;RMeCab&lt;/code&gt;で、この処理は形態素解析と呼ばれます。&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;そして、&lt;code&gt;RMeCab&lt;/code&gt;とはこの&lt;code&gt;MeCab&lt;/code&gt;のラッパーになります。&lt;code&gt;R&lt;/code&gt;から&lt;code&gt;MeCab&lt;/code&gt;を使用するには&lt;code&gt;RMeCab&lt;/code&gt;を使用する必要があります。使用方法は簡単で、&lt;code&gt;RMeCabC&lt;/code&gt;関数に文章を渡すだけです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
library(RMeCab)

RMeCabC(&amp;quot;これはペンです。&amp;quot;) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   名詞   助詞   名詞 助動詞   記号 
## &amp;quot;これ&amp;quot;   &amp;quot;は&amp;quot; &amp;quot;ペン&amp;quot; &amp;quot;です&amp;quot;   &amp;quot;。&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;RMeCabC&lt;/code&gt;関数は結果をリストで返してくるので、&lt;code&gt;unlist&lt;/code&gt;でcharacterに変換しています。&lt;/p&gt;
&lt;div id=&#34;固有名詞に弱いデフォルトのmecab&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;固有名詞に弱いデフォルトのMeCab&lt;/h3&gt;
&lt;p&gt;便利なように思えるのですが、デフォルトの&lt;code&gt;MeCab&lt;/code&gt;は固有名詞に弱いという弱点があります。例えば、「欅坂46が赤いきつねを食べている。」という文章を形態素解析してみましょう。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;欅坂46が赤いきつねを食べている。&amp;quot;) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     名詞     名詞     名詞     助詞   形容詞     名詞     助詞     動詞 
##     &amp;quot;欅&amp;quot;     &amp;quot;坂&amp;quot;     &amp;quot;46&amp;quot;     &amp;quot;が&amp;quot;   &amp;quot;赤い&amp;quot; &amp;quot;きつね&amp;quot;     &amp;quot;を&amp;quot;   &amp;quot;食べ&amp;quot; 
##     助詞     動詞     記号 
##     &amp;quot;て&amp;quot;   &amp;quot;いる&amp;quot;     &amp;quot;。&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;予想していた結果は「欅坂46/が/赤いきつね/を/食べ/て/いる/。」でしょう。ただ、固有名詞を上手く形態素解析することができていないため、不必要なところで分割がなされており、「赤い/きつね/を/食べ」という部分については「赤い（動物の）きつね」を食べているかのような解析結果になっています。赤いきつねの「きつね」と動物の「きつね」を同等に扱ってしまうので、問題があります。また、経済においても以下のように日経平均株価が分割され、新聞社の「日経」と株価の「日経」が、大統領の「トランプ」とカードの「トランプ」が区別できないといったことが想定されます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;今週の日経平均株価は、上値抵抗線を突破して上昇！&amp;quot;) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   名詞   助詞   名詞   名詞   名詞   助詞   記号   名詞   名詞   名詞   助詞 
## &amp;quot;今週&amp;quot;   &amp;quot;の&amp;quot; &amp;quot;日経&amp;quot; &amp;quot;平均&amp;quot; &amp;quot;株価&amp;quot;   &amp;quot;は&amp;quot;   &amp;quot;、&amp;quot; &amp;quot;上値&amp;quot; &amp;quot;抵抗&amp;quot;   &amp;quot;線&amp;quot;   &amp;quot;を&amp;quot; 
##   名詞   動詞   助詞   名詞   記号 
## &amp;quot;突破&amp;quot;   &amp;quot;し&amp;quot;   &amp;quot;て&amp;quot; &amp;quot;上昇&amp;quot;   &amp;quot;！&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;トランプ政権による経済活動再開の指針発表が評価される&amp;quot;) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       名詞       名詞       助詞       名詞       名詞       名詞       助詞 
## &amp;quot;トランプ&amp;quot;     &amp;quot;政権&amp;quot;   &amp;quot;による&amp;quot;     &amp;quot;経済&amp;quot;     &amp;quot;活動&amp;quot;     &amp;quot;再開&amp;quot;       &amp;quot;の&amp;quot; 
##       名詞       名詞       助詞       名詞       動詞       動詞 
##     &amp;quot;指針&amp;quot;     &amp;quot;発表&amp;quot;       &amp;quot;が&amp;quot;     &amp;quot;評価&amp;quot;       &amp;quot;さ&amp;quot;     &amp;quot;れる&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これはデフォルトで&lt;code&gt;MeCab&lt;/code&gt;が使用している&lt;em&gt;辞書&lt;/em&gt;に原因があります。IPA辞書(&lt;code&gt;ipadic&lt;/code&gt;)と呼ばれる物で、奈良先端科学技術大学院大学が公開している茶筌と呼ばれる形態素解析ソフト用に作られました。&lt;a href=&#34;https://ja.osdn.net/projects/ipadic/releases/&#34;&gt;こちら&lt;/a&gt;を見ると辞書の更新は2007年でストップしており、新語や流行語がアップデートされていないために上記の固有名詞が上手く形態素解析できないことがわかります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;新語に強いneologd辞書&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;新語に強いNEologd辞書&lt;/h3&gt;
&lt;p&gt;最近&lt;code&gt;MeCab&lt;/code&gt;でよく使用されている辞書に&lt;code&gt;NEologd&lt;/code&gt;辞書というものがあります。&lt;code&gt;NEologd&lt;/code&gt;辞書とは、Web上から得た新語に対応しており、頻繁に更新される&lt;code&gt;MeCab&lt;/code&gt;用のシステム辞書です。&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;Twitterのアカウントには、&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;特色は語彙の多さと更新頻度、新語の採録の速さ、読み仮名の正確さ、表記揺れへの対応。おもな解析対象はWeb上のニュース記事や流行した出来事。
おもな用途は文書分類、文書ベクトル作成、単語埋め込みベクトル作成、読み仮名付与。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;と記載されており、上述したIPA辞書の弱点を補完する辞書となっています。今回の記事はその辞書のインストール方法についてですが、インストールした辞書の威力を先にお見せしておきます。実行するには、&lt;code&gt;RMeCab&lt;/code&gt;関数に辞書ファイル(.dicファイル)のパスを渡してやれば良いです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dic_directory &amp;lt;- &amp;quot;C:\\hogehoge\\mecab-user-dict-seed.yyyymmdd.dic&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;欅坂46が赤いきつねを食べている。&amp;quot;,dic=dic_directory) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         名詞         助詞         名詞         助詞         動詞         助詞 
##     &amp;quot;欅坂46&amp;quot;         &amp;quot;が&amp;quot; &amp;quot;赤いきつね&amp;quot;         &amp;quot;を&amp;quot;       &amp;quot;食べ&amp;quot;         &amp;quot;て&amp;quot; 
##         動詞         記号 
##       &amp;quot;いる&amp;quot;         &amp;quot;。&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;今週の日経平均株価は、上値抵抗線を突破して上昇！&amp;quot;,dic=dic_directory) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           名詞           助詞           名詞           助詞           記号 
##         &amp;quot;今週&amp;quot;           &amp;quot;の&amp;quot; &amp;quot;日経平均株価&amp;quot;           &amp;quot;は&amp;quot;           &amp;quot;、&amp;quot; 
##           名詞           名詞           名詞           助詞           名詞 
##         &amp;quot;上値&amp;quot;         &amp;quot;抵抗&amp;quot;           &amp;quot;線&amp;quot;           &amp;quot;を&amp;quot;         &amp;quot;突破&amp;quot; 
##           動詞           助詞           名詞           記号 
##           &amp;quot;し&amp;quot;           &amp;quot;て&amp;quot;         &amp;quot;上昇&amp;quot;           &amp;quot;！&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;トランプ政権による経済活動再開の指針発表が評価される&amp;quot;,dic=dic_directory) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           名詞           助詞           名詞           名詞           助詞 
## &amp;quot;トランプ政権&amp;quot;       &amp;quot;による&amp;quot;     &amp;quot;経済活動&amp;quot;         &amp;quot;再開&amp;quot;           &amp;quot;の&amp;quot; 
##           名詞           名詞           助詞           名詞           動詞 
##         &amp;quot;指針&amp;quot;         &amp;quot;発表&amp;quot;           &amp;quot;が&amp;quot;         &amp;quot;評価&amp;quot;           &amp;quot;さ&amp;quot; 
##           動詞 
##         &amp;quot;れる&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上値抵抗線は1語カウントしてほしいところではありますが、それ以外は上手く形態素解析できていそうです。
この&lt;code&gt;NEologd&lt;/code&gt;辞書ですが、&lt;em&gt;Windowsのインストールが想定されていません&lt;/em&gt;。つまり、Windowsユーザーは直接インストールすることができないのです。Windowsユーザーは少々ややこしい手順を踏まなければなりません。今回はそのややこしい手順を解説する記事です。Linuxでインストールを行い、それをWindows環境にコピー、その後辞書ファイルを作成します。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;インストール手順&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. インストール手順&lt;/h2&gt;
&lt;div id=&#34;a.-windows-subsystem-for-linuxwslのインストール&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A. &lt;code&gt;Windows Subsystem for Linux&lt;/code&gt;(WSL)のインストール&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Windows Power Shell&lt;/code&gt;を&lt;em&gt;管理者権限&lt;/em&gt;で開き、&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を実行する。WSLをインストールすることができます。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../my_blog/post/post21_files/powershell.PNG&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;powerShellでの実行の様子&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-ubuntu-linuxのインストール&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B. Ubuntu Linuxのインストール&lt;/h3&gt;
&lt;p&gt;Microsoft Storeよりubuntuをダウンロードする。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;ubuntu.PNG&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;ubuntuの画面&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;インストールが完了したらubuntuを起動し、初期設定を完了させる(ID、パスワード)。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-ubuntu-linuxにmecabをインストール&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;C. Ubuntu LinuxにMeCabをインストール&lt;/h3&gt;
&lt;p&gt;ubuntuのコマンドプロンプトで、&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt-get update
sudo apt install mecab
sudo apt install libmecab-dev
sudo apt install mecab-ipadic-utf8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を入力し、&lt;code&gt;MeCab&lt;/code&gt;をインストール。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;ubuntumecab.PNG&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;ubuntuでmecabをインストール&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;d.-ubuntu-for-linuxにneologd辞書をインストール&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;D. Ubuntu for Linuxに&lt;code&gt;NEologd&lt;/code&gt;辞書をインストール&lt;/h3&gt;
&lt;p&gt;ubuntsuのコマンドプロンプトで&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt install make
git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git
cd mecab-ipadic-neologd
sudo bin/install-mecab-ipadic-neologd -n -a&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を実行。&lt;code&gt;NEologd&lt;/code&gt;辞書ファイルが(&lt;code&gt;/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/&lt;/code&gt;)にインストールできます。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e.-ubuntu-for-linuxからwindowsへ辞書ファイルをコピー&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;E. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー&lt;/h3&gt;
&lt;p&gt;ubuntuのコマンドプロンプトで&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;explorer.exe .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を入力。エクスプローラーが立ち上がるので、&lt;code&gt;/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/(ディレクトリ)&lt;/code&gt;をWindowsの任意のディレクトリにコピーする。完了したらUbuntuは閉じる。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;explorer.PNG&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;エクスプローラーでubuntu内部ディレクトリを確認する図&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;f.-windows内で辞書ファイルのコンパイルshift-jisを行う&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;F. Windows内で辞書ファイルのコンパイル(&lt;code&gt;SHIFT-JIS&lt;/code&gt;)を行う。&lt;/h3&gt;
&lt;p&gt;コピーしたディレクトリ内の以下のcsvを辞書ファイル(.dic)にコンパイルします。(yyyymmddは辞書の最終更新日なので変更してください)&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;-mecab-ipadic-neologd-buildmecab-ipadic-2.7.0-20070801-neologd-yyyymmdd-mecab-user-dict-seed.yyyymmdd.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;その際、元ファイルは&lt;strong&gt;エンコーディングが&lt;code&gt;UTF-8&lt;/code&gt;となっているので、&lt;code&gt;SHIFT-JIS&lt;/code&gt;へ変換することに注意&lt;/strong&gt;です。これをしないと、&lt;code&gt;RMeCab&lt;/code&gt;を実行したときに結果が文字化けします。コンパイルにはMeCabのmecab-dict-indexというバイナリファイルを使用します。自分は以下のディレクトリに存在しました。&lt;/p&gt;
&lt;p&gt;C:
-Program Files (x86)
-MeCab
-bin
-mecab-dict-index.exe&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mecab_dict_index.PNG&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;コマンドプロンプトを立ち上げ、&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;C:\Program Files (x86)\MeCab\bin\mecab-dict-index.exe -d .../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmdd(ファイルの保存場所) -u NEologd.yyyymmdd.dic -f utf-8 -t shift-jis mecab-ipadic-neologd\buildmecab\ipadic-2.7.0-20070801-neologd-yyyymmdd\mecab-user-dict-seed.yyyymmdd.csv &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を適宜変更の上、入力。&lt;code&gt;.../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmdd&lt;/code&gt;に辞書がコンパイルされ、&lt;code&gt;NEologd.yyyymmdd.dic&lt;/code&gt;ができます。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;まとめ&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. まとめ&lt;/h2&gt;
&lt;p&gt;以上で、&lt;code&gt;NEologd&lt;/code&gt;辞書が使用できるようになります。非常に強力なツールなので使用してみてください。なお、辞書がアップデートされた際は同じ手続きを行う必要があります。&lt;/p&gt;
&lt;p&gt;&lt;後日談&gt;
Ubuntu for Linuxを使用しなくてもダウンロードできそうな方法ありました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://qiita.com/zincjp/items/c61c441426b9482b5a48&#34; class=&#34;uri&#34;&gt;https://qiita.com/zincjp/items/c61c441426b9482b5a48&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ただ、自分は実行していないので実際にできるかはわかりません。辞書が更新されたら、やってみたいと思います。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;MeCab内部の仕組みについては&lt;a href=&#34;https://techlife.cookpad.com/entry/2016/05/11/170000&#34;&gt;こちら&lt;/a&gt;の記事が参考になります。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;2020/4/19時点の最近版は2020/3/15更新の辞書です。&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>LightGBMを使用して競馬結果を予想してみる</title>
      <link>/post/post16/</link>
      <pubDate>Sat, 29 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/post16/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#データインポート&#34;&gt;1.データインポート&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#予測モデルの作成&#34;&gt;2. 予測モデルの作成&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#shapでの結果解釈&#34;&gt;3. shapでの結果解釈&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#最後に&#34;&gt;4. 最後に&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。かなり久しぶりではありますが、Pythonの勉強をかねて以前yahoo.keibaで収集した競馬のレース結果データから、レース結果を予想するモデルを作成したいと思います。&lt;/p&gt;
&lt;div id=&#34;データインポート&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.データインポート&lt;/h2&gt;
&lt;p&gt;まず、前回&lt;code&gt;sqlite&lt;/code&gt;に保存したレース結果データを&lt;code&gt;pandas&lt;/code&gt;データフレームへ保存します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;conn = sqlite3.connect(r&amp;#39;C:\hogehoge\horse_data.db&amp;#39;)
sql = r&amp;#39;SELECT * FROM race_result&amp;#39;
df = pd.read_sql(con=conn,sql=sql)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;データの中身を確認してみましょう。列は以下のようになっています。orderが着順となっています。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.columns&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Index([&amp;#39;order&amp;#39;, &amp;#39;frame_number&amp;#39;, &amp;#39;horse_number&amp;#39;, &amp;#39;trainer&amp;#39;, &amp;#39;passing_rank&amp;#39;,
##        &amp;#39;last_3F&amp;#39;, &amp;#39;time&amp;#39;, &amp;#39;margin&amp;#39;, &amp;#39;horse_name&amp;#39;, &amp;#39;horse_age&amp;#39;, &amp;#39;horse_sex&amp;#39;,
##        &amp;#39;horse_weight&amp;#39;, &amp;#39;horse_weight_change&amp;#39;, &amp;#39;brinker&amp;#39;, &amp;#39;jockey&amp;#39;,
##        &amp;#39;jockey_weight&amp;#39;, &amp;#39;jockey_weight_change&amp;#39;, &amp;#39;odds&amp;#39;, &amp;#39;popularity&amp;#39;,
##        &amp;#39;race_date&amp;#39;, &amp;#39;race_course&amp;#39;, &amp;#39;race_name&amp;#39;, &amp;#39;race_distance&amp;#39;, &amp;#39;type&amp;#39;,
##        &amp;#39;race_turn&amp;#39;, &amp;#39;race_condition&amp;#39;, &amp;#39;race_weather&amp;#39;, &amp;#39;colour&amp;#39;, &amp;#39;owner&amp;#39;,
##        &amp;#39;farm&amp;#39;, &amp;#39;locality&amp;#39;, &amp;#39;horse_birthday&amp;#39;, &amp;#39;father&amp;#39;, &amp;#39;mother&amp;#39;, &amp;#39;prize&amp;#39;,
##        &amp;#39;http&amp;#39;],
##       dtype=&amp;#39;object&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;orderの中身を確認してみると、括弧（）がついている物が多く、また取消や中止、失格などが存在するため、文字型に認識されていることがわかります。ちなみに括弧（）内の順位は入線順位というやつで、他馬の走行を妨害したりして順位が降着させられたことを意味します（&lt;a href=&#34;http://www.jra.go.jp/judge/&#34; class=&#34;uri&#34;&gt;http://www.jra.go.jp/judge/&lt;/a&gt;）。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df.loc[:,&amp;#39;order&amp;#39;].unique()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([&amp;#39;1&amp;#39;, &amp;#39;7&amp;#39;, &amp;#39;2&amp;#39;, &amp;#39;8&amp;#39;, &amp;#39;5&amp;#39;, &amp;#39;15&amp;#39;, &amp;#39;6&amp;#39;, &amp;#39;12&amp;#39;, &amp;#39;11&amp;#39;, &amp;#39;14&amp;#39;, &amp;#39;3&amp;#39;, &amp;#39;13&amp;#39;,
##        &amp;#39;4&amp;#39;, &amp;#39;16&amp;#39;, &amp;#39;9&amp;#39;, &amp;#39;10&amp;#39;, &amp;#39;取消&amp;#39;, &amp;#39;中止&amp;#39;, &amp;#39;除外&amp;#39;, &amp;#39;17&amp;#39;, &amp;#39;18&amp;#39;, &amp;#39;4(3)&amp;#39;, &amp;#39;2(1)&amp;#39;,
##        &amp;#39;3(2)&amp;#39;, &amp;#39;6(4)&amp;#39;, &amp;#39;失格&amp;#39;, &amp;#39;9(8)&amp;#39;, &amp;#39;16(6)&amp;#39;, &amp;#39;12(12)&amp;#39;, &amp;#39;13(9)&amp;#39;, &amp;#39;6(3)&amp;#39;,
##        &amp;#39;10(7)&amp;#39;, &amp;#39;6(5)&amp;#39;, &amp;#39;9(3)&amp;#39;, &amp;#39;11(8)&amp;#39;, &amp;#39;13(2)&amp;#39;, &amp;#39;12(9)&amp;#39;, &amp;#39;14(7)&amp;#39;,
##        &amp;#39;10(1)&amp;#39;, &amp;#39;16(8)&amp;#39;, &amp;#39;14(6)&amp;#39;, &amp;#39;10(3)&amp;#39;, &amp;#39;12(1)&amp;#39;, &amp;#39;13(6)&amp;#39;, &amp;#39;7(1)&amp;#39;,
##        &amp;#39;12(6)&amp;#39;, &amp;#39;6(2)&amp;#39;, &amp;#39;11(2)&amp;#39;, &amp;#39;15(6)&amp;#39;, &amp;#39;13(10)&amp;#39;, &amp;#39;14(4)&amp;#39;, &amp;#39;7(5)&amp;#39;,
##        &amp;#39;17(4)&amp;#39;, &amp;#39;9(7)&amp;#39;, &amp;#39;16(14)&amp;#39;, &amp;#39;12(11)&amp;#39;, &amp;#39;14(2)&amp;#39;, &amp;#39;8(2)&amp;#39;, &amp;#39;9(5)&amp;#39;,
##        &amp;#39;11(5)&amp;#39;, &amp;#39;12(7)&amp;#39;, &amp;#39;11(1)&amp;#39;, &amp;#39;12(8)&amp;#39;, &amp;#39;7(4)&amp;#39;, &amp;#39;5(4)&amp;#39;, &amp;#39;13(12)&amp;#39;,
##        &amp;#39;14(3)&amp;#39;, &amp;#39;10(2)&amp;#39;, &amp;#39;11(10)&amp;#39;, &amp;#39;18(3)&amp;#39;, &amp;#39;10(4)&amp;#39;, &amp;#39;15(8)&amp;#39;, &amp;#39;8(3)&amp;#39;,
##        &amp;#39;5(1)&amp;#39;, &amp;#39;10(5)&amp;#39;, &amp;#39;7(3)&amp;#39;, &amp;#39;5(2)&amp;#39;, &amp;#39;9(1)&amp;#39;, &amp;#39;13(3)&amp;#39;, &amp;#39;16(11)&amp;#39;,
##        &amp;#39;11(3)&amp;#39;, &amp;#39;18(15)&amp;#39;, &amp;#39;11(6)&amp;#39;, &amp;#39;10(6)&amp;#39;, &amp;#39;14(12)&amp;#39;, &amp;#39;12(5)&amp;#39;, &amp;#39;15(14)&amp;#39;,
##        &amp;#39;17(8)&amp;#39;, &amp;#39;18(6)&amp;#39;, &amp;#39;4(2)&amp;#39;, &amp;#39;18(10)&amp;#39;, &amp;#39;16(7)&amp;#39;, &amp;#39;13(1)&amp;#39;, &amp;#39;16(10)&amp;#39;,
##        &amp;#39;15(7)&amp;#39;, &amp;#39;9(4)&amp;#39;, &amp;#39;15(5)&amp;#39;, &amp;#39;12(3)&amp;#39;, &amp;#39;8(7)&amp;#39;, &amp;#39;15(2)&amp;#39;, &amp;#39;12(10)&amp;#39;,
##        &amp;#39;14(9)&amp;#39;, &amp;#39;3(1)&amp;#39;, &amp;#39;6(1)&amp;#39;, &amp;#39;14(5)&amp;#39;, &amp;#39;15(4)&amp;#39;, &amp;#39;11(4)&amp;#39;, &amp;#39;12(4)&amp;#39;,
##        &amp;#39;16(4)&amp;#39;, &amp;#39;9(2)&amp;#39;, &amp;#39;13(5)&amp;#39;, &amp;#39;12(2)&amp;#39;, &amp;#39;15(1)&amp;#39;, &amp;#39;4(1)&amp;#39;, &amp;#39;14(13)&amp;#39;,
##        &amp;#39;14(1)&amp;#39;, &amp;#39;13(7)&amp;#39;, &amp;#39;5(3)&amp;#39;, &amp;#39;8(6)&amp;#39;, &amp;#39;15(13)&amp;#39;, &amp;#39;7(2)&amp;#39;, &amp;#39;15(11)&amp;#39;,
##        &amp;#39;10(9)&amp;#39;, &amp;#39;11(9)&amp;#39;, &amp;#39;8(4)&amp;#39;, &amp;#39;15(3)&amp;#39;, &amp;#39;13(4)&amp;#39;, &amp;#39;16(12)&amp;#39;, &amp;#39;16(5)&amp;#39;,
##        &amp;#39;18(11)&amp;#39;, &amp;#39;10(8)&amp;#39;, &amp;#39;18(8)&amp;#39;, &amp;#39;14(8)&amp;#39;, &amp;#39;16(9)&amp;#39;, &amp;#39;8(5)&amp;#39;, &amp;#39;8(1)&amp;#39;,
##        &amp;#39;14(11)&amp;#39;, &amp;#39;9(6)&amp;#39;, &amp;#39;16(13)&amp;#39;, &amp;#39;16(15)&amp;#39;, &amp;#39;11(11)&amp;#39;, &amp;#39;15(10)&amp;#39;, &amp;#39;7(6)&amp;#39;],
##       dtype=object)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;まずここを修正しましょう。括弧を除去してint型に型変更し、入線順位は新たな列&lt;code&gt;arriving order&lt;/code&gt;として追加します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;arriving order&amp;#39;] = df[df.order.str.contains(r&amp;#39;\d*\(\d*\)&amp;#39;,regex=True)][&amp;#39;order&amp;#39;].replace(r&amp;#39;\d+\(&amp;#39;,r&amp;#39;&amp;#39;,regex=True).replace(r&amp;#39;\)&amp;#39;,r&amp;#39;&amp;#39;,regex=True).astype(&amp;#39;float64&amp;#39;)
df[&amp;#39;arriving order&amp;#39;].unique()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([nan,  3.,  1.,  2.,  4.,  8.,  6., 12.,  9.,  7.,  5., 10., 14.,
##        11., 15., 13.])&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;order&amp;#39;] = df[&amp;#39;order&amp;#39;].replace(r&amp;#39;\(\d+\)&amp;#39;,r&amp;#39;&amp;#39;,regex=True)
df = df[lambda df: ~df.order.str.contains(r&amp;#39;(取消|中止|除外|失格)&amp;#39;,regex=True)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## C:\Users\aashi\Anaconda3\envs\umanalytics\lib\site-packages\pandas\core\strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.
##   return func(self, *args, **kwargs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;order&amp;#39;] = df[&amp;#39;order&amp;#39;].astype(&amp;#39;float64&amp;#39;)
df[&amp;#39;order&amp;#39;].unique()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## array([ 1.,  7.,  2.,  8.,  5., 15.,  6., 12., 11., 14.,  3., 13.,  4.,
##        16.,  9., 10., 17., 18.])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;きれいな&lt;code&gt;float&lt;/code&gt;型に処理することができました。では、次にラスト3Fのタイムの前処理に移ります。前走のラスト3Fのタイムを予測に使用します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
df[&amp;#39;last_3F&amp;#39;] = df[&amp;#39;last_3F&amp;#39;].replace(r&amp;#39;character(0)&amp;#39;,np.nan,regex=False).astype(&amp;#39;float64&amp;#39;)
df[&amp;#39;last_3F&amp;#39;] = df.groupby(&amp;#39;horse_name&amp;#39;)[&amp;#39;last_3F&amp;#39;].shift(-1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;前走のレースと順位、追加順位もデータセットへ含めましょう。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;prerace&amp;#39;] = df.groupby(&amp;#39;horse_name&amp;#39;)[&amp;#39;race_name&amp;#39;].shift(-1)
df[&amp;#39;preorder&amp;#39;] = df.groupby(&amp;#39;horse_name&amp;#39;)[&amp;#39;order&amp;#39;].shift(-1)
df[&amp;#39;prepassing&amp;#39;] = df.groupby(&amp;#39;horse_name&amp;#39;)[&amp;#39;passing_rank&amp;#39;].shift(-1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;出走時点で獲得している累積賞金額も追加します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;preprize&amp;#39;] = df.groupby(&amp;#39;horse_name&amp;#39;)[&amp;#39;prize&amp;#39;].shift(-1)
df[&amp;#39;preprize&amp;#39;] = df[&amp;#39;preprize&amp;#39;].fillna(0)
df[&amp;#39;margin&amp;#39;] = df.groupby(&amp;#39;horse_name&amp;#39;)[&amp;#39;margin&amp;#39;].shift(-1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;その他、欠損値やデータ型の修正、カテゴリデータのラベルエンコーディングです。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df[&amp;#39;horse_weight&amp;#39;] = df[&amp;#39;horse_weight&amp;#39;].astype(&amp;#39;float64&amp;#39;)
df[&amp;#39;margin&amp;#39;] = df[&amp;#39;margin&amp;#39;].replace(r&amp;#39;character(0)&amp;#39;,np.nan,regex=False)
df[&amp;#39;horse_age&amp;#39;] = df[&amp;#39;horse_age&amp;#39;].astype(&amp;#39;float64&amp;#39;)
df[&amp;#39;horse_weight_change&amp;#39;] = df[&amp;#39;horse_weight_change&amp;#39;].astype(&amp;#39;float64&amp;#39;)
df[&amp;#39;jockey_weight&amp;#39;] = df[&amp;#39;jockey_weight&amp;#39;].astype(&amp;#39;float64&amp;#39;)
df[&amp;#39;race_distance&amp;#39;] = df[&amp;#39;race_distance&amp;#39;].replace(r&amp;#39;m&amp;#39;,r&amp;#39;&amp;#39;,regex=True).astype(&amp;#39;float64&amp;#39;)
df[&amp;#39;race_turn&amp;#39;] = df[&amp;#39;race_turn&amp;#39;].replace(r&amp;#39;character(0)&amp;#39;,np.nan,regex=False)
df.loc[df[&amp;#39;order&amp;#39;]!=1,&amp;#39;order&amp;#39;] = 0

df[&amp;#39;race_turn&amp;#39;] = df[&amp;#39;race_turn&amp;#39;].fillna(&amp;#39;missing&amp;#39;)
df[&amp;#39;colour&amp;#39;] = df[&amp;#39;colour&amp;#39;].fillna(&amp;#39;missing&amp;#39;)
df[&amp;#39;prepassing&amp;#39;] = df[&amp;#39;prepassing&amp;#39;].fillna(&amp;#39;missing&amp;#39;)
df[&amp;#39;prerace&amp;#39;] = df[&amp;#39;prerace&amp;#39;].fillna(&amp;#39;missing&amp;#39;)
df[&amp;#39;father&amp;#39;] = df[&amp;#39;father&amp;#39;].fillna(&amp;#39;missing&amp;#39;)
df[&amp;#39;mother&amp;#39;] = df[&amp;#39;mother&amp;#39;].fillna(&amp;#39;missing&amp;#39;)

from sklearn import preprocessing
cat_list = [&amp;#39;trainer&amp;#39;, &amp;#39;horse_name&amp;#39;, &amp;#39;horse_sex&amp;#39;, &amp;#39;brinker&amp;#39;, &amp;#39;jockey&amp;#39;, &amp;#39;race_course&amp;#39;, &amp;#39;race_name&amp;#39;, &amp;#39;type&amp;#39;, &amp;#39;race_turn&amp;#39;, &amp;#39;race_condition&amp;#39;, &amp;#39;race_weather&amp;#39;, &amp;#39;colour&amp;#39;, &amp;#39;father&amp;#39;, &amp;#39;mother&amp;#39;, &amp;#39;prerace&amp;#39;, &amp;#39;prepassing&amp;#39;]
for column in cat_list:
    target_column = df[column]
    le = preprocessing.LabelEncoder()
    le.fit(target_column)
    label_encoded_column = le.transform(target_column)
    df[column] = pd.Series(label_encoded_column).astype(&amp;#39;category&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()
## LabelEncoder()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas_profiling as pdq
profile = pdq.ProfileReport(df)
profile&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;予測モデルの作成&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. 予測モデルの作成&lt;/h2&gt;
&lt;p&gt;では&lt;code&gt;LightGBM&lt;/code&gt;で予測モデルを作ってみます。&lt;code&gt;optuna&lt;/code&gt;の&lt;code&gt;LightGBM&lt;/code&gt;を使用して、ハイパーパラメータチューニングを行い、学習したモデルを用いて計算したテストデータの予測値と実績値の&lt;code&gt;confusion matrix&lt;/code&gt;ならびに正解率を算出します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import optuna.integration.lightgbm as lgb
from sklearn.model_selection import train_test_split

y = df[&amp;#39;order&amp;#39;]
x = df.drop([&amp;#39;order&amp;#39;,&amp;#39;passing_rank&amp;#39;,&amp;#39;time&amp;#39;,&amp;#39;odds&amp;#39;,&amp;#39;popularity&amp;#39;,&amp;#39;owner&amp;#39;,&amp;#39;farm&amp;#39;,&amp;#39;locality&amp;#39;,&amp;#39;horse_birthday&amp;#39;,&amp;#39;http&amp;#39;,&amp;#39;prize&amp;#39;,&amp;#39;race_date&amp;#39;,&amp;#39;margin&amp;#39;],axis=1)

X_train, X_test, y_train, y_test = train_test_split(x, y)
X_train, x_val, y_train, y_val = train_test_split(X_train, y_train)

lgb_train = lgb.Dataset(X_train, y_train)
lgb_eval = lgb.Dataset(x_val, y_val)
lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)

lgbm_params = {
        &amp;#39;objective&amp;#39;: &amp;#39;binary&amp;#39;,
        &amp;#39;boost_from_average&amp;#39;: False
    }

best_params, history = {}, []
model = lgb.train(lgbm_params, lgb_train, categorical_feature = cat_list,valid_sets = lgb_eval, num_boost_round=100,early_stopping_rounds=20,best_params=best_params,tuning_history=history, verbose_eval=False)
best_params

def calibration(y_proba, beta):
    return y_proba / (y_proba + (1 - y_proba) / beta)

sampling_rate = y_train.sum() / len(y_train)
y_proba = model.predict(X_test, num_iteration=model.best_iteration)
y_proba_calib = calibration(y_proba, sampling_rate)

y_pred = np.vectorize(lambda x: 1 if x &amp;gt; 0.49 else 0)(y_proba_calib)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;可視化パートです。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns

# AUC (Area Under the Curve) を計算する
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
auc = auc(fpr, tpr)

# ROC曲線をプロット
plt.plot(fpr, tpr, label=&amp;#39;ROC curve (area = %.2f)&amp;#39;%auc)
plt.legend()
plt.title(&amp;#39;ROC curve&amp;#39;)
plt.xlabel(&amp;#39;False Positive Rate&amp;#39;)
plt.ylabel(&amp;#39;True Positive Rate&amp;#39;)
plt.grid(True)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()

# Confusion Matrixを生成
ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000004F873388&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()

accuracy_score(y_test, y_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.9300814465677578&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;precision_score(y_test, y_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.9426605504587156&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;accuracy_score&lt;/code&gt;（予測精度）が90%を超え、&lt;code&gt;precision_Score&lt;/code&gt;（適合率、陽=1着と予想したデータの正解率）もいい感じです。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;recall_score(y_test, y_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.01211460236986382&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;f1_score(y_test, y_pred)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 0.02392177405273267&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一方、&lt;code&gt;recall_score&lt;/code&gt;(再現性、陽=1着のサンプルのうち実際に正解した割合)が低く偽陰性が高いことが確認できます。その結果、&lt;code&gt;F1&lt;/code&gt;値も低くなっていますね。競馬予測モデルの場合、偽陰性が高いことは偽陽性が高いことよりはましなのですが、回収率を上げるためには偽陰性を下げることを頑張らなければいけません。これは今後の課題ですね。次節では&lt;code&gt;shapley&lt;/code&gt;値を使って要因分解をしたいと思います。。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shapでの結果解釈&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. shapでの結果解釈&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shap

shap.initjs()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;IPython.core.display.HTML object&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;explainer = shap.TreeExplainer(model)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Setting feature_perturbation = &amp;quot;tree_path_dependent&amp;quot; because no background data was given.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;shap_values = explainer.shap_values(X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;まず、各特徴量の重要度を見ることにします。&lt;code&gt;summary_plot&lt;/code&gt;メソッドを使用します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;shap.summary_plot(shap_values, X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;横軸は各特徴量の平均的な重要度を表しています(shap値の絶対値)。preprize(前走までの賞金獲得金額)やhorse_age、preorder(前走の着順)などが予測に重要であることが分かります。特にpreprizeの重要度は1着の予測、1着以外の予測どちらに対しても大きいです。horse_ageも同様です。ただ、これでは重要というだけで定性的な評価はできません。例えば、preprizeが大きい→1位になる確率が上昇といった関係が確認できれば、それは重要な情報になり得ます。次にそれを確認します。&lt;code&gt;summary_plot&lt;/code&gt;メソッドを使用します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;shap.summary_plot(shap_values[1], X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;上図も各特徴量の重要度を表しています(今回は絶対値ではありません)。今回はそれぞれの特徴量の重要度がバイオリンプロットによって表されており、かつ特徴量の値の大きさで色分けがされています。例えば、preprizeだと横軸が0以上の部分でのみ赤色の分布が発生しており、ここからpreprizeの特徴量が大きい、つまり前走までの獲得賞金額が多いと平均的に1着の確率が上がるという当たり前の解釈をすることができます。
他にも、horse_age,preorder,last_3Fは特徴量が小さくなるほど1着になる確率があがることも読み取れます。horse_weight, jokey_weightは大きくなるほど1着になる確率が上がるようです。一方、その他は特に定性的な関係を読み取ることはできません。&lt;/p&gt;
&lt;p&gt;次に、特徴量と確率の関係をより詳しく確認してみましょう。先ほど、preprizeは特徴量が大きくなるほど1着になる確率が上昇するということがわかりました。ただ、その確率の上昇は1次関数的に増加するのか、指数的に増大するのか、それとも&lt;span class=&#34;math inline&#34;&gt;\(\log x\)&lt;/span&gt;のように逓減していくのか、わかりません。&lt;code&gt;dependence_plot&lt;/code&gt;を使用してそれを確認してみましょう。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;shap.dependence_plot(ind=&amp;quot;preprize&amp;quot;, shap_values=shap_values[1], features=X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;上図は学習した&lt;code&gt;LightGBM&lt;/code&gt;をpreprizeの関数として見たときの概形をplotしたものです。先に確認したとおり、やはり特徴量が大きくなるにつれ、1着になる確率が上昇していきます。ただ、その上昇は徐々に逓減していき、2000万円を超えるところでほぼ頭打ちとなります。また、上図ではhorse_ageでの色分けを行っており、preprizeとの関係性も確認できるようになっています。やはり、直感と同じく、preprizeが高い馬の中でもhorse_ageが若い馬の1着確率が高くなることが見て取れます。&lt;/p&gt;
&lt;p&gt;preorderの&lt;code&gt;dependence_plot&lt;/code&gt;も確認してみましょう。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;shap.dependence_plot(ind=&amp;quot;preorder&amp;quot;, shap_values=shap_values[1], features=X_test)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;やはり、前走の着順が上位になるほど1着確率が高まることがここからも分かります。また、その確率は6着以上とそれ以外で水準感が変わることも分かります。last_3Fのタイムとの関係性も確認していますが、こちらはあまり関連性はなさそうです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;最後に&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. 最後に&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;LightGBM&lt;/code&gt;を使用し、競馬の予測モデルを作成してみました。さすが&lt;code&gt;LightGBM&lt;/code&gt;といった感じで、予測精度は高かったです。また、&lt;code&gt;shap&lt;/code&gt;値を使用した重要特徴量の検出も上手くいきました。これによって、&lt;code&gt;LightGBM&lt;/code&gt;の気持ちを理解し、より良い特徴量の発見を進めていくことでモデリングの精度を高めていこうと思います。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>10年物長期金利をフィッティングしてみる</title>
      <link>/post/post14/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/post14/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#データ収集&#34;&gt;1. データ収集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#月次解析パート&#34;&gt;2. 月次解析パート&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#日次解析パート&#34;&gt;3. 日次解析パート&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。とある理由で10年物長期金利のフィッティングを行いたいと思いました。というわけで、USデータを用いて解析していきます。まず、データを収集しましょう。&lt;code&gt;quantmod&lt;/code&gt;パッケージを用いて、FREDからデータを落とします。&lt;code&gt;getsymbols(キー,from=開始日,src=&#34;FRED&#34;, auto.assign=TRUE)&lt;/code&gt;で簡単にできちゃいます。ちなみにキーはFREDのHPで確認できます。&lt;/p&gt;
&lt;div id=&#34;データ収集&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. データ収集&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(quantmod)

# data name collected
symbols.name &amp;lt;- c(&amp;quot;10-Year Treasury Constant Maturity Rate&amp;quot;,&amp;quot;Effective Federal Funds Rate&amp;quot;,&amp;quot;
Consumer Price Index for All Urban Consumers: All Items&amp;quot;,&amp;quot;Civilian Unemployment Rate&amp;quot;,&amp;quot;3-Month Treasury Bill: Secondary Market Rate&amp;quot;,&amp;quot;Industrial Production Index&amp;quot;,&amp;quot;
10-Year Breakeven Inflation Rate&amp;quot;,&amp;quot;Trade Weighted U.S. Dollar Index: Broad, Goods&amp;quot;,&amp;quot;
Smoothed U.S. Recession Probabilities&amp;quot;,&amp;quot;Moody&amp;#39;s Seasoned Baa Corporate Bond Yield&amp;quot;,&amp;quot;5-Year, 5-Year Forward Inflation Expectation Rate&amp;quot;,&amp;quot;Personal Consumption Expenditures&amp;quot;)

# Collect economic data
symbols &amp;lt;- c(&amp;quot;GS10&amp;quot;,&amp;quot;FEDFUNDS&amp;quot;,&amp;quot;CPIAUCSL&amp;quot;,&amp;quot;UNRATE&amp;quot;,&amp;quot;TB3MS&amp;quot;,&amp;quot;INDPRO&amp;quot;,&amp;quot;T10YIEM&amp;quot;,&amp;quot;TWEXBMTH&amp;quot;,&amp;quot;RECPROUSM156N&amp;quot;,&amp;quot;BAA&amp;quot;,&amp;quot;T5YIFRM&amp;quot;,&amp;quot;PCE&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, src = &amp;quot;FRED&amp;quot;, auto.assign = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;GS10&amp;quot;          &amp;quot;FEDFUNDS&amp;quot;      &amp;quot;CPIAUCSL&amp;quot;      &amp;quot;UNRATE&amp;quot;       
##  [5] &amp;quot;TB3MS&amp;quot;         &amp;quot;INDPRO&amp;quot;        &amp;quot;T10YIEM&amp;quot;       &amp;quot;TWEXBMTH&amp;quot;     
##  [9] &amp;quot;RECPROUSM156N&amp;quot; &amp;quot;BAA&amp;quot;           &amp;quot;T5YIFRM&amp;quot;       &amp;quot;PCE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;macro_indicator &amp;lt;- merge(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE)
rm(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE,USEPUINDXD)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;月次解析パート&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. 月次解析パート&lt;/h2&gt;
&lt;p&gt;データは
&lt;a href=&#34;htmlwidget/macro_indicator.html&#34;&gt;こちら&lt;/a&gt;
から参照できます。では、推計用のデータセットを作成していきます。被説明変数は&lt;code&gt;10-Year Treasury Constant Maturity Rate(GS10)&lt;/code&gt;です。説明変数は以下の通りです。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;説明変数名&lt;/th&gt;
&lt;th&gt;キー&lt;/th&gt;
&lt;th&gt;代理変数&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Federal Funds Rate&lt;/td&gt;
&lt;td&gt;FEDFUNDS&lt;/td&gt;
&lt;td&gt;短期金利&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Consumer Price Index&lt;/td&gt;
&lt;td&gt;CPIAUCSL&lt;/td&gt;
&lt;td&gt;物価&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment Rate&lt;/td&gt;
&lt;td&gt;UNRATE&lt;/td&gt;
&lt;td&gt;雇用関連&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;3-Month Treasury Bill&lt;/td&gt;
&lt;td&gt;TB3MS&lt;/td&gt;
&lt;td&gt;短期金利&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Industrial Production Index&lt;/td&gt;
&lt;td&gt;INDPRO&lt;/td&gt;
&lt;td&gt;景気&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Breakeven Inflation Rate&lt;/td&gt;
&lt;td&gt;T10YIEM&lt;/td&gt;
&lt;td&gt;物価&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Trade Weighted Dollar Index&lt;/td&gt;
&lt;td&gt;TWEXBMTH&lt;/td&gt;
&lt;td&gt;為替&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Recession Probabilities&lt;/td&gt;
&lt;td&gt;RECPROUSM156N&lt;/td&gt;
&lt;td&gt;景気&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Moody’s Seasoned Baa Corporate Bond Yield&lt;/td&gt;
&lt;td&gt;BAA&lt;/td&gt;
&lt;td&gt;リスクプレミアム&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Inflation Expectation Rate&lt;/td&gt;
&lt;td&gt;T5YIFRM&lt;/td&gt;
&lt;td&gt;物価&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Personal Consumption Expenditures&lt;/td&gt;
&lt;td&gt;PCE&lt;/td&gt;
&lt;td&gt;景気&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Economic Policy Uncertainty Index&lt;/td&gt;
&lt;td&gt;USEPUINDXD&lt;/td&gt;
&lt;td&gt;政治&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;かなり適当な変数選択ではあるんですが、マクロモデリング的に長期金利ってどうやってモデル化するかというとちゃんとやってない場合が多いです。DSGEでは効率市場仮説に従って10年先までの短期金利のパスをリンクしたものと長期金利が等しくなると定式化するのが院生時代のモデリングでした（マクロファイナンスの界隈ではちゃんとやってそう）。そういうわけで、短期金利を説明変数に加えています。そして、短期金利に影響を与えるであろう物価にかかる指標も3つ追加しました。加えて、景気との相関が強いことはよく知られているので景気に関するデータも追加しました。これらはそもそもマクロモデルでは短期金利は以下のようなテイラールールに従うとモデリングすることが一般的であることが背景にあります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_t = \rho r_{t-1} + \alpha \pi_{t} + \beta y_{t}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt;は政策金利（短期金利）、&lt;span class=&#34;math inline&#34;&gt;\(\pi_t\)&lt;/span&gt;はインフレ率、&lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt;はoutputです。&lt;span class=&#34;math inline&#34;&gt;\(\rho, \alpha, \beta\)&lt;/span&gt;はdeep parameterと呼ばれるもので、それぞれ慣性、インフレ率への金利の感応度、outputに対する感応度を表しています。&lt;span class=&#34;math inline&#34;&gt;\(\rho=0,\beta=0\)&lt;/span&gt;の時、&lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;=1\)&lt;/span&gt;でなければ合理的期待均衡解が得られないことは「テイラーの原理」として有名です。
その他、Corporate bondとの裁定関係も存在しそうな&lt;code&gt;Moody&#39;s Seasoned Baa Corporate Bond Yield&lt;/code&gt;も説明変数に追加しています。また、欲を言えば&lt;code&gt;VIX&lt;/code&gt;指数と財政に関する指標を追加したいところです。財政に関する指数はQuateryかAnnualyなので今回のようなmonthlyの推計には使用することができません。この部分は最もネックなところです。なにか考え付いたら再推計します。&lt;/p&gt;
&lt;p&gt;では、推計に入ります。今回は説明変数が多いので&lt;code&gt;lasso&lt;/code&gt;回帰を行い、有効な変数を絞り込みたいと思います。また、比較のために&lt;code&gt;OLS&lt;/code&gt;もやります。説明変数は被説明変数の1期前の値を使用します。おそらく、1期前でもデータの公表時期によっては翌月の推計に間に合わない可能性もありますが、とりあえずこれでやってみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make dataset
traindata &amp;lt;- na.omit(merge(macro_indicator[&amp;quot;2003-01-01::2015-12-31&amp;quot;][,1],stats::lag(macro_indicator[&amp;quot;2003-01-01::2015-12-31&amp;quot;][,-1],1)))
testdata  &amp;lt;- na.omit(merge(macro_indicator[&amp;quot;2016-01-01::&amp;quot;][,1],stats::lag(macro_indicator[&amp;quot;2016-01-01::&amp;quot;][,-1],1)))

# fitting OLS
trial1 &amp;lt;- lm(GS10~.,data = traindata)
summary(trial1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GS10 ~ ., data = traindata)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.76208 -0.21234  0.00187  0.21595  0.70493 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   14.3578405  4.3524691   3.299 0.001226 ** 
## FEDFUNDS      -0.2011132  0.1438774  -1.398 0.164335    
## CPIAUCSL      -0.0702011  0.0207761  -3.379 0.000938 ***
## UNRATE        -0.2093502  0.0796052  -2.630 0.009477 ** 
## TB3MS          0.2970160  0.1413796   2.101 0.037410 *  
## INDPRO        -0.0645376  0.0260343  -2.479 0.014339 *  
## T10YIEM        1.1484487  0.1769925   6.489 1.32e-09 ***
## TWEXBMTH      -0.0317345  0.0118155  -2.686 0.008091 ** 
## RECPROUSM156N -0.0099083  0.0021021  -4.713 5.72e-06 ***
## BAA            0.7793520  0.0868628   8.972 1.49e-15 ***
## T5YIFRM       -0.4551318  0.1897695  -2.398 0.017759 *  
## PCE            0.0009087  0.0002475   3.672 0.000339 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2981 on 143 degrees of freedom
## Multiple R-squared:  0.9203, Adjusted R-squared:  0.9142 
## F-statistic: 150.1 on 11 and 143 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;自由度修正済み決定係数高めですね。2015/12/31までのモデルを使って、アウトサンプルのデータ(2016/01/01~)を予測し、平均二乗誤差を計算します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.OLS.Y &amp;lt;- predict(trial1,testdata[,-1])
Y &amp;lt;- as.matrix(testdata[,1])
mse.OLS &amp;lt;- sum((Y - est.OLS.Y)^2) / length(Y)
mse.OLS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1431734&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;次に&lt;code&gt;lasso&lt;/code&gt;回帰です。Cross Validationを行い、&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;を決める&lt;code&gt;glmnet&lt;/code&gt;パッケージの&lt;code&gt;cv.glmnet&lt;/code&gt;関数を使用します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fitting lasso regression
library(glmnet)
trial2 &amp;lt;- cv.glmnet(as.matrix(traindata[,-1]),as.matrix(traindata[,1]),family=&amp;quot;gaussian&amp;quot;,alpha=1)
plot(trial2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trial2$lambda.min&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.000436523&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(trial2,s=trial2$lambda.min)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 12 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                           1
## (Intercept)   12.0180676188
## FEDFUNDS      -0.1041665345
## CPIAUCSL      -0.0574470880
## UNRATE        -0.1919723880
## TB3MS          0.2110222475
## INDPRO        -0.0610115260
## T10YIEM        1.1688912397
## TWEXBMTH      -0.0242324285
## RECPROUSM156N -0.0095154487
## BAA            0.7600115062
## T5YIFRM       -0.4575241038
## PCE            0.0007486169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Unemployment Rate&lt;/code&gt;、&lt;code&gt;3-Month Treasury Bill&lt;/code&gt;、&lt;code&gt;Breakeven Inflation Rate、Moody&#39;s Seasoned Baa Corporate Bond Yield&lt;/code&gt;、&lt;code&gt;Inflation Expectation Rate&lt;/code&gt;の回帰係数が大きくなるという結果ですね。失業率以外は想定内の結果です。ただ、今回の結果を見る限り景気との相関は低そうです（逆向きにしか効かない？）。MSEを計算します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.lasso.Y &amp;lt;- predict(trial2, newx = as.matrix(testdata[,-1]), s = trial2$lambda.min, type = &amp;#39;response&amp;#39;)
mse.lasso &amp;lt;- sum((Y - est.lasso.Y)^2) / length(Y)
mse.lasso&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1318541&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;lasso&lt;/code&gt;回帰のほうが良い結果になりました。&lt;code&gt;lasso&lt;/code&gt;回帰で計算した予測値と実績値を時系列プロットしてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

ggplot(gather(data.frame(actual=Y[,1],lasso_prediction=est.lasso.Y[,1],OLS_prediction=est.OLS.Y,date=as.POSIXct(rownames(Y))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +
  geom_line(size=1.5) +
  scale_x_datetime(breaks = &amp;quot;6 month&amp;quot;,date_labels = &amp;quot;%Y-%m&amp;quot;) +
  scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,3.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;方向感はいい感じです。一方で、2016年1月からや2018年12月以降の急激な金利低下は予測できていません。この部分については何か変数を考えるorローリング推計を実施する、のいずれかをやってみないと精度が上がらなそうです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;日次解析パート&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. 日次解析パート&lt;/h2&gt;
&lt;p&gt;月次での解析に加えて日次での解析もやりたいとおもいます。日次データであればデータの公表は市場が閉まり次第の場合が多いので、いわゆる&lt;code&gt;jagged edge&lt;/code&gt;の問題が起こりにくいと思います。まずは日次データの収集から始めます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data name collected
symbols.name &amp;lt;- c(&amp;quot;10-Year Treasury Constant Maturity Rate&amp;quot;,&amp;quot;Effective Federal Funds Rate&amp;quot;,&amp;quot;
6-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar&amp;quot;,&amp;quot;NASDAQ Composite Index&amp;quot;,&amp;quot;3-Month Treasury Bill: Secondary Market Rate&amp;quot;,&amp;quot;Economic Policy Uncertainty Index for United States&amp;quot;,&amp;quot;
10-Year Breakeven Inflation Rate&amp;quot;,&amp;quot;Trade Weighted U.S. Dollar Index: Broad, Goods&amp;quot;,&amp;quot;Moody&amp;#39;s Seasoned Baa Corporate Bond Yield&amp;quot;,&amp;quot;5-Year, 5-Year Forward Inflation Expectation Rate&amp;quot;)

# Collect economic data
symbols &amp;lt;- c(&amp;quot;DGS10&amp;quot;,&amp;quot;DFF&amp;quot;,&amp;quot;USD6MTD156N&amp;quot;,&amp;quot;NASDAQCOM&amp;quot;,&amp;quot;DTB3&amp;quot;,&amp;quot;USEPUINDXD&amp;quot;,&amp;quot;T10YIE&amp;quot;,&amp;quot;DTWEXB&amp;quot;,&amp;quot;DBAA&amp;quot;,&amp;quot;T5YIFR&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, src = &amp;quot;FRED&amp;quot;, auto.assign = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;DGS10&amp;quot;       &amp;quot;DFF&amp;quot;         &amp;quot;USD6MTD156N&amp;quot; &amp;quot;NASDAQCOM&amp;quot;   &amp;quot;DTB3&amp;quot;       
##  [6] &amp;quot;USEPUINDXD&amp;quot;  &amp;quot;T10YIE&amp;quot;      &amp;quot;DTWEXB&amp;quot;      &amp;quot;DBAA&amp;quot;        &amp;quot;T5YIFR&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NASDAQCOM.r &amp;lt;- ROC(na.omit(NASDAQCOM))
macro_indicator.d &amp;lt;- merge(DGS10,DFF,USD6MTD156N,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)
rm(DGS10,DFF,USD6MTD156N,NASDAQCOM,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;次にデータセットを構築します。学習用と訓練用にデータを分けます。実際の予測プロセスを考え、2営業日前のデータを説明変数に用いています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make dataset
traindata.d &amp;lt;- na.omit(merge(macro_indicator.d[&amp;quot;1980-01-01::2010-12-31&amp;quot;][,1],stats::lag(macro_indicator.d[&amp;quot;1980-01-01::2010-12-31&amp;quot;][,-1],2)))
testdata.d  &amp;lt;- na.omit(merge(macro_indicator.d[&amp;quot;2010-01-01::&amp;quot;][,1],stats::lag(macro_indicator.d[&amp;quot;2010-01-01::&amp;quot;][,-1],2)))

# fitting OLS
trial1.d &amp;lt;- lm(DGS10~.,data = traindata.d)
summary(trial1.d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = DGS10 ~ ., data = traindata.d)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.82445 -0.12285  0.00469  0.14332  0.73789 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -3.5051208  0.1690503 -20.734  &amp;lt; 2e-16 ***
## DFF          0.0818269  0.0239371   3.418 0.000653 ***
## USD6MTD156N -0.0135771  0.0233948  -0.580 0.561799    
## NASDAQCOM   -0.3880217  0.4367334  -0.888 0.374483    
## DTB3         0.1227984  0.0280283   4.381 1.29e-05 ***
## USEPUINDXD  -0.0006611  0.0001086  -6.087 1.58e-09 ***
## T10YIE       0.6980971  0.0355734  19.624  &amp;lt; 2e-16 ***
## DTWEXB       0.0270128  0.0012781  21.135  &amp;lt; 2e-16 ***
## DBAA         0.2988122  0.0182590  16.365  &amp;lt; 2e-16 ***
## T5YIFR       0.3374944  0.0381111   8.856  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2173 on 1114 degrees of freedom
## Multiple R-squared:  0.8901, Adjusted R-squared:  0.8892 
## F-statistic:  1002 on 9 and 1114 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;依然決定係数は高めです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.OLS.Y.d &amp;lt;- predict(trial1.d,testdata.d[,-1])
Y.d &amp;lt;- as.matrix(testdata.d[,1])
mse.OLS.d &amp;lt;- sum((Y.d - est.OLS.Y.d)^2) / length(Y.d)
mse.OLS.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8003042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;次に&lt;code&gt;lasso&lt;/code&gt;回帰です。&lt;code&gt;CV&lt;/code&gt;で&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;を決定。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fitting lasso regression
trial2.d &amp;lt;- cv.glmnet(as.matrix(traindata.d[,-1]),as.matrix(traindata.d[,1]),family=&amp;quot;gaussian&amp;quot;,alpha=1)
plot(trial2.d)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trial2.d$lambda.min&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.001472377&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(trial2.d,s=trial2.d$lambda.min)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                         1
## (Intercept) -3.4186530904
## DFF          0.0707022021
## USD6MTD156N  .           
## NASDAQCOM   -0.2675513858
## DTB3         0.1204092358
## USEPUINDXD  -0.0006506183
## T10YIE       0.6915446819
## DTWEXB       0.0270389569
## DBAA         0.2861031504
## T5YIFR       0.3376304446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;libor&lt;/code&gt;の係数値が0になりました。MSEは&lt;code&gt;OLS&lt;/code&gt;の方が高い結果に。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.lasso.Y.d &amp;lt;- predict(trial2.d, newx = as.matrix(testdata.d[,-1]), s = trial2.d$lambda.min, type = &amp;#39;response&amp;#39;)
mse.lasso.d &amp;lt;- sum((Y.d - est.lasso.Y.d)^2) / length(Y.d)
mse.lasso.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8378427&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;予測値をプロットします。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gather(data.frame(actual=Y.d[,1],lasso_prediction=est.lasso.Y.d[,1],OLS_prediction=est.OLS.Y.d,date=as.POSIXct(rownames(Y.d))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +
  geom_line(size=1.5) +
  scale_x_datetime(breaks = &amp;quot;2 year&amp;quot;,date_labels = &amp;quot;%Y-%m&amp;quot;) +
  scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;月次と同じく、&lt;code&gt;OLS&lt;/code&gt;と&lt;code&gt;lasso&lt;/code&gt;で予測値に差はほとんどありません。なかなかいい感じに変動を捉えることができていますが、2011年の&lt;code&gt;United States federal government credit-rating downgrades&lt;/code&gt;による金利低下や2013年の景気回復に伴う金利上昇は捉えることができていません。日次の景気指標はPOSデータくらいしかないんですが、強いて言うなら最近使用した夜間光の衛星画像データなんかは使えるかもしれません。時間があればやってみます。とりあえず、いったんこれでこの記事は終わりたいと思います。ここまで読み進めていただき、ありがとうございました。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Google Earth Engine APIで衛星画像データを取得し、景況感をナウキャスティングしてみる</title>
      <link>/post/post12/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/post12/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#earth-engineを使うための事前準備&#34;&gt;1. Earth Engineを使うための事前準備&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#python-apiを用いた衛星画像データの取得&#34;&gt;2. Python APIを用いた衛星画像データの取得&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#image&#34;&gt;Image…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imagecollection&#34;&gt;ImageCollection…&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#featurecollection&#34;&gt;FeatureCollection…&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;皆さんおはこんばんにちわ。前回、GPLVMモデルを用いたGDP予測モデルを構築しました。ただ、ナウキャスティングというからにはオルタナティブデータを用いた解析を行いたいところではあります。ふと、以下の記事を見つけました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jp.reuters.com/article/gdp-u-tokyo-idJPKBN15M0NH&#34;&gt;焦点：ナウキャストのＧＤＰ推計、世界初の衛星画像利用　利用拡大も&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;こちらは東京大学の渡辺努先生が人工衛星画像を用いてGDP予測モデルを開発したというものです。記事には&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;米国の海洋大気庁が運営する気象衛星「スオミＮＰＰ」が日本上空を通過する毎日午前１時３０分時点の画像を購入し、縦、横７２０メートル四方のマス目ごとの明るさを計測する。同じ明るさでも、農地、商業用地、工業用地など土地の用途によって経済活動の大きさが異なるため、国土地理院の土地利用調査を参照。土地の用途と、明るさが示す経済活動の相関を弾き出し、この結果を考慮した上で、明るさから経済活動の大きさを試算する。
（中略）衛星画像のように誰もが入手可能な公表データであれば、政府、民間の区別なく分析が可能であるため、渡辺氏はこれを「統計の民主化」と呼び、世界的な潮流になると予想している。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;と書かれており、衛星写真を用いた分析に興味を惹かれました。 衛星写真って誰でも利用可能か？というところですが、Googleが&lt;code&gt;Earth Engine&lt;/code&gt;というサービスを提供していることがわかりました。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine3.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://earthengine.google.com/&#34; class=&#34;uri&#34;&gt;https://earthengine.google.com/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;（拙訳）Google Earth Engineは、数ペタバイトの衛星画像群と地理空間データセットを惑星規模の解析機能と組み合わせ、科学者、研究者、開発者が変化を検出し、傾向を射影し、地球の変容を定量化することを可能にします。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;研究・教育・非営利目的ならば、なんと&lt;strong&gt;無料&lt;/strong&gt;で衛星写真データを解析することができます。具体的に何ができるのかは以下の動画を見てください。&lt;/p&gt;
&lt;iframe src=&#34;//www.youtube.com/embed/gKGOeTFHnKY&#34; width=&#34;100%&#34; height=&#34;500&#34; seamless frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;今回はそんなEath Engineのpython APIを用いて衛星画像データを取得し、解析していきたいと思います。&lt;/p&gt;
&lt;div id=&#34;earth-engineを使うための事前準備&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Earth Engineを使うための事前準備&lt;/h2&gt;
&lt;p&gt;Earth Engineを使用するためには、Google Accountを使って申請を行う必要があります。先ほどの画像の右上の「Sign Up」からできます。申請を行って、Gmailに以下のようなメールが来るととりあえずEarth Engineは使用できるようになります。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine4.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;とりあえずというのはWEB上の&lt;code&gt;Earth Engine&lt;/code&gt; コードエディタは使用できるということです。コードエディタというのは以下のようなもので、ブラウザ上でデータを取得したり、解析をしたり、解析結果をMAPに投影したりすることができる便利ツールです。&lt;code&gt;Earth Engine&lt;/code&gt;の本体はむしろこいつで、APIは副次的なものと考えています。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine5.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;真ん中のコードエディタにコードを打っていきますが、言語はjavascriptです(APIは&lt;code&gt;python&lt;/code&gt;と&lt;code&gt;javascript&lt;/code&gt;両方あるんですけどね)。解析結果をMAPに投影したり、reference（左）を参照したり、Consoleに吐き出したデータを確認することができるのでかなり便利です。が、データを落とした後で高度な解析を行いたい場合はpythonを使ったほうが慣れているので今回はAPIを使用しています。
話が脱線しました。さて、&lt;code&gt;Earth Engine&lt;/code&gt;の承認を得たら、&lt;code&gt;pip&lt;/code&gt;で&lt;code&gt;earthengine-api&lt;/code&gt;をインストールしておきます。そして、コマンドプロンプト上で、&lt;code&gt;earthengine authenticate&lt;/code&gt;と打ちます。そうすると、勝手にブラウザが立ち上がり、以下のように&lt;code&gt;python api&lt;/code&gt;のauthenticationを行う画面がでますので「次へ」を押下します。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine1.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;次に以下のような画面にいきますので、そのまま承認します。これでauthenticationの完成です。&lt;code&gt;python&lt;/code&gt;からAPIが使えます。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine2.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;python-apiを用いた衛星画像データの取得&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Python APIを用いた衛星画像データの取得&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Python&lt;/code&gt; APIを使用する準備ができました。ここからは衛星画像データを取得していきます。以下にあるように&lt;code&gt;Earth Engine&lt;/code&gt;にはたくさんのデータセットが存在します。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/earth-engine/datasets/&#34; class=&#34;uri&#34;&gt;https://developers.google.com/earth-engine/datasets/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;今回は&lt;code&gt;VIIRS Stray Light Corrected Nighttime Day/Night Band Composites Version 1&lt;/code&gt;というデータセットを使用します。このデータセットは世界中の夜間光の光量を月次単位で平均し、提供するものです。サンプル期間は2014-01~現在です。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Earth Engine&lt;/code&gt;にはいくつかの固有なデータ型が存在します。覚えておくべきものは以下の3つです。&lt;/p&gt;
&lt;div id=&#34;image&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Image…&lt;/h3&gt;
&lt;p&gt;ある１時点における&lt;code&gt;raste&lt;/code&gt;rデータです。&lt;code&gt;image&lt;/code&gt;オブジェクトはいくつかの&lt;code&gt;band&lt;/code&gt;で構成されています。この&lt;code&gt;band&lt;/code&gt;はデータによって異なりますが、おおよそのデータは&lt;code&gt;band&lt;/code&gt;それぞれがRGB値を表していたりします。&lt;code&gt;Earth Engine&lt;/code&gt;を使用する上で最も基本的なデータです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;imagecollection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ImageCollection…&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Image&lt;/code&gt;オブジェクトを時系列に並べたオブジェクトです。今回は時系列解析をするのでこのデータを使用します。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;featurecollection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;FeatureCollection…&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;GeoJSON Feature&lt;/code&gt;です。地理情報を表す&lt;code&gt;Geometry&lt;/code&gt;オブジェクトやそのデータのプロパティ（国名等）が格納されています。今回は日本の位置情報を取得する際に使用しています。&lt;/p&gt;
&lt;p&gt;ではコーディングしていきます。まず、日本の地理情報の&lt;code&gt;FeatureCollection&lt;/code&gt;オブジェクトを取得します。地理情報は&lt;code&gt;Fusion Tables&lt;/code&gt;に格納されていますので、IDで引っ張りCountryがJapanのものを抽出します。&lt;code&gt;ee.FeatureCollection()&lt;/code&gt;の引数にIDを入力すれば簡単に取得できます。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import ee
from dateutil.parser import parse

ee.Initialize()

# get Japan geometory as FeatureCollection from fusion table
japan = ee.FeatureCollection(&amp;#39;ft:1tdSwUL7MVpOauSgRzqVTOwdfy17KDbw-1d9omPw&amp;#39;).filter(ee.Filter.eq(&amp;#39;Country&amp;#39;, &amp;#39;Japan&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;次に夜間光の衛星画像を取得してみます。こちらも&lt;code&gt;ee.ImageCollection()&lt;/code&gt;にデータセットのIDを渡すと取得できます。なお、ここでは&lt;code&gt;band&lt;/code&gt;を月次の平均光量である&lt;code&gt;avg_rad&lt;/code&gt;に抽出しています。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get night-light data from earth engine from 2014-01-01 to 2019-01-01
dataset = ee.ImageCollection(&amp;#39;NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG&amp;#39;).filter(ee.Filter.date(&amp;#39;2014-01-01&amp;#39;,&amp;#39;2019-01-01&amp;#39;)).select(&amp;#39;avg_rad&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;取得した衛星画像を日本周辺に切り出し、画像ファイルとして出力してみましょう。画像ファイルの出力は&lt;code&gt;image&lt;/code&gt;オブジェクトで可能です（そうでないと画像がたくさん出てきてしまいますからね。。。）。今取得したのは&lt;code&gt;ImageCollection&lt;/code&gt;オブジェクトですから&lt;code&gt;Image&lt;/code&gt;オブジェクトへ圧縮してやる必要があります（上が&lt;code&gt;ImageCollection&lt;/code&gt;オブジェクト、下が圧縮された&lt;code&gt;Image&lt;/code&gt;オブジェクト）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://developers.google.com/earth-engine/images/Reduce_ImageCollection.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ここでは、&lt;code&gt;ImageCollection&lt;/code&gt;オブジェクトの中にあるの&lt;code&gt;Image&lt;/code&gt;オブジェクトの平均値をとってサンプル期間の平均的な画像を出力してみたいと思います。&lt;code&gt;ImageCollection.mean()&lt;/code&gt;でできます。また、&lt;code&gt;.visualize({min:0.5})&lt;/code&gt;でピクセル値が0.5以上でフィルターをかけています。こうしないと雲と思われるものやゴミ？みたいなものがついてしまいます。次に、ここまで加工した画像データをダウンロードするurlを&lt;code&gt;.getDownloadURL&lt;/code&gt;メソッドで取得しています。その際、&lt;code&gt;region&lt;/code&gt;で切り出す範囲をポリゴン値で指定し、&lt;code&gt;scale&lt;/code&gt;でデータの解像度を指定しています（&lt;code&gt;scale&lt;/code&gt;が小さすぎると処理が重すぎるらしくエラーが出て処理できません）。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dataset.mean().visualize(min=0.5).getDownloadURL(dict(name=&amp;#39;thumbnail&amp;#39;,region=[[[120.3345348936478, 46.853488838010854],[119.8071911436478, 24.598157870729043],[148.6353161436478, 24.75788466523463],[149.3384411436478, 46.61252884462868]]],scale=5000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;取得した画像が以下です。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine6.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;やはり、東京を中心とした関東圏、大阪を中心とした関西圏、愛知、福岡、北海道（札幌周辺）の光量が多く、経済活動が活発であることがわかります。また、陸内よりも沿岸部で光量が多い地域があることがわかります。これは経済活動とは直接関係しない現象のような気もします。今回は分析対象外ですが、北緯38度を境に北側が真っ暗になるのが印象的です。これは言うまでもなく北朝鮮と韓国の境界線ですから、両国の経済活動水準の差が視覚的にコントラストされているのでしょう。今回使用したデータセットは2014年からのものですが、他のデータセットでは1990年代からのデータが取得できるものもあります（その代わり最近のデータは取れませんが）。それらを用いて朝鮮半島や中国の経済発展を観察するのも面白いかもしれません。&lt;/p&gt;
&lt;p&gt;さて、画像は取得できましたがこのままでは解析ができません。ここからは夜間光をピクセル値にマッピングしたデータを取得し、数値的な解析を試みます。ただ、先ほどとはデータ取得の手続きが少し変わります。というのも、今度は日本各地で各ピクセル単位ごとにさまざまな値をとる夜間光を&lt;strong&gt;集約&lt;/strong&gt;し、1つの代用値にしなければならないからです。ピクセルごとの数値を手に入れたところで解析するには手に余ってしまいますからね。イメージは以下のような感じです（Earth Engineサイトから引用）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://developers.google.com/earth-engine/images/Reduce_region_diagram.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;先ほど取得した夜間光の&lt;code&gt;ImageCollection&lt;/code&gt;のある1時点の衛星画像が左です。その中に日本という&lt;code&gt;Region&lt;/code&gt;が存在し、それを&lt;code&gt;ee.Reducer&lt;/code&gt;によって定量的に集約（aggregate）します。Earth Engine APIには&lt;code&gt;.reduceRegions()&lt;/code&gt;メソッドが用意されていますのでそれを用いればいいです。引数は、&lt;code&gt;reducer&lt;/code&gt;=集約方法（ここでは合計値）、&lt;code&gt;collection&lt;/code&gt;=集約をかける&lt;code&gt;region&lt;/code&gt;（&lt;code&gt;FeatureCollection&lt;/code&gt;オブジェクト）、&lt;code&gt;scale&lt;/code&gt;=解像度、です。以下では、&lt;code&gt;ImageCollection&lt;/code&gt;（dataset）の中にある1番目の&lt;code&gt;Image&lt;/code&gt;オブジェクトに&lt;code&gt;.reduceRegions()&lt;/code&gt;メソッドをかけています。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# initialize output box
time0 = dataset.first().get(&amp;#39;system:time_start&amp;#39;);
first = dataset.first().reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(&amp;#39;time_start&amp;#39;, time0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我々は時系列データが欲しいわけですから、&lt;code&gt;ImageCollection&lt;/code&gt;内にある&lt;code&gt;Image&lt;/code&gt;それぞれに対して同じ処理を行う必要があります。Earth Engineには&lt;code&gt;iterate&lt;/code&gt;という便利な関数があり、引数に処理したい関数を渡せばfor文いらずでこの処理を行ってくれます。ここでは&lt;code&gt;Image&lt;/code&gt;オブジェクトに&lt;code&gt;reduceRegions&lt;/code&gt;メソッドを処理した&lt;code&gt;Computed Object&lt;/code&gt;を以前に処理したものとmergeする&lt;code&gt;myfunc&lt;/code&gt;という関数を定義し、それを&lt;code&gt;iterate&lt;/code&gt;に渡しています。最後に、先ほどと同じく生成したデータを&lt;code&gt;getDownloadURL&lt;/code&gt;メソッドを用いてurlを取得しています（ファイル形式はcsv）。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# define reduceRegions function for iteration
def myfunc(image,first):
  added = image.reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(&amp;#39;time_start&amp;#39;, image.get(&amp;#39;system:time_start&amp;#39;))
  return ee.FeatureCollection(first).merge(added)

# implement iteration
nightjp = dataset.filter(ee.Filter.date(&amp;#39;2014-02-01&amp;#39;,&amp;#39;2019-01-01&amp;#39;)).iterate(myfunc,first)

# get url to download
ee.FeatureCollection(nightjp).getDownloadURL(filetype=&amp;#39;csv&amp;#39;,selectors=ee.FeatureCollection(nightjp).first().propertyNames().getInfo())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;CSVファイルのurlが取得できました。この時系列をプロットして今日は終わりにしたいと思います。
データを読み込むとこんな感じです。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import matplotlib.pyplot as plt
import os

os.environ[&amp;#39;QT_QPA_PLATFORM_PLUGIN_PATH&amp;#39;] = &amp;#39;C:/Users/aashi/Anaconda3/Library/plugins/platforms&amp;#39;

plt.style.use(&amp;#39;ggplot&amp;#39;)

nightjp_csv.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   system:index          sum Country  Unnamed: 3  Unnamed: 4
## 0     2014/1/1  881512.4572   Japan         NaN         NaN
## 1     2014/2/1  827345.3551   Japan         NaN         NaN
## 2     2014/3/1  729110.4619   Japan         NaN         NaN
## 3     2014/4/1  612665.8866   Japan         NaN         NaN
## 4     2014/5/1  661434.5027   Japan         NaN         NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.plot(pd.to_datetime(nightjp_csv[&amp;#39;system:index&amp;#39;]),nightjp_csv[&amp;#39;sum&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;かなり季節性がありますね。冬場は日照時間が少ないこともあって光量が増えているみたいです。それにしても急激な増え方ですが。次回はこのデータと景況感の代理変数となる経済統計を元に統計解析を行いたいと思います。おたのしみに。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>rvestでyahoo競馬にある過去のレース結果をスクレイピングしてみた（2回目）</title>
      <link>/post/post11/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/post/post11/</guid>
      <description>&lt;p&gt;2回目になりますが、またrvestで過去のレース結果を落としてみたいと思います。過去の記事を見てないという人は先にそちらをご覧になられることをお勧めします。&lt;/p&gt;
&lt;p&gt;今回データを取り直そうと思ったのは、競馬の分析をした際により多くの項目を説明変数に加えて、分析をしたいと思ったからです。なので、今回は前回のRスクリプトに追記を行う形でプログラムを作成しました。新たに追加したデータ項目は以下の14個です。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;芝かダートか&lt;/li&gt;
&lt;li&gt;右回りか左回りか&lt;/li&gt;
&lt;li&gt;レースコンディション（良や稍重など）&lt;/li&gt;
&lt;li&gt;天候&lt;/li&gt;
&lt;li&gt;馬の毛色（栗毛、鹿毛など）&lt;/li&gt;
&lt;li&gt;馬主&lt;/li&gt;
&lt;li&gt;生産者&lt;/li&gt;
&lt;li&gt;産地&lt;/li&gt;
&lt;li&gt;生年月日&lt;/li&gt;
&lt;li&gt;父馬&lt;/li&gt;
&lt;li&gt;母馬&lt;/li&gt;
&lt;li&gt;そのレースまでの獲得賞金（2003年から入手可能）&lt;/li&gt;
&lt;li&gt;ジョッキーの体重&lt;/li&gt;
&lt;li&gt;ジョッキーの体重の増減&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;実はまだデータ収集は終わっていなくて、Rのプログラムがずっと実行中になっています（3日くらい回しています）。しかし、プログラム自体はきっちり回っているのでスクリプトの紹介をしていこうと思います。もしかしたら追記で結果を書くかもしれません。&lt;/p&gt;
&lt;div id=&#34;スクリプトの中身&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. スクリプトの中身&lt;/h2&gt;
&lt;p&gt;まずはパッケージの呼び出しです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rvestによる競馬データのwebスクレイピング

#install.packages(&amp;quot;rvest&amp;quot;)
#if (!require(&amp;quot;pacman&amp;quot;)) install.packages(&amp;quot;pacman&amp;quot;)
#install.packages(&amp;quot;beepr&amp;quot;)
#install.packages(&amp;quot;RSQLite&amp;quot;)
pacman::p_load(qdapRegex)
library(rvest)
library(stringr)
library(dplyr)
library(beepr)
library(RSQLite)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;かなりwarnningが出るのでそれを禁止し、SQLiteに接続しています&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# warnning禁止
options(warn=-1)

# SQLiteへの接続
con = dbConnect(SQLite(), &amp;quot;horse_data.db&amp;quot;, synchronous=&amp;quot;off&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;1994年からしかオッズが取れないので、1994年から直近までのデータを取得します。yahoo競馬では月ごとにレースがまとめられているので、それを変数として使用しながらデータをとっていきます。基本的には、&lt;a href=&#34;https://keiba.yahoo.co.jp/schedule/list/2018/?month=7&#34;&gt;該当年、該当月のレース結果一覧&lt;/a&gt;へアクセスし、そのページ上の各日の&lt;a href=&#34;https://keiba.yahoo.co.jp/race/list/18020106/&#34;&gt;個々の競馬場ごとのタイムテーブル&lt;/a&gt;へのリンクを取得します。個々の競馬場でレースはだいたい12ほどあるので、そのリンクを取得し、各レースの&lt;a href=&#34;https://keiba.yahoo.co.jp/race/result/1802010601/&#34;&gt;レース結果ページ&lt;/a&gt;にアクセスします。そして、レース結果を取得していきます。まず、各日の個々の競馬場ごとのタイムテーブルへのリンクの取得方法です。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(year in 1994:2019){
  start.time &amp;lt;- Sys.time() # 計算時間を図る
  # yahoo競馬のレース結果一覧ページの取得
  for (k in 1:12){ # kは月を表す
    
    tryCatch(
      {
        keiba.yahoo &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp/schedule/list/&amp;quot;, year,&amp;quot;/?month=&amp;quot;,k)) # 該当年、該当月のレース結果一覧にアクセス
        Sys.sleep(2)
        race_lists &amp;lt;- keiba.yahoo %&amp;gt;%
          html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% 
          html_attr(&amp;quot;href&amp;quot;) # 全urlを取得
        
        # 競馬場ごとの各日のレースリストを取得
        race_lists &amp;lt;- race_lists[str_detect(race_lists, pattern=&amp;quot;race/list/\\d+/&amp;quot;)==1] # 「result」が含まれるurlを抽出
      }
      , error = function(e){signal &amp;lt;- 1}
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ここでは、取得したリンクのurlにresultという文字が含まれているものだけを抽出しています。要はそれが各競馬場のレーステーブルへのリンクとなります。ここからは取得した競馬場のレーステーブルのリンクを用いて、そのページにアクセスし、全12レースそれぞれのレース結果が掲載されているページのリンクを取得していきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    for (j in 1:length(race_lists)){ # jは当該年月にあったレーステーブルへのリンクを表す
      
      tryCatch(
        {
          race_list &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,race_lists[j]))
          race_url &amp;lt;- race_list %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) # 全urlを取得
          
          # レース結果のurlを取得
          race_url &amp;lt;- race_url[str_detect(race_url, pattern=&amp;quot;result&amp;quot;)==1] # 「result」が含まれるurlを抽出
        }
        , error = function(e){signal &amp;lt;- 1}
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;各レース結果へのリンクが取得できたので、ここからはいよいよレース結果の取得とその整形パートに入ります。かなり長ったらしく複雑なコードになってしまいました。レース結果は以下のようなテーブル属性に格納されているので、まずそれを単純に引っ張ってきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;      for (i in 1:length(race_url)){ # iは当該年月当該競馬場で開催されたレースを表す
        
        print(str_c(&amp;quot;現在、&amp;quot;, year, &amp;quot;年&amp;quot;, k, &amp;quot;月&amp;quot;,j, &amp;quot;グループ、&amp;quot;, i,&amp;quot;番目のレースの保存中です&amp;quot;))
        
        tryCatch(
          {
            race1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,race_url[i])) # レース結果のurlを取得
            signal &amp;lt;- 0
            Sys.sleep(2)
          }
          , error = function(e){signal &amp;lt;- 1}
        )
        
        # レースが中止orこれまでの過程でエラーでなければ処理を実行
        if (identical(race1 %&amp;gt;%
                      html_nodes(xpath = &amp;quot;//div[@class = &amp;#39;resultAtt mgnBL fntSS&amp;#39;]&amp;quot;) %&amp;gt;%
                      html_text(),character(0)) == TRUE &amp;amp;&amp;amp; signal == 0){
          
          # レース結果をスクレイピング
          race_result &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//table[@id = &amp;#39;raceScore&amp;#39;]&amp;quot;) %&amp;gt;%
            html_table()
          race_result &amp;lt;- do.call(&amp;quot;data.frame&amp;quot;,race_result) # リストをデータフレームに変更
          
          colnames(race_result) &amp;lt;- c(&amp;quot;order&amp;quot;,&amp;quot;frame_number&amp;quot;,&amp;quot;horse_number&amp;quot;,&amp;quot;horse_name/age&amp;quot;,&amp;quot;time/margin&amp;quot;,&amp;quot;passing_rank/last_3F&amp;quot;,&amp;quot;jockey/weight&amp;quot;,&amp;quot;popularity/odds&amp;quot;,&amp;quot;trainer&amp;quot;) #　列名変更&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;tableをただ取得しただけでは以下のように、一つのセルに複数の情報が入っていたりと分析には使えないデータとなっています。なので、これを成型する必要が出てきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          # 通過順位と上り3Fのタイム
          race_result &amp;lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;(\\d{2}-\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2})&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;\\d{2}\\.\\d&amp;quot;)))
          race_result &amp;lt;- race_result[-6]
          
          # タイムと着差
          race_result &amp;lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;\\d\\.\\d{2}\\.\\d|\\d{2}\\.\\d&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+&amp;quot;)))
          race_result$margin[race_result$order==1] &amp;lt;- &amp;quot;トップ&amp;quot;
          race_result$margin[race_result$margin==&amp;quot;character(0)&amp;quot;] &amp;lt;- &amp;quot;大差&amp;quot;
          race_result$margin[race_result$order==0] &amp;lt;- NA
          race_result &amp;lt;- race_result[-5]
          
          # 馬名、馬齢、馬体重
          race_result &amp;lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;[ァ-ヴー・]+&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;牡\\d+|牝\\d+|せん\\d+&amp;quot;)))
          race_result$horse_sex &amp;lt;- str_extract(race_result$horse_age, pattern = &amp;quot;牡|牝|せん&amp;quot;)
          race_result$horse_age &amp;lt;- str_extract(race_result$horse_age, pattern = &amp;quot;\\d&amp;quot;)
          race_result &amp;lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\d{3}&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\([\\+|\\-]\\d+\\)|\\([\\d+]\\)&amp;quot;)))
          race_result$horse_weight_change &amp;lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
          race_result &amp;lt;- dplyr::mutate(race_result,brinker=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;B&amp;quot;)))
          race_result$brinker[race_result$brinker!=&amp;quot;B&amp;quot;] &amp;lt;- &amp;quot;N&amp;quot;
          race_result &amp;lt;- race_result[-4]
          
          # ジョッキー
          race_result &amp;lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,&amp;quot;[ぁ-ん一-龠]+\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,jockey_weight=as.character(str_extract_all(race_result$`jockey/weight`,&amp;quot;\\d{2}&amp;quot;)))
          race_result$jockey_weight_change &amp;lt;- 0
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;☆&amp;quot;)==1] &amp;lt;- 1
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;△&amp;quot;)==1] &amp;lt;- 2
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;△&amp;quot;)==1] &amp;lt;- 3
          race_result &amp;lt;- race_result[-4]
          
          # オッズと人気
          race_result &amp;lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\(.+\\)&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\d+[^(\\d+.\\d)]&amp;quot;)))
          race_result$odds &amp;lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
          race_result &amp;lt;- race_result[-4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;次に、今取得したtable以外の情報も取り込むことにします。具体的には、レース名や天候、馬場状態、日付、競馬場などです。これらの情報はレース結果ページの上部に掲載されています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          # レース情報
          race_date &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/p[@id = &amp;#39;raceTitDay&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
          race_name &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/h1[@class = &amp;#39;fntB&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
          race_distance &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//p[@id = &amp;#39;raceTitMeta&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
        
          race_result &amp;lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,&amp;quot;\\d+年\\d+月\\d+日&amp;quot;)))
          race_result$race_date &amp;lt;- str_replace_all(race_result$race_date,&amp;quot;年&amp;quot;,&amp;quot;/&amp;quot;)
          race_result$race_date &amp;lt;- str_replace_all(race_result$race_date,&amp;quot;月&amp;quot;,&amp;quot;/&amp;quot;)
          race_result$race_date &amp;lt;- as.Date(race_result$race_date)
          race_course &amp;lt;- as.character(str_extract_all(race_date,pattern = &amp;quot;札幌|函館|福島|新潟|東京|中山|中京|京都|阪神|小倉&amp;quot;))
          race_result$race_course &amp;lt;- race_course
          race_result &amp;lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,&amp;quot;\\s&amp;quot;,&amp;quot;&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,race_distance=as.character(str_extract_all(race_distance,&amp;quot;\\d+m&amp;quot;)))
          race_type=as.character(str_extract_all(race_distance,pattern = &amp;quot;芝|ダート&amp;quot;))
          race_result$type &amp;lt;- race_type
          race_turn &amp;lt;- as.character(str_extract_all(race_distance,pattern = &amp;quot;右|左&amp;quot;))
          race_result$race_turn &amp;lt;- race_turn
          
          if(length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg ryou&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;良&amp;quot;
          } else if (length(race1 %&amp;gt;% 
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg yayaomo&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;稍重&amp;quot;
          } else if (length(race1 %&amp;gt;%
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg omo&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;重&amp;quot;
          } else if (length(race1 %&amp;gt;% 
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg furyou&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;不良&amp;quot;
          } else race_result$race_condition &amp;lt;- &amp;quot;NA&amp;quot;
          
          if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg hare&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;晴れ&amp;quot;
          } else if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg ame&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;曇り&amp;quot;
          } else if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg kumori&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;雨&amp;quot;
          } else race_result$race_weather &amp;lt;- &amp;quot;その他&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;次は各馬の情報です。 実はさきほど取得したtableの馬名はリンクになっており、そのリンクをたどると&lt;a href=&#34;https://keiba.yahoo.co.jp/directory/horse/2015105508/&#34;&gt;各馬の情報&lt;/a&gt;が取得できます（毛色や生年月日など）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          horse_url &amp;lt;- race1 %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) 
          horse_url &amp;lt;- horse_url[str_detect(horse_url, pattern=&amp;quot;directory/horse&amp;quot;)==1] # 馬情報のリンクだけ抽出する
          
          for (l in 1:length(horse_url)){
            tryCatch(
              {
                horse1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,horse_url[l]))
                Sys.sleep(0.5)
                horse_name &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;dirTitName&amp;#39;]/h1[@class = &amp;#39;fntB&amp;#39;]&amp;quot;) %&amp;gt;% 
                  html_text()
                horse &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;dirTitName&amp;#39;]/ul&amp;quot;) %&amp;gt;% 
                  html_text()
                race_result$colour[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;毛色：.+&amp;quot;)) 
                race_result$owner[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;馬主：.+&amp;quot;))
                race_result$farm[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;生産者：.+&amp;quot;))
                race_result$locality[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;産地：.+&amp;quot;))
                race_result$horse_birthday[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;\\d+年\\d+月\\d+日&amp;quot;))
                race_result$father[race_result$horse_name==horse_name] &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;bloodM&amp;#39;][@rowspan = &amp;#39;4&amp;#39;]&amp;quot;) %&amp;gt;% html_text()
                race_result$mother[race_result$horse_name==horse_name] &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;bloodF&amp;#39;][@rowspan = &amp;#39;4&amp;#39;]&amp;quot;) %&amp;gt;% html_text()
              }
              , error = function(e){
                race_result$colour[race_result$horse_name==horse_name] &amp;lt;- NA 
                race_result$owner[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$farm[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$locality[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$horse_birthday[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$father[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$mother[race_result$horse_name==horse_name] &amp;lt;- NA
                }
            )
          }
          
          race_result$colour &amp;lt;- str_replace_all(race_result$colour,&amp;quot;毛色：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$owner &amp;lt;- str_replace_all(race_result$owner,&amp;quot;馬主：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$farm &amp;lt;- str_replace_all(race_result$farm,&amp;quot;生産者：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$locality &amp;lt;- str_replace_all(race_result$locality,&amp;quot;産地：&amp;quot;,&amp;quot;&amp;quot;)
          #race_result$horse_birthday &amp;lt;- str_replace_all(race_result$horse_birthday,&amp;quot;年&amp;quot;,&amp;quot;/&amp;quot;)
          #race_result$horse_birthday &amp;lt;- str_replace_all(race_result$horse_birthday,&amp;quot;月&amp;quot;,&amp;quot;/&amp;quot;)
          #race_result$horse_birthday &amp;lt;- as.Date(race_result$horse_birthday)
          
          race_result &amp;lt;- dplyr::arrange(race_result,horse_number) # 馬番順に並べる&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;次にそのレースまでに獲得した賞金額を落としに行きます。これはレース結果のページの&lt;a href=&#34;https://keiba.yahoo.co.jp/race/denma/1802010601/&#34;&gt;出馬表&lt;/a&gt;と書かれたリンクをたどるとアクセスできます。ここに賞金があるのでそれを取得します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          yosou_url &amp;lt;- race1 %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) 
          yosou_url &amp;lt;- yosou_url[str_detect(yosou_url, pattern=&amp;quot;denma&amp;quot;)==1]
          
          if (length(yosou_url)==1){
          yosou1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,yosou_url)) 
          Sys.sleep(2)
          yosou &amp;lt;- yosou1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;txC&amp;#39;]&amp;quot;) %&amp;gt;% as.character()
          prize &amp;lt;- yosou[grepl(&amp;quot;万&amp;quot;,yosou)==TRUE] %&amp;gt;% str_extract_all(&amp;quot;\\d+万&amp;quot;)
          prize &amp;lt;- t(do.call(&amp;quot;data.frame&amp;quot;,prize)) %&amp;gt;% as.character()
          race_result$prize &amp;lt;- prize
          race_result$prize &amp;lt;- str_replace_all(race_result$prize,&amp;quot;万&amp;quot;,&amp;quot;&amp;quot;) %&amp;gt;% as.numeric()
          } else race_result$prize &amp;lt;- NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;取得した各レース結果を格納するdatasetというデータフレームを作成し、データを格納していきます。1年ごとにそれをSQLite
へ保存していきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          ## ファイル貯めるのかく
          if (k == 1 &amp;amp;&amp;amp; i == 1 &amp;amp;&amp;amp; j == 1){
            dataset &amp;lt;- race_result
          } else {
            dataset &amp;lt;- rbind(dataset,race_result)
          } # if文2の終わり
        }else
        {
          print(&amp;quot;保存できませんでした&amp;quot;) 
        }# if文1の終わり
      } # iループの終わり
    } # jループ終わり
  } # kループの終わり
  beep(3)
  write.csv(dataset,&amp;quot;race_result2.csv&amp;quot;, row.names = FALSE)
  
  if (year == 1994){
    dbWriteTable(con, &amp;quot;race_result&amp;quot;, dataset)
  } else {
    dbWriteTable(con, &amp;quot;temp&amp;quot;, dataset)
    dbSendQuery(con, &amp;quot;INSERT INTO race_result select * from temp&amp;quot;)
    dbSendQuery(con, &amp;quot;DROP TABLE temp&amp;quot;)
  } # ifの終わり
} # yearループの終わり
end.time &amp;lt;- Sys.time()
print(str_c(&amp;quot;処理時間は&amp;quot;,end.time-start.time,&amp;quot;です。&amp;quot;))
beep(5)

options(warn = 1)

dbDisconnect(con)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;以上です。取れたデータは以下のようになりました。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(race_result)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   order frame_number horse_number   trainer passing_rank last_3F   time
## 1    10            1            1   田中 剛        09-09    39.0 1.14.3
## 2    16            1            2 天間 昭一        11-11    40.3 1.15.7
## 3    15            2            3 田中 清隆        14-14    39.4 1.15.1
## 4     9            2            4 中舘 英二        08-08    39.1 1.14.3
## 5    12            3            5 根本 康広        11-11    39.0 1.14.4
## 6     4            3            6 杉浦 宏昭        04-04    38.4 1.13.2
##      margin         horse_name horse_age horse_sex horse_weight
## 1    アタマ     サトノジョニー         3        牡          512
## 2 3 1/2馬身       ツギノイッテ         3        牡          464
## 3     3馬身           ギュウホ         3        牡          444
## 4 2 1/2馬身 セイウンメラビリア         3        牝          466
## 5      クビ サバイバルトリック         3        牝          450
## 6    アタマ       ステイホット         3        牝          474
##   horse_weight_change brinker      jockey jockey_weight jockey_weight_change
## 1                 +30       N   松岡 正海            56                    0
## 2                  +8       N 西田 雄一郎            56                    0
## 3                  +8       N   杉原 誠人            56                    0
## 4                 +10       N   村田 一誠            54                    0
## 5                  -2       N 野中 悠太郎            51                    0
## 6                  -2       N   大野 拓弥            54                    0
##    odds popularity  race_date race_course       race_name race_distance   type
## 1  40.3         9  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 2 340.9        16  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 3 283.1        14  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 4 299.7        15  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 5  26.7         8  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 6   2.4         1  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
##   race_turn race_condition race_weather colour                           owner
## 1        右             良         晴れ   栗毛 株式会社 サトミホースカンパニー
## 2        右             良         晴れ 黒鹿毛                     西村 新一郎
## 3        右             良         晴れ   鹿毛           有限会社 ミルファーム
## 4        右             良         晴れ 青鹿毛                       西山 茂行
## 5        右             良         晴れ 黒鹿毛                       福田 光博
## 6        右             良         晴れ   栗毛                       小林 善一
##           farm   locality horse_birthday                 father
## 1   千代田牧場 新ひだか町  2016年1月29日         オルフェーヴル
## 2    織笠 時男     青森県  2016年4月17日 スクワートルスクワート
## 3    神垣 道弘 新ひだか町  2016年4月19日     ジャングルポケット
## 4  石郷岡 雅樹     新冠町  2016年4月21日     キンシャサノキセキ
## 5     原田牧場     日高町  2016年4月30日       リーチザクラウン
## 6 社台ファーム     千歳市  2016年3月13日     キャプテントゥーレ
##               mother prize
## 1 スパークルジュエル     0
## 2   エプソムアイリス     0
## 3     デライトシーン     0
## 4     ドリームシップ     0
## 5   フリーダムガール   180
## 6     ステイアライヴ   455&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>GPLVMでマルチファクターモデルを構築してみた</title>
      <link>/post/post8/</link>
      <pubDate>Sun, 26 May 2019 00:00:00 +0000</pubDate>
      <guid>/post/post8/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gplvmとは&#34;&gt;1. GPLVMとは&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#最もprimitiveなgp-lvm&#34;&gt;2. &lt;span&gt;最もPrimitiveなGP-LVM&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rでの実装&#34;&gt;3. Rでの実装&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。
ずいぶん前にGianonne et al (2008)のマルチファクターモデルで四半期GDPの予想を行いました。
結果としては、ある程度は予測精度が出ていたものの彼らの論文ほどは満足のいくものではありませんでした。原因としてはクロスセクショナルなデータ不足が大きいと思われ、現在収集方法についてもEXCELを用いて改修中です。しかし一方で、マルチファクターモデルの改善も考えたいと思っています。前回は月次経済統計を主成分分析（実際にはカルマンフィルタ）を用いて次元削減を行い、主成分得点を説明変数としてGDPに回帰しました。今回はこの主成分分析のド発展版である&lt;code&gt;Gaussian Process Latent Variable Model&lt;/code&gt;(GPLVM)を用いてファクターを計算し、それをGDPに回帰したいと思います。&lt;/p&gt;
&lt;div id=&#34;gplvmとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. GPLVMとは&lt;/h2&gt;
&lt;p&gt;GPLVMとは、&lt;code&gt;Gaussian Process&lt;/code&gt; Modelの一種です。以前、&lt;code&gt;Gaussian Process Regression&lt;/code&gt;の記事を書きました。&lt;/p&gt;
&lt;p&gt;最も基本的な&lt;code&gt;Gaussian Process&lt;/code&gt; Modelは上の記事のようなモデルで、非説明変数&lt;span class=&#34;math inline&#34;&gt;\(Y=(y_{1},y_{2},...,y_{n})\)&lt;/span&gt;と説明変数&lt;span class=&#34;math inline&#34;&gt;\(X=(\textbf{x}_{1},\textbf{x}_{2},...,\textbf{x}_{n})\)&lt;/span&gt;があり、以下のような関係式で表される際にそのモデルを直接推定することなしに新たな説明変数&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;の入力に対し、非説明変数&lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;の予測値をはじき出すというものでした。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle y_{i}  = \textbf{w}^{T}\phi(\textbf{x}_{i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の説明変数ベクトル、&lt;span class=&#34;math inline&#34;&gt;\(\phi(・)\)&lt;/span&gt;は非線形関数、 &lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}^{T}\)&lt;/span&gt;は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x}_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\)&lt;/span&gt;を想定しています（&lt;span class=&#34;math inline&#34;&gt;\(x_{1,i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の入力データ&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{i}\)&lt;/span&gt;の１番目の変数）。詳しくは過去記事を参照してください。&lt;/p&gt;
&lt;p&gt;今回やるGPLVMは説明変数ベクトルが観測できない潜在変数（Latent Variable）であるところが特徴です。以下のスライドが非常にわかりやすいですが、GP-LVMは確率的主成分分析（PPCA）の非線形版という位置付けになっています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.slideshare.net/antiplastics/pcagplvm&#34;&gt;資料&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では具体的な説明に移ります。GPLVMは主成分分析の発展版ですので、主に次元削減のために行われることを想定しています。つまり、データセットがあったとして、サンプルサイズ&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;よりも変数の次元&lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;が大きいような場合を想定しています。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;最もprimitiveなgp-lvm&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. &lt;a href=&#34;http://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf&#34;&gt;最もPrimitiveなGP-LVM&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;先述したようにGPLVMはPPCAの非線形版です。なので、GPLVMを説明するスタートはPPCAになります。観測可能な&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt;次元データセットを&lt;span class=&#34;math inline&#34;&gt;\(\{\textbf{y}_{n}\}_{n=1}^{N}\)&lt;/span&gt;とします。そして、潜在変数を&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;とおきます。今、データセットと潜在変数の間には以下のような関係があるとします。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{y}_{n} = \textbf{W}\textbf{x}_{n} + \epsilon_{n}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;はウェイト行列、&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{n}\)&lt;/span&gt;はかく乱項で&lt;span class=&#34;math inline&#34;&gt;\(N(0,\beta^{-1}\textbf{I})\)&lt;/span&gt;に従います（被説明変数が多次元になることに注意）。また、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(N(0,\textbf{I})\)&lt;/span&gt;に従います。このとき、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{y}_{n}\)&lt;/span&gt;の尤度を&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;を周辺化することで表現すると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\displaystyle p(\textbf{y}_{n}|\textbf{W},\beta) &amp;amp;=&amp;amp; \int p(\textbf{y}_{n}|\textbf{x}_{n},\textbf{W},\beta)N(0,\textbf{I})d\textbf{x}_{n} \\
\displaystyle &amp;amp;=&amp;amp; \int N(\textbf{W}\textbf{x}_{n},\beta^{-1}\textbf{I})N(0,\textbf{I})d\textbf{x}_{n} \\
&amp;amp;=&amp;amp; N(0,\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}) \\
\displaystyle &amp;amp;=&amp;amp; \frac{1}{(2\pi)^{DN/2}|\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}|^{N/2}}\exp(\frac{1}{2}\textbf{tr}( (\textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I})^{-1}\textbf{YY}^{T}))
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y}_{n}|\textbf{x}_{n},\textbf{W},\beta)=N(\textbf{W}\textbf{x}_{n},\beta^{-1}\textbf{I})\)&lt;/span&gt;です。平均と分散は以下から求めました。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
E(\textbf{y}_{n}|\textbf{W},\beta) &amp;amp;=&amp;amp; E(\textbf{W}\textbf{x}_{n} + \epsilon_{n}) \\
&amp;amp;=&amp;amp; E(\textbf{W}\textbf{x}_{n}) + E(\epsilon_{n}) \\
&amp;amp;=&amp;amp; \textbf{W}E(\textbf{x}_{n}) + E(\epsilon_{n}) = 0
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
E[(\textbf{y}_{n}|\textbf{W},\beta)(\textbf{y}_{n}|\textbf{W},\beta)^{T}] &amp;amp;=&amp;amp; E[ (\textbf{W}\textbf{x}_{n} + \epsilon_{n} - 0)(\textbf{W}\textbf{x}_{n} + \epsilon_{n} - 0)^{T} ] \\
&amp;amp;=&amp;amp; E[ (\textbf{W}\textbf{x}_{n} + \epsilon_{n})(\textbf{W}\textbf{x}_{n} + \epsilon_{n})^{T} ] \\
&amp;amp;=&amp;amp; E[ \textbf{W}\textbf{x}_{n}(\textbf{W}\textbf{x}_{n})^{T} + \textbf{W}\textbf{x}_{n}\epsilon_{n}^{T} + \epsilon_{n}\textbf{W}\textbf{x}_{n}^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;amp;=&amp;amp; E[ \textbf{W}\textbf{x}_{n}(\textbf{W}\textbf{x}_{n})^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;amp;=&amp;amp; E[ \textbf{W}\textbf{x}_{n}\textbf{x}_{n}^{T}\textbf{W}^{T} + \epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;amp;=&amp;amp; E[ \textbf{W}\textbf{x}_{n}\textbf{x}_{n}^{T}\textbf{W}^{T}] + E[\epsilon_{n}\epsilon_{n}^{T} ] \\
&amp;amp;=&amp;amp; \textbf{W}\textbf{W}^{T} + \beta^{-1}\textbf{I}
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を求めるためには&lt;span class=&#34;math inline&#34;&gt;\(\textbf{y}_{n}\)&lt;/span&gt;がi.i.d.と仮定し、以下のようなデータセット全体の尤度を最大化すれば良いことになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{Y}|\textbf{W},\beta) = \prod_{n=1}^{N}p(\textbf{y}_{n}|\textbf{W},\beta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(N×D\)&lt;/span&gt;の計画行列です。このように、PPCAでは&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;を周辺化し、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を最適化します。逆に、Lawrence(2004)では&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を周辺化し、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_{n}\)&lt;/span&gt;します（理由は後述）。&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を周辺化するために、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;に事前分布を与えましょう。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{W}) = \prod_{i=1}^{D}N(\textbf{w}_{i}|0,\alpha^{-1}\textbf{I})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}_{i}\)&lt;/span&gt;はウェイト行列&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;の&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の列です。では、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;を周辺化して&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;の尤度関数を導出してみます。やり方はさっきとほぼ同じなので省略します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{Y}|\textbf{X},\beta) = \frac{1}{(2\pi)^{DN/2}|K|^{D/2}}\exp(\frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T}))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}=\alpha^2\textbf{X}\textbf{X}^{T} + \beta^{-1}\textbf{I}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{Y}|\textbf{X},\beta)\)&lt;/span&gt;の分散共分散行列で、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}=(\textbf{x}_{1},\textbf{x}_{2},...,\textbf{x}_{N})^{T}\)&lt;/span&gt;は入力ベクトルです。対数尤度は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle L = - \frac{DN}{2}\ln{2\pi} - \frac{1}{2}\ln{|\textbf{K}|} - \frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;周辺化のおかげでウェイト&lt;span class=&#34;math inline&#34;&gt;\(\textbf{W}\)&lt;/span&gt;が消えたのでこれを&lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;で微分してみましょう。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle\frac{\partial L}{ \partial \textbf{X}} = \alpha^2 \textbf{K}^{-1}\textbf{Y}\textbf{Y}^{T}\textbf{K}^{-1}\textbf{X} - \alpha^2 D\textbf{K}^{-1}\textbf{X}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここから、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{1}{D}\textbf{Y}\textbf{Y}^{T}\textbf{K}^{-1}\textbf{X} = \textbf{X}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、特異値分解を用いると&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{X} = \textbf{ULV}^{T}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。&lt;span class=&#34;math inline&#34;&gt;\(\textbf{U} = (\textbf{u}_{1},\textbf{u}_{2},...,\textbf{u}_{q})\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(N×q\)&lt;/span&gt;直交行列、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{L} = diag(l_{1},l_{2},..., l_{q})\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(q×q\)&lt;/span&gt;の特異値を対角成分に並べた行列、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{V}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(q×q\)&lt;/span&gt;直交行列です。これを先ほどの式に代入すると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\textbf{K}^{-1}\textbf{X} &amp;amp;=&amp;amp; (\alpha^2\textbf{X}\textbf{X}^{T} + \beta^{-1}\textbf{I})^{-1}\textbf{X} \\
&amp;amp;=&amp;amp; \textbf{X}(\alpha^2\textbf{X}^{T}\textbf{X} + \beta^{-1}\textbf{I})^{-1} \\
&amp;amp;=&amp;amp; \textbf{ULV}^{T}(\alpha^2\textbf{VLU}^{T}\textbf{ULV}^{T} + \beta^{-1}\textbf{I})^{-1} \\
&amp;amp;=&amp;amp; \textbf{ULV}^{T}\textbf{V}(\alpha^2\textbf{LU}^{T}\textbf{UL} + \beta^{-1}\textbf{I}^{-1})\textbf{V}^{T} \\
&amp;amp;=&amp;amp; \textbf{UL}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1}\textbf{V}^{T}
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;なので、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\displaystyle \frac{1}{D}\textbf{Y}\textbf{Y}^{T}\textbf{UL}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1})\textbf{V}^{T} &amp;amp;=&amp;amp; \textbf{ULV}^{T}\\
\displaystyle \textbf{Y}\textbf{Y}^{T}\textbf{UL} &amp;amp;=&amp;amp; D\textbf{U}(\alpha^2\textbf{L}^{2} + \beta^{-1}\textbf{I})^{-1}\textbf{L} \\
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。&lt;span class=&#34;math inline&#34;&gt;\(l_{j}\)&lt;/span&gt;が0でなければ、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\textbf{u}_{j} = D(\alpha^2 l_{j}^{2} + \beta^{-1})\textbf{u}_{j}\)&lt;/span&gt;となり、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{U}\)&lt;/span&gt;のそれぞれの列は&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\)&lt;/span&gt;の固有ベクトルであり、対応する固有値&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{j}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(D(\alpha^2 l_{j}^{2} + \beta^{-1})\)&lt;/span&gt;となります。つまり、未知であった&lt;span class=&#34;math inline&#34;&gt;\(X=ULV\)&lt;/span&gt;が実は&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\)&lt;/span&gt;の固有値問題から求めることが出来るというわけです。&lt;span class=&#34;math inline&#34;&gt;\(l_{j}\)&lt;/span&gt;は上式を利用して、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle l_{j} = (\frac{\lambda_{j}}{D\alpha^2} - \frac{1}{\beta\alpha^2})^{1/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\)&lt;/span&gt;の固有値&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{j}\)&lt;/span&gt;とパラメータから求められることがわかります。よって、&lt;span class=&#34;math inline&#34;&gt;\(X=ULV\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{X} = \textbf{U}_{q}\textbf{L}\textbf{V}^{T}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{U}_{q}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\textbf{Y}^{T}\)&lt;/span&gt;の固有ベクトルを&lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;個取り出したものです。&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;が限りなく大きければ（=観測誤差が限りなく小さければ）通常のPCAと一致します。&lt;/p&gt;
&lt;p&gt;以上がPPCAです。GPLVMはPPCAで確率モデルとして想定していた以下のモデルを拡張します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{y}_{n} = \textbf{W}^{T}\textbf{x}_{n} + \epsilon_{n}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;具体的には、通常のガウス過程と同様、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \textbf{y}_{n}  = \textbf{W}^{T}\phi(\textbf{x}_{n})+ \epsilon_{n}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;という風に基底関数&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x}_{n})\)&lt;/span&gt;をかませて拡張します。&lt;span class=&#34;math inline&#34;&gt;\(\phi(・)\)&lt;/span&gt;は平均&lt;span class=&#34;math inline&#34;&gt;\(\textbf{0}\)&lt;/span&gt;、分散共分散行列&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}_{\textbf{x}}\)&lt;/span&gt;のガウス過程と仮定します。分散共分散行列&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}_{\textbf{x}}\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\textbf{K}_{\textbf{x}} = \alpha^2\phi(\textbf{x})\phi(\textbf{x})^T
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;であり、入力ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{・})\)&lt;/span&gt;で非線形変換した特徴量&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x})\)&lt;/span&gt;が近いほど、出力値&lt;span class=&#34;math inline&#34;&gt;\(\textbf{Y}\)&lt;/span&gt;も近くなりやすいという性質があることになります。GPLVMではこの性質を逆に利用しています。つまり、出力値&lt;span class=&#34;math inline&#34;&gt;\(Y_i\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(Y_j\)&lt;/span&gt;が近い→&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x}_i)\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{x}_j)\)&lt;/span&gt;が近い（内積が大きい）→&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}_{x,ij}\)&lt;/span&gt;が大きい→観測不可能なデータ&lt;span class=&#34;math inline&#34;&gt;\(X_{i}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(X_{j}\)&lt;/span&gt;は近い値（or同じようなパターン）をとる。
この議論からもわかるように、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{K}_{\textbf{x}}\)&lt;/span&gt;は入力ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;それぞれの距離を表したものになります。分散共分散行列の計算には入力ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\textbf{X}\)&lt;/span&gt;を基底関数&lt;span class=&#34;math inline&#34;&gt;\(\phi(\textbf{・})\)&lt;/span&gt;で非線形変換した後、内積を求めるといったことをする必要はなく、カーネル関数を計算するのみでOKです。今回は王道中の王道RBFカーネルを使用していますので、これを例説明します。&lt;/p&gt;
&lt;p&gt;RBFカーネル（スカラーに対する）
&lt;span class=&#34;math display&#34;&gt;\[
\theta_{1}\exp(-\frac{1}{\theta_{2}}(x-x^T)^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;このRBFカーネルは以下の基底関数と対応しています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\phi(x)_h = \tau\exp(-\frac{1}{r}(x-h)^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;例えば、この基底関数で入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;を変換したものを&lt;span class=&#34;math inline&#34;&gt;\(2H^2+1\)&lt;/span&gt;個並べた関数を&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\phi(x) = (\phi(x)_{-H^2}, ..., \phi(x)_{0},...,\phi(x)_{H^2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;の特徴量だとすると&lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;との共分散&lt;span class=&#34;math inline&#34;&gt;\(K_{x}(x,x&amp;#39;)\)&lt;/span&gt;は内積の和なので&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
K_{x}(x,x&amp;#39;) = \sum_{h=-H^2}^{H^2}\phi_{h}(x)\phi_{h}(x&amp;#39;)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(H \to \infty\)&lt;/span&gt;とし、グリッドを極限まで細かくしてみます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
K_{x}(x,x&amp;#39;) &amp;amp;=&amp;amp; \lim_{H \to \infty}\sum_{h=-H^2}^{H^2}\phi_{h}(x)\phi_{h}(x&amp;#39;) \\
&amp;amp;\to&amp;amp;\int_{-\infty}^{\infty}\tau\exp(-\frac{1}{r}(x-h)^2)\tau\exp(-\frac{1}{r}(x&amp;#39;-h)^2)dh \\
&amp;amp;=&amp;amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{1}{r}\{(x-h)^2+(x&amp;#39;-h)^2\})dh \\
&amp;amp;=&amp;amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{1}{r}\{2(h-\frac{x+x&amp;#39;}{2})^2+\frac{1}{2}(x-x&amp;#39;)^2\})dh \\
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。&lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt;に関係のない部分を積分の外に出します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
&amp;amp;=&amp;amp; \tau^2 \int_{-\infty}^{\infty}\exp(-\frac{2}{r}(h-\frac{x+x&amp;#39;}{2})^2)dh\exp(-\frac{1}{2r}(x-x&amp;#39;)^2) \\
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;残った積分を見ると、正規分布の正規化定数と等しいことがわかります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\int_{-\infty}^{\infty}\exp(-\frac{1}{2\sigma}(h-\frac{x+x&amp;#39;}{2})^2)dh &amp;amp;=&amp;amp;  \int_{-\infty}^{\infty}\exp(-\frac{2}{r}(h-\frac{x+x&amp;#39;}{2})^2)dh\\
\sigma &amp;amp;=&amp;amp; \frac{r}{4}
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となるので、ガウス積分の公式を用いて&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
&amp;amp;=&amp;amp; \tau^2 \sqrt{\frac{\pi r}{2}}\exp(-\frac{1}{2r}(x-x&amp;#39;)^2)　\\
&amp;amp;=&amp;amp; \theta_{1}\exp(-\frac{1}{\theta_{2}}(x-x’)^2)
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となり、RBFカーネルと等しくなることがわかります。よって、RBFカーネルで計算した共分散は上述した基底関数で入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;を無限次元へ拡張した特徴量ベクトルの内積から計算した共分散と同値になることがわかります。つまり、入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(x&amp;#39;\)&lt;/span&gt;のスカラーの計算のみで&lt;span class=&#34;math inline&#34;&gt;\(K_{x}(x,x&amp;#39;)\)&lt;/span&gt;ができてしまうという夢のような計算効率化が可能になるわけです。無限次元特徴量ベクトルの回帰問題なんて普通計算できませんからね。。。カーネル関数は偉大です。
前の記事にも載せましたが、RBFカーネルで分散共分散行列を計算したガウス過程のサンプルパスは以下通りです（&lt;span class=&#34;math inline&#34;&gt;\(\theta_1=1,\theta_2=0.5\)&lt;/span&gt;）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define Kernel function
Kernel_Mat &amp;lt;- function(X,sigma,beta){
  N &amp;lt;- NROW(X)
  K &amp;lt;- matrix(0,N,N)
  for (i in 1:N) {
    for (k in 1:N) {
      if(i==k) kdelta = 1 else kdelta = 0
      K[i,k] &amp;lt;- K[k,i] &amp;lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta
    }
  }
  return(K)
}

N &amp;lt;- 10 # max value of X
M &amp;lt;- 1000 # sample size
X &amp;lt;- matrix(seq(1,N,length=M),M,1) # create X
testK &amp;lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix

library(MASS)

P &amp;lt;- 6 # num of sample path
Y &amp;lt;- matrix(0,M,P) # define Y

for(i in 1:P){
  Y[,i] &amp;lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y
}

# Plot
matplot(x=X,y=Y,type = &amp;quot;l&amp;quot;,lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;非常に滑らかな関数となっていることがわかります。RBFのほかにもカーネル関数は存在します。カーネル関数を変えると基底関数が変わりますから、サンプルパスは大きく変わることになります。&lt;/p&gt;
&lt;p&gt;GPLVMの推定方法に話を進めましょう。PPCAの時と同じく、以下の尤度関数を最大化する観測不能な入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;を推定値とします。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle L = - \frac{DN}{2}\ln{2\pi} - \frac{1}{2}\ln{|\textbf{K}|} - \frac{1}{2}\textbf{tr}(\textbf{K}^{-1}\textbf{YY}^{T})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ただ、PPCAとは異なり、その値は解析的に求めることができません。尤度関数の導関数は今や複雑な関数であり、展開することができないからです。よって、共役勾配法を用いて数値的に計算するのが主流なようです（自分は準ニュートン法で実装）。解析的、数値的のどちらにせよ導関数を求めておくことは必要なので、導関数を求めてみます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial L}{\partial \textbf{K}_x} = \frac{1}{2}(\textbf{K}_x^{-1}\textbf{Y}\textbf{Y}^T\textbf{K}_x^{-1}-D\textbf{K}_x^{-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;なので、チェーンルールから&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial L}{\partial \textbf{x}} = \frac{\partial L}{\partial \textbf{K}_x}\frac{\partial \textbf{K}_x}{\partial \textbf{x}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;なので、カーネル関数を決め、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}\)&lt;/span&gt;に初期値を与えてやれば勾配法によって尤度&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;が最大となる点を探索することができます。今回使用するRBFカーネルで&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial \textbf{K}_x}{\partial x_{nj}}\)&lt;/span&gt;を計算してみます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray*}
\frac{\partial \textbf{K}_x(\textbf{x}_n,\textbf{x}_n&amp;#39;)}{x_{nj}}&amp;amp;=&amp;amp;\frac{\partial\theta_{1}\exp(-\frac{|\textbf{x}_n-\textbf{x}_n&amp;#39;|^2}{\theta_{2}})}{\partial x_{nk}} \\
&amp;amp;=&amp;amp; \frac{\partial\theta_{1}\exp(-\frac{(\textbf{x}_n-\textbf{x}_n&amp;#39;)^T(\textbf{x}_n-\textbf{x}_n&amp;#39;)}{\theta_{2}})}{\partial x_{nk}} \\
&amp;amp;=&amp;amp; \frac{\partial\theta_{1}\exp(-\frac{-(\textbf{x}_n^T\textbf{x}_n-2\textbf{x}_n&amp;#39;^T\textbf{x}_n+\textbf{x}_n&amp;#39;^T\textbf{x}_n&amp;#39;)}{\theta_{2}})}{\partial x_{nk}} \\
&amp;amp;=&amp;amp; -2\textbf{K}_x(\textbf{x}_n\textbf{x}_n&amp;#39;)\frac{(x_{nj}-x_{n&amp;#39;j})}{\theta_2}
\end{eqnarray*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;番目の潜在変数の&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;番目のサンプルそれぞれに導関数を計算し、それを分散共分散行列と同じ行列に整理したものと&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial L}{\partial \textbf{K}_x}\)&lt;/span&gt;との要素ごとの積を足し合わせたものが勾配となります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rでの実装&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Rでの実装&lt;/h2&gt;
&lt;p&gt;GPLVMを&lt;code&gt;R&lt;/code&gt;で実装します。使用するデータは以前giannoneの記事で使用したものと同じものです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ESTIMATE_GPLVM &amp;lt;- function(Y,P,sigma){
  # 1. Set initial value
  Y &amp;lt;- as.matrix(Y)
  eigenvector &amp;lt;- eigen(cov(Y))$vectors
  X &amp;lt;- Y%*%eigenvector[,1:P] # initial value
  N &amp;lt;- NROW(Y) # Sample Size
  D &amp;lt;- NCOL(Y) # Dimention of dataset
  X0 &amp;lt;- c(as.vector(X))
  sigma &amp;lt;- var(matrix(Y,dim(Y)[1]*dim(Y)[2],1))
  
  # 2. Define log likelihood function
  loglik &amp;lt;- function(X0,Y,N,P,D,beta,sigma){
    X &amp;lt;- matrix(X0,N,P)
    K &amp;lt;- matrix(0,N,N)
    scale &amp;lt;- diag(sqrt(3/((apply(X, 2, max) -apply(X, 2, min))^2)))
    X &amp;lt;- X%*%scale
    for (i in 1:N) {
      for (k in 1:N) {
         if(i==k) kdelta = 1 else kdelta = 0
         K[i,k] &amp;lt;- K[k,i] &amp;lt;- sigma*exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)
       }
     }
 
    L &amp;lt;- - D*N/2*log(2*pi) - D/2*log(det(K)) - 1/2*sum(diag(ginv(K)%*%Y%*%t(Y))) #loglikelihood

    return(L)
  }
  
  # 3. Define derivatives of log likelihood function
  dloglik &amp;lt;- function(X0,P,D,N,Y,beta,sigma){
    X &amp;lt;- matrix(X0,N,P)
    K &amp;lt;- matrix(0,N,N)
    for (i in 1:N) {
      for (k in 1:N) {
        if(i==k) kdelta = 1 else kdelta = 0
        K[i,k] &amp;lt;- K[k,i] &amp;lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])*0.5) + beta^(-1)*kdelta + beta^(-1)
      }
    }
    invK &amp;lt;- ginv(K)
    dLdK &amp;lt;- invK%*%Y%*%t(Y)%*%invK - D*invK
    dLdx &amp;lt;- matrix(0,N,P)
    
    for (j in 1:P){
      for(i in 1:N){
        dKdx &amp;lt;- matrix(0,N,N)
        for (k in 1:N){
          dKdx[i,k] &amp;lt;- dKdx[k,i] &amp;lt;- -exp(-(t(X[i,]-X[k,])%*%(X[i,]-X[k,]))*0.5)*((X[i,j]-X[k,j])*0.5)
        }
        dLdx[i,j] &amp;lt;- sum(dLdK*dKdx)
      }
    }
    
    return(dLdx)
  }
  
  # 4. Optimization
  res &amp;lt;- optim(X0, loglik, dloglik, Y = Y, N=N, P=P, D=D, beta = exp(2), sigma = sigma,
               method = &amp;quot;BFGS&amp;quot;, control = list(fnscale = -1,trace=1000,maxit=10000))
  output &amp;lt;- matrix(res$par,N,P)
  result &amp;lt;- list(output,res,P)
  names(result) &amp;lt;- c(&amp;quot;output&amp;quot;,&amp;quot;res&amp;quot;,&amp;quot;P&amp;quot;)
  return(result)
}

GPLVM_SELECT &amp;lt;- function(Y){
  D &amp;lt;- NCOL(Y)
  library(stringr)
  for (i in 1:D){
    if (i == 1){
      result &amp;lt;- ESTIMATE_GPLVM(Y,i)
      P &amp;lt;- 2
      print(str_c(&amp;quot;STEP&amp;quot;, i, &amp;quot; loglikelihood &amp;quot;, as.numeric(result$res$value)))
    }else{
      temp &amp;lt;- ESTIMATE_GPLVM(Y,i)
      print(str_c(&amp;quot;STEP&amp;quot;, i, &amp;quot; loglikelihood &amp;quot;, as.numeric(temp$res$value)))
      if (result$res$value &amp;lt; temp$res$value){
        result &amp;lt;- temp
        P &amp;lt;- i
      }
    }
  }
  print(str_c(&amp;quot;The optimal number of X is &amp;quot;, P))
  print(str_c(&amp;quot;loglikelihood &amp;quot;, as.numeric(result$res$value)))
  return(result)
}

result &amp;lt;- ESTIMATE_GPLVM(scale(Y),5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## initial  value 613.864608 
## final  value 609.080329 
## converged&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

ggplot(gather(as.data.frame(result$output),key = name,value = value),
       aes(x=rep(dataset1$publication,5),y=value,colour=name)) + 
  geom_line(size=1) +
  xlab(&amp;quot;Date&amp;quot;) +
  ggtitle(&amp;quot;5 economic factors&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(xts)
X.xts &amp;lt;- xts(result$output,order.by = dataset1$publication)
X.q.xts &amp;lt;- apply.quarterly(X.xts,mean)
X.3m.xts &amp;lt;- X.xts[endpoints(X.xts,on=&amp;quot;quarters&amp;quot;),]

if (months(index(X.q.xts)[NROW(X.q.xts)]) %in% c(&amp;quot;3月&amp;quot;,&amp;quot;6月&amp;quot;,&amp;quot;9月&amp;quot;,&amp;quot;12月&amp;quot;)){
} else X.q.xts &amp;lt;- X.q.xts[-NROW(X.q.xts),]
if (months(index(X.3m.xts)[NROW(X.3m.xts)]) %in% c(&amp;quot;3月&amp;quot;,&amp;quot;6月&amp;quot;,&amp;quot;9月&amp;quot;,&amp;quot;12月&amp;quot;)){
} else X.3m.xts &amp;lt;- X.3m.xts[-NROW(X.3m.xts),]

colnames(X.xts) &amp;lt;- c(&amp;quot;factor1&amp;quot;,&amp;quot;factor2&amp;quot;,&amp;quot;factor3&amp;quot;,&amp;quot;factor4&amp;quot;,&amp;quot;factor5&amp;quot;) 

GDP$publication &amp;lt;- GDP$publication + months(2)
GDP.q &amp;lt;- GDP[GDP$publication&amp;gt;=index(X.q.xts)[1] &amp;amp; GDP$publication&amp;lt;=index(X.q.xts)[NROW(X.q.xts)],]

rg &amp;lt;- lm(scale(GDP.q$GDP)~X.q.xts[-54])
rg2 &amp;lt;- lm(scale(GDP.q$GDP)~X.3m.xts[-54])

summary(rg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = scale(GDP.q$GDP) ~ X.q.xts[-54])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.31774 -0.06927 -0.03224  0.06690  0.31062 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)      0.0445000  0.0317777   1.400    0.177    
## X.q.xts[-54]X.1 -0.3181108  0.0106335 -29.916  &amp;lt; 2e-16 ***
## X.q.xts[-54]X.2  0.0004621  0.0175913   0.026    0.979    
## X.q.xts[-54]X.3  0.1737612  0.0249186   6.973 9.09e-07 ***
## X.q.xts[-54]X.4 -0.0060035  0.0376324  -0.160    0.875    
## X.q.xts[-54]X.5  0.0646009  0.0430678   1.500    0.149    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.1617 on 20 degrees of freedom
## Multiple R-squared:  0.9791, Adjusted R-squared:  0.9739 
## F-statistic: 187.3 on 5 and 20 DF,  p-value: 4.402e-16&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(rg2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = scale(GDP.q$GDP) ~ X.3m.xts[-54])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.35613 -0.11430  0.00258  0.15171  0.36506 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)    -0.003798   0.041789  -0.091    0.928    
## X.3m.xts[-54]1 -0.312411   0.014008 -22.303 1.34e-15 ***
## X.3m.xts[-54]2  0.022873   0.025672   0.891    0.384    
## X.3m.xts[-54]3  0.152350   0.031080   4.902 8.62e-05 ***
## X.3m.xts[-54]4 -0.019237   0.047809  -0.402    0.692    
## X.3m.xts[-54]5 -0.032419   0.055662  -0.582    0.567    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.21 on 20 degrees of freedom
## Multiple R-squared:  0.9647, Adjusted R-squared:  0.9559 
## F-statistic: 109.3 on 5 and 20 DF,  p-value: 8.102e-14&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;決定係数が大幅に改善しました。
3ヶ月の平均をとったファクターの方がパフォーマンスが良さそうなので、こちらで実際のGDPと予測値のプロットを行ってみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gather(data.frame(fit=rg$fitted.values,actual=scale(GDP.q$GDP),Date=GDP.q$publication),key,value,-Date),aes(y=value,x=Date,colour=key)) +
  geom_line(size=1) +
  ggtitle(&amp;quot;fit v.s. actual GDP&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;いかがでしょうか。個人的にはかなりフィッティングできている印象があります（もはや経済理論など不要なのでしょうか）。ただ、最も新しい値を除いては未来の値が情報量として加味された上で推計されていることになりますから、フェアではありません。正しく予測能力を検証するためには四半期ごとに逐次的に回帰を行う必要があります。&lt;/p&gt;
&lt;p&gt;というわけで、アウトサンプルの予測力がどれほどあるのかをテストしてみたいと思います。まず、2005年4月から2007年3月までの月次統計データでファクターを計算し、データ頻度を四半期に集約します。そして、2007年1Qのデータを除いて、GDPに回帰します。回帰したモデルの係数を用いて、2007年1Qのファクターデータで同時点のGDPの予測値を計算し、それを実績値と比較します。次は2005年4月から2007年6月までのデータを用いて･･･という感じでアウトサンプルの予測を行ってみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(lubridate)
test_df &amp;lt;- data.frame()
for (i in as.list(seq(as.Date(&amp;quot;2015-04-01&amp;quot;),as.Date(&amp;quot;2019-03-01&amp;quot;),by=&amp;quot;quarter&amp;quot;))){
  day(i) &amp;lt;- days_in_month(i)
  traindata &amp;lt;- dataset1[dataset1$publication&amp;lt;=i,]
  X_train &amp;lt;- ESTIMATE_GPLVM(scale(traindata[,-2]),5)
  X_train.xts &amp;lt;- xts(X_train$output,order.by = traindata$publication)
  X_train.q.xts &amp;lt;- apply.quarterly(X_train.xts,mean)
  
  if (months(index(X_train.q.xts)[NROW(X_train.q.xts)]) %in% c(&amp;quot;3月&amp;quot;,&amp;quot;6月&amp;quot;,&amp;quot;9月&amp;quot;,&amp;quot;12月&amp;quot;)){
  } else X_train.q.xts &amp;lt;- X_train.q.xts[-NROW(X_train.q.xts),]
  
  colnames(X_train.q.xts) &amp;lt;- c(&amp;quot;factor1&amp;quot;,&amp;quot;factor2&amp;quot;,&amp;quot;factor3&amp;quot;,&amp;quot;factor4&amp;quot;,&amp;quot;factor5&amp;quot;) 

  GDP_train.q &amp;lt;- scale(GDP[GDP$publication&amp;gt;=index(X_train.q.xts)[1] &amp;amp; GDP$publication&amp;lt;=index(X_train.q.xts)[NROW(X_train.q.xts)],2])

  rg_train &amp;lt;- lm(GDP_train.q[-NROW(GDP_train.q)]~.,data=X_train.q.xts[-NROW(X_train.q.xts)])
  summary(rg_train)
  test_df &amp;lt;- rbind(test_df,data.frame(predict(rg_train,X_train.q.xts[NROW(X_train.q.xts)],interval = &amp;quot;prediction&amp;quot;,level=0.90),GDP=GDP_train.q[NROW(GDP_train.q)]))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;計算できました。グラフにしてみましょう。先ほどのグラフよりは精度が悪くなりました。特にリーマンの後は予測値の信頼区間（90%）が大きく拡大しており、不確実性が増大していることもわかります。2010年以降に関しては実績値は信頼区間にほど入っており、予測モデルとしての性能はまあまあなのかなと思います。ただ、リーマンのような金融危機もズバッと当てるところにロマンがあると思うので元データの改善を図りたいと思います。この記事はいったんここで終了です。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gather(data.frame(test_df,Date=as.Date(rownames(test_df))),,,-c(lwr,upr,Date)),aes(y=value,x=Date,colour=key)) +
  geom_ribbon(aes(ymax=upr,ymin=lwr,fill=&amp;quot;band&amp;quot;),alpha=0.1,linetype=&amp;quot;blank&amp;quot;) +
  geom_line(size=1) +
  ggtitle(&amp;quot;out-sample test&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Asset Allocation ModelをRで組んでみた。</title>
      <link>/post/post2/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/post2/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#最小分散ポートフォリオ&#34;&gt;1. 最小分散ポートフォリオ&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#分散共分散行列をどのように求めるか&#34;&gt;2. 分散共分散行列をどのように求めるか&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a.-constant-conditional-correlation-ccc-model&#34;&gt;A. Constant conditional correlation (CCC) model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#b.-dynamic-conditional-correlation-dcc-model&#34;&gt;B. Dynamic Conditional Correlation (DCC) model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#c.-dynamic-equicorrelation-deco-model&#34;&gt;C. Dynamic Equicorrelation (DECO) model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#テスト用データの収集&#34;&gt;3. テスト用データの収集&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。勤め先で、アセットアロケーションに関するワークショップに参加したので、この分野は完全なる専門外ですがシミュレーションをしてみたいと思います。今回は、最小分散ポートフォリオ(minimum variance portfolio)を基本ポートフォリオとしたうえで、その分散共分散行列（予測値）をどのように推計するのかという点について先行研究を参考にエクササイズしていきたいと思います。先行研究は以下の論文です（オペレーションリサーチのジャーナルでした）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2947643&#34;&gt;Asset Allocation with Correlation: A Composite Trade-Off&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;最小分散ポートフォリオ&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. 最小分散ポートフォリオ&lt;/h2&gt;
&lt;p&gt;最小分散ポートフォリオの詳しい説明はここでは割愛しますが、要は各資産（内株、外株、内債、外債、オルタナ）のリターンの平均と分散を計算し、それらを縦軸平均値、横軸分散の二次平面にプロットしたうえで、投資可能範囲を計算し、その集合の中で最も分散が小さくなるポートフォリオの事らしいです（下図参照）。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Minimum_variance_flontier_of_MPT.svg/1280px-Minimum_variance_flontier_of_MPT.svg.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;minimum variance portfolio&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;先行研究のCarroll et. al. (2017)では、&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;this paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;と記載されており、現状でも期待リターンの推計は難しく、それを必要としない最小分散ポートフォリオは有益で実践的な手法であるといえます。最小分散ポートフォリオの目的関数は、その名の通り「分散を最小化すること」です。今、各資産のリターンを集めたベクトルを[tex:r]、各資産の保有ウェイトを&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;、ポートフォリオリターンを&lt;span class=&#34;math inline&#34;&gt;\(R_{p}\)&lt;/span&gt;で表すことにすると、ポートフォリオ全体の分散&lt;span class=&#34;math inline&#34;&gt;\(var(R_{p})\)&lt;/span&gt;は以下のように記述できます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
var(R_{p}) = var(r^{T}\theta) = E( (r^{T}\theta)(r^{T}\theta)^{T}) = \theta^{T}\Sigma\theta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで&lt;span class=&#34;math inline&#34;&gt;\(Sigma\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;の分散共分散行列です。よって、最小化問題は以下になります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\theta}(\theta^{T}\Sigma\theta) \\
s.t 1^{T}\theta = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここでは、フルインベストメントを制約条件に加えています。ラグランジュ未定乗数法を用いてこの問題を解いてみましょう。ラグランジュ関数&lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt;は以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L = \theta^{T}\Sigma\theta + \lambda(1^{T}\theta - 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1階の条件は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle\frac{\partial L}{\partial \theta} = 2\Sigma\theta + 1\lambda = 0 \\
\displaystyle \frac{\partial L}{\partial \lambda} = 1^{T} \theta = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1本目の式を&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;について解くと、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta = \Sigma^{-1}1\lambda^{*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}=-1/2\lambda\)&lt;/span&gt;です。これを2本目の式に代入し、&lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}\)&lt;/span&gt;について解きます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
1^{T}\Sigma1\lambda^{*} = 1 \\
\displaystyle \lambda^{*} = \frac{1}{1^{T}\Sigma^{-1}1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\theta = \Sigma^{-1}1\lambda^{*}\)&lt;/span&gt;だったので、&lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}\)&lt;/span&gt;を消去すると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \theta_{gmv} = \frac{\Sigma^{-1}1}{1^{T}\Sigma^{-1}1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となり、最適なウェイトを求めることができました。とりあえず、これをRで実装しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gmv &amp;lt;- function(r_dat,r_cov){
  library(MASS)
  i &amp;lt;- matrix(1,NCOL(r_dat),1)
  r_weight &amp;lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i)
  wr_dat &amp;lt;- r_dat*as.numeric(r_weight)
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;) 
  return(result)
}

nlgmv &amp;lt;- function(r_dat,r_cov){
  qp.out &amp;lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))),
           bvec=c(1,rep(0,NCOL(r_dat))),meq=1)
  r_weight &amp;lt;- qp.out$solution
  wr_dat &amp;lt;- r_dat*r_weight
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;入力は各資産のリターンと分散共分散行列になっています。出力はウェイト、リターン、リスクです。&lt;code&gt;nlgmv&lt;/code&gt;は最小分散ポートフォリオの空売り制約バージョンです。解析的な解は得られないので、数値的に買いを求めています。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;分散共分散行列をどのように求めるか&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. 分散共分散行列をどのように求めるか&lt;/h2&gt;
&lt;p&gt;最小分散ポートフォリオの計算式は求めることができました。次は、その入力である分散共分散行列をどうやって求めるのかについて分析したいと思います。一番原始的な方法はその時点以前に利用可能なリターンデータを標本として分散共分散行列を求め、その値を固定して最小分散ポートフォリオを求めるというヒストリカルなアプローチかと思います（つまりウェイトも固定）。ただ、これはあくまで過去の平均値を将来の予想値に使用するため、いろいろ問題が出てくるかと思います。専門外の私が思いつくものとしては、前日ある資産Aのリターンが大きく下落したという場面で明日もこの資産の分散は大きくなることが予想されるにも関わらず、平均値を使用するため昨日の効果が薄められてしまうことでしょうか。それに、ウェイトを最初から変更しないというのも時間がたつにつれ、最適点から離れていく気がします。ただ、ではどう推計するのかついてはこの分野でも試行錯誤が行われているようです。Carroll et. al. (2017)でも、&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;と述べられていました。この論文では以下のようなモデルを用いて推計が行われています。いずれも、分散共分散行列を時変としているところに特徴があります。&lt;/p&gt;
&lt;div id=&#34;a.-constant-conditional-correlation-ccc-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A. Constant conditional correlation (CCC) model&lt;/h3&gt;
&lt;p&gt;元論文は&lt;a href=&#34;https://www.jstor.org/stable/2109358?read-now=1&amp;amp;seq=3#page_scan_tab_contents&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;まず、分散共分散行列と相関行列の関係性から、&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t} = D_{t}R_{t}D_{t}\)&lt;/span&gt;となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;は相関行列、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(diag(\sigma_{1,t},...,\sigma_{N,t})\)&lt;/span&gt;で各資産&lt;span class=&#34;math inline&#34;&gt;\(tt\)&lt;/span&gt;期の標準偏差&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{i,t}\)&lt;/span&gt;を対角成分に並べた行列です。ここから、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;を分けて推計していきます。まず、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;ですが、こちらは以下のような多変量GARCHモデル(1,1)で推計します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_{t} = \mu + u_{t} \\
u_{t} = \sigma_{t}\epsilon \\
\sigma_{t}^{2} = \alpha_{0} + \alpha_{1}u_{t-1}^{2} + \alpha_{2}\sigma_{t-1}^{2} \\
\epsilon_{t} = NID(0,1) \\
E(u_{t}|u_{t-1}) = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;はリターンの標本平均です。&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{i}\)&lt;/span&gt;は推定すべきパラメータ。&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;をGARCHで推計しているので、リターンの分布が正規分布より裾野の厚い分布に従い、またリターンの変化は一定ではなく前日の分散に依存する関係をモデル化しているといえるのではないでしょうか。とりあえずこれで&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;の推計はできたということにします。次に&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;の推計ですが、このモデルではリターンを標本として求めるヒストリカルなアプローチを取ります。つまり、&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;は定数です。よって、リターン変動の大きさは時間によって変化するが、各資産の相対的な関係性は不変であるという仮定を置いていることになります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-dynamic-conditional-correlation-dcc-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B. Dynamic Conditional Correlation (DCC) model&lt;/h3&gt;
&lt;p&gt;元論文は&lt;a href=&#34;http://www.cass.city.ac.uk/__data/assets/pdf_file/0003/78960/Week7Engle_2002.pdf&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;こちらのモデルでは、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;を求めるところまでは①と同じですが、[&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、&lt;span class=&#34;math inline&#34;&gt;\(tex:t\)&lt;/span&gt;期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;です。ここで、&lt;span class=&#34;math inline&#34;&gt;\(Q_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期での条件付分散共分散行列で以下のように定式化されます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q_{t} = \bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\epsilon_{i,t-1}\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\bar{Q}\)&lt;/span&gt;はヒストリカルな方法で計算した分散共分散行列であり、&lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt;はパラメータです。この方法では、先ほどとは異なり、リターン変動の大きさが時間によって変化するだけでなく、各資産の相対的な関係性も通時的に変化していくという仮定を置いていることになります。金融危機時には全資産のリターンが下落し、各資産の相関が正になる事象も観測されていることから、この定式化は魅力的であるということができるのではないでしょうか。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-dynamic-equicorrelation-deco-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;C. Dynamic Equicorrelation (DECO) model&lt;/h3&gt;
&lt;p&gt;元論文は&lt;a href=&#34;https://faculty.chicagobooth.edu/bryan.kelly/research/pdf/deco.pdf&#34;&gt;こちら&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;この論文はまだきっちり読めていないのですが、相関行列&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;の定義から&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{t} = (1-\rho_{t})I_{N} + \rho_{t}1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となるようです。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\rho_{t}\)&lt;/span&gt;はスカラーでequicorrelationの程度を表す係数です。equicorrelationとは平均的なペアワイズ相関の事であると理解しています。つまりは欠損値がなければ普通の相関と変わりないんじゃないかと。ただ、資産が増えればそのような問題にも対処する必要があるのでその点ではよい推定量のようです。&lt;span class=&#34;math inline&#34;&gt;\(\rho_{t}\)&lt;/span&gt;は以下のように求めることができます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \rho_{t} = \frac{1}{N(N-1)}(\iota^{T}R_{t}^{DCC}\iota - N) = \frac{2}{N(N-1)}\sum_{i&amp;gt;j}\frac{q_{ij,t}}{\sqrt{q_{ii,t} q_{jj,t}}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\iota\)&lt;/span&gt;はN×1ベクトルで要素は全て1です。また、&lt;span class=&#34;math inline&#34;&gt;\(q_{ij,t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(Q_{t}\)&lt;/span&gt;のi,j要素です。&lt;/p&gt;
&lt;p&gt;さて、分散共分散行列のモデル化ができたところで、ここまでを&lt;code&gt;R&lt;/code&gt;で実装しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carroll &amp;lt;- function(r_dat,FLG){
  
  library(rmgarch)
  
  if(FLG == &amp;quot;benchmark&amp;quot;){
    H &amp;lt;- cov(r_dat)
  }else{
    #1. define variables
    N &amp;lt;- NCOL(r_dat) # the number of assets
    
    #2. estimate covariance matrix
    basic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = &amp;#39;sGARCH&amp;#39;), distribution.model = &amp;#39;norm&amp;#39;)
    multi_garch = multispec(replicate(N, basic_garch))
    dcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = &amp;quot;mvnorm&amp;quot;,model = &amp;quot;DCC&amp;quot;)
    fit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE))
    forecast_dcc_garch &amp;lt;- dccforecast(fit_dcc_garch)
    if (FLG == &amp;quot;CCC&amp;quot;){
      #Constant conditional correlation (CCC) model
      D &amp;lt;- sigma(forecast_dcc_garch)
      R_ccc &amp;lt;- cor(r_dat)
      H &amp;lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1])
      colnames(H) &amp;lt;- colnames(r_dat)
      rownames(H) &amp;lt;- colnames(r_dat)
    }
    else{
      #Dynamic Conditional Correlation (DCC) model
      H &amp;lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1])
      if (FLG == &amp;quot;DECO&amp;quot;){
        #Dynamic Equicorrelation (DECO) model
        one &amp;lt;- matrix(1,N,N)
        iota &amp;lt;- rep(1,N)
        Q_dcc &amp;lt;- rcor(forecast_dcc_garch,type=&amp;quot;Q&amp;quot;)[[1]][,,1]
        rho &amp;lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N))
        D &amp;lt;- sigma(forecast_dcc_garch)
        R_deco &amp;lt;- (1-rho)*diag(1,N,N) + rho*one
        H &amp;lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1])
        colnames(H) &amp;lt;- colnames(r_dat)
        rownames(H) &amp;lt;- colnames(r_dat)
      }
    }
  }
  return(H)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;本来であれば、パッケージを使用するべきではないのですが、今日はエクササイズなので推計結果だけを追い求めたいと思います。GARCHについては再来週ぐらいに記事を書く予定です。
これで準備ができました。この関数にリターンデータを入れて、分散共分散行列を計算し、それを用いて最小分散ポートフォリオを計算することができるようになりました。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;テスト用データの収集&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. テスト用データの収集&lt;/h2&gt;
&lt;p&gt;データは以下の記事を参考にしました。&lt;/p&gt;
&lt;p&gt;(Introduction to Asset Allocation)[&lt;a href=&#34;https://www.r-bloggers.com/introduction-to-asset-allocation/&#34; class=&#34;uri&#34;&gt;https://www.r-bloggers.com/introduction-to-asset-allocation/&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;使用したのは、以下のインデックスに連動するETF(iShares)の基準価額データです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S&amp;amp;P500&lt;/li&gt;
&lt;li&gt;NASDAQ100&lt;/li&gt;
&lt;li&gt;MSCI Emerging Markets&lt;/li&gt;
&lt;li&gt;Russell 2000&lt;/li&gt;
&lt;li&gt;MSCI EAFE&lt;/li&gt;
&lt;li&gt;US 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index)&lt;/li&gt;
&lt;li&gt;U.S. Real Estate(the Dow Jones US Real Estate Index)&lt;/li&gt;
&lt;li&gt;gold bullion market&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;まず、データ集めです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(quantmod)

#**************************
# ★8 ASSETS SIMULATION
# SPY - S&amp;amp;P 500 
# QQQ - Nasdaq 100
# EEM - Emerging Markets
# IWM - Russell 2000
# EFA - EAFE
# TLT - 20 Year Treasury
# IYR - U.S. Real Estate
# GLD - Gold
#**************************

# load historical prices from Yahoo Finance
symbol.names = c(&amp;quot;S&amp;amp;P 500&amp;quot;,&amp;quot;Nasdaq 100&amp;quot;,&amp;quot;Emerging Markets&amp;quot;,&amp;quot;Russell 2000&amp;quot;,&amp;quot;EAFE&amp;quot;,&amp;quot;20 Year Treasury&amp;quot;,&amp;quot;U.S. Real Estate&amp;quot;,&amp;quot;Gold&amp;quot;)
symbols = c(&amp;quot;SPY&amp;quot;,&amp;quot;QQQ&amp;quot;,&amp;quot;EEM&amp;quot;,&amp;quot;IWM&amp;quot;,&amp;quot;EFA&amp;quot;,&amp;quot;TLT&amp;quot;,&amp;quot;IYR&amp;quot;,&amp;quot;GLD&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, auto.assign = TRUE)

#gn dates for all symbols &amp;amp; convert to monthly
hist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)
month.ends = endpoints(hist.prices, &amp;#39;day&amp;#39;)
hist.prices = Cl(hist.prices)[month.ends, ]
colnames(hist.prices) = symbols

# remove any missing data
hist.prices = na.omit(hist.prices[&amp;#39;1995::&amp;#39;])

# compute simple returns
hist.returns = na.omit( ROC(hist.prices, type = &amp;#39;discrete&amp;#39;) )

# compute historical returns, risk, and correlation
ia = list()
ia$expected.return = apply(hist.returns, 2, mean, na.rm = T)
ia$risk = apply(hist.returns, 2, sd, na.rm = T)
ia$correlation = cor(hist.returns, use = &amp;#39;complete.obs&amp;#39;, method = &amp;#39;pearson&amp;#39;)

ia$symbols = symbols
ia$symbol.names = symbol.names
ia$n = length(symbols)
ia$hist.returns = hist.returns

# convert to annual, year = 12 months
annual.factor = 12
ia$expected.return = annual.factor * ia$expected.return
ia$risk = sqrt(annual.factor) * ia$risk

rm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;リターンをプロットするとこんな感じです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = &amp;quot;パフォーマンスサマリー&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;次に、バックテストのコーディングを行います。一気にコードを公開します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# BACK TEST
backtest &amp;lt;- function(r_dat,FLG,start_date,span,learning_term,port){
  #-----------------------------------------
  # BACKTEST
  # r_dat - return data(xts object) 
  # FLG - flag(CCC,DCC,DECO)
  # start_date - start date for backtest
  # span - rebalance frequency
  # learning_term - learning term (days)
  # port - method of portfolio optimization
  #-----------------------------------------
  
  library(stringi)

  initial_dat &amp;lt;- r_dat[stri_c(as.Date(start_date)-learning_term,&amp;quot;::&amp;quot;,as.Date(start_date))]
  for (i in NROW(initial_dat):NROW(r_dat)) {
    if (i == NROW(initial_dat)){
      H &amp;lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG)
      if (port == &amp;quot;nlgmv&amp;quot;){
        result &amp;lt;- nlgmv(initial_dat,H)
      }else if (port == &amp;quot;risk parity&amp;quot;){
        result &amp;lt;- risk_parity(initial_dat,H)
      }
      weight &amp;lt;- t(result$weight)
      colnames(weight) &amp;lt;- colnames(initial_dat)
      p_return &amp;lt;- initial_dat[NROW(initial_dat),]*result$weight
    } else {
      if (i %in% endpoints(r_dat,span)){
        H &amp;lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG)
        if (port == &amp;quot;nlgmv&amp;quot;){
          result &amp;lt;- nlgmv(test_dat,H)
        }else if (port == &amp;quot;risk parity&amp;quot;){
          result &amp;lt;- risk_parity(test_dat,H)
        }
        
      }
      weight &amp;lt;- rbind(weight,t(result$weight))
      p_return &amp;lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight)
    }
    if (i != NROW(r_dat)){
      term &amp;lt;- stri_c(index(r_dat[i+1,])-learning_term,&amp;quot;::&amp;quot;,index(r_dat[i+1,])) 
      test_dat &amp;lt;- r_dat[term]
    }
  }
  p_return$portfolio &amp;lt;- xts(apply(p_return,1,sum),order.by = index(p_return))
  weight.xts &amp;lt;- xts(weight,order.by = index(p_return))

  result &amp;lt;- list(p_return,weight.xts)
  names(result) &amp;lt;- c(&amp;quot;return&amp;quot;,&amp;quot;weight&amp;quot;)
  return(result)
}

CCC &amp;lt;- backtest(hist.returns,&amp;quot;CCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DCC &amp;lt;- backtest(hist.returns,&amp;quot;DCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DECO &amp;lt;- backtest(hist.returns,&amp;quot;DECO&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
benchmark &amp;lt;- backtest(hist.returns,&amp;quot;benchmark&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)

result &amp;lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)
colnames(result) &amp;lt;- c(&amp;quot;CCC&amp;quot;,&amp;quot;DCC&amp;quot;,&amp;quot;DECO&amp;quot;,&amp;quot;benchmark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;計算結果をグラフにしてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result,main = &amp;quot;BACKTEST&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;空売り制約を課したので、上で定義した最小分散ポートフォリオは使用していません。どうやらこれだけで解析的に解くのは難しいらしく、数値的に解くことにしています。リバランス期間を週次にしたので、自前のPCでは計算に時間がかかりましたが、結果が計算できました。&lt;/p&gt;
&lt;p&gt;リーマン以降はどうやらベンチマークである等ウェイトポートフォリオよりをアウトパフォームしているようです。特に、DECOはいい感じです。そもそもDECOとDCCはほぼ変わらないパフォーマンスであると思っていたのですが、どうやら自分の理解が足らないらしく、論文の読み返す必要があるようです。Equicorrelationの意味をもう一度考えてみたいと思います。それぞれの組入比率の推移は以下のようになりました。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot allocation weighting
d_allocation &amp;lt;- function(ggweight,title){
  #install.packages(&amp;quot;tidyverse&amp;quot;)
  library(tidyverse)
  ggweight &amp;lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method)
  ggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) +
    geom_area(colour=&amp;quot;black&amp;quot;,size=.1) +
    scale_y_continuous(limits = c(0,1)) +
    labs(title=title) + facet_grid(method~.)
}

gmv_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)))

# plot allocation weighting
d_allocation(gmv_weight,&amp;quot;GMV Asset Allocation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;リーマンの際にTLT、つまり米国債への比率を増やしているようです。CCCとDCCはそれ以外の部分でも米国債への比率が高く、よく挙げられる最小分散ポートフォリオの問題点がここでも発生しているようです。一方、DECOがやはり個性的な組入比率の推移をしており、ここらを考えてももう一度論文を読み返してみる必要がありそうです。&lt;/p&gt;
&lt;p&gt;追記（2019/3/3）
これまでは、最小分散ポートフォリオで分析をしていましたが、リスクパリティの結果も見たいなと言うことで、そのコードも書いてみました。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;risk_parity &amp;lt;- function(r_dat,r_cov){
  fn &amp;lt;- function(weight, r_cov) {
    N &amp;lt;- NROW(r_cov)
    risks &amp;lt;-  weight * (r_cov %*% weight)
    g &amp;lt;- rep(risks, times = N) - rep(risks, each = N)
  return(sum(g^2))
  }
  dfn &amp;lt;- function(weight,r_cov){
    out &amp;lt;- weight
    for (i in 0:length(weight)) {
      up &amp;lt;- dn &amp;lt;- weight
      up[i] &amp;lt;- up[i]+.0001
      dn[i] &amp;lt;- dn[i]-.0001
      out[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002
    }
    return(out)
  }
  std &amp;lt;- sqrt(diag(r_cov)) 
  x0 &amp;lt;- 1/std/sum(1/std)
  res &amp;lt;- nloptr::nloptr(x0=x0,
                 eval_f=fn,
                 eval_grad_f=dfn,
                 eval_g_eq=function(weight,r_cov) { sum(weight) - 1 },
                 eval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) },
                 lb=rep(0,length(std)),ub=rep(1,length(std)),
                 opts = list(&amp;quot;algorithm&amp;quot;=&amp;quot;NLOPT_LD_SLSQP&amp;quot;,&amp;quot;print_level&amp;quot; = 0,&amp;quot;xtol_rel&amp;quot;=1.0e-8,&amp;quot;maxeval&amp;quot; = 1000),
                 r_cov = r_cov)
  r_weight &amp;lt;- res$solution
  names(r_weight) &amp;lt;- colnames(r_cov)
  wr_dat &amp;lt;- r_dat*r_weight
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;)
  return(result)
  }

CCC &amp;lt;- backtest(hist.returns,&amp;quot;CCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DCC &amp;lt;- backtest(hist.returns,&amp;quot;DCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DECO &amp;lt;- backtest(hist.returns,&amp;quot;DECO&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
benchmark &amp;lt;- backtest(hist.returns,&amp;quot;benchmark&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)

result &amp;lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)
colnames(result) &amp;lt;- c(&amp;quot;CCC&amp;quot;,&amp;quot;DCC&amp;quot;,&amp;quot;DECO&amp;quot;,&amp;quot;benchmark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;結果はこんな感じ。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result, main = &amp;quot;BACKTEST COMPARISON&amp;quot;)

library(plotly)

# plot allocation weighting
riskparity_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=&amp;quot;benchmark&amp;quot;,Date=index(benchmark$weight)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result, main = &amp;quot;BACKTEST COMPARISON&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;riskparity_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=&amp;quot;benchmark&amp;quot;,Date=index(benchmark$weight)))

# plot allocation weighting
d_allocation(riskparity_weight, &amp;quot;Risk Parity Asset Allocation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;どの手法もbenchmarkをアウトパーフォームできているという好ましい結果になりました。
やはり、分散共分散行列の推計がうまくいっているようです。また、DECOのパフォーマンスがよいのは、相関行列に各資産ペアの相関係数の平均値を用いているため、他の手法よりもリスク資産の組み入れが多くなったからだと思われます。ウェイトは以下の通りです。&lt;/p&gt;
&lt;p&gt;とりあえず、今日はここまで。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>カルマンフィルタの実装</title>
      <link>/post/post3/</link>
      <pubDate>Sun, 10 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/post3/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#カルマンフィルタとは&#34;&gt;1. カルマンフィルタとは？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#カルマンフィルタアルゴリズムの導出&#34;&gt;2. カルマンフィルタアルゴリズムの導出&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rで実装する&#34;&gt;3. &lt;code&gt;R&lt;/code&gt;で実装する。&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#カルマンスムージング&#34;&gt;4. カルマンスムージング&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。かなり久しぶりの投稿となってしまいました。決して研究をさぼっていたのではなく、&lt;code&gt;BVAR&lt;/code&gt;のコーディングに手こずっていました。あと少しで完成します。さて、今回は&lt;code&gt;BVAR&lt;/code&gt;やこの前のGiannnone et a (2008)のような分析でも大活躍のカルマンフィルタを実装してしまいたいと思います。このブログではパッケージソフトに頼らず、基本的に自分で一から実装を行い、研究することをポリシーとしていますので、これから頻繁に使用するであろうカルマンフィルタを関数として実装してしまうことは非常に有益であると考えます。今回はRで実装をしましたので、そのご報告をします。&lt;/p&gt;
&lt;div id=&#34;カルマンフィルタとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. カルマンフィルタとは？&lt;/h2&gt;
&lt;p&gt;まず、カルマンフィルタに関する簡単な説明を行います。非常にわかりやすい記事があるので、&lt;a href=&#34;https://qiita.com/MoriKen/items/0c80ef75749977767b43&#34;&gt;こちら&lt;/a&gt;を読んでいただいたほうがより分かりやすいかと思います。&lt;/p&gt;
&lt;p&gt;カルマンフィルタとは、状態空間モデルを解くアルゴリズムの一種です。状態空間モデルとは、手元の観測可能な変数から観測できない変数を推定するモデルであり、以下のような形をしています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} \\
\alpha_{t} = T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(g×1\)&lt;/span&gt;ベクトルの観測可能な変数(観測変数)、&lt;span class=&#34;math inline&#34;&gt;\(Z_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(g×k\)&lt;/span&gt;係数行列、&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(k×1\)&lt;/span&gt;ベクトルの観測不可能な変数(状態変数)、&lt;span class=&#34;math inline&#34;&gt;\(T_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(k×k\)&lt;/span&gt;係数行列です。また、&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\)&lt;/span&gt;は観測変数の誤差項、&lt;span class=&#34;math inline&#34;&gt;\(\eta_{t}\)&lt;/span&gt;は状態変数の誤差項です。これらの誤差項はそれぞれ&lt;span class=&#34;math inline&#34;&gt;\(N(0,H_{t})\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(N(0,Q_{t})\)&lt;/span&gt;に従います（&lt;span class=&#34;math inline&#34;&gt;\(H_{t},Q_{t}\)&lt;/span&gt;は分散共分散行列）。&lt;span class=&#34;math inline&#34;&gt;\(d_{t}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(c_{t}\)&lt;/span&gt;は定数項です。1本目の式は観測方程式、2本目の式は遷移方程式と呼ばれます。
状態空間モデルを使用する例として、しばしば池の魚の数を推定する問題が使用されます。今、池の中の魚の全数が知りたいとして、その推定を考えます。観測時点毎に池の中の魚をすべて捕まえてその数を調べるのは現実的に困難なので、一定期間釣りをして釣れた魚をサンプルに全数を推定することを考えます。ここで、釣れた魚は観測変数、池にいる魚の全数は状態変数と考えることができます。今、経験的に釣れた魚の数と全数の間に以下のような関係があるとします。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = 0.01\alpha_{t} + 5 + \epsilon_{t}
\]&lt;/span&gt;
これが観測方程式になります。また、魚の全数は過去の値からそれほど急速には変化しないと考えられるため、以下のようなランダムウォークに従うと仮定します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\alpha_{t} = \alpha_{t-1}  + 500 + \eta_{t}
\]&lt;/span&gt;
これが遷移方程式になります。あとは、これをカルマンフィルタアルゴリズムを用いて計算すれば、観測できない魚の全数を推定することができます。
このように状態空間モデルは非常に便利なモデルであり、また応用範囲も広いです。例えば、販売額から潜在顧客数を推定したり、クレジットスプレッドやトービンのQ等経済モデル上の概念として存在する変数を推定する、&lt;code&gt;BVAR&lt;/code&gt;のように&lt;code&gt;VAR&lt;/code&gt;や回帰式の時変パラメータ推定などにも使用されます。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;カルマンフィルタアルゴリズムの導出&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. カルマンフィルタアルゴリズムの導出&lt;/h2&gt;
&lt;p&gt;さて、非常に便利な状態空間モデルの説明はこれくらいにして、カルマンフィルタの説明に移りたいと思います。カルマンフィルタは状態空間モデルを解くアルゴリズムの一種であると先述しました。つまり、他にも状態空間モデルを解くアルゴリズムは存在します。カルマンフィルタアルゴリズムは一般に誤差項の正規性の仮定を必要としないフィルタリングアルゴリズムであり、観測方程式と遷移方程式の線形性の仮定さえあれば、線形最小分散推定量となります。カルマンフィルタアルゴリズムの導出にはいくつかの方法がありますが、今回はこの線形最小分散推定量としての導出を行います。まず、以下の３つの仮定を置きます。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;初期値&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{0}\)&lt;/span&gt;は正規分布&lt;span class=&#34;math inline&#34;&gt;\(N(a_{0},\Sigma_{0})\)&lt;/span&gt;に従う確率ベクトルである(&lt;span class=&#34;math inline&#34;&gt;\(a_{t}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t}\)&lt;/span&gt;の推定値)。&lt;/li&gt;
&lt;li&gt;誤差項&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(\eta_{s}\)&lt;/span&gt;は全ての&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;,&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;で互いに独立で、初期値ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{0}\)&lt;/span&gt;と無相関である（&lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon_{t}\eta_{s})=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon_{t}\alpha_{0})=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E(\eta_{t}\alpha_{0})=0\)&lt;/span&gt;）。&lt;/li&gt;
&lt;li&gt;2より、&lt;span class=&#34;math inline&#34;&gt;\(E(\epsilon_{t}\alpha_{t}&amp;#39;)=0\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(E(\eta_{t}\alpha_{t-1}&amp;#39;)=0\)&lt;/span&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;まず、&lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt;期の情報集合&lt;span class=&#34;math inline&#34;&gt;\(\Omega_{t-1}\)&lt;/span&gt;が既知の状態での&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;の期待値（予測値）を求めてみましょう。上述した状態空間モデルと誤差項の期待値がどちらもゼロである事実を用いると、以下のように計算することができます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E(\alpha_{t}|\Omega_{t-1}) = a_{t|t-1} = T_{t}a_{t-1|t-1} + c_{t}
E(Y_{t}|\Omega_{t-1}) = Y_{t|t-1} = Z_{t}a_{t|t-1} + d_{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、次に、これらの分散を求めます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E( (\alpha_{t}-a_{t|t-1})(\alpha_{t}-a_{t|t-1})&amp;#39;|\Omega_{t-1}) &amp;amp;=&amp;amp; E( (T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}-a_{t|t-1})(T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}-a_{t|t-1})&amp;#39;|\Omega_{t-1}) \\ 
&amp;amp;=&amp;amp; E(T_{t}\alpha_{t-1}\alpha_{t-1}&amp;#39;T_{t}&amp;#39; + R_{t}\eta_{t}\eta_{t}&amp;#39;R_{t}&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; E(T_{t}\alpha_{t-1}\alpha_{t-1}&amp;#39;T_{t}&amp;#39;|\Omega_{t-1}) + E(R_{t}\eta_{t}\eta_{t}&amp;#39;R_{t}&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; T_{t}E(\alpha_{t-1}\alpha_{t-1}&amp;#39;|\Omega_{t-1})T_{t}&amp;#39; + R_{t}E(\eta_{t}\eta_{t}&amp;#39;|\Omega_{t-1})R_{t}&amp;#39; \\
&amp;amp;=&amp;amp; T_{t}\Sigma_{t-1|t-1}T_{t}&amp;#39; + R_{t}Q_{t}R_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E( (Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})&amp;#39;|\Omega_{t-1}) &amp;amp;=&amp;amp; E( (Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t}-Y_{t|t-1})(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t}-Y_{t|t-1})&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; E(Z_{t}\alpha_{t}\alpha_{t}&amp;#39;Z_{t}&amp;#39; + S_{t}\epsilon_{t}\epsilon_{t}&amp;#39;S_{t}&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; E(Z_{t}\alpha_{t}\alpha_{t}&amp;#39;Z_{t}&amp;#39;|\Omega_{t-1}) + E(S_{t}\epsilon_{t}\epsilon_{t}&amp;#39;S_{t}&amp;#39;|\Omega_{t-1}) \\
&amp;amp;=&amp;amp; Z_{t}E(\alpha_{t}\alpha_{t}&amp;#39;|\Omega_{t-1})Z_{t}&amp;#39; + S_{t}E(\epsilon_{t}\epsilon_{t}&amp;#39;|\Omega_{t-1})S_{t}&amp;#39; \\
&amp;amp;=&amp;amp; Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39; \\
&amp;amp;=&amp;amp; F_{t|t-1}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期の情報集合&lt;span class=&#34;math inline&#34;&gt;\(\Omega_{t}\)&lt;/span&gt;が得られたとします（つまり、観測値&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;を入手）。カルマンフィルタでは、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期の情報である観測値&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;を用いて&lt;span class=&#34;math inline&#34;&gt;\(a_{t|t-1}\)&lt;/span&gt;を以下の方程式で更新します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E(\alpha_{t}|\Omega_{t}) = a_{t|t} = a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1})
\]&lt;/span&gt;
つまり、観測値と&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;の期待値（予測値）の差をあるウェイト&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;（&lt;span class=&#34;math inline&#34;&gt;\(k×g\)&lt;/span&gt;行列）でかけたもので補正をかけるわけです。よって、観測値と予測値が完全に一致していた場合は補正は行われないことになります。ここで重要なのは、ウエイト&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;をどのように決めるのかです。&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;は更新後の状態変数の分散&lt;span class=&#34;math inline&#34;&gt;\(E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&amp;#39;)= \Sigma_{t|t}\)&lt;/span&gt;を最小化するよう決定します。これが、カルマンフィルタが線形最小分散推定量である根拠です。最小化にあたっては以下のベクトル微分が必要になりますので、おさらいをしておきましょう。今回使用するのは以下の事実です。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial a&amp;#39;b}{\partial b} = \frac{\partial b&amp;#39;a}{\partial b} = a \\
\displaystyle \frac{\partial b&amp;#39;Ab}{\partial b} = 2Ab
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt;はベクトル（それぞれ&lt;span class=&#34;math inline&#34;&gt;\(n×1\)&lt;/span&gt;ベクトル、&lt;span class=&#34;math inline&#34;&gt;\(1×n\)&lt;/span&gt;ベクトル）、&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(n×n\)&lt;/span&gt;の対称行列です。まず、１つ目から証明していきます。&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y = a&amp;#39;b = b&amp;#39;a = \sum_{i=1}^{n}a_{i}b_{i}\)&lt;/span&gt;とします。
このとき、&lt;span class=&#34;math inline&#34;&gt;\(\frac{\partial y}{\partial b_{i}}=a_{i}\)&lt;/span&gt;なので、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial a&amp;#39;b}{\partial b} = \frac{\partial b&amp;#39;a}{\partial b} = a
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;次に２つ目です。&lt;span class=&#34;math inline&#34;&gt;\(y = b&amp;#39;Ab = \sum_{i=1}^{n}\sum_{j=1}^{n}a_{ij}b_{i}b_{j}\)&lt;/span&gt;とします。このとき、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial y}{\partial b_{i}} = \sum_{j=1}^{n}a_{ij}b_{j} + \sum_{j=1}^{n}a_{ji}b_{j} = 2\sum_{j=1}^{n}a_{ij}b_{j} = 2a_{i}&amp;#39;b
\]&lt;/span&gt;
よって、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial y}{\partial b} =
\left(
    \begin{array}{cccc}
      \frac{\partial y}{\partial b_{1}} \\
      \vdots \\
      \frac{\partial y}{\partial b_{n}} \\
    \end{array}
  \right) = 2
\left(
    \begin{array}{cccc}
      \sum_{j=1}^{n}a_{1j}b_{j} \\
      \vdots \\
      \sum_{j=1}^{n}a_{nj}b_{j} \\
    \end{array}
  \right) = 2
\left(
    \begin{array}{cccc}
      a_{1}&amp;#39;b \\
      \vdots \\
      a_{n}&amp;#39;b \\
    \end{array}
  \right)
= 2Ab
\]&lt;/span&gt;
さて、準備ができたので、更新後の状態変数の分散&lt;span class=&#34;math inline&#34;&gt;\(E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&amp;#39;)\)&lt;/span&gt;を求めてみましょう。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E( (\alpha_{t} - a_{t|t})(\alpha_{t} - a_{t|t})&amp;#39;) &amp;amp;=&amp;amp; \Sigma_{t|t} \\
&amp;amp;=&amp;amp; E\{ (\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))(\alpha_{t} - a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}))&amp;#39;\} \\
&amp;amp;=&amp;amp; E\{ ( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + d_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1} - d_{t}) )\} \\
&amp;amp;=&amp;amp; E\{ ( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1}) )( (\alpha_{t} - a_{t|t-1}) - k_{t}(Z_{t}\alpha_{t} + S_{t}\epsilon_{t} - Z_{t}a_{t|t-1}) )&amp;#39;\} \\
&amp;amp;=&amp;amp; E\{ ( (I - k_{t}Z_{t})\alpha_{t} - k_{t}S_{t}\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})( (I - k_{t}Z_{t})\alpha_{t} - k_{t}S_{t}\epsilon_{t} - (I - k_{t}Z_{t})a_{t|t-1})&amp;#39; \} \\
&amp;amp;=&amp;amp; E\{( (I - k_{t}Z_{t})(\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\epsilon_{t})( (I - k_{t}Z_{t})(\alpha_{t}-a_{t|t-1}) - k_{t}S_{t}\epsilon_{t})&amp;#39;\} \\
&amp;amp;=&amp;amp; (I - k_{t}Z_{t})\Sigma_{t|t-1}(I - k_{t}Z_{t})&amp;#39; + k_{t}S_{t}H_{t}S_{t}&amp;#39;k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; (\Sigma_{t|t-1} - k_{t}Z_{t}\Sigma_{t|t-1})(I - k_{t}Z_{t})&amp;#39; + k_{t}S_{t}H_{t}S_{t}&amp;#39;k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}(k_{t}Z_{t})&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; + k_{t}S_{t}H_{t}S_{t}&amp;#39;k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}(Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39;)k_{t}&amp;#39; \\
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;１回目の式変形で、&lt;span class=&#34;math inline&#34;&gt;\(a_{t|t}\)&lt;/span&gt;に上述した更新式を代入し、２回目の式変形で観測方程式と上で計算した&lt;span class=&#34;math inline&#34;&gt;\(E(Y_{t}|\Omega_{t-1})\)&lt;/span&gt;を代入しています。さて、更新後の状態変数の分散&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t|t}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;の関数として書き表すことができたので、これを&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;で微分し、0と置き、&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t|t}\)&lt;/span&gt;を最小化する&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;を求めます。先述した公式で、&lt;span class=&#34;math inline&#34;&gt;\(a=\Sigma_{t|t-1}Z_{t}&amp;#39;\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(b=k_{t}&amp;#39;\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(A=(Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39;)\)&lt;/span&gt;とすると（&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;は分散共分散行列の和なので対称行列）、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial \Sigma_{t|t}}{\partial k_{t}&amp;#39;} = -2(Z_{t}\Sigma_{t|t-1})&amp;#39; + 2(Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39;)k_{t} = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここから、&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;を解きなおすと、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
k_{t} &amp;amp;=&amp;amp; \Sigma_{t|t-1}Z_{t}&amp;#39;(Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39;)^{-1} \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1}Z_{t}&amp;#39;F_{t|t-1}^{-1}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;突然、&lt;span class=&#34;math inline&#34;&gt;\(F_{t|t-1}\)&lt;/span&gt;が出てきました。これは観測変数の予測値の分散&lt;span class=&#34;math inline&#34;&gt;\(E((Y_{t}-Y_{t|t-1})(Y_{t}-Y_{t|t-1})&amp;#39;|\Omega_{t-1})\)&lt;/span&gt;でした。一方、&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t|t-1}Z_{t}\)&lt;/span&gt;は状態変数の予測値の分散を観測変数のスケールに調整したものです（観測空間に写像したもの）。つまり、カルマンゲイン&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;は状態変数と観測変数の予測値の分散比となっているのです。観測変数にはノイズがあり、観測方程式はいつも誤差０で満たされるわけではありません。また、状態方程式にも誤差項が存在します。状態の遷移も100%モデル通りにはいかないということです。&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期の観測変数&lt;span class=&#34;math inline&#34;&gt;\(Y_{t}\)&lt;/span&gt;が得られたとして、それをどれほど信頼して状態変数を更新するかは観測変数のノイズが状態変数のノイズに比べてどれほど小さいかによります。つまり、相対的に観測方程式が遷移方程式よりも信頼できる場合には状態変数を大きく更新するのです。このように、カルマンフィルタでは、観測方程式と遷移方程式の相対的な信頼度によって、更新の度合いを毎期調整しています。その度合いが分散比であり、カルマンゲインだというわけです。ちなみに欠損値が発生した場合には、観測変数の分散を無限大にし、状態変数の更新を全く行わないという対処を行います。観測変数に信頼がないので当たり前の処置です。この場合は遷移方程式を100%信頼します。これがカルマンフィルタのコアの考え方になります。
更新後の分散を計算しておきます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\Sigma_{t|t} &amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + k_{t}F_{t|t-1}k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + (\Sigma_{t|t-1}Z_{t}&amp;#39;F_{t|t-1}^{-1})F_{t|t-1}k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; - k_{t}Z_{t}\Sigma_{t|t-1} + \Sigma_{t|t-1}Z_{t}&amp;#39;k_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - k_{t}Z_{t}\Sigma_{t|t-1} \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - k_{t}F_{t|t-1}F_{t|t-1}^{-1}Z_{t}\Sigma_{t|t-1} \\
&amp;amp;=&amp;amp; \Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}&amp;#39;
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;では、最終的に導出されたアルゴリズムをまとめたいと思います。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
a_{t|t-1} &amp;amp;=&amp;amp; T_{t}a_{t-1|t-1} + c_{t} \\
\Sigma_{t|t-1} &amp;amp;=&amp;amp; T_{t}\Sigma_{t-1|t-1}T_{t}&amp;#39; + R_{t}Q_{t}R_{t}&amp;#39; \\
Y_{t|t-1} &amp;amp;=&amp;amp; Z_{t}a_{t|t-1} + d_{t} \\
F_{t|t-1} &amp;amp;=&amp;amp;  Z_{t}\Sigma_{t|t-1}Z_{t}&amp;#39; + S_{t}H_{t}S_{t}&amp;#39; \\
k_{t} &amp;amp;=&amp;amp; \Sigma_{t|t-1}Z_{t}&amp;#39;F_{t|t-1}^{-1} \\
a_{t|t} &amp;amp;=&amp;amp; a_{t|t-1} + k_{t}(Y_{t} - Y_{t|t-1}) \\
\Sigma_{t|t} &amp;amp;=&amp;amp;  \Sigma_{t|t-1} - k_{t}F_{t|t-1}k_{t}&amp;#39;
\end{eqnarray}
\]&lt;/span&gt;
初期値&lt;span class=&#34;math inline&#34;&gt;\(a_{0},\Sigma_{0}\)&lt;/span&gt;が所与の元で、まず状態変数の予測値&lt;span class=&#34;math inline&#34;&gt;\(a_{1|0},\Sigma_{1|0}\)&lt;/span&gt;を計算します。その結果を用いて、次は観測変数の予測値&lt;span class=&#34;math inline&#34;&gt;\(Y_{t|t-1},F_{t|t-1}\)&lt;/span&gt;を計算し、カルマンゲイン&lt;span class=&#34;math inline&#34;&gt;\(k_{t}\)&lt;/span&gt;を得ます。&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;期の観測可能なデータを入手したら、更新方程式を用いて&lt;span class=&#34;math inline&#34;&gt;\(a_{t|t},\Sigma_{t|t}\)&lt;/span&gt;を更新します。これをサンプル期間繰り返していくことになります。ちなみに、遷移方程式の誤差項&lt;span class=&#34;math inline&#34;&gt;\(\eta_{t}\)&lt;/span&gt;と定数項&lt;span class=&#34;math inline&#34;&gt;\(c_{t}\)&lt;/span&gt;がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致します。つまり、新しいサンプルを入手するたびに&lt;code&gt;OLS&lt;/code&gt;をやり直す推計方法ということです（今回はその証明は勘弁してください）。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rで実装する&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. &lt;code&gt;R&lt;/code&gt;で実装する。&lt;/h2&gt;
&lt;p&gt;以下が&lt;code&gt;R&lt;/code&gt;での実装コードです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kalmanfiter &amp;lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){
  #-------------------------------------------------------------------
  # Implemention of Kalman filter
  #   y - observed variable
  #   I - the number of unobserved variable
  #   t - parameter of endogenous variable in state equation
  #   z - parameter of endogenous variable in observable equation
  #   c - constant in state equaion
  #   R - parameter of exogenous variable in state equation
  #   Q - var-cov matrix of exogenous variable in state equation
  #   d - constant in observable equaion
  #   S - parameter of exogenous variable in observable equation
  #   h - var-cov matrix of exogenous variable in observable equation
  #   a_int - initial value of endogenous variable
  #   sig_int - initial value of variance of endogenous variable
  #-------------------------------------------------------------------
  
  library(MASS)
  
  # 1.Define Variable
  if (class(y)!=&amp;quot;matrix&amp;quot;){
    y &amp;lt;- as.matrix(y)
  }
  N &amp;lt;- NROW(y) # sample size
  L &amp;lt;- NCOL(y) # the number of observable variable 
  a_pre &amp;lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable
  a_fil &amp;lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable
  sig_pre &amp;lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable
  sig_fil &amp;lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable
  y_pre &amp;lt;- array(0,dim = c(L,1,N)) # prediction of observed variable
  F_pre &amp;lt;- array(0,dim = c(L,L,N)) # prediction of var-cov mat. of observable variable 
  F_inv &amp;lt;- array(0,dim = c(L,L,N)) # inverse of F_pre
  k &amp;lt;- array(0,dim = c(I,L,N)) # kalman gain
  
  if (any(is.na(a_int))==TRUE){
    a_int &amp;lt;- matrix(0,nrow = I,ncol = 1)
  }
  if (any(is.na(sig_int))==TRUE){
    sig_int &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(R))==TRUE){
    R &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(Q))==TRUE){
    Q &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(S))==TRUE){
    S &amp;lt;- matrix(1,nrow = L,ncol = L)
  }
  if (any(is.na(h))==TRUE){
    H &amp;lt;- array(0,dim = c(L,L,N))
    for(i in 1:N){
      diag(H[,,i]) = 1
    }
  }else if (class(h)!=&amp;quot;array&amp;quot;){
    H &amp;lt;- array(h,dim = c(NROW(h),NCOL(h),N))
  }
  
  # fill infinite if observed data is NA
  for(i in 1:N){
    miss &amp;lt;- is.na(y[i,])
    diag(H[,,i])[miss] &amp;lt;- 1e+32
  }
  y[is.na(y)] &amp;lt;- 0
  
  # 2.Set Initial Value
  a_fil[,,1] &amp;lt;- a_int
  sig_fil[,,1] &amp;lt;- sig_int
  
  # 3.Implement Kalman filter
  for (i in 1:N){
    if(class(z)==&amp;quot;array&amp;quot;){
      Z &amp;lt;- z[,,i]
    }else{
      Z &amp;lt;- z
    }
    a_pre[,,i] &amp;lt;- t%*%a_fil[,,i] + c
    sig_pre[,,i] &amp;lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)
    y_pre[,,i] &amp;lt;- Z%*%a_pre[,,i] + d
    F_pre[,,i] &amp;lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)
    k[,,i] &amp;lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])
    a_fil[,,i+1] &amp;lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])
    sig_fil[,,i+1] &amp;lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])
  }
  
  # 4.Aggregate results
  result &amp;lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)
  names(result) &amp;lt;- c(&amp;quot;state prediction&amp;quot;, &amp;quot;state filtered&amp;quot;, &amp;quot;state var prediction&amp;quot;, 
                     &amp;quot;state var filtered&amp;quot;, &amp;quot;observable prediction&amp;quot;, &amp;quot;kalman gain&amp;quot;,
                     &amp;quot;parameter of state eq&amp;quot;, &amp;quot;parameter of observable eq&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;案外簡単に書けるもんですね。これを使って、Giannone et al (2008)をやり直してみます。データセットは前回記事と変わりません。&lt;/p&gt;
&lt;p&gt;以下、分析用の&lt;code&gt;R&lt;/code&gt;コードです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#------------------------
# Giannone et. al. 2008 
#------------------------

library(MASS)
library(xts)

# ファクターを計算
f &amp;lt;- 3
z &amp;lt;- scale(dataset1)
for (i in 1:nrow(z)){
  eval(parse(text = paste(&amp;quot;S_i &amp;lt;- z[i,]%*%t(z[i,])&amp;quot;,sep = &amp;quot;&amp;quot;)))
  if (i==1){
    S &amp;lt;- S_i
  }else{
    S &amp;lt;- S + S_i
  }
}
S &amp;lt;- (1/nrow(z))*S
gamma &amp;lt;- eigen(S)
D &amp;lt;- diag(gamma$values[1:f])
V &amp;lt;- gamma$vectors[,1:f]
F_t &amp;lt;- matrix(0,nrow(z),f)
for (i in 1:nrow(z)){
  eval(parse(text = paste(&amp;quot;F_t[&amp;quot;,i,&amp;quot;,]&amp;lt;- z[&amp;quot;,i,&amp;quot;,]%*%V&amp;quot;,sep = &amp;quot;&amp;quot;)))
}
lambda_hat &amp;lt;- V
psi &amp;lt;- diag(diag(S-V%*%D%*%t(V)))
R &amp;lt;- diag(diag(cov(z-z%*%V%*%t(V))))

a &amp;lt;- matrix(0,f,f)
b &amp;lt;- matrix(0,f,f)
for(t in 2:nrow(z)){
  a &amp;lt;- a + F_t[t,]%*%t(F_t[t-1,])
  b &amp;lt;- b + F_t[t-1,]%*%t(F_t[t-1,])
}
b_inv &amp;lt;- solve(b)
A_hat &amp;lt;- a%*%b_inv

e &amp;lt;- numeric(f)
for (t in 2:nrow(F_t)){
  e &amp;lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat
}
H &amp;lt;- t(e)%*%e
Q &amp;lt;- diag(1,f,f)
Q[1:f,1:f] &amp;lt;- H

p &amp;lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))

result1 &amp;lt;- kalmanfiter(z,f,A_hat,lambda_hat,c=0,R=NA,Q=Q,d=0,S=NA,h=R,a_int=F_t[1,],sig_int=matrix(p,f,f))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;プロットしてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(tidyverse)

ggplot(gather(data.frame(factor1=result1$`state filtered`[1,1,-dim(result1$`state filtered`)[3]],factor2=result1$`state filtered`[2,1,-dim(result1$`state filtered`)[3]],factor3=result1$`state filtered`[3,1,-dim(result1$`state filtered`)[3]],time=as.Date(rownames(dataset1))),key = factor,value = value,-time),aes(x=time,y=value,colour=factor)) + geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;giannoneの記事を書いた際は、元論文の&lt;code&gt;MATLAB&lt;/code&gt;コードを参考にRで書いたのですが、通常のカルマンフィルタとは観測変数の分散共分散行列の逆数の計算方法が違うらしくグラフの形が異なっています。まあでも、概形はほとんど同じですが（なので、ちゃんと動いているはず）。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;カルマンスムージング&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. カルマンスムージング&lt;/h2&gt;
&lt;p&gt;カルマンフィルタの実装は以上で終了なのですが、誤差項の正規性を仮定すれば&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期までの情報集合&lt;span class=&#34;math inline&#34;&gt;\(\Omega_{T}\)&lt;/span&gt;を用いて、&lt;span class=&#34;math inline&#34;&gt;\(a_{i|i}, \Sigma_{i|i}(i = 1:T)\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(a_{i|T}, \Sigma_{i|T}(i = 1:T)\)&lt;/span&gt;へ更新することができます。これをカルマンスムージングと呼びます。これを導出してみましょう。その準備として、以下のような&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t|t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t+1|t}\)&lt;/span&gt;の混合分布を計算しておきます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
(\alpha_{t|t},\alpha_{t+1|t}) &amp;amp;=&amp;amp; N(
\left(
    \begin{array}{cccc}
      E(\alpha_{t|t}) \\
      E(\alpha_{t+1|t})
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      Var(\alpha_{t|t}), Cov(\alpha_{t|t},\alpha_{t+1|t}) \\
      Cov(\alpha_{t+1|t},\alpha_{t|t}), Var(\alpha_{t+1|t})
    \end{array}
  \right)
) \\
&amp;amp;=&amp;amp; N(
\left(
    \begin{array}{cccc}
      a_{t|t} \\
      a_{t+1|t}
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      \Sigma_{t|t}, \Sigma_{t|t}T_{t}&amp;#39; \\
      T_{t}\Sigma_{t|t}, \Sigma_{t+1|t} 
    \end{array}
  \right)
)
\end{eqnarray}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(Cov(\alpha_{t|t},\alpha_{t+1|t})\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
Cov(\alpha_{t+1|t},\alpha_{t|t}) &amp;amp;=&amp;amp; Cov(T_{t}\alpha_{t-1} + c_{t} + R_{t}\eta_{t}, \alpha_{t|t}) \\
&amp;amp;=&amp;amp; T_{t}Cov(\alpha_{t|t},\alpha_{t|t}) + Cov(c_{t},\alpha_{t|t}) + Cov(R_{t}\eta_{t},\alpha_{t|t}) \\
&amp;amp;=&amp;amp; T_{t}Var(\alpha_{t|t}) = T_{t}\Sigma_{t|t}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、条件付き多変量正規分布は以下のような分布をしていることを思い出しましょう（
&lt;a href=&#34;https://mathwords.net/gaussjoken&#34;&gt;参考&lt;/a&gt;
）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(X_{1},X_{2}) = N(
\left(
    \begin{array}{cccc}
      \mu_{1} \\
      \mu_{2}
    \end{array}
  \right),
\left(
    \begin{array}{cccc}
      \Sigma_{11}, \Sigma_{12} \\
      \Sigma_{21}, \Sigma_{22}
    \end{array}
  \right)
) \\
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
(X_{1}|X_{2}=x_{2}) = N(\mu_{1} + \Sigma_{12}\Sigma_{22}^{-1}(x_{2}-\mu_{2}),\Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;これを用いて、&lt;span class=&#34;math inline&#34;&gt;\((\alpha_{t|t}|\alpha_{t+1|t}=a_{t+1})\)&lt;/span&gt;を計算してみましょう。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
(\alpha_{t|t}|\alpha_{t+1|t}=a_{t+1}) &amp;amp;=&amp;amp; N(a_{t|t} + \Sigma_{t|t}T_{t}&amp;#39;\Sigma_{t+1|t}^{-1}(a_{t+1}-a_{t+1|t}), \Sigma_{t|t}-\Sigma_{t|t}T_{t}&amp;#39;\Sigma_{t+1|t}^{-1}T_{t}\Sigma_{t|t}) \\
&amp;amp;=&amp;amp;N(a_{t|t} + L_{t}(a_{t+1}-a_{t+1|t}), \Sigma_{t|t}-L_{t}\Sigma_{t+1|t}L_{t}&amp;#39;)
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ただし、&lt;span class=&#34;math inline&#34;&gt;\(a_{t+1}\)&lt;/span&gt;の値は観測不可能なので、上式を用いて状態変数を更新することはできません。今、わかるのは&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期における&lt;span class=&#34;math inline&#34;&gt;\(a_{t+1|T}\)&lt;/span&gt;の分布のみです。ということで、&lt;span class=&#34;math inline&#34;&gt;\(a_{t+1}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(a_{t+1|T}\)&lt;/span&gt;で代用し、&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t|T}\)&lt;/span&gt;の分布を求めてみます。では、計算していきます。&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t|T} = N(E(\alpha_{t|T}),Var(\alpha_{t|T}))\)&lt;/span&gt;ですが、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E(\alpha_{t|T}) &amp;amp;=&amp;amp; E_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
Var(\alpha_{t|T}) &amp;amp;=&amp;amp; E_{\alpha_{t+1|T}}(Var(\alpha_{t|t}|\alpha_{t+1|t} = \alpha_{t+1|T})) + Var_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T}))
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;というように、&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t+1|T}\)&lt;/span&gt;も確率変数となるので、繰り返し期待値の法則と繰り返し分散の法則を使用します（&lt;a href=&#34;https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/video-lectures/lecture-12-iterated-expectations-sum-of-a-random-number-of-random-variables/MIT6_041F10_L12.pdf&#34;&gt;こちら&lt;/a&gt;を参照）。&lt;/p&gt;
&lt;p&gt;*繰り返し期待値の法則
&lt;span class=&#34;math inline&#34;&gt;\(E(x) = E_{Z}(E(X|Y=Z))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;*繰り返し分散の法則
&lt;span class=&#34;math inline&#34;&gt;\(Var(X) = E_{Z}(Var(X|Y=Z))+Var_{Z}(E(X|Y=Z))\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;よって、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
E(\alpha_{t|T}) &amp;amp;=&amp;amp; E_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
&amp;amp;=&amp;amp; a_{t|t} + L_{t}(a_{t+1|T}-a_{t+1|t}) \\
Var(\alpha_{t|T}) &amp;amp;=&amp;amp; E_{\alpha_{t+1|T}}(Var(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) + Var_{\alpha_{t+1|T}}(E(\alpha_{t|t}|\alpha_{t+1|t}=\alpha_{t+1|T})) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + E( (a_{t|t} + L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))(a_{t|t} + L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - a_{t|t} - L_{t}(a_{t+1|T}-a_{t+1|t}))&amp;#39;) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + E( (L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))(L_{t}(\alpha_{t+1|T}-a_{t+1|t}) - L_{t}(a_{t+1|T}-a_{t+1|t}))&amp;#39;) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + E( (L_{t}\alpha_{t+1|T} - L_{t}a_{t+1|T})(L_{t}\alpha_{t+1|T} - L_{t}(a_{t+1|T})&amp;#39;) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + E( L_{t}(\alpha_{t+1|T} - a_{t+1|T})(\alpha_{t+1|T} - a_{t+1|T})&amp;#39;L_{t}&amp;#39;) \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + L_{t}E( (\alpha_{t+1|T} - a_{t+1|T})(\alpha_{t+1|T} - a_{t+1|T})&amp;#39;)L_{t}&amp;#39; \\
&amp;amp;=&amp;amp; \Sigma_{t|t} - L_{t}\Sigma_{t+1|t}L_{t}&amp;#39; + L_{t}\Sigma_{t+1|T}L_{t}&amp;#39;
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。カルマンスムージングのアルゴリズムをまとめておきます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
L_{t} &amp;amp;=&amp;amp; \Sigma_{t|t}T_{t}\Sigma_{t+1|t}^{-1} \\
a_{t|T} &amp;amp;=&amp;amp; a_{t|t} + L_{t}(a_{t+1|T} - a_{t+1|t}) \\
\Sigma_{t|T} &amp;amp;=&amp;amp; \Sigma_{t+1|t} + L_{t}(\Sigma_{t+1|T}-\Sigma_{t+1|t})L_{t}&amp;#39;
\end{eqnarray}
\]&lt;/span&gt;
カルマンスムージングの特徴的な点は後ろ向きに計算をしていく点です。つまり、&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期から1期に向けて計算を行っていきます。&lt;span class=&#34;math inline&#34;&gt;\(L_{t}\)&lt;/span&gt;に関してはそもそもカルマンフィルタを回した時点で計算可能ですが、&lt;span class=&#34;math inline&#34;&gt;\(\alpha_{t|T}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期までのデータが手元にないと計算できません。今、&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期まで観測可能なデータが入手できたとしましょう。すると、２番目の方程式を用いて、&lt;span class=&#34;math inline&#34;&gt;\(a_{T-1|T}\)&lt;/span&gt;を計算します。ちなみに&lt;span class=&#34;math inline&#34;&gt;\(a_{T|T}\)&lt;/span&gt;はカルマンフィルタを回した時点ですでに手に入っているので、計算する必要はありません。同時に、３番目の式を用いて&lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{T-1|T}\)&lt;/span&gt;を計算します。そして、&lt;span class=&#34;math inline&#34;&gt;\(a_{T-1|T},\Sigma_{T-1|T}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(L_{T-1}\)&lt;/span&gt;を用いて&lt;span class=&#34;math inline&#34;&gt;\(a_{T-2|T},\Sigma_{T-2|T}\)&lt;/span&gt;を計算、というように1期に向けて後ろ向きに計算をしていくのです。さきほど、遷移方程式の誤差項&lt;span class=&#34;math inline&#34;&gt;\(\eta_{t}\)&lt;/span&gt;と定数項&lt;span class=&#34;math inline&#34;&gt;\(c_{t}\)&lt;/span&gt;がなく、遷移方程式のパラメータが単位行列のカルマンフィルタは逐次最小自乗法と一致すると書きましたが、カルマンスムージングの場合は&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;期までのサンプルで&lt;code&gt;OLS&lt;/code&gt;を行った結果と一致します。
&lt;code&gt;R&lt;/code&gt;で実装してみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kalmansmoothing &amp;lt;- function(filter){
  #-------------------------------------------------------------------
  # Implemention of Kalman smoothing
  #   t - parameter of endogenous variable in state equation
  #   z - parameter of endogenous variable in observable equation
  #   a_pre - prediction of state
  #   a_fil - filtered value of state
  #   sig_pre - prediction of var of state
  #   sig_fil - filtered value of state
  #-------------------------------------------------------------------
  
  library(MASS)
  
  # 1.Define variable
  a_pre &amp;lt;- filter$`state prediction`
  a_fil &amp;lt;- filter$`state filtered`
  sig_pre &amp;lt;- filter$`state var prediction`
  sig_fil &amp;lt;- filter$`state var filtered`
  t &amp;lt;- filter$`parameter of state eq`
  C &amp;lt;- array(0,dim = dim(sig_pre))
  a_sm &amp;lt;- array(0,dim = dim(a_pre))
  sig_sm &amp;lt;- array(0,dim = dim(sig_pre))
  N &amp;lt;- dim(C)[3]
  a_sm[,,N] &amp;lt;- a_fil[,,N]
  sig_sm[,,N] &amp;lt;- sig_fil[,,N]
  
  for (i in N:2){
    C[,,i-1] &amp;lt;- sig_fil[,,i-1]%*%t(t)%*%ginv(sig_pre[,,i])
    a_sm[,,i-1] &amp;lt;- a_fil[,,i-1] + C[,,i-1]%*%(a_sm[,,i]-a_pre[,,i])
    sig_sm[,,i-1] &amp;lt;- sig_fil[,,i-1] + C[,,i-1]%*%(sig_sm[,,i]-sig_pre[,,i])%*%t(C[,,i-1])
  }
  
  
  result &amp;lt;- list(a_sm,sig_sm,C)
  names(result) &amp;lt;- c(&amp;quot;state smoothed&amp;quot;, &amp;quot;state var smoothed&amp;quot;, &amp;quot;c&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;先ほどのコードの続きで&lt;code&gt;R&lt;/code&gt;コードを書いてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result2 &amp;lt;- kalmansmoothing(result1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;かなりシンプルですね。ちなみにグラフにしましたが、１個目とほぼ変わりませんでした。とりあえず、今日はここまで。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>ガウス回帰の実装をやってみた</title>
      <link>/post/post1/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/post1/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gprとは&#34;&gt;1. &lt;code&gt;GPR&lt;/code&gt;とは&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gprの実装&#34;&gt;2. &lt;code&gt;GPR&lt;/code&gt;の実装&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;　おはこんばんにちは。昨日、&lt;code&gt;Bayesian Vector Autoregression&lt;/code&gt;の記事を書きました。&lt;br /&gt;
　その中でハイパーパラメータのチューニングの話が出てきて、なにか効率的にチューニングを行う方法はないかと探していた際に&lt;code&gt;Bayesian Optimization&lt;/code&gt;を発見しました。日次GDPでも機械学習の手法を利用しようと思っているので、&lt;code&gt;Bayesian Optimization&lt;/code&gt;はかなり使える手法ではないかと思い、昨日徹夜で理解しました。&lt;br /&gt;
　その内容をここで実装しようとは思うのですが、&lt;code&gt;Bayesian Optimization&lt;/code&gt;ではガウス回帰（&lt;code&gt;Gaussian Pocess Regression&lt;/code&gt;,以下&lt;code&gt;GPR&lt;/code&gt;）を使用しており、まずその実装を行おうと持ったのがこのエントリを書いた動機です。&lt;code&gt;Bayesian Optimization&lt;/code&gt;の実装はこのエントリの後にでも書こうかなと思っています。&lt;/p&gt;
&lt;div id=&#34;gprとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. &lt;code&gt;GPR&lt;/code&gt;とは&lt;/h2&gt;
&lt;p&gt;　&lt;code&gt;GRP&lt;/code&gt;とは簡単に言ってしまえば「&lt;strong&gt;ベイズ推定を用いた非線形回帰手法の１種&lt;/strong&gt;」です。モデル自体は線形ですが、&lt;strong&gt;カーネルトリックを用いて入力変数を無限個非線形変換したもの&lt;/strong&gt;を説明変数として推定できるところが特徴です（カーネルになにを選択するかによります）。&lt;br /&gt;
　&lt;code&gt;GPR&lt;/code&gt;が想定しているのは、学習データとして入力データと教師データがそれぞれN個得られており、また入力データに関しては&lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;個目のデータも得られている状況です。この状況から、&lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;個目の教師データを予測します。&lt;br /&gt;
　教師データにはノイズが含まれており、以下のような確率モデルに従います。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
t_{i} = y_{i} + \epsilon_{i}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の観測可能な教師データ（スカラー）、&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;は観測できない出力データ（スカラー）、&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{i}\)&lt;/span&gt;は測定誤差で正規分布&lt;span class=&#34;math inline&#34;&gt;\(N(0,\beta^{-1})\)&lt;/span&gt;に従います。&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;は以下のような確率モデルに従います。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle y_{i}  = \textbf{w}^{T}\phi(x_{i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt;はi番目の入力データベクトル、&lt;span class=&#34;math inline&#34;&gt;\(\phi(・)\)&lt;/span&gt;は非線形関数、 &lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}^{T}\)&lt;/span&gt;は各入力データに対する重み係数（回帰係数）ベクトルです。非線形関数としては、&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i}) = (x_{1,i}, x_{1,i}^{2},...,x_{1,i}x_{2,i},...)\)&lt;/span&gt;を想定しています（&lt;span class=&#34;math inline&#34;&gt;\(x_{1,i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の入力データ&lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt;の１番目の変数）。教師データの確率モデルから、&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の出力データ&lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt;が得られたうえで&lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt;が得られる条件付確率は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 p(t_{i}|y_{i}) = N(t_{i}|y_{i},\beta^{-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{t} = (t_{1},...,t_{n})^{T}\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y} = (y_{1},...,y_{n})^{T}\)&lt;/span&gt;とすると、上式を拡張することで&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{t}|\textbf{y}) = N(\textbf{t}|\textbf{y},\beta^{-1}\textbf{I}_{N})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;と書けます。また、事前分布として&lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}\)&lt;/span&gt;の期待値は0、分散は全て&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;と仮定します。&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;はガウス過程に従うと仮定します。ガウス過程とは、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;の同時分布が多変量ガウス分布に従うもののことです。コードで書くと以下のようになります。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define Kernel function
Kernel_Mat &amp;lt;- function(X,sigma,beta){
  N &amp;lt;- NROW(X)
  K &amp;lt;- matrix(0,N,N)
  for (i in 1:N) {
    for (k in 1:N) {
      if(i==k) kdelta = 1 else kdelta = 0
      K[i,k] &amp;lt;- K[k,i] &amp;lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta
    }
  }
  return(K)
}

N &amp;lt;- 10 # max value of X
M &amp;lt;- 1000 # sample size
X &amp;lt;- matrix(seq(1,N,length=M),M,1) # create X
testK &amp;lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix

library(MASS)

P &amp;lt;- 6 # num of sample path
Y &amp;lt;- matrix(0,M,P) # define Y

for(i in 1:P){
  Y[,i] &amp;lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y
}

# Plot
matplot(x=X,y=Y,type = &amp;quot;l&amp;quot;,lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;　Kernel_Matについては後述しますが、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;の各要素&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y_{i} = \textbf{w}^{T}\phi(x_{i})\)&lt;/span&gt;の間の共分散行列&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;を入力&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;からカーネル法を用いて計算しています。そして、この&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;と平均0から、多変量正規乱数を6系列生成し、それをプロットしています。&lt;/p&gt;
&lt;p&gt;　これらの系列は共分散行列から計算されるので、&lt;strong&gt;各要素の共分散が正に大きくなればなるほど同じ値をとりやすくなる&lt;/strong&gt;ようモデリングされていることになります。また、グラフを見ればわかるように非常になめらかなグラフが生成されており、かつ非常に柔軟な関数を表現できていることがわかります。コードでは計算コストの関係上、入力を0から10に限定して1000個の入力点をサンプルし、作図を行っていますが、原理的には&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;は実数空間で定義されるものであるので、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt;は無限次元の多変量正規分布に従います。
以上のように、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;はガウス過程に従うと仮定するので同時確率&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt;は平均0、分散共分散行列が&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;の多変量正規分布&lt;span class=&#34;math inline&#34;&gt;\(N(\textbf{y}|0,K)\)&lt;/span&gt;に従います。ここで、&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;の各要素&lt;span class=&#34;math inline&#34;&gt;\(K_{i,j}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
K_{i,j} &amp;amp;=&amp;amp; cov[y_{i},y_{j}] = cov[\textbf{w}\phi(x_{i}),\textbf{w}\phi(x_{j})] \\
&amp;amp;=&amp;amp;\phi(x_{i})\phi(x_{j})cov[\textbf{w},\textbf{w}]=\phi(x_{i})\phi(x_{j})\alpha
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;です。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\phi(x_{j})\alpha\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\)&lt;/span&gt;の&lt;strong&gt;次元が大きくなればなるほど計算量が多く&lt;/strong&gt;なります（つまり、非線形変換をかければかけるほど計算が終わらない）。しかし、カーネル関数&lt;span class=&#34;math inline&#34;&gt;\(k(x,x&amp;#39;)\)&lt;/span&gt;を用いると、計算量は高々入力データ&lt;span class=&#34;math inline&#34;&gt;\(x_{i},x_{j}\)&lt;/span&gt;のサンプルサイズの次元になるので、計算がしやすくなります。カーネル関数を用いて&lt;span class=&#34;math inline&#34;&gt;\(K_{i,j} = k(x_{i},x_{j})\)&lt;/span&gt;となります。カーネル関数としてはいくつか種類がありますが、以下のガウスカーネルがよく使用されます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
k(x,x&amp;#39;) = a \exp(-b(x-x&amp;#39;)^{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;の同時確率が定義できたので、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{t}\)&lt;/span&gt;の同時確率を求めることができます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\displaystyle p(\textbf{t}) &amp;amp;=&amp;amp; \int p(\textbf{t}|\textbf{y})p(\textbf{y}) d\textbf{y} \\
 \displaystyle &amp;amp;=&amp;amp; \int N(\textbf{t}|\textbf{y},\beta^{-1}\textbf{I}_{N})N(\textbf{y}|0,K)d\textbf{y} \\
 &amp;amp;=&amp;amp; N(\textbf{y}|0,\textbf{C}_{N})
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{C}_{N} = K + \beta^{-1}\textbf{I}_{N}\)&lt;/span&gt;です。なお、最後の式展開は正規分布の再生性を利用しています（証明は正規分布の積率母関数から容易に導けます）。要は、両者は独立なので共分散は2つの分布の共分散の和となると言っているだけです。個人的には、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt;が先ほど説明したガウス過程の事前分布であり、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t}|\textbf{y})\)&lt;/span&gt;が尤度関数で、&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t})\)&lt;/span&gt;は事後分布をというようなイメージです。事前分布&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt;は制約の緩い分布でなめらかであることのみが唯一の制約です。
&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;個の観測可能な教師データ&lt;span class=&#34;math inline&#34;&gt;\(\textbf{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(t_{N+1}\)&lt;/span&gt;の同時確率は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 p(\textbf{t},t_{N+1}) = N(\textbf{t},t_{N+1}|0,\textbf{C}_{N+1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{C}_{N+1}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \textbf{C}_{N+1} = \left(
    \begin{array}{cccc}
      \textbf{C}_{N} &amp;amp; \textbf{k} \\
      \textbf{k}^{T} &amp;amp; c \\
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;です。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\textbf{k} = (k(x_{1},x_{N+1}),...,k(x_{N},x_{N+1}))\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(c = k(x_{N+1},x_{N+1})\)&lt;/span&gt;です。&lt;span class=&#34;math inline&#34;&gt;\(\textbf{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(t_{N+1}\)&lt;/span&gt;の同時分布から条件付分布&lt;span class=&#34;math inline&#34;&gt;\(p(t_{N+1}|\textbf{t})\)&lt;/span&gt;を求めることができます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(t_{N+1}|\textbf{t}) = N(t_{N+1}|\textbf{k}^{T}\textbf{C}_{N+1}^{-1}\textbf{t},c-\textbf{k}^{T}\textbf{C}_{N+1}^{-1}\textbf{k})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;条件付分布の計算においては、&lt;a href=&#34;https://qiita.com/kilometer/items/34249479dc2ac3af5706:title&#34;&gt;条件付多変量正規分布の性質&lt;/a&gt;を利用しています。上式を見ればわかるように、条件付分布&lt;span class=&#34;math inline&#34;&gt;\(p(t_{N+1}|\textbf{t})\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;個の入力データ、&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;個の教師データ、カーネル関数のパラメータ&lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt;が既知であれば計算可能となっていますので、任意の点を入力データとして与えてやれば、元のData Generating Processを近似することが可能になります。&lt;code&gt;GPR&lt;/code&gt;の良いところは上で定義した確率モデル&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y_{i} = \textbf{w}^{T}\phi(x_{i})\)&lt;/span&gt;を直接推定しなくても予測値が得られるところです。確率モデルには&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\)&lt;/span&gt;があり、非線形変換により入力データを高次元ベクトルへ変換しています。よって、次元が高くなればなるほど&lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\phi(x_{j})\alpha\)&lt;/span&gt;の計算量は大きくなっていきますが、&lt;code&gt;GPR&lt;/code&gt;ではカーネルトリックを用いているので高々入力データベクトルのサンプルサイズの次元の計算量で事足りることになります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;gprの実装&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. &lt;code&gt;GPR&lt;/code&gt;の実装&lt;/h2&gt;
&lt;p&gt;　とりあえずここまでを&lt;code&gt;R&lt;/code&gt;で実装してみましょう。PRMLのテストデータで実装しているものがあったので、それをベースにいじってみました。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(grid)

# 1.Gaussian Process Regression

# PRML&amp;#39;s synthetic data set
curve_fitting &amp;lt;- data.frame(
  x=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000),
  t=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502))

f &amp;lt;- function(beta, sigma, xmin, xmax, input, train) {
  kernel &amp;lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function
  K &amp;lt;- outer(input, input, kernel); # calc gram matrix
  C_N &amp;lt;- K + diag(length(input))/beta
  m &amp;lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean 
  m_sig &amp;lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance
  x &amp;lt;- seq(xmin,xmax,length=100)
  output &amp;lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),
                              tx=input,ty=train),
                   aes(x=x1,y=m)) + 
    geom_line() +
    geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +
    geom_point(aes(x=tx,y=ty))
  return(output)
}

grid.newpage() # make a palet
pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2
print(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))
print(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2))
print(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))
print(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta^{-1}\)&lt;/span&gt;は測定誤差を表しています。&lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;が大きい（つまり、測定誤差が小さい）とすでに得られているデータとの誤差が少なくなるように予測値をはじき出すので、over fitting しやすくなります。&lt;/strong&gt;上図の左上がそうなっています。左上は&lt;span class=&#34;math inline&#34;&gt;\(\beta=400\)&lt;/span&gt;で、現時点で得られているデータに過度にfitしていることがわかります。逆に&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;が小さいと教師データとの誤差を無視するように予測値をはじき出しますが、汎化性能は向上するかもしれません。右上の図がそれです。&lt;span class=&#34;math inline&#34;&gt;\(\beta=4\)&lt;/span&gt;で、得られているデータ点を平均はほとんど通っていません。&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;は現時点で得られているデータが周りに及ぼす影響の広さを表しています。&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;が小さいと、隣接する点が互いに強く影響を及ぼし合うため、精度は下がるが汎化性能は上がるかもしれません。逆に、&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;が大きいと、個々の点にのみフィットする不自然な結果になります。これは右下の図になります（&lt;span class=&#34;math inline&#34;&gt;\(b=\frac{1}{0.03},\beta=25\)&lt;/span&gt;）。御覧の通り、&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;が大きいのでoverfitting気味であり、なおかつ&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;も大きいので個々の点のみにfitし、無茶苦茶なグラフになっています。左下のグラフが最もよさそうです。&lt;span class=&#34;math inline&#34;&gt;\(b=\frac{1}{0.3},\beta=2\)&lt;/span&gt;となっています。試しに、このグラフのx区間を[0,2]へ伸ばしてみましょう。すると、以下のようなグラフがかけます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid.newpage() # make a palet
pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2
print(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) 
print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) 
print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))
print(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;これを見ればわかるように、左下以外のグラフはすぐに95%信頼区間のバンドが広がり、データ点がないところではまったく使い物にならないことがわかります。一方、左下のグラフは1.3~1.4ぐらいまではそこそこのバンドがかけており、我々が直感的に理解する関数とも整合的な点を平均値が通っているように思えます。また、観測可能なデータ点から離れすぎるとパラメータに何を与えようと平均０、分散１の正規分布になることもわかるがわかります。
さて、このようにパラメータの値に応じて、アウトサンプルの予測精度が異なることを示したわけですが、ここで問題となるのはこれらハイパーパラメータをどのようにして推計するかです。これは対数尤度関数&lt;span class=&#34;math inline&#34;&gt;\(\ln p(\textbf{t}|a,b)\)&lt;/span&gt;を最大にするハイパーパラメータを勾配法により求めます((&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;は少しタイプが異なるようで、発展的な議論では他のチューニング方法をとる模様。まだ、そのレベルにはいけていないのでここではカリブレートすることにします。))。&lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t}) = N(\textbf{y}|0,\textbf{C}_{N})\)&lt;/span&gt;なので、対数尤度関数は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \ln p(\textbf{t}|a,b,\beta) = -\frac{1}{2}\ln|\textbf{C}_{N}| - \frac{N}{2}\ln(2\pi) - \frac{1}{2}\textbf{t}^{T}\textbf{C}_{N}^{-1}\textbf{k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となります。あとは、これをパラメータで微分し、得られた連立方程式を解くことで最尤推定量が得られます。ではまず導関数を導出してみます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial}{\partial \theta_{i}} \ln p(\textbf{t}|\theta) = -\frac{1}{2}Tr(\textbf{C}_{N}^{-1}\frac{\partial \textbf{C}_{N}}{\partial \theta_{i}}) + \frac{1}{2}\textbf{t}^{T}\textbf{C}_{N}^{-1}
\frac{\partial\textbf{C}_{N}}{\partial\theta_{i}}\textbf{C}_{N}^{-1}\textbf{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;はパラメータセットで、&lt;span class=&#34;math inline&#34;&gt;\(\theta_{i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目のパラメータを表しています。この導関数が理解できない方は&lt;a href=&#34;http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf:title=PRML&#34;&gt;こちら&lt;/a&gt;の補論にある(C.21)式と(C.22)式をご覧になると良いと思います。今回はガウスカーネルを用いているため、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial k(x,x&amp;#39;)}{\partial a} = \exp(-b(x-x&amp;#39;)^{2}) \\
\displaystyle \frac{\partial k(x,x&amp;#39;)}{\partial b} = -a(x-x&amp;#39;)^{2}\exp(-b(x-x&amp;#39;)^{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;を上式に代入すれば良いだけです。ただ、今回は勾配法により最適なパラメータを求めます。以下、実装のコードです（かなり迷走しています）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- function(xmin, xmax, input, train){
  # initial value
  beta = 100
  b = 1
  a = 1
  learning_rate = 0.1
  itermax &amp;lt;- 1000
  if (class(input) == &amp;quot;numeric&amp;quot;){
    N &amp;lt;- length(input)
  } else
  {
    N &amp;lt;- NROW(input)
  }
  kernel &amp;lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel
  derivative_a &amp;lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2)
  derivative_b &amp;lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)
  dloglik_a &amp;lt;- function(C_N,y,x1,x2) {
    -sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y 
  }
  dloglik_b &amp;lt;- function(C_N,y,x1,x2) {
    -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y 
  }
  # loglikelihood function
  likelihood &amp;lt;- function(b,a,x,y){
    kernel &amp;lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2)
    K &amp;lt;- outer(x, x, kernel)
    C_N &amp;lt;- K + diag(N)/beta
    itermax &amp;lt;- 1000
    l &amp;lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y
    return(l)
  }
  K &amp;lt;- outer(input, input, kernel) 
  C_N &amp;lt;- K + diag(N)/beta
  for (i in 1:itermax){
    kernel &amp;lt;- function(x1, x2) a*exp(-b*(x1-x2)^2)
    derivative_b &amp;lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)
    dloglik_b &amp;lt;- function(C_N,y,x1,x2) {
      -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y 
    }
    K &amp;lt;- outer(input, input, kernel) # calc gram matrix
    C_N &amp;lt;- K + diag(N)/beta
    l &amp;lt;- 0
    if(abs(l-likelihood(b,a,input,train))&amp;lt;0.0001&amp;amp;i&amp;gt;2){
      break
    }else{
      a &amp;lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input))
      b &amp;lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input))
    }
    l &amp;lt;- likelihood(b,a,input,train)
  }
  K &amp;lt;- outer(input, input, kernel)
  C_N &amp;lt;- K + diag(length(input))/beta
  m &amp;lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train)  
  m_sig &amp;lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel))))
  x &amp;lt;- seq(xmin,xmax,length=100)
  output &amp;lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),
                              tx=input,ty=train),
                   aes(x=x1,y=m)) + 
    geom_line() +
    geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +
    geom_point(aes(x=tx,y=ty))
  return(output)
}

print(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;たしかに、良さそうな感じがします（笑）
とりあえず、今日はここまで。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>BVARについて</title>
      <link>/post/post5/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/post/post5/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#unrestricted-varについて&#34;&gt;1. Unrestricted VARについて&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bvarについて&#34;&gt;2. BVARについて&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#具体的な推定方法カルマンフィルタ&#34;&gt;具体的な推定方法（カルマンフィルタ）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#カルマンフィルタの初期値をどのようにきめるか&#34;&gt;カルマンフィルタの初期値をどのようにきめるか&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ハイパーパラメータの決定方法とその評価尺度&#34;&gt;ハイパーパラメータの決定方法とその評価尺度&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rでの実装&#34;&gt;3. Rでの実装&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;　おはこんばんにちは。日次GDP推計を休日に進めているのですが、今日は少し勉強編で&lt;code&gt;BVAR&lt;/code&gt;についての記事を書きたいと思います。この&lt;code&gt;BVAR&lt;/code&gt;はFRBアトランタ連銀の&lt;code&gt;GDPNow&lt;/code&gt;でも使用されていることから、日次GDP推計との親和性も高いと思われます。そもそも、時系列でアウトサンプルの予測精度を上げたいということになると真っ先に思いつくのが&lt;code&gt;BVAR&lt;/code&gt;です。Doan, Litterman and Sims(1984)で提案されたこのモデルは予測精度が良いので、非常に有効な手段になると思われます。&lt;code&gt;BVAR&lt;/code&gt;は&lt;code&gt;Bayesian Vector Autoregression&lt;/code&gt;の略で、ベクトル自己回帰モデル（&lt;code&gt;VAR&lt;/code&gt;）の派生版です。&lt;code&gt;VAR&lt;/code&gt;とネットで調べるとまず&lt;code&gt;Value at Risk&lt;/code&gt;（&lt;code&gt;VaR&lt;/code&gt;）が出てくると思いますが、それとは違います。よく見るとaが小文字になっていることに気づくかと思います。
　さて、&lt;code&gt;BVAR&lt;/code&gt;の説明をこれから行おうとするのですが、その前にまず基本的な&lt;code&gt;VAR&lt;/code&gt;の説明からしたいと思います。ただし、歴史的な背景（大型マクロ計量モデルからの経緯など）には触れません。あくまで、&lt;code&gt;BVAR&lt;/code&gt;を説明するうえで必要な知識について触れたいと思います。&lt;/p&gt;
&lt;div id=&#34;unrestricted-varについて&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Unrestricted VARについて&lt;/h2&gt;
&lt;p&gt;　まず、注意点を一点。この投稿では、もっとも基本的な&lt;code&gt;VAR&lt;/code&gt;のことをUnrestricted VAR（UVAR）と呼ぶことにします。UVARはSims(1980)の論文が有名です。&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;このモデルには、理論的な基礎づけは原則ありません。あくまで実証的なモデルです。UVARは一般系は以下のような形をしています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = A_{0} + A_{1}Y_{t-1} + ... + A_{K}Y_{t-K} + U_{t}, ~~ U_{t} ～ N(0,\Omega)
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = \left(
    \begin{array}{cccc}
      y_{1,t} \\
      y_{2,t} \\
      \vdots  \\
      y_{J,t} \\
    \end{array}
  \right),
       A_{0} = \left(
    \begin{array}{cccc}
      a_{10} \\
      a_{20} \\
      \vdots  \\
      a_{J0} \\
    \end{array}
  \right),
      A_{k} = \left(
    \begin{array}{cccc}
      a_{11,k} &amp;amp; a_{12,k} &amp;amp; \ldots &amp;amp; a_{1J,k} \\
      a_{21,k} &amp;amp; a_{22,k} &amp;amp; \ldots &amp;amp; a_{2J,k} \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      a_{J1,k} &amp;amp; a_{J2,k} &amp;amp; \ldots &amp;amp; a_{JJ,k}
    \end{array}
  \right),
    U_{t} = \left(
    \begin{array}{cccc}
      u_{1,t} \\
      u_{2,t} \\
      \vdots  \\
      u_{J,t} \\
    \end{array}
  \right)
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;は時点、&lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;は変数の数、&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;はラグ数を表しています。上式を見ると、UVARは自己回帰＋他変数のラグでt期の変数&lt;span class=&#34;math inline&#34;&gt;\(y_{j,t}\)&lt;/span&gt;を説明しようとするモデルであると言えます。しばしば、経済の実証分析で使用され、インサンプルの当てはまりが良いことも知られています（GDP、消費、投資、金利、マネーサプライの５変数&lt;code&gt;VAR&lt;/code&gt;で金融政策の波及経路を分析したり･･･）。推定するパラメータの個数は、回帰式1本だけでJK+1個（定数項込み）の係数を含むので、J本になればJ(JK+1)個になります。また、&lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;がJ(J+1)/2個のパラメータを持っているので、合計J(JK+1)+J(J+1)/2個のパラメータを推定することになり、かなりパラメータ数が多い印象です（これは後々重要になってきます）。具体的な推計方法ですが、UVARは同時方程式体系ではないのでそこまで面倒ではありません。UVAR自体は&lt;code&gt;Seemingly Unrestricted Regression Equation&lt;/code&gt;（SUR）の一種でそれぞれの方程式は誤差項の相関を通じて関係してはいますが（&lt;span class=&#34;math inline&#34;&gt;\(\Omega\)&lt;/span&gt;の部分）、全ての回帰式が同じ説明変数を持つため、各方程式を最小二乗法（&lt;code&gt;OLS&lt;/code&gt;）によって推定するだけで良いことが知られています。
　この事実を説明してみましょう（&lt;code&gt;BVAR&lt;/code&gt;が気になる方は読み飛ばしてもらって構いません）。説明のために今、UVARをSURの一般系に書き直します。上式はt期の&lt;code&gt;VAR&lt;/code&gt;(K)システムですが、UVARを推定する際はこれらJ本の方程式がサンプル数Tセット分存在するので、実際のシステム体系は以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
   \left(
    \begin{array}{cccc}
      Y_{1} \\
      Y_{2} \\
      \vdots  \\
      Y_{J} \\
    \end{array}
  \right) = 
 \left(
    \begin{array}{cccc}
      X_{1} &amp;amp; 0 &amp;amp; \ldots &amp;amp; 0 \\
      0 &amp;amp; X_{2} &amp;amp; \ldots &amp;amp; 0 \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; X_{J}
    \end{array}
  \right) 
  \left(
    \begin{array}{cccc}
      A_{1} \\
      A_{2} \\
      \vdots  \\
      A_{J} \\
    \end{array}
  \right) +
  \left(
    \begin{array}{cccc}
      U_{1} \\
      U_{2} \\
      \vdots  \\
      U_{J} \\
    \end{array}
  \right) = \overline{X}A + U (1式)
\]&lt;/span&gt;
ここで、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{j} = \left(
    \begin{array}{cccc}
      y_{t,j} \\
      y_{t+1,j} \\
      \vdots  \\
      y_{T,j} \\
    \end{array}
  \right) ,
X_{j} = X = \left(
    \begin{array}{cccc}
      1 &amp;amp; y_{t-1,1} &amp;amp; \ldots &amp;amp; y_{t-K,1} &amp;amp; \ldots &amp;amp; y_{t-K,J} \\
      1 &amp;amp; y_{t,1} &amp;amp; \ldots &amp;amp; y_{t-K+1,1} &amp;amp; \ldots &amp;amp; y_{t-K+1,J} \\
      \vdots &amp;amp; \vdots &amp;amp; \ldots &amp;amp; \ldots &amp;amp; \ddots &amp;amp; \vdots \\
      1 &amp;amp; y_{T-1,1} &amp;amp; \ldots &amp;amp; y_{T-K,1} &amp;amp; \ldots &amp;amp; y_{T-K,J} \\
    \end{array}
  \right),
 A_{j} = \left(
    \begin{array}{cccc}
      a_{00,j} \\
      a_{11,j} \\
      \vdots  \\
      a_{JK,j} \\
    \end{array}
  \right)
\]&lt;/span&gt;
です。上式とは違い、変数順で並べられていることに注意してください（つまり、各方程式を並べる優先順位は１番目にj、２番目にtとなっている）。また、&lt;span class=&#34;math inline&#34;&gt;\(X_{j}\)&lt;/span&gt;が全ての変数&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;について等しいことに注目してください（jに依存していません、&lt;code&gt;VAR&lt;/code&gt;なので当たり前ですが）。これが後々非常に重要になってきます。&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;の分散共分散行列は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[UU&amp;#39;|X_{1}, X_{2},..,X_{J}] = \Omega = 
\left(
    \begin{array}{cccc}
      \sigma_{11}I &amp;amp; \sigma_{12}I &amp;amp; \ldots &amp;amp; \sigma_{1J}I \\
      \sigma_{21}I &amp;amp; \sigma_{22}I &amp;amp; \ldots &amp;amp; \sigma_{2J}I \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      \sigma_{J1}I &amp;amp; \sigma_{J2}I &amp;amp; \ldots &amp;amp; \sigma_{JJ}I \\
    \end{array}
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となっており、それぞれの変数は誤差項の相関を通じて関係しています（ただし、同じ変数内の異なる時点間の相関はないと仮定します）。このような場合、一般化最小二乗法（&lt;code&gt;GLS&lt;/code&gt;）を用いて推計を行うことになりますが、これら方程式体系において説明変数が同じであるならば、&lt;code&gt;GLS&lt;/code&gt;推定量と&lt;code&gt;OLS&lt;/code&gt;推定量は同値になります。それを確かめてみましょう。上述した方程式体系(1)の&lt;code&gt;GLS&lt;/code&gt;推定量は以下になります（&lt;a href=&#34;http://user.keio.ac.jp/~nagakura/zemi/GLS.pdf&#34;&gt;参考&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
A_{GLS} = (\overline{X}&amp;#39;\Omega^{-1}\overline{X})^{-1}\overline{X}&amp;#39;\Omega^{-1}Y = (\overline{X}&amp;#39;(\Sigma^{-1}\otimes I)\overline{X})^{-1}\overline{X}&amp;#39;(\Sigma^{-1}\otimes I)Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt;は誤差項の分散共分散行列のうち、スカラーである分散、共分散を取り出した行列です。先ほど確認したように、&lt;span class=&#34;math inline&#34;&gt;\(X_{1}=X_{2}=...=X_{J}=X\)&lt;/span&gt;なので&lt;span class=&#34;math inline&#34;&gt;\(\overline{X}=I \otimes X\)&lt;/span&gt;であり、その転置も&lt;span class=&#34;math inline&#34;&gt;\(\overline{X}&amp;#39;=I \otimes X\)&lt;/span&gt;となります。よって、&lt;a href=&#34;https://mathwords.net/kuronekaseki&#34;&gt;この&lt;/a&gt;の性質を用いると、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
A_{GLS} &amp;amp;=&amp;amp; [(I \otimes X&amp;#39;)(\Sigma^{-1}\otimes I)(I \otimes X)]^{-1}(I \otimes X&amp;#39;)(\Sigma^{-1}\otimes I)Y \\
&amp;amp;=&amp;amp; [(\Sigma^{-1}\otimes X&amp;#39;)(I \otimes X)]^{-1}(\Sigma^{-1}\otimes X&amp;#39;)y \\
&amp;amp;=&amp;amp; (\Sigma^{-1}\otimes(X&amp;#39;X))^{-1}(\Sigma^{-1}\otimes X&amp;#39;)y \\
&amp;amp;=&amp;amp; (I \otimes (X&amp;#39;X)^{-1}X&amp;#39;)y \\
&amp;amp;=&amp;amp; (\overline{X}&amp;#39;\overline{X})^{-1}\overline{X}&amp;#39;Y \\
&amp;amp;=&amp;amp;  \left(
    \begin{array}{cccc}
      (X&amp;#39;X)^{-1}X&amp;#39; &amp;amp; 0 &amp;amp; \ldots &amp;amp; 0 \\
      0 &amp;amp; (X&amp;#39;X)^{-1}X&amp;#39; &amp;amp; \ldots &amp;amp; 0 \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; (X&amp;#39;X)^{-1}X&amp;#39;
    \end{array}
  \right) 
\left(
    \begin{array}{cccc}
      Y_{1} \\
      Y_{2} \\
      \vdots  \\
      Y_{J} \\
    \end{array}
  \right)\\ 
&amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      (X&amp;#39;X)^{-1}X&amp;#39;Y_{1} \\
      (X&amp;#39;X)^{-1}X&amp;#39;Y_{2} \\
      \vdots  \\
      (X&amp;#39;X)^{-1}X&amp;#39;Y_{J} \\
    \end{array}
  \right) \\
&amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      A_{1,OLS} \\
      A_{2,OLS} \\
      \vdots  \\
      A_{J,OLS} \\
    \end{array}
  \right)
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;となり、各経済変数の方程式を別個に&lt;code&gt;OLS&lt;/code&gt;推計していけばよいことがわかります。
　&lt;code&gt;OLS&lt;/code&gt;推定に際し、&lt;code&gt;VAR&lt;/code&gt;の次数Kの選択を選択する必要がありますが、次数Kは&lt;code&gt;AIC&lt;/code&gt;（&lt;code&gt;BIC&lt;/code&gt;）を評価軸に探索的に決定します。つまり、いろいろな値をKに設定し、&lt;code&gt;OLS&lt;/code&gt;推計を行い、計算された&lt;code&gt;VAR&lt;/code&gt;(K)の&lt;code&gt;AIC&lt;/code&gt;(&lt;code&gt;BIC&lt;/code&gt;)のうちで最も値が大きいモデルの次数を真のモデルの次数Kとして採用するということです。&lt;code&gt;R&lt;/code&gt;にも&lt;code&gt;VARselect()&lt;/code&gt;という関数があり、引数にラグの探索最大数とデータを渡すことで最適な次数を計算してくれます（便利）。
　このようにUVARは&lt;code&gt;OLS&lt;/code&gt;推計で各変数間の相互依存関係をデータから推計できる手軽な手法です。私が知っている分野ですと財政政策乗数の推計に使用されていました。GDP、消費、投資、政府支出の４変数で&lt;code&gt;VAR&lt;/code&gt;を推定し、推定した&lt;code&gt;VAR&lt;/code&gt;でインパルス応答を見ることで１単位の財政支出の増加がGDP等に与える影響を定量的にシミュレートすることができたりします（指導教官が論文を書いてました）。そもそも私の専門の&lt;code&gt;DSGE&lt;/code&gt;も誘導系に書き直せば&lt;code&gt;VAR&lt;/code&gt;形式になり、インパルス応答などは基本的に一緒です。また、経済変数は慣性が強いので（特に我が国の場合）、インサンプルのモデルの当てはまりもいいです。ただし、あくまでインサンプルです。&lt;b&gt;アウトサンプルの当てはまりはそれほど良い印象はありません。&lt;/b&gt;なぜなら、推定パラメータが多すぎるからです。UVARの予測に関する問題点は&lt;code&gt;over-parametrization&lt;/code&gt;です。例えば、先ほどの４変数UVARでラグが６期だったとすると、推定すべきパラメータは３１個になります。よって、データ数にもよりますが、パラメータ数がデータ数に近づくとインサンプルの補間に近づき、過学習を引き起こす危険性があります。日次GDP推計は大量の変数を使用するのでこの問題は非常に致命的になります。&lt;code&gt;BVAR&lt;/code&gt;はこの問題を解決することに主眼を置いています。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bvarについて&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. BVARについて&lt;/h2&gt;
&lt;p&gt;　UVARの問題点は&lt;code&gt;over-parametrization&lt;/code&gt;であると述べました。&lt;code&gt;BVAR&lt;/code&gt;はこの問題を防ぐために不必要な説明変数（のパラメータ）をそぎ落とそうとします。ただ、不要なパラメータを推定する前に０と仮置き（カリブレート）するのではなく、１階の自己に関わるパラメータは１周り、その他変数のパラメータは０周りに正規分布するという形の制約を与えます。このモデルの最大の仮定は「各経済変数は多かれ少なかれドリフト付き１階のランダムウォークに従う」というものであり、上述したような事前分布を先験的に与えた上で推定を行うのです。砕けた言い方をすると、&lt;code&gt;BVAR&lt;/code&gt;の考え方は「経済変数の挙動は基本はランダムウォークだけど他変数のラグに予測力向上に資するものがあればそれも取り入れるよ」というものなんです。さて、前置きが長くなりましたが、具体的な説明に移りたいと思います。&lt;/p&gt;
&lt;div id=&#34;具体的な推定方法カルマンフィルタ&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;具体的な推定方法（カルマンフィルタ）&lt;/h3&gt;
&lt;p&gt;　上述した事前分布に加えて、&lt;code&gt;BVAR&lt;/code&gt;がUVARと異なる点はパラメータが&lt;code&gt;time-varying&lt;/code&gt;であるということです。なんとなく、パラメータがずっと固定よりもサンプルが増えるたびにその値が更新されるほうが予測精度が上がりそうですよね（笑）。推定手法としてはUVARの時のように&lt;code&gt;OLS&lt;/code&gt;をそれぞれにかけることはせず、&lt;a href=&#34;https://qiita.com/MoriKen/items/0c80ef75749977767b43&#34;&gt;カルマンフィルタ&lt;/a&gt; と呼ばれるアルゴリズムを用いて推定を行います。&lt;code&gt;BVAR&lt;/code&gt;は以下のような状態空間モデルとして定義されます（各ベクトル、行列の次元はUVAR時と同じです）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y_{t} = X_{t}B_{t} + u_{t} \\
B_{t} = \Phi B_{t-1} + \epsilon_{t} \\
B_{t} = \left(
    \begin{array}{cccc}
      \beta_{00,t} \\
      \beta_{11,t} \\
      \vdots  \\
      \beta_{JK,t} \\
    \end{array}
  \right), 
\Phi = \left(
    \begin{array}{cccc}
      \phi_{00} &amp;amp; 0 &amp;amp; \ldots &amp;amp; 0 \\
      0 &amp;amp; \phi_{11} &amp;amp; \ldots &amp;amp; 0 \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; \phi_{JK}
    \end{array}
  \right),
u_{t} = \left(
    \begin{array}{cccc}
      u_{1,t} \\
      u_{2,t} \\
      \vdots  \\
      u_{J,t} \\
    \end{array}
  \right),
\epsilon_{t} = \left(
    \begin{array}{cccc}
      \epsilon_{00,t} \\
      \epsilon_{11,t} \\
      \vdots  \\
      \epsilon_{JK,t} \\
    \end{array}
  \right)
\]&lt;/span&gt;
状態空間モデルは観測可能なデータ（ex.経済統計）を用いて、観測不可能なデータ（ex.リスクプレミアムや限界消費性向等）を推定します。観測可能なデータと不可能なデータを関連付ける方程式を観測方程式、観測不可能なデータの挙動をモデル化した方程式を遷移方程式と呼びます。ここでは、1本目が観測方程式、2本目が遷移方程式となります。御覧の通り、観測方程式は通常の&lt;code&gt;VAR&lt;/code&gt;の形をしている一方、遷移方程式は&lt;code&gt;AR(1)&lt;/code&gt;となっており、これによってパラメータ&lt;span class=&#34;math inline&#34;&gt;\(B_{t}\)&lt;/span&gt;は過去の値を引きずりながら&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\)&lt;/span&gt;によって確率的に変動します。&lt;span class=&#34;math inline&#34;&gt;\(\Phi\)&lt;/span&gt;は自己回帰係数です。誤差項&lt;span class=&#34;math inline&#34;&gt;\(u_{t}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{t}\)&lt;/span&gt;は平均０の正規分布に従い、その分散は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
Var(u_{t}) &amp;amp;=&amp;amp; \sigma_{u}^{2} \\
Var(\epsilon_{t}) &amp;amp;=&amp;amp; \sigma_{\epsilon}^{2}R
\end{eqnarray}
\]&lt;/span&gt;
で与えられます。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\Phi,R,\sigma_{u}^{2},\sigma_{\epsilon}^{2}\)&lt;/span&gt;は既知であるとします。今、t-1期までのデータが入手可能であるとすると、そのデータをカルマンフィルタアルゴリズムで推定した&lt;span class=&#34;math inline&#34;&gt;\(B_{t-1|t-1}\)&lt;/span&gt;とその分散共分散行列&lt;span class=&#34;math inline&#34;&gt;\(P_{t-1|t-1}\)&lt;/span&gt;を用いて、t期の予想値を以下のように計算します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
B_{t|t-1} &amp;amp;=&amp;amp; \Phi B_{t-1|t-1} \\
P_{t|t-1} &amp;amp;=&amp;amp; \Phi P_{t|t-1} \Phi&amp;#39; + \sigma_{\epsilon}^{2}R
\end{eqnarray}
\]&lt;/span&gt;
観測可能な変数の予測値はこの値を用いて計算します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{Y_{t}} = X_{t} B_{t|t-1}
\]&lt;/span&gt;
次にt期の観測値が得られると次の更新方程式を用いて&lt;span class=&#34;math inline&#34;&gt;\(B_{t|t}, P_{t|t}\)&lt;/span&gt;を計算します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
B_{t|t} &amp;amp;=&amp;amp; B_{t|t-1} + P_{t|t-1}X_{t}&amp;#39;(X_{t}P_{t|t-1}X_{t}&amp;#39; + \sigma_{u}^{2})^{-1}(Y_{t}-X_{t}B_{t|t-1}) \\
P_{t|t} &amp;amp;=&amp;amp; P_{t|t-1} + P_{t|t-1}X_{t}&amp;#39;(X_{t}P_{t|t-1}X_{t}&amp;#39; + \sigma_{u}^{2})^{-1}X_{t}P_{t|t-1}
\end{eqnarray}
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\epsilon}^{2}R = 0\)&lt;/span&gt;かつ&lt;span class=&#34;math inline&#34;&gt;\(\Phi = I\)&lt;/span&gt;の場合は&lt;code&gt;逐次最小二乗法&lt;/code&gt;に一致します。要は入手できるサンプル増えるたびに&lt;code&gt;OLS&lt;/code&gt;をやり直していくことと同値だということです。こうして推計を行うのが&lt;code&gt;BVAR&lt;/code&gt;なのですが、カルマンフィルタは漸化式なので初期値&lt;span class=&#34;math inline&#34;&gt;\(B_{0|0}, P_{0|0}\)&lt;/span&gt;を決めてやる必要があります。&lt;code&gt;BVAR&lt;/code&gt;の２つ目の特徴は初期値の計算に混合推定法を用いているところであり、ここに前述した事前分布が関係してきます。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;カルマンフィルタの初期値をどのようにきめるか&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;カルマンフィルタの初期値をどのようにきめるか&lt;/h3&gt;
&lt;p&gt;初期値&lt;span class=&#34;math inline&#34;&gt;\(B_{0|0}, P_{0|0}\)&lt;/span&gt;をどうやって計算するのかを考えた際にすぐ思いつく方法としては、カルマンフィルタのスタート地点tの前に、初期値推計期間をある程度用意し、&lt;span class=&#34;math inline&#34;&gt;\(Y = XB + \epsilon\)&lt;/span&gt;で&lt;span class=&#34;math inline&#34;&gt;\(B_{0|0}, P_{0|0}\)&lt;/span&gt;を推定する方法があります。つまり、観測方程式を&lt;code&gt;GLS&lt;/code&gt;推計し、そのパラメータを初期値とする方法です。その際の&lt;code&gt;GLS&lt;/code&gt;推定量は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{B} = (X&amp;#39;V^{-1}X)^{-1}X&amp;#39;V^{-1}Y
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;です。これでもいいんですが、これではただ時変パラメータを推計しているだけで先ほど述べた&lt;code&gt;over-parametrization&lt;/code&gt;の問題にはアプローチできていません。そこで、ここではパラメータ&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;に対してなにか先験的な情報が得られているとしましょう。つまり、先述した「経済変数の挙動は基本はランダムウォークだけど他変数のラグに予測力向上に資するものがあればそれも取り入れるよ」という予想です。これを定式化してみましょう。つまり、パラメータ&lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt;に対して以下の制約式を課します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r = RB + \nu
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(E(\nu) = 0,Var(\nu) = V_{0}\)&lt;/span&gt;です。また、&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(RB\)&lt;/span&gt;の予想値であり、&lt;span class=&#34;math inline&#34;&gt;\(V_{0}\)&lt;/span&gt;はその予想値の周りでのばらつきを表しています。取っ付きにくいかもしれませんが、&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;は１階自己回帰係数に関わる部分は1、それ以外は0となるベクトルで、&lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;は単位行列&lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt;だと思ってもらえばいいです。つまり、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\left(
    \begin{array}{cccc}
      0 \\
      1 \\
      0 \\
      \vdots  \\
      1 \\
      \vdots  \\
      0 \\
    \end{array}
  \right)
 &amp;amp;=&amp;amp; 
\left(
    \begin{array}{cccc}
　  \beta_{0,1}^{1} \\
      \beta_{1,1}^{1} \\
      \beta_{2,1}^{1} \\
      \vdots  \\
      \beta_{j,1}^{j} \\
      \vdots  \\
      \beta_{J,K}^{J} \\
    \end{array}
  \right)
+
\left(
    \begin{array}{cccc}
      \nu_{0,1}^{1} \\
      \nu_{1,1}^{1} \\
      \nu_{2,1}^{1}  \\
\vdots  \\
      \nu_{j,1}^{j}  \\
\vdots  \\
      \nu_{J,K}^{J} \\
    \end{array}
  \right)
\end{eqnarray}
\]&lt;/span&gt;
みたいな感じです。ここで&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{i}\)&lt;/span&gt;はi番目の方程式のj番目の変数のk次ラグにかかるパラメータを表しています。混合推定では、正規分布に従う観測値とこの事前分布が独立であるという仮定の下で観測方程式&lt;span class=&#34;math inline&#34;&gt;\(Y = XB + \epsilon\)&lt;/span&gt;と$ r = RB + $を以下のように組み合わせます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\left(
    \begin{array}{cccc}
      Y \\
      r 
    \end{array}
  \right) = \left(
    \begin{array}{cccc}
      X \\
      R 
    \end{array}
  \right)B + 
\left(
    \begin{array}{cccc}
      \epsilon \\
      \nu 
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E\left(
    \begin{array}{cccc}
      \epsilon \\
      \nu 
    \end{array}
  \right) = 0 \\
Var\left(
    \begin{array}{cccc}
      \epsilon \\
      \nu 
    \end{array}
  \right) = \left(
    \begin{array}{cccc}
      \sigma^{2}V &amp;amp; 0 \\
      0 &amp;amp; V_{0} \\
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;そして、このシステム体系を&lt;code&gt;GLS&lt;/code&gt;で推計するのです。こうすることで、事前分布を考慮した初期値の推定を行うことができます。&lt;code&gt;GLS&lt;/code&gt;推定量は以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \hat{B}_{M} = (\frac{1}{\sigma^{2}}X&amp;#39;V^{-1}X + R&amp;#39;V_{0}^{-1}R)^{-1}(\frac{1}{\sigma^{2}}X&amp;#39;V^{-1}y + R&amp;#39;V_{0}^{-1}r)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ご覧になればわかるように、&lt;code&gt;GLS&lt;/code&gt;推定量は上記2本の連立方程式(実際には行列なので何本もありますが)それぞれのGLS推定量を按分したような推定量になります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}\)&lt;/span&gt;は既知ではないので、いったん&lt;code&gt;OLS&lt;/code&gt;で推計しその推定量を用います。問題は&lt;span class=&#34;math inline&#34;&gt;\(V_{0}\)&lt;/span&gt;の置き方です。&lt;span class=&#34;math inline&#34;&gt;\(V_{0}\)&lt;/span&gt;の各要素を小さくとれば、事前分布に整合的な推定量が得られます（つまり、ランダムウォーク）。逆に、大きくとれば通常の&lt;code&gt;GLS&lt;/code&gt;推定量に近づいていきます。Doan, Litternam and Sims (1984)では以下のように分散を置いています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\displaystyle Var(\beta_{j,k}^{j}) &amp;amp;=&amp;amp; \frac{\pi_{5}・\pi_{1}}{k・exp(\pi_{4}w_{i}^{i})} \\
\displaystyle Var(\beta_{j,k}^{i}) &amp;amp;=&amp;amp; \frac{\pi_{5}・\pi_{2}・\sigma_{i}^{2}}{k・exp(\pi_{4}w_{j}^{i})・\sigma_{j}^{2}} \\
Var(c^{i}) &amp;amp;=&amp;amp; \pi_{5}・\pi_{3}・\sigma_{i}^{2}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{j}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目の方程式の自己回帰パラメータ、&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{i}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;番目方程式の他変数ラグ項にかかるパラメータ、&lt;span class=&#34;math inline&#34;&gt;\(c^{i}\)&lt;/span&gt;は定数項です。そして、&lt;span class=&#34;math inline&#34;&gt;\(\pi_{1},\pi_{2},\pi_{3},\pi_{4},\pi_{5}\)&lt;/span&gt;はハイパーパラメータと呼ばれるもので、カルマンフィルタにかけるパラメータの初期値の事前分布の分布の広がりを決定するパラメータとなっています。これらは初期値の推定を行う前に値を指定する必要があります。各ハイパーパラメータの具体的な特徴は以下の通りです。&lt;span class=&#34;math inline&#34;&gt;\(\pi_{1}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{j}\)&lt;/span&gt;の分散にのみ出現することから自己回帰係数に影響を与えるパラメータとなっています。具体的には、&lt;span class=&#34;math inline&#34;&gt;\(\pi_{1}\)&lt;/span&gt;が大きくなればなるほど自己回帰係数は事前分布から大きく離れた辺りを取りうることになります。&lt;span class=&#34;math inline&#34;&gt;\(\pi_{2}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\beta_{j,k}^{i}\)&lt;/span&gt;の分散にのみ出現することから他変数のラグ項に影響を与えるパラメータとなっています。こちらも値が大きくなればなるほど係数は事前分布から大きく離れた辺りを取りうることになります。&lt;span class=&#34;math inline&#34;&gt;\(\pi_{3}\)&lt;/span&gt;は定数項の事前分布に影響を与えるパラメータでこちらも考え方は同じです。&lt;span class=&#34;math inline&#34;&gt;\(\pi_{4}\)&lt;/span&gt;はラグ項の分散(つまり定数項以外)に影響を与えるパラメータで、値が大きくなるにつれ、初期値は事前分布に近づいていきます。最後に&lt;span class=&#34;math inline&#34;&gt;\(\pi_{5}\)&lt;/span&gt;ですが、こちらは全体にかかるパラメータで、値が大きくなるにつれ、初期値は事前分布から遠ざかります。&lt;/p&gt;
&lt;p&gt;　上式には、ハイパーパラメータ以外にもパラメータや変数が存在します。&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{i}, \sigma_{j}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(y_{j,t}, y_{i,t}\)&lt;/span&gt;をそれぞれ&lt;code&gt;AR(m)&lt;/code&gt;でフィッティングをかけた時の残差の標準偏差の推定値です。変数間のスケーリングの違いを考慮するために、&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{i}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{j}\)&lt;/span&gt;で割ったものを使用しています。本来ならば、&lt;code&gt;VAR&lt;/code&gt;の残差の標準偏差を使用すべきなのですが、推定する前にわかるわけもないので、それぞれ&lt;code&gt;AR(m)&lt;/code&gt;で推定をかけ、その標準偏差を使用しています((ランダムウォークが先験情報なので整合性は取れているような気がします))。&lt;span class=&#34;math inline&#34;&gt;\(w_{j}^{i}\)&lt;/span&gt;は完全に恣意的なパラメータで、DLSではrelative weightsと呼ばれているものです。i番目の方程式のj番目の変数のラグ項にかかるパラメータが０であるかどうかについて、分析者の先験情報を反映するためのパラメータです。分散の式を見ればわかるように、relative weightが大きくなれば分散は小さくなり、推定値は事前分布に近づいていきます。DLSでは、ほとんどの変数は&lt;span class=&#34;math inline&#34;&gt;\(w_{i}^{i}=0,w_{j}^{i}=1\)&lt;/span&gt;でよいと主張されています。つまり、自己ラグにかかる事前分布の分散に関しては確信をもってランダムウォークであるといえる一方、他変数ラグについては予測力向上に役立つもののあることを考え、値を１と置いているのです。一方、為替レートや株価はランダムウォーク色が強いということから大きい値を使用しています。最後に、&lt;span class=&#34;math inline&#34;&gt;\(Var(\beta_{j,k}^{j})\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(Var(\beta_{j,k}^{i})\)&lt;/span&gt;には分母にkがついています。つまり、ラグ次数kが大きくなればなるほど、その係数は０に近づいていくことを先験的情報として仮定していることになります((このおかげで&lt;code&gt;VAR&lt;/code&gt;の次数をこちらで指定する必要がなくなります。適当に大きい次数を指定しておけば、必要のない次数の大きいパラメータに関しては事前と値が０になるので。))。
　
　これらがいわゆる&lt;code&gt;Minnesota Prior&lt;/code&gt;の正体です((実はこれに加えて自己回帰パラメータの総和が１、他変数のラグ項にかかるパラメータの総和が０となる制約を課すのですが、話が複雑になりすぎるので今回は割愛しました))。初期値が事前分布に近づけば&lt;code&gt;BVAR&lt;/code&gt;はランダムウォークに近づきますし、離れるとUVARに近づきます。現実はその間となるのですが、なにを評価尺度としてハイパーパラメータの値を決めるかというと、それは当てはまりの良さということになります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ハイパーパラメータの決定方法とその評価尺度&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ハイパーパラメータの決定方法とその評価尺度&lt;/h3&gt;
&lt;p&gt;当てはまりの良さと言ってもいろいろありますが、DLSはその時点で観測可能なデータから予測できるk期先の予測値の当てはまりの良さを基準としています。DLSでは以下の予測誤差ベクトル&lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}_{t+k|t}\)&lt;/span&gt;のクロス積和を最小化することを目的関数として、ハイパーパラメータのチューニングを行っています。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\epsilon}_{t+k|t} = \hat{Y}_{t+k|t}-Y_{t+k}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(\hat{Y}_{t+k|t}\)&lt;/span&gt;はカルマンフィルタによるk期先の予測値です（kをいくつ先にすれば良いかは不明）。このクロス積は&lt;span class=&#34;math inline&#34;&gt;\(\hat{\epsilon}_{t+k|t}\hat{\epsilon}&amp;#39;_{t+k|t}\)&lt;/span&gt;であり、これをフィルタリングをかけるt=1期からサンプル期間であるt=T期まで計算していくので、最終的にはT個の予測誤差ベクトルを得ることになり、以下のようなこれらの総和を最小化します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \sum_{t=1}^{T-k}\hat{\epsilon}_{t+k|t}\hat{\epsilon}&amp;#39;_{t+k|t}
\]&lt;/span&gt;
より厳密にはこのクロス積和の対数値を最小化するハイパーパラメータの値をグリッドサーチやランダムサーチで探索していくことになります((ここは機械学習等で用いられているベイズ最適化を利用するとより高速に収束させることが可能かもです。))。
とまあ、&lt;code&gt;BVAR&lt;/code&gt;の推定方法はこんな感じです。他の&lt;code&gt;VAR&lt;/code&gt;と違い、恣意的であり、また推定方法が機械学習に近い点が特徴ではないかと思います。そもそも、&lt;code&gt;BVAR&lt;/code&gt;は予測に特化した&lt;code&gt;VAR&lt;/code&gt;ですから、他の&lt;code&gt;VAR&lt;/code&gt;とは別物と考える方が良いかもです。&lt;/p&gt;
&lt;p&gt;（追記　2019/4/29）&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;rでの実装&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Rでの実装&lt;/h2&gt;
&lt;p&gt;ここまでを&lt;code&gt;R&lt;/code&gt;で実装したいと思います。まずは、&lt;code&gt;vars&lt;/code&gt;パッケージに準備されているCanadaデータを使用して、通常の&lt;code&gt;VAR&lt;/code&gt;の推定をやってみます。これはカナダの1980～2000年の労働生産性、雇用、失業率、実質賃金をまとめたものになります。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(vars)

data(Canada)
Canada.var &amp;lt;- VAR(Canada,p=VARselect(Canada,lag.max = 4)$selection[1])
summary(Canada.var)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## VAR Estimation Results:
## ========================= 
## Endogenous variables: e, prod, rw, U 
## Deterministic variables: const 
## Sample size: 81 
## Log Likelihood: -150.609 
## Roots of the characteristic polynomial:
## 1.004 0.9283 0.9283 0.7437 0.7437 0.6043 0.6043 0.5355 0.5355 0.2258 0.2258 0.1607
## Call:
## VAR(y = Canada, p = VARselect(Canada, lag.max = 4)$selection[1])
## 
## 
## Estimation results for equation e: 
## ================================== 
## e = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const 
## 
##           Estimate Std. Error t value Pr(&amp;gt;|t|)    
## e.l1       1.75274    0.15082  11.622  &amp;lt; 2e-16 ***
## prod.l1    0.16962    0.06228   2.723 0.008204 ** 
## rw.l1     -0.08260    0.05277  -1.565 0.122180    
## U.l1       0.09952    0.19747   0.504 0.615915    
## e.l2      -1.18385    0.23517  -5.034 3.75e-06 ***
## prod.l2   -0.10574    0.09425  -1.122 0.265858    
## rw.l2     -0.02439    0.06957  -0.351 0.727032    
## U.l2      -0.05077    0.24534  -0.207 0.836667    
## e.l3       0.58725    0.16431   3.574 0.000652 ***
## prod.l3    0.01054    0.06384   0.165 0.869371    
## rw.l3      0.03824    0.05365   0.713 0.478450    
## U.l3       0.34139    0.20530   1.663 0.100938    
## const   -150.68737   61.00889  -2.470 0.016029 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## Residual standard error: 0.3399 on 68 degrees of freedom
## Multiple R-Squared: 0.9988,  Adjusted R-squared: 0.9985 
## F-statistic:  4554 on 12 and 68 DF,  p-value: &amp;lt; 2.2e-16 
## 
## 
## Estimation results for equation prod: 
## ===================================== 
## prod = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const 
## 
##           Estimate Std. Error t value Pr(&amp;gt;|t|)    
## e.l1      -0.14880    0.28913  -0.515   0.6085    
## prod.l1    1.14799    0.11940   9.615 2.65e-14 ***
## rw.l1      0.02359    0.10117   0.233   0.8163    
## U.l1      -0.65814    0.37857  -1.739   0.0866 .  
## e.l2      -0.18165    0.45083  -0.403   0.6883    
## prod.l2   -0.19627    0.18069  -1.086   0.2812    
## rw.l2     -0.20337    0.13337  -1.525   0.1319    
## U.l2       0.82237    0.47034   1.748   0.0849 .  
## e.l3       0.57495    0.31499   1.825   0.0723 .  
## prod.l3    0.04415    0.12239   0.361   0.7194    
## rw.l3      0.09337    0.10285   0.908   0.3672    
## U.l3       0.40078    0.39357   1.018   0.3121    
## const   -195.86985  116.95813  -1.675   0.0986 .  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## Residual standard error: 0.6515 on 68 degrees of freedom
## Multiple R-Squared:  0.98,   Adjusted R-squared: 0.9765 
## F-statistic: 277.5 on 12 and 68 DF,  p-value: &amp;lt; 2.2e-16 
## 
## 
## Estimation results for equation rw: 
## =================================== 
## rw = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const 
## 
##           Estimate Std. Error t value Pr(&amp;gt;|t|)    
## e.l1    -4.716e-01  3.373e-01  -1.398    0.167    
## prod.l1 -6.500e-02  1.393e-01  -0.467    0.642    
## rw.l1    9.091e-01  1.180e-01   7.702 7.63e-11 ***
## U.l1    -7.941e-04  4.417e-01  -0.002    0.999    
## e.l2     6.667e-01  5.260e-01   1.268    0.209    
## prod.l2 -2.164e-01  2.108e-01  -1.027    0.308    
## rw.l2   -1.457e-01  1.556e-01  -0.936    0.353    
## U.l2    -3.014e-01  5.487e-01  -0.549    0.585    
## e.l3    -1.289e-01  3.675e-01  -0.351    0.727    
## prod.l3  2.140e-01  1.428e-01   1.498    0.139    
## rw.l3    1.902e-01  1.200e-01   1.585    0.118    
## U.l3     1.506e-01  4.592e-01   0.328    0.744    
## const   -1.167e+01  1.365e+02  -0.086    0.932    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## Residual standard error: 0.7601 on 68 degrees of freedom
## Multiple R-Squared: 0.9989,  Adjusted R-squared: 0.9987 
## F-statistic:  5239 on 12 and 68 DF,  p-value: &amp;lt; 2.2e-16 
## 
## 
## Estimation results for equation U: 
## ================================== 
## U = e.l1 + prod.l1 + rw.l1 + U.l1 + e.l2 + prod.l2 + rw.l2 + U.l2 + e.l3 + prod.l3 + rw.l3 + U.l3 + const 
## 
##          Estimate Std. Error t value Pr(&amp;gt;|t|)    
## e.l1     -0.61773    0.12508  -4.939 5.39e-06 ***
## prod.l1  -0.09778    0.05165  -1.893 0.062614 .  
## rw.l1     0.01455    0.04377   0.332 0.740601    
## U.l1      0.65976    0.16378   4.028 0.000144 ***
## e.l2      0.51811    0.19504   2.656 0.009830 ** 
## prod.l2   0.08799    0.07817   1.126 0.264279    
## rw.l2     0.06993    0.05770   1.212 0.229700    
## U.l2     -0.08099    0.20348  -0.398 0.691865    
## e.l3     -0.03006    0.13627  -0.221 0.826069    
## prod.l3  -0.01092    0.05295  -0.206 0.837180    
## rw.l3    -0.03909    0.04450  -0.879 0.382733    
## U.l3      0.06684    0.17027   0.393 0.695858    
## const   114.36732   50.59802   2.260 0.027008 *  
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## 
## Residual standard error: 0.2819 on 68 degrees of freedom
## Multiple R-Squared: 0.9736,  Adjusted R-squared: 0.969 
## F-statistic: 209.2 on 12 and 68 DF,  p-value: &amp;lt; 2.2e-16 
## 
## 
## 
## Covariance matrix of residuals:
##             e     prod       rw        U
## e     0.11550 -0.03161 -0.03681 -0.07034
## prod -0.03161  0.42449  0.05589  0.01494
## rw   -0.03681  0.05589  0.57780  0.03660
## U    -0.07034  0.01494  0.03660  0.07945
## 
## Correlation matrix of residuals:
##            e     prod      rw        U
## e     1.0000 -0.14276 -0.1425 -0.73426
## prod -0.1428  1.00000  0.1129  0.08136
## rw   -0.1425  0.11286  1.0000  0.17084
## U    -0.7343  0.08136  0.1708  1.00000&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(predict(Canada.var,n.ahead=20,ci=0.95))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/r-1.png&#34; width=&#34;480&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;パッケージが整備されているので簡単に実行できました。上で見たようにやっていることは別々の&lt;code&gt;OLS&lt;/code&gt;を4本推定しているだけです。
次に&lt;code&gt;BVAR&lt;/code&gt;です。まず、カルマンフィルタと事前分布を定義します。カルマンフィルタは以前記事でご紹介したものと同じです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(seasonal)

kalmanfiter &amp;lt;- function(y,I,t,z,c=0,R=NA,Q=NA,d=0,S=NA,h=NA,a_int=NA,sig_int=NA){
  #-------------------------------------------------------------------
  # Implemention of Kalman filter
  #   y - observed variable
  #   I - the number of unobserved variable
  #   t - parameter of endogenous variable in state equation
  #   z - parameter of endogenous variable in observable equation
  #   c - constant in state equaion
  #   R - parameter of exogenous variable in state equation
  #   Q - var-cov matrix of exogenous variable in state equation
  #   d - constant in observable equaion
  #   S - parameter of exogenous variable in observable equation
  #   h - var-cov matrix of exogenous variable in observable equation
  #   a_int - initial value of endogenous variable
  #   sig_int - initial value of variance of endogenous variable
  #-------------------------------------------------------------------
  
  library(MASS)
    
  # 1.Define Variable
  if (class(y)!=&amp;quot;matrix&amp;quot;){
    y &amp;lt;- as.matrix(y)
  }
  N &amp;lt;- NROW(y) # sample size
  L &amp;lt;- NCOL(y) # the number of observable variable 
  a_pre &amp;lt;- array(0,dim = c(I,1,N)) # prediction of unobserved variable
  a_fil &amp;lt;- array(0,dim = c(I,1,N+1)) # filtered of unobserved variable
  sig_pre &amp;lt;- array(0,dim = c(I,I,N)) # prediction of var-cov mat. of unobserved variable
  sig_fil &amp;lt;- array(0,dim = c(I,I,N+1)) # filtered of var-cov mat. of unobserved variable
  y_pre &amp;lt;- array(0,dim = c(L,1,N)) # prediction of observed variable
  F_pre &amp;lt;- array(0,dim = c(L,L,N)) # auxiliary variable 
  k &amp;lt;- array(0,dim = c(I,L,N)) # kalman gain
  
  if (any(is.na(a_int))){
    a_int &amp;lt;- matrix(0,nrow = I,ncol = 1)
  }
  if (any(is.na(sig_int))){
    sig_int &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(R))){
    R &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if (any(is.na(Q))){
    Q &amp;lt;- diag(1,nrow = I,ncol = I)
  }
  if(any(is.na(S))){
    S &amp;lt;- matrix(1,nrow = L,ncol = L)
  }
  if (any(is.na(h))){
    H &amp;lt;- array(0,dim = c(L,L,N))
    for(i in 1:N){
      diag(H[,,i]) = 1
    }
  }else if (class(h)!=&amp;quot;array&amp;quot;){
    H &amp;lt;- array(h,dim = c(NROW(h),NCOL(h),N))
  }
  
  # fill infinite if observed data is NA
  for(i in 1:N){
    miss &amp;lt;- is.na(y[i,])
    diag(H[,,i])[miss] &amp;lt;- 1e+32
  }
  y[is.na(y)] &amp;lt;- 0
  
  # 2.Set Initial Value
  a_fil[,,1] &amp;lt;- a_int
  sig_fil[,,1] &amp;lt;- sig_int
  
  # 3.Implement Kalman filter
  for (i in 1:N){
    if(class(z)==&amp;quot;array&amp;quot;){
      Z &amp;lt;- z[,,i]
    }else{
      Z &amp;lt;- z
    }
    a_pre[,,i] &amp;lt;- t%*%a_fil[,,i] + c
    sig_pre[,,i] &amp;lt;- t%*%sig_fil[,,i]%*%t(t) + R%*%Q%*%t(R)
    y_pre[,,i] &amp;lt;- Z%*%a_pre[,,i] + d
    F_pre[,,i] &amp;lt;- Z%*%sig_pre[,,i]%*%t(Z) + S%*%H[,,i]%*%t(S)
    k[,,i] &amp;lt;- sig_pre[,,i]%*%t(Z)%*%ginv(F_pre[,,i])
    a_fil[,,i+1] &amp;lt;- a_pre[,,i] + k[,,i]%*%(y[i,]-y_pre[,,i])
    sig_fil[,,i+1] &amp;lt;- sig_pre[,,i] - k[,,i]%*%F_pre[,,i]%*%t(k[,,i])
  }
  
  # 4.Aggregate results
  result &amp;lt;- list(a_pre,a_fil,sig_pre,sig_fil,y_pre,k,t,z)
  names(result) &amp;lt;- c(&amp;quot;state prediction&amp;quot;, &amp;quot;state filtered&amp;quot;, &amp;quot;state var prediction&amp;quot;, 
                     &amp;quot;state var filtered&amp;quot;, &amp;quot;observable prediction&amp;quot;, &amp;quot;kalman gain&amp;quot;,
                     &amp;quot;parameter of state eq&amp;quot;, &amp;quot;parameter of observable eq&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;事前分布を計算する関数を定義します。まず、引数を計算しやすい形に変換しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  # set pi
  pi &amp;lt;- abs(rnorm(5)) # hyperparameter
  x &amp;lt;- Canada # dataset
  order &amp;lt;- 4 # order
  
  library(MASS)
    
  # 1. Process row data
  for (i in 1:5){
    eval(parse(text = paste0(&amp;quot;pi&amp;quot;,i,&amp;quot;=pi[[&amp;quot;,i,&amp;quot;]]&amp;quot;)))
  }
  if (class(x)!=&amp;quot;matrix&amp;quot;){
    x &amp;lt;- as.matrix(x)
  }
  dependent &amp;lt;- as.matrix(x[(order+1):NROW(x),1])
  for (i in 2:NCOL(x)){
    dependent &amp;lt;- rbind(dependent,as.matrix(x[(order+1):NROW(x),i]))
  }
  explanatory &amp;lt;- embed(cbind(1,x),order)[,-((order*NCOL(x)+order-NCOL(x)):(order*NCOL(x)+order))]
  explanatory &amp;lt;- diag(1,nrow = NCOL(x),ncol = NCOL(x))%x%as.matrix(explanatory)
  npara &amp;lt;- NCOL(explanatory) # the number of parameters/ ncol(x)*(ncol(x)*order + 1(const))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;dependentは非説明変数、explanatoryは説明変数です。ちょうど1式を再現した形になります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
   \left(
    \begin{array}{cccc}
      Y_{1} \\
      Y_{2} \\
      \vdots  \\
      Y_{J} \\
    \end{array}
  \right) = 
 \left(
    \begin{array}{cccc}
      X_{1} &amp;amp; 0 &amp;amp; \ldots &amp;amp; 0 \\
      0 &amp;amp; X_{2} &amp;amp; \ldots &amp;amp; 0 \\
      \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
      0 &amp;amp; 0 &amp;amp; \ldots &amp;amp; X_{J}
    \end{array}
  \right) 
  \left(
    \begin{array}{cccc}
      A_{1} \\
      A_{2} \\
      \vdots  \\
      A_{J} \\
    \end{array}
  \right) +
  \left(
    \begin{array}{cccc}
      U_{1} \\
      U_{2} \\
      \vdots  \\
      U_{J} \\
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 2. Make Prior matrixes
  r &amp;lt;- as.matrix(numeric(npara)) # a part of dependent variable in mixed estimation
  iter1 &amp;lt;- numeric(NCOL(x))
  n &amp;lt;- 2
  for (i in 1:length(iter1)){
    iter1[i] &amp;lt;- n
    n &amp;lt;- n + order*NCOL(x) + 1
  }
  for (i in iter1){
    r[i] &amp;lt;- 1
  }
  R &amp;lt;- diag(1,nrow = npara,ncol = npara) # a part of explanatory variables in mixed estimation
  V &amp;lt;- diag(1,nrow = NROW(explanatory),ncol = NROW(explanatory)) # coefficient matrix of var-cov matrix in mixed estimation
  
  V0 &amp;lt;- matrix(0,nrow = npara,ncol = npara) # Prior for var matrix
  
  # PRIOR FOR VAR OF CONSTANT
  sigi &amp;lt;- numeric(NCOL(x)) # var of AR(m)
  for (i in 1:NCOL(x)){
    AR &amp;lt;- ar(x[,i],order.max = order)
    sigi[i] &amp;lt;- AR$var.pred
  }
  sigma_AR &amp;lt;- diag(sigi,nrow = NCOL(x),ncol = NCOL(x))
  iter2 &amp;lt;- seq(1,npara,(NCOL(x)*order+1))
  n &amp;lt;- 1
  for (i in iter2){
    V0[i,i] &amp;lt;- pi5*pi3*sigi[n]
    n &amp;lt;- n + 1
  }
  
  # PRIOR FOR VAR OF AUTOREGRESSIVE PARAMETER
  wi &amp;lt;- 0 # prior weight on s.d. of autoregressive parameter
  k &amp;lt;- 0 # decay parameter
  iter3 &amp;lt;- numeric((NCOL(x)*order))
  n &amp;lt;- 2;iter3[1] &amp;lt;- n
  for (i in 2:length(iter3)){
    if ((i-1)%%order == 0){
      n &amp;lt;- n + NCOL(x) + 2 # const
      iter3[i] &amp;lt;- n 
    }else{
      n &amp;lt;- n + NCOL(x)
      iter3[i] &amp;lt;- n
    }
  }
  for (i in iter3) {
    k &amp;lt;- k + 1
    V0[i,i] &amp;lt;- pi5*pi1/(k*exp(pi4*wi))
    if (k == order){
      k = 0
    }
  }
  
  # PRIOR FOR VAR OF DISTRIBUTED LAG PARAMETER
  wj &amp;lt;- 1 # prior weight on s.d. of distributed lag parameter
  k &amp;lt;- 1 # decay parameter
  count &amp;lt;- 0
  n &amp;lt;- 1
  ndis &amp;lt;- npara - NCOL(x) - length(iter3) # the number of distibuted lag in each equation
  
  l &amp;lt;- rep(1:NCOL(x),length=npara)
  temp &amp;lt;- numeric(length(iter3))
  j &amp;lt;- 1;temp[1] &amp;lt;- j
  for (i in 2:length(temp)){
    if ((i-1)%%order == 0){
      j &amp;lt;- j + NCOL(x) + 1 # const
      temp[i] &amp;lt;- j 
    }else{
      j &amp;lt;- j + NCOL(x)
      temp[i] &amp;lt;- j
    }
  }
  l &amp;lt;- l[-c(temp)]; l &amp;lt;- l[1:(ndis)]
  
  iter4 &amp;lt;- 1:npara; iter4 &amp;lt;- iter4[-c(iter2,iter3)]
  for (i in iter4){
    count &amp;lt;- count + 1
    V0[i,i] &amp;lt;- pi5*pi2*sigi[n]/(k*exp(pi4*wj)*sigi[l[count]])
    if(count%%(order-1) == 0){
      k &amp;lt;- k + 1
    }
    if (count%%(ndis/NCOL(x)) == 0){
      n = n + 1
      k = 1
    }
  }

  # 3. Estimate OLS
  beta &amp;lt;- ginv((t(explanatory)%*%explanatory))%*%t(explanatory)%*%dependent
  sig &amp;lt;- as.numeric((NROW(explanatory)-NCOL(explanatory))^(-1)*
                      t(dependent-explanatory%*%beta)%*%ginv(V)%*%
                      (dependent-explanatory%*%beta))
  
  # 4. Implement Mixed Estimation for initial values
  beta_M &amp;lt;- ginv((1/sig)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)%*%
    ((sig)^(-1)*t(explanatory)%*%ginv(V)%*%dependent + t(R)%*%ginv(V0)%*%r)
  sig_M &amp;lt;- sig*ginv((sig)^(-1)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)
  
  # for kalman filter
  observable &amp;lt;- as.matrix(x[(order+1):NROW(x),]) # dependent variable
  coefficient &amp;lt;- array(0,dim = c(nrow = NCOL(x),ncol = npara,(NROW(x)-order))) # explanatory variable
  for(i in 1:(NROW(x)-order)){
    for (k in 1:NCOL(x)){
      coefficient[k,(1+(k-1)*npara/NCOL(x)):(k*npara/NCOL(x)),i] &amp;lt;- explanatory[i,1:(npara/NCOL(x))]
    }
  }
  
  result &amp;lt;- list(observable, coefficient, beta_M, sig_M, sigma_AR, V0, order, npara)
  names(result) &amp;lt;- c(&amp;quot;observable&amp;quot;, &amp;quot;coefficient&amp;quot;,&amp;quot;beta_M&amp;quot;, &amp;quot;sig_M&amp;quot;,&amp;quot;sigma_AR&amp;quot;, &amp;quot;V0&amp;quot;,&amp;quot;order&amp;quot;,&amp;quot;npara&amp;quot;)
  
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MPrior &amp;lt;- function(x,order,pi){
  #-------------------------------------------------------------------
  # Make Minnesota Prior
  #   x - dataset
  #   order - max lag of autoregression
  #   pi - a set of hyper-parameter of prior matrix
  #-------------------------------------------------------------------

  library(MASS)
    
  # 1. Process row data
  for (i in 1:5){
    eval(parse(text = paste0(&amp;quot;pi&amp;quot;,i,&amp;quot;=pi[[&amp;quot;,i,&amp;quot;]]&amp;quot;)))
  }
  if (class(x)!=&amp;quot;matrix&amp;quot;){
    x &amp;lt;- as.matrix(x)
  }
  dependent &amp;lt;- as.matrix(x[1:(NROW(x)-order),1])
  for (i in 2:NCOL(x)){
    dependent &amp;lt;- rbind(dependent,as.matrix(x[(order+1):NROW(x),i]))
  }
  explanatory &amp;lt;- cbind(1,embed(x,order)[-1,])
  explanatory &amp;lt;- diag(1,nrow = NCOL(x),ncol = NCOL(x))%x%as.matrix(explanatory)
  npara &amp;lt;- NCOL(explanatory) # the number of parameters/ ncol(x)*(ncol(x)*order + 1(const))
  
  # 2. Make Prior matrixes
  r &amp;lt;- as.matrix(numeric(npara)) # a part of dependent variable in mixed estimation
  iter1 &amp;lt;- numeric(NCOL(x))
  n &amp;lt;- 2
  for (i in 1:length(iter1)){
    iter1[i] &amp;lt;- n
    n &amp;lt;- n + order*NCOL(x) + 1
  }
  for (i in iter1){
    r[i] &amp;lt;- 1
  }
  R &amp;lt;- diag(1,nrow = npara,ncol = npara) # a part of explanatory variables in mixed estimation
  V &amp;lt;- diag(1,nrow = NROW(explanatory),ncol = NROW(explanatory)) # coefficient matrix of var-cov matrix in mixed estimation
  
  V0 &amp;lt;- matrix(0,nrow = npara,ncol = npara) # Prior for var matrix
  
  # PRIOR FOR VAR OF CONSTANT
  sigi &amp;lt;- numeric(NCOL(x)) # var of AR(m)
  for (i in 1:NCOL(x)){
    AR &amp;lt;- ar(x[,i],order.max = order)
    sigi[i] &amp;lt;- AR$var.pred
  }
  sigma_AR &amp;lt;- diag(sigi,nrow = NCOL(x),ncol = NCOL(x))
  iter2 &amp;lt;- seq(1,npara,(NCOL(x)*order+1))
  n &amp;lt;- 1
  for (i in iter2){
    V0[i,i] &amp;lt;- pi5*pi3*sigi[n]
    n &amp;lt;- n + 1
  }
  
  # PRIOR FOR VAR OF AUTOREGRESSIVE PARAMETER
  wi &amp;lt;- 0 # prior weight on s.d. of autoregressive parameter
  k &amp;lt;- 0 # decay parameter
  iter3 &amp;lt;- numeric((NCOL(x)*order))
  n &amp;lt;- 2;iter3[1] &amp;lt;- n
  for (i in 2:length(iter3)){
    if ((i-1)%%order == 0){
      n &amp;lt;- n + NCOL(x) + 2 # const
      iter3[i] &amp;lt;- n 
    }else{
      n &amp;lt;- n + NCOL(x)
      iter3[i] &amp;lt;- n
    }
  }
  for (i in iter3) {
    k &amp;lt;- k + 1
    V0[i,i] &amp;lt;- pi5*pi1/(k*exp(pi4*wi))
    if (k == order){
      k = 0
    }
  }
  
  # PRIOR FOR VAR OF DISTRIBUTED LAG PARAMETER
  wj &amp;lt;- 1 # prior weight on s.d. of distributed lag parameter
  k &amp;lt;- 1 # decay parameter
  count &amp;lt;- 0
  n &amp;lt;- 1
  ndis &amp;lt;- npara - NCOL(x) - length(iter3) # the number of distibuted lag in each equation
  
  l &amp;lt;- rep(1:NCOL(x),length=npara)
  temp &amp;lt;- numeric(length(iter3))
  j &amp;lt;- 1;temp[1] &amp;lt;- j
  for (i in 2:length(temp)){
    if ((i-1)%%order == 0){
      j &amp;lt;- j + NCOL(x) + 1 # const
      temp[i] &amp;lt;- j 
    }else{
      j &amp;lt;- j + NCOL(x)
      temp[i] &amp;lt;- j
    }
  }
  l &amp;lt;- l[-c(temp)]; l &amp;lt;- l[1:(ndis)]
  
  iter4 &amp;lt;- 1:npara; iter4 &amp;lt;- iter4[-c(iter2,iter3)]
  for (i in iter4){
    count &amp;lt;- count + 1
    V0[i,i] &amp;lt;- pi5*pi2*sigi[n]/(k*exp(pi4*wj)*sigi[l[count]])
    if(count%%(order-1) == 0){
      k &amp;lt;- k + 1
    }
    if (count%%(ndis/NCOL(x)) == 0){
      n = n + 1
      k = 1
    }
  }

  # 3. Estimate OLS
  beta &amp;lt;- ginv((t(explanatory)%*%explanatory))%*%t(explanatory)%*%dependent
  sig &amp;lt;- as.numeric((NROW(explanatory)-NCOL(explanatory))^(-1)*
                      t(dependent-explanatory%*%beta)%*%ginv(V)%*%
                      (dependent-explanatory%*%beta))
  
  # 4. Implement Mixed Estimation for initial values
  beta_M &amp;lt;- ginv((1/sig)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)%*%
    ((sig)^(-1)*t(explanatory)%*%ginv(V)%*%dependent + t(R)%*%ginv(V0)%*%r)
  sig_M &amp;lt;- sig*ginv((sig)^(-1)*t(explanatory)%*%ginv(V)%*%explanatory + t(R)%*%ginv(V0)%*%R)
  
  # for kalman filter
  observable &amp;lt;- as.matrix(x[(order+1):NROW(x),]) # dependent variable
  coefficient &amp;lt;- array(0,dim = c(nrow = NCOL(x),ncol = npara,(NROW(x)-order))) # explanatory variable
  for(i in 1:(NROW(x)-order)){
    for (k in 1:NCOL(x)){
      coefficient[k,(1+(k-1)*npara/NCOL(x)):(k*npara/NCOL(x)),i] &amp;lt;- explanatory[i,1:(npara/NCOL(x))]
    }
  }
  
  result &amp;lt;- list(observable, coefficient, beta_M, sig_M, sigma_AR, V0, order, npara)
  names(result) &amp;lt;- c(&amp;quot;observable&amp;quot;, &amp;quot;coefficient&amp;quot;,&amp;quot;beta_M&amp;quot;, &amp;quot;sig_M&amp;quot;,&amp;quot;sigma_AR&amp;quot;, &amp;quot;V0&amp;quot;,&amp;quot;order&amp;quot;,&amp;quot;npara&amp;quot;)
  
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BVAR &amp;lt;- function(m){
  #-------------------------------------------------------------------
  # Estimation of Bayesian Vector Auto-Regression
  #   m - dataset
  #-------------------------------------------------------------------
  
  # 1. Process data
  observable &amp;lt;- m$observable
  coefficient &amp;lt;- m$coefficient
  beta_M &amp;lt;- m$beta_M
  sig_M &amp;lt;- m$sig_M
  sigma_AR &amp;lt;- m$sigma_AR
  V0 &amp;lt;- m$V0
  order &amp;lt;- m$order
  npara &amp;lt;- m$npara
  
  # 2. Run Kalmanfilter
  results &amp;lt;- kalmanfiter(observable,npara,diag(1,nrow = npara,ncol = npara),
                         coefficient,0,NA,V0,0,NA,sigma_AR,beta_M,sig_M)
  return(results)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;m &amp;lt;- MPrior(Canada,3,abs(rnorm(5)))
results &amp;lt;- BVAR(m)

BVAR_Prediction &amp;lt;- function(results,horizon){
  
  state_pred &amp;lt;- results$`state prediction`
  state_fil &amp;lt;- results$`state filtered`
  obs_pred &amp;lt;- results$`observable prediction`
  A &amp;lt;- results$`parameter of state eq`
    
  nparaall &amp;lt;- dim(state_pred)[1]
  num &amp;lt;- dim(obs_pred)[1]
  npara &amp;lt;- order*num+1
  order &amp;lt;- (nparaall-num)/(num^2)
  samplesize &amp;lt;- dim(state_pred)[3]
  
  y &amp;lt;- matrix(0,horizon,num)
  B &amp;lt;- matrix(0,horizon,nparaall)
  x &amp;lt;- results$`parameter of observable eq`[1,1:npara,samplesize]
  
  B[1,] &amp;lt;- state_fil[,1,samplesize]
  
  for (i in 1:(horizon-1)) {
    B[i+1,] &amp;lt;- A%*%B[i,]
    y[i+1,] &amp;lt;- B[i+1,]%*%(diag(1,num)%x%x)
    x &amp;lt;- c(1,y[i+1,],x[-c(1,seq(from=(npara-order),to=npara))])
  }
  
  result &amp;lt;- list(B,y)
  names(result) &amp;lt;- c(&amp;quot;B&amp;quot;,&amp;quot;y&amp;quot;)
  
  return(result)
  
}

pre &amp;lt;- BVAR_Prediction(results,20)

BVAR_OPT &amp;lt;- function(x){
  int_pi &amp;lt;- rnorm(5)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Sims, Christopher A, 1980. “Macroeconomics and Reality,” Econometrica, Econometric Society, vol. 48(1), pages 1-48, January.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Gianonne et. al. (2008)のマルチファクターモデルで四半期GDPを予想してみた</title>
      <link>/post/post6/</link>
      <pubDate>Mon, 16 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/post/post6/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gianonne-et.-al.-2008版マルチファクターモデル&#34;&gt;1. Gianonne et. al. (2008)版マルチファクターモデル&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rで実装する&#34;&gt;2. Rで実装する&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。
前回、統計ダッシュボードからAPI接続で統計データを落とすという記事を投稿しました。
今回はそのデータを、Gianonne et. al. (2008)のマルチファクターモデルにかけ、四半期GDPの予測を行いたいと思います。&lt;/p&gt;
&lt;div id=&#34;gianonne-et.-al.-2008版マルチファクターモデル&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Gianonne et. al. (2008)版マルチファクターモデル&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dept.ku.edu/~empirics/Courses/Econ844/papers/Nowcasting%20GDP.pdf&#34;&gt;元論文&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;前回の投稿でも書きましたが、この論文はGiannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標を2つのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しています。
まず、このモデルについてご紹介します。このモデルでは2段階推計を行います。まず主成分分析により経済統計を統計間の相関が0となるファクターへ変換します（&lt;a href=&#34;https://datachemeng.com/principalcomponentanalysis/&#34;&gt;参考&lt;/a&gt;）。そして、その後の状態空間モデルでの推計で必要になるパラメータを&lt;code&gt;OLS&lt;/code&gt;推計し、そのパラメータを使用してカルマンフィルタ＆カルマンスムーザーを回し、ファクターを推計しています。では、具体的な説明に移ります。
統計データを&lt;span class=&#34;math inline&#34;&gt;\(x_{i,t|v_j}\)&lt;/span&gt;と定義します。ここで、&lt;span class=&#34;math inline&#34;&gt;\(i=1,...,n\)&lt;/span&gt;は経済統計を表し（つまり&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;が全統計数）、&lt;span class=&#34;math inline&#34;&gt;\(t=1,...,T_{iv_j}\)&lt;/span&gt;は統計&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;のサンプル期間の時点を表しています（つまり、&lt;span class=&#34;math inline&#34;&gt;\(T_{iv_j}\)&lt;/span&gt;は統計&lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;のその時点での最新データ日付を表す）。また、&lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt;はある時点&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;（2005年など）で得られる情報集合（vintage）を表しています。統計データ&lt;span class=&#34;math inline&#34;&gt;\(x_{i,t|v_j}\)&lt;/span&gt;は以下のようにファクター&lt;span class=&#34;math inline&#34;&gt;\(f_{r,t}\)&lt;/span&gt;の線形結合で表すことができます（ここで&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;はファクターの数を表す）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_{i,t|v\_j} = \mu_i + \lambda_{i1}f_{1,t} + ... + \lambda_{ir}f_{r,t} + \xi_{i,t|v_j} \tag{1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mu\_i\)&lt;/span&gt;は定数項、&lt;span class=&#34;math inline&#34;&gt;\(\lambda\_{ir}\)&lt;/span&gt;はファクターローディング、&lt;span class=&#34;math inline&#34;&gt;\(\xi\_{i,t|v\_j}\)&lt;/span&gt;はホワイトノイズの誤差項を表しています。これを行列形式で書くと以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_{t|v_j}  = \mu + \Lambda F_t + \xi_{t|v_j} = \mu + \chi_t + \xi_{t|v_j} \tag{2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(x_{t|v_j} = (x_{1,t|v_j}, ..., x_{n,t|v_j} )^{\mathrm{T}}\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\xi_{t|v_j}=(\xi_{1,t|v_j}, ..., \xi_{n,t|v_j})^{\mathrm{T}}\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(F_t = (f_{1,t}, ..., f_{r,t})^{\mathrm{T}}\)&lt;/span&gt;であり、&lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;は各要素が$ _{ij}&lt;span class=&#34;math inline&#34;&gt;\(の\)&lt;/span&gt;nr&lt;span class=&#34;math inline&#34;&gt;\(行列のファクターローディングを表しています。また、\)&lt;/span&gt;&lt;em&gt;t = F_t&lt;span class=&#34;math inline&#34;&gt;\(です。よって、ファクター\)&lt;/span&gt; F_t&lt;span class=&#34;math inline&#34;&gt;\(を推定するためには、データ\)&lt;/span&gt;x&lt;/em&gt;{i,t|v_j}$を以下のように基準化したうえで、分散共分散行列を計算し、その固有値問題を解けばよいという事になります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle z_{it} = \frac{1}{\hat{\sigma}_i}(x_{it} - \hat{\mu}_{it}) \tag{3}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \hat{\mu}_{it} = 1/T \sum_{t=1}^T x_{it}\)&lt;/span&gt;であり、&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}_i = \sqrt{1/T \sum_{t=1}^T (x_{it}-\hat{\mu_{it}})^2}\)&lt;/span&gt;です（ここで&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;はサンプル期間）。分散共分散行列&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;を以下のように定義します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle S = \frac{1}{T} \sum_{t=1}^T z_t z_t^{\mathrm{T}} \tag{4}
\]&lt;/span&gt;
次に、&lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;のうち、固有値を大きい順に&lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;個取り出し、それを要素にした$ r r&lt;span class=&#34;math inline&#34;&gt;\(対角行列を\)&lt;/span&gt; D&lt;span class=&#34;math inline&#34;&gt;\(、それに対応する固有ベクトルを\)&lt;/span&gt;n r&lt;span class=&#34;math inline&#34;&gt;\(行列にしたものを\)&lt;/span&gt; V&lt;span class=&#34;math inline&#34;&gt;\(と定義します。ファクター\)&lt;/span&gt; _t$は以下のように推計できます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{F}_t = V^{\mathrm{T}} z_t \tag{5}
\]&lt;/span&gt;
ファクターローディング&lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;と誤差項の共分散行列&lt;span class=&#34;math inline&#34;&gt;\(\Psi = \mathop{\mathbb{E}} [\xi_t\xi^{\mathrm{T}}_t]\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(\tilde{F}_t\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(z_t\)&lt;/span&gt;に回帰することで推計します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \hat{\Lambda} = \sum_{t=1}^T z_t \tilde{F}^{\mathrm{T}}_t (\sum_{t=1}^T\tilde{F}_t\tilde{F}^{\mathrm{T}}_t)^{-1} = V \tag{6}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\Psi} = diag(S - VDV) \tag{7}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;注意して頂きたいのは、ここで推計した&lt;span class=&#34;math inline&#34;&gt;\(\tilde{F}_t\)&lt;/span&gt;は、以下の状態空間モデルでの推計に必要なパラメータを計算するための一時的な推計値であるという事です（２段階推計の１段階目という事）。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
x_{t|v_j}  = \mu + \Lambda F\_t + \xi_{t|v_j} = \mu + \chi_t + \xi_{t|v_j} \tag{2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
F\_t = AF\_{t-1} + u\_t \tag{8}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(u_t\)&lt;/span&gt;は平均0、分散&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;のホワイトノイズです。再掲している(2)式が観測方程式、(8)式が遷移方程式となっています。推定すべきパラメータは&lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt;、&lt;span class=&#34;math inline&#34;&gt;\(\Psi\)&lt;/span&gt;以外に&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;があります（&lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt;としています）。&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;は主成分分析により計算した&lt;span class=&#34;math inline&#34;&gt;\(\tilde{F}_t\)&lt;/span&gt;を&lt;code&gt;VAR(1)&lt;/code&gt;にかけることで推定します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{A} = \sum_{t=2}^T\tilde{F}_t\tilde{F}_{t-1}^{\mathrm{T}} (\sum_{t=2}^T\tilde{F}_{t-1}\tilde{F}_{t-1}^{\mathrm{T}})^{-1} \tag{9}
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;は今推計した&lt;code&gt;VAR(1)&lt;/code&gt;の誤差項の共分散行列から計算します。これで必要なパラメータの推定が終わりました。次にカルマンフィルタを回します。カルマンフィルタに関しては&lt;a href=&#34;https://qiita.com/MoriKen/items/0c80ef75749977767b43&#34;&gt;こちら&lt;/a&gt;を参考にしてください。わかりやすいです。これで最終的に&lt;span class=&#34;math inline&#34;&gt;\(\hat{F}_{t|v_j}\)&lt;/span&gt;の推計ができるわけです。
GDPがこれらのファクターで説明可能であり（つまり固有の変動がない）、GDPと月次経済指標がjointly normalであれば以下のような単純なOLS推計でGDPを予測することができます。もちろん月次経済指標の方が早く公表されるので、内生性の問題はないと考えられます。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{y}_{3k|v_j} = \alpha + \beta^{\mathrm{T}} \hat{F}_{3k|v_j} \tag{10}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(3k\)&lt;/span&gt;は四半期の最終月を示しています（3月、6月など）&lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_{3k|v_j}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;時点で得られる情報集合&lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt;での四半期GDPを表しており、&lt;span class=&#34;math inline&#34;&gt;\(\hat{F}_{3k|v_j}\)&lt;/span&gt;はその時点で推定したファクターを表しています（四半期最終月の値だけを使用している点に注意）。これで推計方法の説明は終わりです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rで実装する&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Rで実装する&lt;/h2&gt;
&lt;p&gt;では実装します。前回記事で得られたデータ（dataset）が読み込まれている状態からスタートします。まず、主成分分析でファクターを計算します。なお、前回の記事で3ファクターの累積寄与度が80%を超えたため、今回もファクター数は3にしています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#------------------------
# Giannone et. al. 2008 
#------------------------

library(xts)
library(MASS)
library(tidyverse)

# 主成分分析でファクターを計算
f &amp;lt;- 3 # ファクター数を定義
a &amp;lt;- which(dataset1$publication == &amp;quot;2012-04-01&amp;quot;) # サンプル開始期間を2012年に設定。
dataset2 &amp;lt;- dataset1[a:nrow(dataset1),]
rownames(dataset2) &amp;lt;- dataset2$publication
dataset2 &amp;lt;- dataset2[,-2]
z &amp;lt;- scale(dataset2) # zは基準化されたサンプルデータ
for (i in 1:nrow(z)){
  eval(parse(text = paste(&amp;quot;S_i &amp;lt;- z[i,]%*%t(z[i,])&amp;quot;,sep = &amp;quot;&amp;quot;)))
  if (i==1){
    S &amp;lt;- S_i
  }else{
    S &amp;lt;- S + S_i
  }
}
S &amp;lt;- (1/nrow(z))*S # 分散共分散行列を計算 (4)式
gamma &amp;lt;- eigen(S) 
D &amp;lt;- diag(gamma$values[1:f])
V &amp;lt;- gamma$vectors[,1:f]
F_t &amp;lt;- matrix(0,nrow(z),f)
for (i in 1:nrow(z)){
  eval(parse(text = paste(&amp;quot;F_t[&amp;quot;,i,&amp;quot;,]&amp;lt;- z[&amp;quot;,i,&amp;quot;,]%*%V&amp;quot;,sep = &amp;quot;&amp;quot;))) # (5)式を実行
}
F_t.xts &amp;lt;- xts(F_t,order.by = as.Date(row.names(z)))
plot.zoo(F_t.xts,col = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;yellow&amp;quot;,&amp;quot;purple&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lambda_hat &amp;lt;- V
psi &amp;lt;- diag(S-V%*%D%*%t(V)) # (7)式
R &amp;lt;- diag(diag(cov(z-z%*%V%*%t(V)))) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;推計したファクター&lt;span class=&#34;math inline&#34;&gt;\(\tilde{F}\_t\)&lt;/span&gt;の時系列プロットは以下のようになり、前回&lt;code&gt;princomp&lt;/code&gt;関数で計算したファクターと完全一致します（じゃあ&lt;code&gt;princomp&lt;/code&gt;でいいやんと思われるかもしれませんが実装しないと勉強になりませんので）。&lt;/p&gt;
&lt;p&gt;次に、&lt;code&gt;VAR(1)&lt;/code&gt;を推計し、パラメータを取り出します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# VAR(1)モデルを推計
a &amp;lt;- matrix(0,f,f)
b &amp;lt;- matrix(0,f,f)
for(t in 2:nrow(z)){
  a &amp;lt;- a + F_t[t,]%*%t(F_t[t-1,])
  b &amp;lt;- b + F_t[t-1,]%*%t(F_t[t-1,])
}
b_inv &amp;lt;- solve(b)
A_hat &amp;lt;- a%*%b_inv # (9)式

e &amp;lt;- numeric(f)
for (t in 2:nrow(F_t)){
  e &amp;lt;- e + F_t[t,]-F_t[t-1,]%*%A_hat
}
H &amp;lt;- t(e)%*%e
Q &amp;lt;- diag(1,f,f)
Q[1:f,1:f] &amp;lt;- H&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;VAR(1)&lt;/code&gt;に関しても&lt;code&gt;var&lt;/code&gt;関数とパラメータの数値が一致することを確認済みです。いよいよカルマンフィルタを実行します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# カルマンフィルタを実行
RR &amp;lt;- array(0,dim = c(ncol(z),ncol(z),nrow(z))) # RRは観測値の分散行列（相関はないと仮定）
for(i in 1:nrow(z)){
  miss &amp;lt;- is.na(z[i,])
  R_temp &amp;lt;- diag(R)
  R_temp[miss] &amp;lt;- 1e+32 # 欠損値の分散は無限大にする
  RR[,,i] &amp;lt;- diag(R_temp)
}
zz &amp;lt;- z; zz[is.na(z)] &amp;lt;- 0 # 欠損値（NA）に0を代入（計算結果にはほとんど影響しない）。
a_t &amp;lt;- matrix(0,nrow(zz),f) # a_tは状態変数の予測値
a_tt &amp;lt;- matrix(0,nrow(zz),f) # a_ttは状態変数の更新後の値
a_tt[1,] &amp;lt;- F_t[1,] # 状態変数の初期値には主成分分析で推計したファクターを使用
sigma_t &amp;lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の予測値
sigma_tt &amp;lt;- array(0,dim = c(f,f,nrow(zz))) # sigma_tは状態変数の分散の更新値
p &amp;lt;- ginv(diag(nrow(kronecker(A_hat,A_hat)))-kronecker(A_hat,A_hat))
sigma_tt[,,1] &amp;lt;- matrix(p,3,3) # 状態変数の分散の初期値はVAR(1)の推計値から計算
y_t &amp;lt;- matrix(0,nrow(zz),ncol(zz)) # y_tは観測値の予測値
K_t &amp;lt;- array(0,dim = c(f,ncol(zz),nrow(zz))) # K_tはカルマンゲイン
data.m &amp;lt;- as.matrix(dataset2)
# カルマンフィルタを実行
for (t in 2:nrow(zz)){
  a_t[t,] &amp;lt;- A_hat%*%a_tt[t-1,]
  sigma_t[,,t] &amp;lt;- A_hat%*%sigma_tt[,,t-1]%*%t(A_hat) + Q
  y_t[t,] &amp;lt;- as.vector(V%*%a_t[t,])
  S_t &amp;lt;- V%*%sigma_tt[,,t-1]%*%t(V)+RR[,,t]
  GG &amp;lt;- t(V)%*%diag(1/diag(RR[,,t]))%*%V
  Sinv &amp;lt;- diag(1/diag(RR[,,t])) - diag(1/diag(RR[,,t]))%*%V%*%ginv(diag(nrow(A_hat))+sigma_t[,,t]%*%GG)%*%sigma_t[,,t]%*%t(V)%*%diag(1/diag(RR[,,t]))
  K_t[,,t] &amp;lt;- sigma_t[,,t]%*%t(V)%*%Sinv
  a_tt[t,] &amp;lt;- a_t[t,] + K_t[,,t]%*%(zz[t,]-y_t[t,])
  sigma_tt[,,t] &amp;lt;- sigma_t[,,t] - K_t[,,t]%*%V%*%sigma_tt[,,t-1]%*%t(V)%*%t(K_t[,,t])
  }

F.xts &amp;lt;- xts(a_tt,order.by = as.Date(rownames(data.m)))
plot.zoo(F.xts, col = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;yellow&amp;quot;,&amp;quot;purple&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 得られた推計値を時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;カルマンフィルタにより推計したファクターの時系列プロットが以下です。遷移方程式がAR(1)だったからかかなり平準化された値となっています。&lt;/p&gt;
&lt;p&gt;では、この得られたファクターをOLSにかけます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 得られたファクターとGDPをOLSにかける
F_q &amp;lt;- as.data.frame(a_tt[seq(3,nrow(a_tt),3),]) # 四半期の終わり月の値だけを引っ張ってくる 
colnames(F_q) &amp;lt;- c(&amp;quot;factor1&amp;quot;,&amp;quot;factor2&amp;quot;,&amp;quot;factor3&amp;quot;)
colnames(GDP) &amp;lt;- c(&amp;quot;publication&amp;quot;,&amp;quot;GDP&amp;quot;)
t &amp;lt;- which(GDP$publication==&amp;quot;2012-04-01&amp;quot;)
t2 &amp;lt;- which(GDP$publication==&amp;quot;2015-01-01&amp;quot;) # 2012-2q~2015-1qまでのデータが学習データ、それ以降がテストデータ
GDP_q &amp;lt;- GDP[t:nrow(GDP),]
dataset.q &amp;lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),])
test &amp;lt;- lm(GDP~factor1 + factor2 + factor3,data=dataset.q)
summary(test)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GDP ~ factor1 + factor2 + factor3, data = dataset.q)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1350.34  -361.67   -31.63   375.61  1105.07 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 125729.4     4134.2  30.412 1.07e-08 ***
## factor1       -199.0     1651.3  -0.121    0.907    
## factor2      -1699.7      960.2  -1.770    0.120    
## factor3      -2097.6     3882.5  -0.540    0.606    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 897.5 on 7 degrees of freedom
## Multiple R-squared:  0.783,  Adjusted R-squared:   0.69 
## F-statistic: 8.419 on 3 and 7 DF,  p-value: 0.0101&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;out_of_sample &amp;lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成
test.pred &amp;lt;-  predict(test, out_of_sample, interval=&amp;quot;prediction&amp;quot;)
pred.GDP.xts &amp;lt;- xts(cbind(test.pred[,1],out_of_sample$GDP),order.by = out_of_sample$publication)
plot.zoo(pred.GDP.xts,col = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 予測値と実績値を時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;OLSの推計結果はfactor1（赤）とfactor2（青）が有意との結果。前回の投稿でも言及したように、factor1（赤）はリスクセンチメントを表していそうなので、係数の符号が負であることは頷ける。ただし、factor2（青）も符号が負なのではなぜなのか…。このファクターは生産年齢人口など経済の潜在能力を表していると思っていたのに。かなり謎。まあとりあえず予測に移りましょう。このモデルを使用したGDPの予測値と実績値の推移はいかのようになりました。直近の精度は悪くない？&lt;/p&gt;
&lt;p&gt;というか、これ完全に単位根の問題を無視してOLSしてしまっているな。ファクターもGDPも完全に単位根を持つけど念のため単位根検定をかけてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tseries)

adf.test(F_q$factor1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$factor1
## Dickey-Fuller = -2.8191, Lag order = 2, p-value = 0.2603
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$factor2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$factor2
## Dickey-Fuller = -2.6749, Lag order = 2, p-value = 0.3153
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$factor3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$factor3
## Dickey-Fuller = -2.8928, Lag order = 2, p-value = 0.2323
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(GDP_q$GDP)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  GDP_q$GDP
## Dickey-Fuller = 1.5034, Lag order = 3, p-value = 0.99
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;はい。全部単位根もってました…。階差をとったのち、単位根検定を行います。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;GDP_q &amp;lt;- GDP_q %&amp;gt;% mutate(growth.rate=(GDP/lag(GDP)-1)*100)
F_q &amp;lt;- F_q %&amp;gt;% mutate(f1.growth.rate=(factor1/lag(factor1)-1)*100,
                      f2.growth.rate=(factor2/lag(factor2)-1)*100,
                      f3.growth.rate=(factor3/lag(factor3)-1)*100)

adf.test(GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  GDP_q$growth.rate[2:NROW(GDP_q$growth.rate)]
## Dickey-Fuller = -0.31545, Lag order = 3, p-value = 0.9838
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$f1.growth.rate[2:NROW(F_q$f1.growth.rate)]
## Dickey-Fuller = -2.7762, Lag order = 2, p-value = 0.2767
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$f2.growth.rate[2:NROW(F_q$f2.growth.rate)]
## Dickey-Fuller = -2.6156, Lag order = 2, p-value = 0.3379
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;adf.test(F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##  Augmented Dickey-Fuller Test
## 
## data:  F_q$f3.growth.rate[2:NROW(F_q$f3.growth.rate)]
## Dickey-Fuller = -2.9893, Lag order = 2, p-value = 0.1955
## alternative hypothesis: stationary&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;factor1だけは5%有意水準で帰無仮説を棄却できない…。困りました。有意水準を10%ということにして、とりあえず階差で&lt;code&gt;OLS&lt;/code&gt;してみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset.q &amp;lt;- cbind(GDP_q[1:(t2-t),],F_q[1:(t2-t),])
colnames(dataset.q) &amp;lt;- c(&amp;quot;publication&amp;quot;,&amp;quot;GDP&amp;quot;,&amp;quot;growth.rate&amp;quot;,&amp;quot;factor1&amp;quot;,&amp;quot;factor2&amp;quot;,&amp;quot;factor3&amp;quot;,&amp;quot;f1.growth.rate&amp;quot;,&amp;quot;f2.growth.rate&amp;quot;,&amp;quot;f3.growth.rate&amp;quot;)
test1 &amp;lt;- lm(growth.rate~f1.growth.rate + f2.growth.rate + f3.growth.rate,data=dataset.q)
summary(test1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = growth.rate ~ f1.growth.rate + f2.growth.rate + 
##     f3.growth.rate, data = dataset.q)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.6940 -0.2411  0.2041  0.4274  1.0904 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&amp;gt;|t|)
## (Intercept)     8.978e-02  4.588e-01   0.196    0.851
## f1.growth.rate -6.353e-03  1.375e-02  -0.462    0.660
## f2.growth.rate  6.155e-04  6.026e-03   0.102    0.922
## f3.growth.rate -9.249e-05  5.152e-04  -0.180    0.863
## 
## Residual standard error: 0.956 on 6 degrees of freedom
##   (1 observation deleted due to missingness)
## Multiple R-squared:  0.04055,    Adjusted R-squared:  -0.4392 
## F-statistic: 0.08452 on 3 and 6 DF,  p-value: 0.966&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;推計結果がわるくなりました…。予測値を計算し、実績値とプロットしてみます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;out_of_sample1 &amp;lt;- cbind(GDP_q[(t2-t+1):nrow(GDP_q),],F_q[(t2-t+1):nrow(GDP_q),]) # out of sampleのデータセットを作成
test1.pred &amp;lt;- predict(test1, out_of_sample1, interval=&amp;quot;prediction&amp;quot;)
pred1.GDP.xts &amp;lt;- xts(cbind(test1.pred[,1],out_of_sample1$growth.rate),order.by = out_of_sample1$publication)
plot.zoo(pred1.GDP.xts,col = c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 予測値と実績値を時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ん～、これはやり直しですね。今日はここまでで勘弁してください…。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>日次GDP推計に使用する経済統計を統計ダッシュボードから集めてみた</title>
      <link>/post/post7/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      <guid>/post/post7/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#先行研究と具体的にやりたいこと&#34;&gt;1. 先行研究と具体的にやりたいこと&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#統計ダッシュボードからのデータの収集&#34;&gt;2. 統計ダッシュボードからのデータの収集&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#得られたデータを主成分分析にかけてみる&#34;&gt;3. 得られたデータを主成分分析にかけてみる&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんちわ。&lt;/p&gt;
&lt;p&gt;最近、競馬ばっかりやってましたが、そろそろ本業のマクロの方もやらないとなということで今回は日次GDP推計に使用するデータを総務省が公開している統計ダッシュボードから取ってきました。
そもそも、前の記事では四半期GDP速報の精度が低いことをモチベーションに高頻度データを用いてより精度の高い予測値をはじき出すモデルを作れないかというテーマで研究を進めていました。しかし、先行研究を進めていくうちに、どうやら大規模な経済指標を利用することで日次で四半期GDPの予測値を計算することが可能であることが判明しました。しかも、精度も良い(米国ですが)ということで、なんとかこの方向で研究を進めていけないかということになりました。&lt;/p&gt;
&lt;div id=&#34;先行研究と具体的にやりたいこと&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. 先行研究と具体的にやりたいこと&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dept.ku.edu/~empirics/Courses/Econ844/papers/Nowcasting%20GDP.pdf&#34;&gt;先行研究&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Giannoneらが2008年にパブリッシュした論文です(JME)。彼らはアメリカの経済指標を用いて四半期GDPを日次で推計し、予測指標としての有用性を示しました。指標間の連動性(colinearity)を利用して、多数ある経済指標をいくつかのファクターに圧縮し、そのファクターを四半期GDPにフィッティングさせることによって高い予測性を実現しました。なお、ファクターの計算にはカルマンスムージングを用いています(詳しい推計方法は論文&amp;amp;Technical Appendixを参照)。理論的な定式化は無いのですが、なかなか当たります。そもそも私がこの研究に興味を持ったのは、以下の本を立ち読みした際に参考文献として出てきたからで、いよいよ運用機関などでも使用され始めるのかと思い、やっておこうと思った次第です。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.amazon.co.jp/exec/obidos/ASIN/4532134811/hatena-blog-22/&#34;&gt;実践 金融データサイエンス 隠れた構造をあぶり出す6つのアプローチ&lt;/a&gt;
&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51KIE5GeV+L._SX350_BO1,204,203,200_.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;とりあえずはGiannoneの日本版をやろうかなと思っています。実はこの後に、ファクターモデルとDSGEを組み合わせたモデルがありましてそこまで発展させたいなーなんて思っておりますが。とにかく、ファクターを計算するための経済統計が必要ですので、今回はそれを集めてきたというのがこの記事の趣旨です。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;統計ダッシュボードからのデータの収集&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. 統計ダッシュボードからのデータの収集&lt;/h2&gt;
&lt;p&gt;政府や日銀が公表しているデータの一部は統計ダッシュボードから落とすことができます。これは総務省統計局が提供しているもので、これまで利用しにくかった経済統計をより身近に使用してもらおうというのが一応のコンセプトとなっています。似たものに総務省統計局が提供しているestatがありますが、日銀の公表データがなかったり、メールアドレスの登録が必要だったりと非常に使い勝手が悪いです(個人的感想)。ただ、estatにはestatapiというRパッケージがあり、データを整形するのは比較的容易であると言えます。今回、統計ダッシュボードを選択した理由はそうは言っても日銀のデータがないのはダメだろうという理由で、データの整形に関しては関数を組みました。
そもそも統計ダッシュボードは経済統計をグラフなどで見て楽しむ？ものですが、私のような研究をしたい者を対象にAPIを提供してくれています。取得できるデータは大きく分けて6つあります。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;20180714160029.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;やり方は簡単で、ベースのurlと欲しい統計のIDをGET関数で渡すことによって、データを取得することができます。公式にも以下のように書かれています。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;基本的な使い方としては、まず①「統計メタ情報（系列）取得」で取得したいデータの[系列コード]を検索し、 その後⑥「統計データ取得」で[系列コード]を検索条件に指定し、その系列の情報を取得します。
（②③④⑤は補助的な情報として独立して取得できるようにしています。データのみ必要な場合は当該機能は不要です。）
具体的な使い方は、以下の「WebAPIの詳細仕様」に記載する[ベースURL]に検索条件となる[パラメータ]を“&amp;amp;”で連結し、HTTPリクエスト（GET）を送信することで目的のデータを取得できます。
各パラメータは「パラメータ名=値」のように名称と値を’=‘で結合し、複数のパラメータを指定する場合は「パラメータ名=値&amp;amp;パラメータ名=値&amp;amp;…」のようにそれぞれのパラメータ指定を’&amp;amp;’で結合してください。
また、パラメータ値は必ずURLエンコード(文字コードUTF-8)してから結合してください。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;今回も以下の文献を参考にデータを取ってきたいと思います。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.amazon.co.jp/dp/486354216X&#34;&gt;Rによるスクレイピング入門&lt;/a&gt;
&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51ZBnu8oSvL._SX350_BO1,204,203,200_.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;まず、最初にこのAPIからデータを取得し、得られた結果を分析しやすいように整形する関数を定義したいと思います。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(httr)
library(estatapi)
library(dplyr)
library(XML)
library(stringr)
library(xts)
library(GGally)
library(ggplot2)
library(seasonal)
library(dlm)
library(vars)
library(MASS)

# 関数を定義
get_dashboard &amp;lt;- function(ID){
  base_url &amp;lt;- &amp;quot;https://dashboard.e-stat.go.jp/api/1.0/JsonStat/getData?&amp;quot;
  res &amp;lt;- GET(
    url = base_url,
    query = list(
      IndicatorCode=ID
    )
  )
  result &amp;lt;- content(res)
  x &amp;lt;- result$link$item[[1]]$value
  x &amp;lt;- t(do.call(&amp;quot;data.frame&amp;quot;,x))
  date_x &amp;lt;- result$link$item[[1]]$dimension$Time$category$label
  date_x &amp;lt;- t(do.call(&amp;quot;data.frame&amp;quot;,date_x))
  date_x &amp;lt;- str_replace_all(date_x, pattern=&amp;quot;年&amp;quot;, replacement=&amp;quot;/&amp;quot;)
  date_x &amp;lt;- str_replace_all(date_x, pattern=&amp;quot;月&amp;quot;, replacement=&amp;quot;&amp;quot;)
  date_x &amp;lt;- as.Date(gsub(&amp;quot;([0-9]+)/([0-9]+)&amp;quot;, &amp;quot;\\1/\\2/1&amp;quot;, date_x))
  date_x &amp;lt;- as.Date(date_x, format = &amp;quot;%m/%d/%Y&amp;quot;)
  date_x &amp;lt;- as.numeric(date_x)
  date_x &amp;lt;- as.Date(date_x, origin=&amp;quot;1970-01-01&amp;quot;)
  #x &amp;lt;- cbind(x,date_x)
  x &amp;lt;- data.frame(x)
  x[,1] &amp;lt;- as.character(x[,1])%&amp;gt;%as.numeric(x[,1])
  colnames(x) &amp;lt;- c(result$link$item[[1]]$label)
  x &amp;lt;- x %&amp;gt;% mutate(&amp;quot;publication&amp;quot; = date_x)
  return(x)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;まずベースのurlを定義しています。今回はデータが欲しいので⑥統計データのベースurlを使用します（
&lt;a href=&#34;https://dashboard.e-stat.go.jp/static/api&#34;&gt;参考&lt;/a&gt;）。次にベースurlと統計ID（IndicatorCode）を&lt;code&gt;GET&lt;/code&gt;関数で渡し、結果を取得しています。統計IDについてはエクセルファイルで公開されています。得られた結果の中身（リスト形式）をresultに格納し、リストの深層にある原数値データ（value）をxに格納します。原数値データもリスト形式なので、それを&lt;code&gt;do.call&lt;/code&gt;でデータフレームに変換しています。次に、データ日付を取得します。resultの中を深くたどるとTime→category→labelというデータがあり、そこに日付データが保存されているので、それをdate_xに格納し、同じようにデータフレームへ変換します。データの仕様上、日付は「yyyy年mm月」になっていますが、これだと&lt;code&gt;R&lt;/code&gt;は日付データとして読み取ってくれないので、&lt;code&gt;str_replace_all&lt;/code&gt;等で変換したのち、&lt;code&gt;Date&lt;/code&gt;型に変換しています。列名にデータ名（result→link→item[[1]]→label）をつけ、データ日付をxに追加したら完成です。
そのほか、&lt;code&gt;data_connect&lt;/code&gt;という関数も定義しています。これはデータ系列によれば、たとえば推計方法の変更などで1980年～2005年の系列と2003年～2018年までの系列の2系列があるようなデータも存在し、この2系列を接続するための関数です。これは単純に接続しているだけなので、説明は省略します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data_connect &amp;lt;- function(x){
  a &amp;lt;- min(which(x[,ncol(x)] != &amp;quot;NA&amp;quot;))
  b &amp;lt;- x[a,ncol(x)]/x[a,1]
  c &amp;lt;- x[1:a-1,1]*b
  return(c)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、実際にデータを取得していきます。今回取得するデータは月次データとなっています。これは統計dashboardが月次以下のデータがとれないからです。なので、例えば日経平均などは月末の終値を引っ張っています。ただし、GDPは四半期データとなっています。さきほど定義したget_dashboardの使用方法は簡単で、引数に統計ダッシュボードで公開されている統計IDを入力するだけでデータが取れます。今回使用するデータを以下の表にまとめました。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;20180714212330.png&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# データを取得
Nikkei &amp;lt;- get_dashboard(&amp;quot;0702020501000010010&amp;quot;)
callrate &amp;lt;- get_dashboard(&amp;quot;0702020300000010010&amp;quot;)
TOPIX &amp;lt;- get_dashboard(&amp;quot;0702020590000090010&amp;quot;)
kikai &amp;lt;- get_dashboard(&amp;quot;0701030000000010010&amp;quot;)
kigyo.bukka &amp;lt;- get_dashboard(&amp;quot;0703040300000090010&amp;quot;)
money.stock1 &amp;lt;- get_dashboard(&amp;quot;0702010201000010030&amp;quot;)
money.stock2 &amp;lt;- get_dashboard(&amp;quot;0702010202000010030&amp;quot;)
money.stock &amp;lt;- dplyr::full_join(money.stock1,money.stock2,by=&amp;quot;publication&amp;quot;)
c &amp;lt;- data_connect(money.stock)
a &amp;lt;- min(which(money.stock[,ncol(money.stock)] != &amp;quot;NA&amp;quot;))
money.stock[1:a-1,ncol(money.stock)] &amp;lt;- c
money.stock &amp;lt;- money.stock[,c(2,3)]
cpi &amp;lt;- get_dashboard(&amp;quot;0703010401010090010&amp;quot;)
export.price &amp;lt;- get_dashboard(&amp;quot;0703050301000090010&amp;quot;)
import.price &amp;lt;- get_dashboard(&amp;quot;0703060301000090010&amp;quot;)
import.price$`輸出物価指数（総平均）（円ベース）2015年基準` &amp;lt;- NULL
public.expenditure1 &amp;lt;- get_dashboard(&amp;quot;0802020200000010010&amp;quot;)
public.expenditure2 &amp;lt;- get_dashboard(&amp;quot;0802020201000010010&amp;quot;)
public.expenditure &amp;lt;- dplyr::full_join(public.expenditure1,public.expenditure2,by=&amp;quot;publication&amp;quot;)
c &amp;lt;- data_connect(public.expenditure)
a &amp;lt;- min(which(public.expenditure[,ncol(public.expenditure)] != &amp;quot;NA&amp;quot;))
public.expenditure[1:a-1,ncol(public.expenditure)] &amp;lt;- c
public.expenditure &amp;lt;- public.expenditure[,c(2,3)]
export.service &amp;lt;- get_dashboard(&amp;quot;1601010101000010010&amp;quot;)
working.population &amp;lt;- get_dashboard(&amp;quot;0201010010000010020&amp;quot;)
yukoukyuujinn &amp;lt;- get_dashboard(&amp;quot;0301020001000010010&amp;quot;)
hours_worked &amp;lt;- get_dashboard(&amp;quot;0302010000000010000&amp;quot;)
nominal.wage &amp;lt;- get_dashboard(&amp;quot;0302020000000010000&amp;quot;) 
iip &amp;lt;- get_dashboard(&amp;quot;0502070101000090010&amp;quot;)
shukka.shisu &amp;lt;- get_dashboard(&amp;quot;0502070102000090010&amp;quot;)
zaiko.shisu &amp;lt;- get_dashboard(&amp;quot;0502070103000090010&amp;quot;)
sanji.sangyo &amp;lt;- get_dashboard(&amp;quot;0603100100000090010&amp;quot;)
retail.sells &amp;lt;- get_dashboard(&amp;quot;0601010201010010000&amp;quot;)
GDP1 &amp;lt;- get_dashboard(&amp;quot;0705020101000010000&amp;quot;)
GDP2 &amp;lt;- get_dashboard(&amp;quot;0705020301000010000&amp;quot;)
GDP &amp;lt;- dplyr::full_join(GDP1,GDP2,by=&amp;quot;publication&amp;quot;)
c &amp;lt;- data_connect(GDP)
a &amp;lt;- min(which(GDP[,ncol(GDP)] != &amp;quot;NA&amp;quot;))
GDP[1:a-1,ncol(GDP)] &amp;lt;- c
GDP &amp;lt;- GDP[,c(2,3)]
yen &amp;lt;- get_dashboard(&amp;quot;0702020401000010010&amp;quot;)
household.consumption &amp;lt;- get_dashboard(&amp;quot;0704010101000010001&amp;quot;)
JGB10y &amp;lt;- get_dashboard(&amp;quot;0702020300000010020&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;今取得したデータは原数値系列のデータが多いので、それらは季節調整をかけます。なぜ季節調整済みのデータを取得しないのかというとそれらのデータは何故か極端にサンプル期間が短くなってしまうからです。ここらへんは使い勝手が悪いです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 季節調整をかける
Sys.setenv(X13_PATH = &amp;quot;C:\\Program Files\\WinX13\\x13as&amp;quot;)
checkX13()
seasoning &amp;lt;- function(data,i,start.y,start.m){
  timeseries &amp;lt;- ts(data[,i],frequency = 12,start=c(start.y,start.m))
  m &amp;lt;- seas(timeseries)
  summary(m$data)
  return(m$series$s11)
}
k &amp;lt;- seasoning(kikai,1,2005,4)
kikai$`機械受注額（船舶・電力を除く民需）` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(kigyo.bukka,1,1960,1)
kigyo.bukka$`国内企業物価指数（総平均）2015年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(cpi,1,1970,1)
cpi$`消費者物価指数（生鮮食品を除く総合）2015年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(export.price,1,1960,1)
export.price$`輸出物価指数（総平均）（円ベース）2015年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(import.price,1,1960,1)
import.price$`輸入物価指数（総平均）（円ベース）2015年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(public.expenditure,2,2004,4)
public.expenditure$公共工事受注額 &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(export.service,1,1996,1)
export.service$`貿易・サービス収支` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(yukoukyuujinn,1,1963,1)
yukoukyuujinn$有効求人倍率 &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(hours_worked,1,1990,1)
hours_worked$総実労働時間 &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(nominal.wage,1,1990,1)
nominal.wage$現金給与総額 &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(iip,1,1978,1)
iip$`鉱工業生産指数　2010年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(shukka.shisu,1,1990,1)
shukka.shisu$`鉱工業出荷指数　2010年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(zaiko.shisu,1,1990,1)
zaiko.shisu$`鉱工業在庫指数　2010年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(sanji.sangyo,1,1988,1)
sanji.sangyo$`第３次産業活動指数　2010年基準` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(retail.sells,1,1980,1)
retail.sells$`小売業販売額（名目）` &amp;lt;- as.numeric(k)
k &amp;lt;- seasoning(household.consumption,1,2010,1)
household.consumption$`二人以上の世帯　消費支出（除く住居等）` &amp;lt;- as.numeric(k)
GDP.ts &amp;lt;- ts(GDP[,2],frequency = 4,start=c(1980,1))
m &amp;lt;- seas(GDP.ts)
GDP$`国内総生産（支出側）（実質）2011年基準` &amp;lt;- m$series$s11&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ここでは詳しく季節調整のかけ方は説明しません。x13arimaを使用しています。上述のコードを回す際はx13arimaがインストールされている必要があります。以下の記事を参考にしてください。&lt;/p&gt;
&lt;p&gt;[&lt;a href=&#34;http://sinhrks.hatenablog.com/entry/2014/11/09/003632&#34; class=&#34;uri&#34;&gt;http://sinhrks.hatenablog.com/entry/2014/11/09/003632&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;では、データ日付を基準に落としてきたデータを結合し、データセットを作成します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# データセットに結合
dataset &amp;lt;- dplyr::full_join(kigyo.bukka,callrate,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,kikai,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,Nikkei,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,money.stock,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,cpi,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,export.price,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,import.price,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,public.expenditure,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,export.service,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,working.population,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,yukoukyuujinn,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,hours_worked,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,nominal.wage,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,iip,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,shukka.shisu,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,zaiko.shisu,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,sanji.sangyo,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,retail.sells,by=&amp;quot;publication&amp;quot;)
dataset &amp;lt;- dplyr::full_join(dataset,yen,by=&amp;quot;publication&amp;quot;)
colnames(dataset) &amp;lt;- c(&amp;quot;DCGPI&amp;quot;,&amp;quot;publication&amp;quot;,&amp;quot;callrate&amp;quot;,&amp;quot;Machinery_Orders&amp;quot;,
                       &amp;quot;Nikkei225&amp;quot;,&amp;quot;money_stock&amp;quot;,&amp;quot;CPI&amp;quot;,&amp;quot;export_price&amp;quot;,
                       &amp;quot;import_price&amp;quot;,&amp;quot;public_works_order&amp;quot;,
                       &amp;quot;trade_service&amp;quot;,&amp;quot;working_population&amp;quot;,
                       &amp;quot;active_opening_ratio&amp;quot;,&amp;quot;hours_worked&amp;quot;,
                       &amp;quot;wage&amp;quot;,&amp;quot;iip_production&amp;quot;,&amp;quot;iip_shipment&amp;quot;,&amp;quot;iip_inventory&amp;quot;,
                       &amp;quot;ITIA&amp;quot;,&amp;quot;retail_sales&amp;quot;,&amp;quot;yen&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最後に列名をつけています。datasetはそれぞれのデータの公表開始時期が異なるために大量の&lt;code&gt;NA&lt;/code&gt;を含むデータフレームとなっているので、&lt;code&gt;NA&lt;/code&gt;を削除するために最もデータの開始時期が遅い機械受注統計に合わせてデータセットを再構築します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- min(which(dataset$Machinery_Orders != &amp;quot;NA&amp;quot;))
dataset1 &amp;lt;- dataset[a:nrow(dataset),]
dataset1 &amp;lt;- na.omit(dataset1)
rownames(dataset1) &amp;lt;- dataset1$publication
dataset1 &amp;lt;- dataset1[,-2]
dataset1.xts &amp;lt;- xts(dataset1,order.by = as.Date(rownames(dataset1)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これでとりあえずデータの収集は終わりました。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;得られたデータを主成分分析にかけてみる&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. 得られたデータを主成分分析にかけてみる&lt;/h2&gt;
&lt;p&gt;本格的な分析はまた今後にしたいのですが、データを集めるだけでは面白くないので、Gianonneらのように主成分分析を行いたいと思います。主成分分析をこれまでに学んだことのない方は以下を参考にしてください。個人的にはわかりやすいと思っています。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://datachemeng.com/principalcomponentanalysis/&#34;&gt;主成分分析(Principal Component Analysis, PCA)～データセットの見える化・可視化といったらまずはこれ！～&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では主成分分析を実行してみます。&lt;code&gt;R&lt;/code&gt;では&lt;code&gt;princomp&lt;/code&gt;関数を使用することで非常に簡単に主成分分析を行うことができます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 主成分分析を実行
factor.pca &amp;lt;- princomp(~.,cor = TRUE,data = dataset1) # cor = TRUEでデータの基準化を自動で行ってくれる。
summary(factor.pca)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Importance of components:
##                           Comp.1    Comp.2    Comp.3     Comp.4     Comp.5
## Standard deviation     3.2538097 1.8606047 1.5142917 1.05061250 0.88155352
## Proportion of Variance 0.5293639 0.1730925 0.1146540 0.05518933 0.03885683
## Cumulative Proportion  0.5293639 0.7024563 0.8171103 0.87229965 0.91115648
##                           Comp.6     Comp.7     Comp.8      Comp.9     Comp.10
## Standard deviation     0.7504599 0.63476742 0.48794520 0.413374289 0.358909911
## Proportion of Variance 0.0281595 0.02014648 0.01190453 0.008543915 0.006440816
## Cumulative Proportion  0.9393160 0.95946246 0.97136699 0.979910904 0.986351721
##                            Comp.11     Comp.12     Comp.13      Comp.14
## Standard deviation     0.296125594 0.254294037 0.233119328 0.1394055697
## Proportion of Variance 0.004384518 0.003233273 0.002717231 0.0009716956
## Cumulative Proportion  0.990736239 0.993969512 0.996686743 0.9976584386
##                             Comp.15      Comp.16      Comp.17     Comp.18
## Standard deviation     0.1356026290 0.1104356585 0.0861468897 0.070612034
## Proportion of Variance 0.0009194036 0.0006098017 0.0003710643 0.000249303
## Cumulative Proportion  0.9985778423 0.9991876440 0.9995587083 0.999808011
##                            Comp.19      Comp.20
## Standard deviation     0.053565104 3.115371e-02
## Proportion of Variance 0.000143461 4.852767e-05
## Cumulative Proportion  0.999951472 1.000000e+00&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;screeplot(factor.pca)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pc &amp;lt;- predict(factor.pca,dataset1)[,1:3] # 主成分を計算
pc.xts &amp;lt;- xts(pc,order.by = as.Date(rownames(dataset1)))
plot.zoo(pc.xts,col=c(&amp;quot;red&amp;quot;,&amp;quot;blue&amp;quot;,&amp;quot;green&amp;quot;,&amp;quot;purple&amp;quot;,&amp;quot;yellow&amp;quot;),plot.type = &amp;quot;single&amp;quot;) # 主成分を時系列プロット&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;第3主成分まででデータの約80％が説明できる結果を得たので、第3主成分までのプロットをお見せします。第1主成分（赤）はリーマンショックや東日本大震災、消費税増税のあたりで急上昇しています。ゆえに経済全体のリスクセンチメントを表しているのではないかと思っています。第2主成分（青）と第3主成分（緑）はリーマンショックのあたりで大きく落ち込んでいることは共通していますが2015年～現在の動きが大きく異なっています。また、第2主成分（青）はサンプル期間を通して過去トレンドを持つことから日本経済の潜在能力のようなものを表しているのではないでしょうか（そうするとリーマンショックまで上昇傾向にあることが疑問なのですが）。第3主成分（緑）はいまだ解読不能です（物価＆為替動向を表しているのではないかと思っています）。とりあえず今日はこれまで。次回はGianonne et. al.(2008)の日本版の再現を行いたいと思います。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>rvestでyahoo競馬にある過去のレース結果をクローリングしてみた</title>
      <link>/post/post9/</link>
      <pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate>
      <guid>/post/post9/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rvestとは&#34;&gt;1. Rvestとは&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#レース結果をスクレイピングしてみる&#34;&gt;2. レース結果をスクレイピングしてみる&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;みなさん、おはこんばんにちは。&lt;/p&gt;
&lt;p&gt;競馬のレース結果を的中させるモデルを作ろうということで研究をはじめましたが、まずはデータを自分で取ってくるところからやろうとおもいます。どこからデータを取ってくるのかという点が重要になるわけですが、データ先としてはdatascisotistさんがまとめられた非常にわかりやすい記事があります。どこからデータが取れるのかというと大きく分けて二つで、①JRA提供のJRA-VANや電子競馬新聞でおなじみの？JRJDといったデータベース、②netkeiba、yahoo競馬とといった競馬情報サイト、となっています。②の場合は自分でコードを書き、クローリングを行う必要があります。今回は②を選択し、yahoo競馬のデータをクローリングで落としてきたいと思います。Rでクローリングを行うパッケージとしては、rvest, httr, XMLがありますが、今回は1番簡単に使えるrvestを用います。yahoo競馬では以下のように各レース結果が表にまとめられています（5月の日本ダービーの結果）。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://keiba.yahoo.co.jp/race/result/1805021210/&#34;&gt;yahoo競馬&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;各馬のざっくりとした特徴やレース結果（通過順位等含む）、オッズが掲載されています。とりあえず、このぐらい情報があれば良いのではないかと思います（オッズの情報はもう少し欲しいのですが）。ただ、今後は少しずつ必要になった情報を拡充していこうとも思っています。1986年までのレース結果が格納されており、全データ数は50万件を超えるのではないかと思っています。ただ、単勝オッズが利用できるのは1994年からのようなので今回は1994年から直近までのデータを落としてきます。今回のゴールは、このデータをcsvファイル or SQLに格納することです。&lt;/p&gt;
&lt;div id=&#34;rvestとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Rvestとは&lt;/h2&gt;
&lt;p&gt;Rvestとは、webスクレイピングパッケージの一種でdplyrでおなじみのHadley Wickhamさんによって作成されたパッケージです。たった数行でwebスクレイピングができる優れものとなっており、操作が非常に簡単であるのが特徴です。今回は以下の本を参考にしました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.amazon.co.jp/exec/obidos/ASIN/486354216X/hatena-blog-22/&#34;&gt;Rによるスクレイピング入門&lt;/a&gt;
&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51ZBnu8oSvL._SX350_BO1,204,203,200_.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;そもそも、htmlも大学1年生にやった程度でほとんど忘れていたのですが、この本はそこも非常にわかりやすく解説されており、非常に実践的な本だと思います。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;レース結果をスクレイピングしてみる&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. レース結果をスクレイピングしてみる&lt;/h2&gt;
&lt;p&gt;実際にyahoo競馬からデータを落としてみたいと思います。コードは以下のようになっています。ご留意頂きたいのはこのコードをそのまま使用してスクレイピングを行うことはご遠慮いただきたいという事です。webスクレイピングは高速でサイトにアクセスするため、サイトへの負荷が大きくなる可能性があります。スクレイピングを行う際は、時間を空けるコーディングするなどその点に留意をして行ってください（最悪訴えられる可能性がありますが、こちらは一切の責任を取りません）。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rvestによる競馬データのwebスクレイピング

#install.packages(&amp;quot;rvest&amp;quot;)
#if (!require(&amp;quot;pacman&amp;quot;)) install.packages(&amp;quot;pacman&amp;quot;)
pacman::p_load(qdapRegex)
library(rvest)
library(stringr)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;使用するパッケージは&lt;code&gt;qdapRegex&lt;/code&gt;、&lt;code&gt;rvest&lt;/code&gt;、&lt;code&gt;stringr&lt;/code&gt;、&lt;code&gt;dplyr&lt;/code&gt;です。&lt;code&gt;qdapRegex&lt;/code&gt;はカッコ内の文字を取り出すために使用しています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;keiba.yahoo &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp/schedule/list/2016/?month=&amp;quot;,k))
race_url &amp;lt;- keiba.yahoo %&amp;gt;%
html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
html_attr(&amp;quot;href&amp;quot;) # 全urlを取得

# レース結果のをurlを取得
race_url &amp;lt;- race_url[str_detect(race_url, pattern=&amp;quot;result&amp;quot;)==1] # 「result」が含まれるurlを抽出&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;まず、read_htmlでyahoo競馬のレース結果一覧のhtml構造を引っ張ってきます（リンクは2016年1月の全レース）。ここで、kと出ているのは月を表し、k=1であれば2016年1月のレース結果を引っ張ってくるということです。keiba.yahooを覗いてみると以下のようにそのページ全体のhtml構造が格納されているのが分かります。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;keiba.yahoo&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $node
## &amp;lt;pointer: (nil)&amp;gt;
## 
## $doc
## &amp;lt;pointer: (nil)&amp;gt;
## 
## attr(,&amp;quot;class&amp;quot;)
## [1] &amp;quot;xml_document&amp;quot; &amp;quot;xml_node&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;race_urlにはyahoo.keibaのうちの2016年k月にあった全レース結果のリンクを格納しています。html_nodeとはhtml構造のうちどの要素を引っ張るかを指定し、それを引っ張る関数で、簡単に言えばほしいデータの住所を入力する関数であると認識しています（おそらく正しくない）。ここではa要素を引っ張ることにしています。注意すべきことは、html_nodeは欲しい情報をhtml形式で引っ張ることです。なので、テキストデータとしてリンクを保存するためにはhtml_attrを使用する必要があります。html_attrの引数として、リンク属性を表すhrefを渡しています。これでレース結果のurlが取れたと思いきや、実はこれでは他のリンクもとってしまっています。一番わかりやすいのが広告のリンクです。こういったリンクは除外する必要があります。レース結果のurlには“result”が含まれているので、この文字が入っている要素だけを抽出したのが一番最後のコードです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for (i in 1:length(race_url)){
  race1 &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,race_url[i])) # レース結果のurlを取得

  # レース結果をスクレイピング
  race_result &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//table[@id = &amp;#39;raceScore&amp;#39;]&amp;quot;) %&amp;gt;%
  html_table()
  race_result &amp;lt;- do.call(&amp;quot;data.frame&amp;quot;,race_result) # リストをデータフレームに変更
  colnames(race_result) &amp;lt;- c(&amp;quot;order&amp;quot;,&amp;quot;frame_number&amp;quot;,&amp;quot;horse_number&amp;quot;,&amp;quot;horse_name/age&amp;quot;,&amp;quot;time/margin&amp;quot;,&amp;quot;passing_rank/last_3F&amp;quot;,&amp;quot;jockey/weight&amp;quot;,&amp;quot;popularity/odds&amp;quot;,&amp;quot;trainer&amp;quot;) #　列名変更&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;さて、いよいよレース結果のスクレイピングを行います。さきほど取得したリンク先のhtml構造を一つ一つ取得し、その中で必要なテキスト情報を引っ張るという作業をRに実行させます（なのでループを使う）。race_1にはあるレース結果ページのhtml構造が格納されおり、race_resultにはその結果が入っています。html_nodesの引数に入っているxpathですが、これはXLMフォーマットのドキュメントから効率的に要素を抜き出す言語です。先ほど説明した住所のようなものと思っていただければ良いと思います。その横に書いてある&lt;code&gt;//table[@id = &#39;raceScore&#39;]&lt;/code&gt;が住所です。これはwebブラウザから簡単に探すことができます。Firefoxの説明になりますが、ほかのブラウザでも同じような機能があると思います。スクレイプしたい画面で&lt;code&gt;Ctrl+Shift+C&lt;/code&gt;を押すと下のような画面が表示されます。&lt;/p&gt;
&lt;p&gt;このインスペクターの横のマークをクリックすると、カーソルで指した部分のhtml構造（住所）が表示されます。この場合だと、レース結果はtable属性の&lt;code&gt;id&lt;/code&gt;がraceScoreの場所に格納されていることが分かります。なので、上のコードでは&lt;code&gt;xpath=&lt;/code&gt;のところにそれを記述しているのです。そして、レース結果は表（table）形式でドキュメント化されているので、&lt;code&gt;html_table&lt;/code&gt;でごっそりとスクレイプしました。基本的にリスト形式で返されるので、それをデータフレームに変換し、適当に列名をつけています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 通過順位と上り3Fのタイム
  race_result &amp;lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;(\\d{2}-\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2})&amp;quot;)))
  race_result &amp;lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;\\d{2}\\.\\d&amp;quot;)))
  race_result &amp;lt;- race_result[-6]

# タイムと着差
  race_result &amp;lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;\\d\\.\\d{2}\\.\\d|\\d{2}\\.\\d&amp;quot;)))
  race_result &amp;lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+&amp;quot;)))
  race_result &amp;lt;- race_result[-5]

# 馬名、馬齢、馬体重
  race_result &amp;lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;[ァ-ヴー・]+&amp;quot;)))
  race_result &amp;lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;牡\\d+|牝\\d+|せん\\d+&amp;quot;)))
  race_result &amp;lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\d{3}&amp;quot;)))
  race_result &amp;lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\([\\+|\\-]\\d+\\)|\\([\\d+]\\)&amp;quot;)))
  race_result$horse_weight_change &amp;lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
  race_result &amp;lt;- race_result[-4]

# ジョッキー
  race_result &amp;lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,&amp;quot;[ぁ-ん一-龠]+\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+&amp;quot;)))
  race_result &amp;lt;- race_result[-4]

# オッズと人気
  race_result &amp;lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\(.+\\)&amp;quot;)))
  race_result &amp;lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\d+[^(\\d+.\\d)]&amp;quot;)))
  race_result$odds &amp;lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
  race_result &amp;lt;- race_result[-4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ここまででデータは取得できたわけなのですが、そのデータは綺麗なものにはなっていません。 上のコードでは、その整形作業を行っています。現在、取得したデータは以下のようになっています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(race_result)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;order&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;frame_number&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;horse_number&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;trainer&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;passing_rank&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;last_3F&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;time&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;margin&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;horse_name&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;horse_age&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;horse_weight&#34;],&#34;name&#34;:[11],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;horse_weight_change&#34;],&#34;name&#34;:[12],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;jockey&#34;],&#34;name&#34;:[13],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;odds&#34;],&#34;name&#34;:[14],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;popularity&#34;],&#34;name&#34;:[15],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;race_date&#34;],&#34;name&#34;:[16],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;race_name&#34;],&#34;name&#34;:[17],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;2&#34;,&#34;3&#34;:&#34;2&#34;,&#34;4&#34;:&#34;河野 通文&#34;,&#34;5&#34;:&#34;06-07-06-06&#34;,&#34;6&#34;:&#34;35.1&#34;,&#34;7&#34;:&#34;3.19.7&#34;,&#34;8&#34;:&#34;character(0)&#34;,&#34;9&#34;:&#34;センゴクシルバー&#34;,&#34;10&#34;:&#34;牡6&#34;,&#34;11&#34;:&#34;478&#34;,&#34;12&#34;:&#34;+2&#34;,&#34;13&#34;:&#34;田中 勝春&#34;,&#34;14&#34;:&#34;2.3&#34;,&#34;15&#34;:&#34;1&#34;,&#34;16&#34;:&#34;1994年1月31日&#34;,&#34;17&#34;:&#34;第44回ダイヤモンドステークス（GIII）&#34;,&#34;_rn_&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;6&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;武 邦彦&#34;,&#34;5&#34;:&#34;06-05-04-04&#34;,&#34;6&#34;:&#34;35.7&#34;,&#34;7&#34;:&#34;3.19.9&#34;,&#34;8&#34;:&#34;1 1/4馬身&#34;,&#34;9&#34;:&#34;ジャムシード&#34;,&#34;10&#34;:&#34;牡6&#34;,&#34;11&#34;:&#34;480&#34;,&#34;12&#34;:&#34;0&#34;,&#34;13&#34;:&#34;柴田 政人&#34;,&#34;14&#34;:&#34;14&#34;,&#34;15&#34;:&#34;7&#34;,&#34;16&#34;:&#34;1994年1月31日&#34;,&#34;17&#34;:&#34;第44回ダイヤモンドステークス（GIII）&#34;,&#34;_rn_&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;3&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;10&#34;,&#34;4&#34;:&#34;白井 寿昭&#34;,&#34;5&#34;:&#34;08-07-06-06&#34;,&#34;6&#34;:&#34;35.7&#34;,&#34;7&#34;:&#34;3.20.1&#34;,&#34;8&#34;:&#34;1 1/2馬身&#34;,&#34;9&#34;:&#34;ホクセツギンガ&#34;,&#34;10&#34;:&#34;牡6&#34;,&#34;11&#34;:&#34;500&#34;,&#34;12&#34;:&#34;+4&#34;,&#34;13&#34;:&#34;小屋敷 昭&#34;,&#34;14&#34;:&#34;7.5&#34;,&#34;15&#34;:&#34;3&#34;,&#34;16&#34;:&#34;1994年1月31日&#34;,&#34;17&#34;:&#34;第44回ダイヤモンドステークス（GIII）&#34;,&#34;_rn_&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;4&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;11&#34;,&#34;4&#34;:&#34;矢野 進&#34;,&#34;5&#34;:&#34;03-03-03-02&#34;,&#34;6&#34;:&#34;36.3&#34;,&#34;7&#34;:&#34;3.20.4&#34;,&#34;8&#34;:&#34;1 3/4馬身&#34;,&#34;9&#34;:&#34;サマーワイン&#34;,&#34;10&#34;:&#34;牝5&#34;,&#34;11&#34;:&#34;422&#34;,&#34;12&#34;:&#34;-4&#34;,&#34;13&#34;:&#34;木幡 初広&#34;,&#34;14&#34;:&#34;32.6&#34;,&#34;15&#34;:&#34;9&#34;,&#34;16&#34;:&#34;1994年1月31日&#34;,&#34;17&#34;:&#34;第44回ダイヤモンドステークス（GIII）&#34;,&#34;_rn_&#34;:&#34;4&#34;},{&#34;1&#34;:&#34;5&#34;,&#34;2&#34;:&#34;1&#34;,&#34;3&#34;:&#34;1&#34;,&#34;4&#34;:&#34;秋山 史郎&#34;,&#34;5&#34;:&#34;12-12-12-12&#34;,&#34;6&#34;:&#34;35.6&#34;,&#34;7&#34;:&#34;3.20.5&#34;,&#34;8&#34;:&#34;1/2馬身&#34;,&#34;9&#34;:&#34;ダイワジェームス&#34;,&#34;10&#34;:&#34;牡6&#34;,&#34;11&#34;:&#34;472&#34;,&#34;12&#34;:&#34;+6&#34;,&#34;13&#34;:&#34;大塚 栄三郎&#34;,&#34;14&#34;:&#34;5.9&#34;,&#34;15&#34;:&#34;2&#34;,&#34;16&#34;:&#34;1994年1月31日&#34;,&#34;17&#34;:&#34;第44回ダイヤモンドステークス（GIII）&#34;,&#34;_rn_&#34;:&#34;5&#34;},{&#34;1&#34;:&#34;6&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;7&#34;,&#34;4&#34;:&#34;須貝 彦三&#34;,&#34;5&#34;:&#34;11-10-06-06&#34;,&#34;6&#34;:&#34;36.1&#34;,&#34;7&#34;:&#34;3.20.5&#34;,&#34;8&#34;:&#34;ハナ&#34;,&#34;9&#34;:&#34;シゲノランボー&#34;,&#34;10&#34;:&#34;牡7&#34;,&#34;11&#34;:&#34;438&#34;,&#34;12&#34;:&#34;-6&#34;,&#34;13&#34;:&#34;須貝 尚介&#34;,&#34;14&#34;:&#34;8.7&#34;,&#34;15&#34;:&#34;4&#34;,&#34;16&#34;:&#34;1994年1月31日&#34;,&#34;17&#34;:&#34;第44回ダイヤモンドステークス（GIII）&#34;,&#34;_rn_&#34;:&#34;6&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;ご覧のように、&lt;code&gt;\n&lt;/code&gt;が入っていたり、通過順位と上り3ハロンのタイムが一つのセルに入っていたりとこのままでは分析ができません。不要なものを取り除いたり、データを二つに分割する作業が必要になります。今回の記事ではこの部分について詳しくは説明しません。この部分は正規表現を駆使する必要がありますが、私自身全く詳しくないからです。今回も手探りでやりました。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# レース情報
  race_date &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/p[@id = &amp;#39;raceTitDay&amp;#39;]&amp;quot;) %&amp;gt;%
    html_text()
  race_name &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/h1[@class = &amp;#39;fntB&amp;#39;]&amp;quot;) %&amp;gt;%
    html_text()

  race_result &amp;lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,&amp;quot;\\d+年\\d+月\\d+日&amp;quot;)))
  race_result &amp;lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,&amp;quot;\\s&amp;quot;,&amp;quot;&amp;quot;)))

# ファイル格納
  if (k ==1 &amp;amp;&amp;amp; i == 1){
    dataset &amp;lt;- race_result
  } else {
    dataset &amp;lt;- rbind(dataset,race_result)
  }# if文の終わり
} # iループの終わり

    write.csv(race_result,&amp;quot;race_result.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最後に、レース日時とレース名を抜き出し、データを一時的に格納するコードとcsvファイルに書き出すコードを書いて終了です。完成データセットは以下のような状態になっています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(dataset)&lt;/code&gt;&lt;/pre&gt;
&lt;div data-pagedtable=&#34;false&#34;&gt;
&lt;script data-pagedtable-source type=&#34;application/json&#34;&gt;
{&#34;columns&#34;:[{&#34;label&#34;:[&#34;&#34;],&#34;name&#34;:[&#34;_rn_&#34;],&#34;type&#34;:[&#34;&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;order&#34;],&#34;name&#34;:[1],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;frame_number&#34;],&#34;name&#34;:[2],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;horse_number&#34;],&#34;name&#34;:[3],&#34;type&#34;:[&#34;int&#34;],&#34;align&#34;:[&#34;right&#34;]},{&#34;label&#34;:[&#34;trainer&#34;],&#34;name&#34;:[4],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;passing_rank&#34;],&#34;name&#34;:[5],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;last_3F&#34;],&#34;name&#34;:[6],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;time&#34;],&#34;name&#34;:[7],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;margin&#34;],&#34;name&#34;:[8],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;horse_name&#34;],&#34;name&#34;:[9],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;horse_age&#34;],&#34;name&#34;:[10],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;horse_weight&#34;],&#34;name&#34;:[11],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;horse_weight_change&#34;],&#34;name&#34;:[12],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;jockey&#34;],&#34;name&#34;:[13],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;odds&#34;],&#34;name&#34;:[14],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;popularity&#34;],&#34;name&#34;:[15],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;race_date&#34;],&#34;name&#34;:[16],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]},{&#34;label&#34;:[&#34;race_name&#34;],&#34;name&#34;:[17],&#34;type&#34;:[&#34;chr&#34;],&#34;align&#34;:[&#34;left&#34;]}],&#34;data&#34;:[{&#34;1&#34;:&#34;1&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;11&#34;,&#34;4&#34;:&#34;森安 弘昭&#34;,&#34;5&#34;:&#34;01-01-01-01&#34;,&#34;6&#34;:&#34;35.7&#34;,&#34;7&#34;:&#34;2.00.7&#34;,&#34;8&#34;:&#34;character(0)&#34;,&#34;9&#34;:&#34;ヒダカハヤト&#34;,&#34;10&#34;:&#34;牡8&#34;,&#34;11&#34;:&#34;488&#34;,&#34;12&#34;:&#34;-2&#34;,&#34;13&#34;:&#34;大塚 栄三郎&#34;,&#34;14&#34;:&#34;29&#34;,&#34;15&#34;:&#34;10&#34;,&#34;16&#34;:&#34;1994年1月5日&#34;,&#34;17&#34;:&#34;第43回日刊スポーツ賞金杯（GIII）&#34;,&#34;_rn_&#34;:&#34;1&#34;},{&#34;1&#34;:&#34;2&#34;,&#34;2&#34;:&#34;5&#34;,&#34;3&#34;:&#34;6&#34;,&#34;4&#34;:&#34;矢野 進&#34;,&#34;5&#34;:&#34;06-06-04-03&#34;,&#34;6&#34;:&#34;35.3&#34;,&#34;7&#34;:&#34;2.00.8&#34;,&#34;8&#34;:&#34;3/4馬身&#34;,&#34;9&#34;:&#34;ステージチャンプ&#34;,&#34;10&#34;:&#34;牡5&#34;,&#34;11&#34;:&#34;462&#34;,&#34;12&#34;:&#34;+14&#34;,&#34;13&#34;:&#34;岡部 幸雄&#34;,&#34;14&#34;:&#34;3.2&#34;,&#34;15&#34;:&#34;1&#34;,&#34;16&#34;:&#34;1994年1月5日&#34;,&#34;17&#34;:&#34;第43回日刊スポーツ賞金杯（GIII）&#34;,&#34;_rn_&#34;:&#34;2&#34;},{&#34;1&#34;:&#34;3&#34;,&#34;2&#34;:&#34;4&#34;,&#34;3&#34;:&#34;4&#34;,&#34;4&#34;:&#34;新関 力&#34;,&#34;5&#34;:&#34;03-03-02-02&#34;,&#34;6&#34;:&#34;35.8&#34;,&#34;7&#34;:&#34;2.01.1&#34;,&#34;8&#34;:&#34;1 3/4馬身&#34;,&#34;9&#34;:&#34;マキノトウショウ&#34;,&#34;10&#34;:&#34;牡5&#34;,&#34;11&#34;:&#34;502&#34;,&#34;12&#34;:&#34;+6&#34;,&#34;13&#34;:&#34;的場 均&#34;,&#34;14&#34;:&#34;6.9&#34;,&#34;15&#34;:&#34;4&#34;,&#34;16&#34;:&#34;1994年1月5日&#34;,&#34;17&#34;:&#34;第43回日刊スポーツ賞金杯（GIII）&#34;,&#34;_rn_&#34;:&#34;3&#34;},{&#34;1&#34;:&#34;4&#34;,&#34;2&#34;:&#34;8&#34;,&#34;3&#34;:&#34;12&#34;,&#34;4&#34;:&#34;大和田 稔&#34;,&#34;5&#34;:&#34;02-02-03-03&#34;,&#34;6&#34;:&#34;35.7&#34;,&#34;7&#34;:&#34;2.01.1&#34;,&#34;8&#34;:&#34;ハナ&#34;,&#34;9&#34;:&#34;ペガサス&#34;,&#34;10&#34;:&#34;牡5&#34;,&#34;11&#34;:&#34;464&#34;,&#34;12&#34;:&#34;+4&#34;,&#34;13&#34;:&#34;安田 富男&#34;,&#34;14&#34;:&#34;5.7&#34;,&#34;15&#34;:&#34;2&#34;,&#34;16&#34;:&#34;1994年1月5日&#34;,&#34;17&#34;:&#34;第43回日刊スポーツ賞金杯（GIII）&#34;,&#34;_rn_&#34;:&#34;4&#34;},{&#34;1&#34;:&#34;5&#34;,&#34;2&#34;:&#34;7&#34;,&#34;3&#34;:&#34;9&#34;,&#34;4&#34;:&#34;田中 和夫&#34;,&#34;5&#34;:&#34;07-07-07-05&#34;,&#34;6&#34;:&#34;35.8&#34;,&#34;7&#34;:&#34;2.01.6&#34;,&#34;8&#34;:&#34;3馬身&#34;,&#34;9&#34;:&#34;シャマードシンボリ&#34;,&#34;10&#34;:&#34;牡7&#34;,&#34;11&#34;:&#34;520&#34;,&#34;12&#34;:&#34;+4&#34;,&#34;13&#34;:&#34;田中 剛&#34;,&#34;14&#34;:&#34;29.6&#34;,&#34;15&#34;:&#34;12&#34;,&#34;16&#34;:&#34;1994年1月5日&#34;,&#34;17&#34;:&#34;第43回日刊スポーツ賞金杯（GIII）&#34;,&#34;_rn_&#34;:&#34;5&#34;},{&#34;1&#34;:&#34;6&#34;,&#34;2&#34;:&#34;3&#34;,&#34;3&#34;:&#34;3&#34;,&#34;4&#34;:&#34;中島 敏文&#34;,&#34;5&#34;:&#34;09-10-10-09&#34;,&#34;6&#34;:&#34;35.5&#34;,&#34;7&#34;:&#34;2.01.7&#34;,&#34;8&#34;:&#34;3/4馬身&#34;,&#34;9&#34;:&#34;モンタミール&#34;,&#34;10&#34;:&#34;牡7&#34;,&#34;11&#34;:&#34;474&#34;,&#34;12&#34;:&#34;+4&#34;,&#34;13&#34;:&#34;蓑田 早人&#34;,&#34;14&#34;:&#34;10.4&#34;,&#34;15&#34;:&#34;6&#34;,&#34;16&#34;:&#34;1994年1月5日&#34;,&#34;17&#34;:&#34;第43回日刊スポーツ賞金杯（GIII）&#34;,&#34;_rn_&#34;:&#34;6&#34;}],&#34;options&#34;:{&#34;columns&#34;:{&#34;min&#34;:{},&#34;max&#34;:[10]},&#34;rows&#34;:{&#34;min&#34;:[10],&#34;max&#34;:[10]},&#34;pages&#34;:{}}}
  &lt;/script&gt;
&lt;/div&gt;
&lt;p&gt;以上です。次回はこのデータセットを使用して、分析を行っていきます。次回までには1994年からのデータを全てスクレイピングしてきます。&lt;/p&gt;
&lt;p&gt;【追記（2018/6/10）】&lt;/p&gt;
&lt;p&gt;上述したスクリプトを用いて、スクレイピングを行ったところエラーが出ました。どうやらレース結果の中には強風などで中止になったものも含まれているらしく、そこでエラーが出る様子（race_resultがcharacter(0)になってしまう）。なので、この部分を修正したスクリプトを以下で公開しておきます。こちらは私の PC環境では正常に作動しています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rvestによる競馬データのwebスクレイピング

#install.packages(&amp;quot;rvest&amp;quot;)
#if (!require(&amp;quot;pacman&amp;quot;)) install.packages(&amp;quot;pacman&amp;quot;)
  install.packages(&amp;quot;beepr&amp;quot;)
  pacman::p_load(qdapRegex)
  library(rvest)
  library(stringr)
  library(dplyr)
  library(beepr)

# pathの設定
  setwd(&amp;quot;C:/Users/assiy/Dropbox/競馬統計解析&amp;quot;)

  for(year in 1994:2018){

  # yahoo競馬のレース結果一覧ページの取得
  for (k in 1:12){

    keiba.yahoo &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp/schedule/list/&amp;quot;, year,&amp;quot;/?month=&amp;quot;,k))
    race_url &amp;lt;- keiba.yahoo %&amp;gt;%
    html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;%
    html_attr(&amp;quot;href&amp;quot;) # 全urlを取得

    # レース結果のをurlを取得
    race_url &amp;lt;- race_url[str_detect(race_url, pattern=&amp;quot;result&amp;quot;)==1] # 「result」が含まれるurlを抽出

    for (i in 1:length(race_url)){

    Sys.sleep(10)
    print(str_c(&amp;quot;現在、&amp;quot;, year, &amp;quot;年&amp;quot;, k, &amp;quot;月&amp;quot;, i,&amp;quot;番目のレースの保存中です&amp;quot;))

    race1 &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,race_url[i])) # レース結果のurlを取得

    # レースが中止でなければ処理を実行
    if (identical(race1 %&amp;gt;%
    html_nodes(xpath = &amp;quot;//div[@class = &amp;#39;resultAtt mgnBL fntSS&amp;#39;]&amp;quot;) %&amp;gt;%
    html_text(),character(0)) == TRUE){

    # レース結果をスクレイピング
    race_result &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//table[@id = &amp;#39;raceScore&amp;#39;]&amp;quot;) %&amp;gt;%
    html_table()
    race_result &amp;lt;- do.call(&amp;quot;data.frame&amp;quot;,race_result) # リストをデータフレームに変更
    colnames(race_result) &amp;lt;- c(&amp;quot;order&amp;quot;,&amp;quot;frame_number&amp;quot;,&amp;quot;horse_number&amp;quot;,&amp;quot;horse_name/age&amp;quot;,&amp;quot;time/margin&amp;quot;,&amp;quot;passing_rank/last_3F&amp;quot;,&amp;quot;jockey/weight&amp;quot;,&amp;quot;popularity/odds&amp;quot;,&amp;quot;trainer&amp;quot;) #　列名変更

    # 通過順位と上り3Fのタイム
    race_result &amp;lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;(\\d{2}-\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2})&amp;quot;)))
    race_result &amp;lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;\\d{2}\\.\\d&amp;quot;)))
    race_result &amp;lt;- race_result[-6]

    # タイムと着差
    race_result &amp;lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;\\d\\.\\d{2}\\.\\d|\\d{2}\\.\\d&amp;quot;)))
    race_result &amp;lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+&amp;quot;)))
    race_result &amp;lt;- race_result[-5]

    # 馬名、馬齢、馬体重
    race_result &amp;lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;[ァ-ヴー・]+&amp;quot;)))
    race_result &amp;lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;牡\\d+|牝\\d+|せん\\d+&amp;quot;)))
    race_result &amp;lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\d{3}&amp;quot;)))
    race_result &amp;lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\([\\+|\\-]\\d+\\)|\\([\\d+]\\)&amp;quot;)))
    race_result$horse_weight_change &amp;lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
    race_result &amp;lt;- race_result[-4]

    # ジョッキー
    race_result &amp;lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,&amp;quot;[ぁ-ん一-龠]+\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+&amp;quot;)))
    race_result &amp;lt;- race_result[-4]

    # オッズと人気
    race_result &amp;lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\(.+\\)&amp;quot;)))
    race_result &amp;lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\d+[^(\\d+.\\d)]&amp;quot;)))
    race_result$odds &amp;lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
    race_result &amp;lt;- race_result[-4]

    # レース情報
    race_date &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/p[@id = &amp;#39;raceTitDay&amp;#39;]&amp;quot;) %&amp;gt;%
    html_text()
    race_name &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/h1[@class = &amp;#39;fntB&amp;#39;]&amp;quot;) %&amp;gt;%
    html_text()

    race_result &amp;lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,&amp;quot;\\d+年\\d+月\\d+日&amp;quot;)))
    race_result &amp;lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,&amp;quot;\\s&amp;quot;,&amp;quot;&amp;quot;)))

    ## ファイル貯めるのかく
    if (k == 1 &amp;amp;&amp;amp; i == 1 &amp;amp;&amp;amp; year == 1994){
    dataset &amp;lt;- race_result
    } else {
    dataset &amp;lt;- rbind(dataset,race_result)
    } # if文2の終わり
    } # if文1の終わり
    } # iループの終わり
    } # kループの終わり
    beep()
    } # yearループの終わり

    write.csv(dataset,&amp;quot;race_result.csv&amp;quot;, row.names = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これを回すのに16時間かかりました（笑）データ数は想定していたよりは少なく、97939になりました。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>はじめまして</title>
      <link>/post/post4/</link>
      <pubDate>Fri, 18 May 2018 00:00:00 +0000</pubDate>
      <guid>/post/post4/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;どうもはじめまして。
東京にある資産運用会社に勤める新卒1年目の葦原彩人と申します。&lt;/p&gt;
&lt;p&gt;簡単に自己紹介をしたいと思います。
大学院卒の24歳です。
専攻はマクロ経済学で特にDSGEモデル、状態空間モデル、カルマンフィルタ、ベイズ推定、MCMCなんかをやっていました。
指導教官の研究サポートとして自然言語処理をやったこともあります。
もともと学者志望でしたが金銭的な問題で就職することになりました。
今は営業サポートの下働きをしています。&lt;/p&gt;
&lt;p&gt;このブログは研究への興味関心を捨てることができない私が趣味として研究を進めていく際の備忘録という位置付けになるのだと思います。&lt;/p&gt;
&lt;p&gt;現在進めている研究は以下の２つです。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;馬券版ファクターモデル&lt;/li&gt;
&lt;li&gt;四半期GDP予測モデル&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1は、馬券市場が株式市場よりも投機的に魅力のある市場であるという観点から、回収率100%超えを目指す馬券ポートフォリオを構築するモデルを作成できないかというものです。株式にはファーマフレンチのようなファクターモデルがありますが、それを応用して馬券版ファクターモデルなるものを作れないかと思っています。&lt;/p&gt;
&lt;p&gt;2は最近の四半期GDP速報の精度が低いという問題意識から、精度の高い予測モデルを新たに構築できないかというものです。特にこれまでのようなマクロ経済理論に基づいたモデルではなく、機械学習を用い、通常マクロ経済学の研究で使用しないようなデータを織り込んだモデルを作成する方向で研究を進めていきたいと思っています。&lt;/p&gt;
&lt;p&gt;まだ、研究は始めたばかりなのですが、
興味を持ってもらえるように頑張りたいと思います…
では、最初はこの辺で。&lt;/p&gt;
&lt;p&gt;よろしくお願いします。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
