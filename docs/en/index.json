[{"authors":null,"categories":null,"content":"\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n  \r\n  \r\n  \r\n    \r\n  \r\n\r\n\r\n\r\n  \r\n  \r\n  \r\n  \r\n  \r\n  \r\n  \r\n    \r\n  \r\n  \r\n\r\n\r\n\r\n\r\n  \r\n  \r\n    \r\n    \r\n    \r\n    \r\n    Started work in 2018; currently in year 8 of professional career. Born on January 19, 1994 and currently 32 years old.\r\n  \r\n\r\n\r\n\n","date":1522454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522454400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/en/author/ayato-ashihara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/ayato-ashihara/","section":"authors","summary":"","tags":null,"title":"Ayato Ashihara","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/en/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/en/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/en/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/en/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"\rClick on the Slides button above to view the built-in slides feature.\r\r\rSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/en/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":["macroeconomics","single-shot"],"content":"Hi. There are several ways to collect macroeconomic data, but collecting data for each country can be a challenge. However, you can automate the tedious process of collecting data from the OECD via API. Today, I will introduce the method.\n1.OECD.Stat Web API OECD.org offers a service called OECD.Stat, which provides a variety of economic data for OECD and certain non-member countries. You can also download the csv data manually by going to the website. However, you only need to use Python or R to do this since OECD provides a web API.\nBelow is a list of implementation details for specific OECD REST SDMX interfaces at this time.\n  Only anonymous queries are supported and there is no authentication.\n  Each response is limited to 1,000,000 observations.\n  The maximum length of the request URL is 1000 characters.\n  Cross-origin requests are supported in the CORS header (see here for more information about CORS).\n  Errors are not returned in the results, but HTTP status codes and messages are set according to the Web Service Guidelines.\n  If a nonexistent dataset is requested, 401 Unauthorized is returned.\n  The source (or Agency ID) parameter of the REST query is required, but the ALL keyword is supported.\n  Versioning is not supported: the latest implementation version is always used.\n  Sorting of data is not supported.\n  The lastNObservations parameter is not supported.\n  Even when dimensionAtObservation=AllDimensions is used, the observations follow a chronological (or import-specific) order.\n  Searching for reference metadata is not supported at this time.\n  2.pandasdmx The Web API is provided in the form of sdmx-json. There is a useful package for using it in Python, which is called pandasdmx. Here\u0026rsquo;s how to download the data.\n Import pandasdmx, pass OECD to Request method as an argument and create api.Request object. Pass the query condition to the data method of the api.Request object, and download the data of sdmx-json format from OECD.org. Format the downloaded data into a pandas data frame with the method to_pandas().  3.implementation Let\u0026rsquo;s do this in practice. What we\u0026rsquo;ll get is the Revisions Analysis Dataset -- Infra-annual Economic Indicators. We have access to all data, including revisions to the Monthly Ecnomic Indicator (MEI), one of the OECD datasets, so we can check key economic variables 1 from preliminary data at initial release to revised, finalized data. The dataset provides a snapshot of major economic indicators, at monthly intervals beginning in February 1999, that were previously available. In other words, the dataset allows us to build predictive models based on the data available at each point in time. The most recent data is useful, but it is preliminary and therefore subject to uncertainty. The problem is that this situation cannot be replicated when backtesting, and the analysis is often done under a better environment than the actual operation. This is the so-called Jagged Edge problem. I think this dataset is very useful because it can reproduce the actual operation situation. This time, you will get the following data items.\n   Indicators Statistical ID Frequency     Gross Domestic Product 101 Quarterly   Index of Industrial Production 201 Monthly   Retail Trade Volume 202 Monthly   Monetary Aggregates 601 Monthly   International Trade in Goods 702+703 Monthly   Balance of Payments 701 Quarterly   Employment 502 Monthly   Harmonised Unemployment Rates 501 Monthly   Hourly Earnings in Manufacturing 503 Monthly   Early Estimates of Unit Labor Cost 504 Quarterly   Production of Construction 203 Monthly    First, we define the functions. The arguments are database ID, other IDs (country IDs and statistical IDs), data acquisition start point and end point.\nimport pandasdmx as sdmx ## C:\\Users\\aashi\\ANACON~1\\lib\\site-packages\\pandasdmx\\remote.py:13: RuntimeWarning: optional dependency requests_cache is not installed; cache options to Session() have no effect\r## RuntimeWarning,\roecd = sdmx.Request(\u0026#39;OECD\u0026#39;) def resp_OECD(dsname,dimensions,start,end): dim_args = [\u0026#39;+\u0026#39;.join(d) for d in dimensions] dim_str = \u0026#39;.\u0026#39;.join(dim_args) resp = oecd.data(resource_id=dsname, key=dim_str + \u0026#34;/all?startTime=\u0026#34; + start + \u0026#34;\u0026amp;endTime=\u0026#34; + end) df = resp.to_pandas().reset_index() return(df) Specify the dimension from which the data will be obtained. Below, (1) country, (2) statistical items, (3) time of acquisition, and (4) frequency are specified with a tuple.\ndimensions = ((\u0026#39;USA\u0026#39;,\u0026#39;JPN\u0026#39;,\u0026#39;GBR\u0026#39;,\u0026#39;FRA\u0026#39;,\u0026#39;DEU\u0026#39;,\u0026#39;ITA\u0026#39;,\u0026#39;CAN\u0026#39;,\u0026#39;NLD\u0026#39;,\u0026#39;BEL\u0026#39;,\u0026#39;SWE\u0026#39;,\u0026#39;CHE\u0026#39;),(\u0026#39;201\u0026#39;,\u0026#39;202\u0026#39;,\u0026#39;601\u0026#39;,\u0026#39;702\u0026#39;,\u0026#39;703\u0026#39;,\u0026#39;701\u0026#39;,\u0026#39;502\u0026#39;,\u0026#39;503\u0026#39;,\u0026#39;504\u0026#39;,\u0026#39;203\u0026#39;),(\u0026#34;202001\u0026#34;,\u0026#34;202002\u0026#34;,\u0026#34;202003\u0026#34;,\u0026#34;202004\u0026#34;,\u0026#34;202005\u0026#34;,\u0026#34;202006\u0026#34;,\u0026#34;202007\u0026#34;,\u0026#34;202008\u0026#34;),(\u0026#34;M\u0026#34;,\u0026#34;Q\u0026#34;)) Let\u0026rsquo;s execute the function.\nresult = resp_OECD(\u0026#39;MEI_ARCHIVE\u0026#39;,dimensions,\u0026#39;2019-Q1\u0026#39;,\u0026#39;2020-Q2\u0026#39;) result.count() ## LOCATION 8266\r## VAR 8266\r## EDI 8266\r## FREQUENCY 8266\r## TIME_PERIOD 8266\r## value 8266\r## dtype: int64\rLet\u0026rsquo;s look at the first few cases of data.\nresult.head() ## LOCATION VAR EDI FREQUENCY TIME_PERIOD value\r## 0 BEL 201 202001 M 2019-01 112.5\r## 1 BEL 201 202001 M 2019-02 111.8\r## 2 BEL 201 202001 M 2019-03 109.9\r## 3 BEL 201 202001 M 2019-04 113.5\r## 4 BEL 201 202001 M 2019-05 112.1\rYou can see that the data is stored in tidy form (long type). The most right value is stored as a value, and the other indexes are\n LOCATION - Country VAR - Items EDI - At the time of acquisition (in the case of MEI_ARCHIVE) FREQUENCY - Frequency (monthly, quarterly, etc.) TIME_PERIOD - Reference point for statistics  Therefore, the same `TIME_PERIOD` exists in rows with different EDIs. For example, above you can see the data for 2019-01~2019-05 available as of 2020/01 for the Belgian (BEL) Industrial Production Index (201). This is very much appreciated as it is provided in Long format, which is also easy to visualize and regress. Here\u0026rsquo;s a visualization of the industrial production index as it is updated.\nimport seaborn as sns import matplotlib.pyplot as plt import pandas as pd result = result[result[\u0026#39;FREQUENCY\u0026#39;]==\u0026#39;M\u0026#39;] result[\u0026#39;TIME_PERIOD\u0026#39;] = pd.to_datetime(result[\u0026#39;TIME_PERIOD\u0026#39;],format=\u0026#39;%Y-%m\u0026#39;) sns.relplot(data=result[lambda df: (df.VAR==\u0026#39;201\u0026#39;) \u0026amp; (pd.to_numeric(df.EDI) \u0026gt; 202004)],x=\u0026#39;TIME_PERIOD\u0026#39;,y=\u0026#39;value\u0026#39;,hue=\u0026#39;LOCATION\u0026#39;,kind=\u0026#39;line\u0026#39;,col=\u0026#39;EDI\u0026#39;) plt.show() While we can see that the line graphs are depressed as the economic damage from the corona increases, there have been subtle but significant revisions to the historical values from preliminary to confirmed. We can also see that there is a lag in the release of statistical data by country. Belgium seems to be the slowest to release the data. When I have time, I would like to add a simple analysis of the forecasting model using this data.\n4. Another matter\u0026hellip; I passed the Python 3 Engineer Certification Data Analysis exam. It was pretty easy, with only a 70% pass rate, but it was a good opportunity to revisit the basics of Python. I haven\u0026rsquo;t even had the opportunity to use Python or R, let alone data analysis, in the work I\u0026rsquo;m doing now, so I\u0026rsquo;m considering the possibility of a career change. In the meantime, I plan to get the following qualifications by the end of this year, and I\u0026rsquo;ll be looking for a post where I can use my skills without focusing on finance. Like a diet, I need to declare and push myself.\n G Test Oracle Database Master Silver SQL Linuc level 1 Fundamental Information Technology Engineer Examination AWS Certified Solutions Architect - Associate  I will report on the status of my acceptance on my blog each time.\n  gross domestic product and its expenditure items, industrial production and construction output indices, balance of payments, composite leading indicators, consumer price index, retail turnover, unemployment rate, number of workers, hourly wages, money supply, trade statistics, etc. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603065600,"objectID":"c0bed9051b28d59784db7575607fe5fe","permalink":"/en/post/post22/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/en/post/post22/","section":"post","summary":"I used the OECD.org API to obtain macroeconomic data.","tags":["Python","Gaussian-regression","preprocessing","Web_scraping","API"],"title":"Get macro panel data from OECD.org via API","type":"post"},{"authors":null,"categories":["single-shot","statistics","programming"],"content":"\r\r0. What I want to do\rAs mentioned above, what I will show you this time is the pre-processing (and analysis) of currency tick data. I won’t go into details since my main focus is to improve efficiency of analysis using Rcpp, but I will give you a rough idea of what I want to do.\nWhat we want to do is to detect jumps in the 5-minute returns of the JPY/USD rate. A jump here is a sudden rise (or fall) in the exchange rate compared to the previous point. During the day, exchange rates move in small increments, but when there is an event, they rise (or fall) significantly. It is very interesting to see what kind of event causes a jump. In order to verify this, we first need to detect jumps. The following paper will be used as a reference.\nSuzanne S. Lee \u0026amp; Per A. Mykland, 2008. “Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,” Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.\nIt is a highly regarded paper with a Citation of 204. I will explain the estimation method in brief. First, let the continuous compound return be \\(d\\log S(t)\\) for \\(t\u0026gt;0\\). where \\(S(t)\\) is the price of the asset at \\(t\\). If there are no jumps in the market, \\(S(t)\\) is assumed to follow the following stochastic process\n\\[\rd\\log S(t) = \\mu(t)dt + \\sigma(t)dW(t) \\tag{1}\r\\]\nwhere \\(W(t)\\) is the standard Brownian motion, \\(\\mu(t)\\) is the drift term, and \\(\\sigma(t)\\) is the spot volatility. Also, when there is a Jump, \\(S(t)\\) is assumed to follow the following stochastic process\n\\[\rd\\log S(t) = \\mu(t)dt + \\sigma(t)dW(t) + Y(t)dJ(t) \\tag{2}\r\\]\nwhere \\(J(t)\\) and \\(W(t)\\) are counting processes independent of each other. 1 \\(Y(t)\\) represents the size of the jump and is a predictable process.\nNext, consider the logarithmic return of \\(S(t)\\). That is, \\(\\log S(t_i)/S(t_{i-1})\\), which follows a normal distribution \\(N(0, \\sigma(t_i))\\). 2 Now, we define the statistic \\(\\mathcal{L(i)}\\) at \\(t_i\\) when there is a jump from \\(t_{i-1}\\) to \\(t_{i}\\) as follows.\n\\[\r\\mathcal{L(i)} \\equiv \\frac{|\\log S(t_i)/S(t_{i-1})|}{\\hat{\\sigma}_{t_i}} \\tag{3}\r\\]\nThis is a simple standardization of the absolute value of the log return, but uses the “Realized Bipower Variation” defined below as an estimator of the standard deviation.\n\\[\r\\hat{\\sigma}_{t_i} = \\frac{1}{K-2}\\sum_{j=i-K+2}^{i-2}|\\log S(t_j)/\\log S(t_{j-1})||\\log S(t_{j-1})/\\log S(t_{j-2})| \\tag{4}\r\\]\nwhere \\(K\\) is the number of sample sizes contained in the Window. If we use a return in 5-minute increments and the jump occurs at 10:00 on 9/10/2020, and \\(K=270\\), then we will calculate using samples from the previous day, 9/9/2020 11:30 to 9/11/2020 09:55. What we’re doing is adding up the absolute value of the return multiplied by the absolute value of the return, which seems to make it difficult for the estimate of the next instant after the jump occurs (i.e., \\(t_{i+1}\\) and so on) to be affected by the jump. Incidentally, \\(K=270\\) is introduced in another paper as a recommended value for returns in 5-minute increments.\nLet’s move on to how the Jump statistic calculated in this way can be used in a statistical test to detect a Jump. This is done by considering the maximum value of \\(\\mathcal{L(i)}\\), and when a value deviates greatly from its distribution (such as the 95th percentile), the return is considered to be a Jump.\nIf we assume that there is no Jump in period \\([t_{i-1},t_i]\\), then let the length of this period \\(\\Delta=t_i-t_{i-1}\\)\rclose to 0, that is, \\(\\Delta\\rightarrow 0\\), the (absolute) maximum of the standard normal variable converges to the Gumbel distribution. Therefore, Jump can be detected if the null hypothesis is rejected when the following conditions are met.\n\\[\r\\mathcal{L(i)} \u0026gt; G^{-1}(1-\\alpha)S_{n} + C_{n} \\tag{5}\r\\]\nwhere \\(G^{-1}(1-\\alpha)\\) is the \\((1-\\alpha)\\) quantile function of the standard Gumbell distribution. If \\(\\alpha=10%\\), \\(G^{-1}(1-\\alpha)=2.25\\). Note that (We won’t derive it, but we can prove it using equations 1 and 2)\n\\[\rS_{n} = \\frac{1}{c(2\\log n)^{0.5}},~ \\\\\rC_{n} = \\frac{(2\\log n)^{0.5}}{c}-\\frac{\\log \\pi+\\log(\\log n)}{2c(2\\log n)^{0.5}}\r\\]\nwhere \\(c=(2/\\pi)^{0.5}\\) and \\(n\\) is the total sample size used for estimation.\rFinally, \\(Jump_{t_i}\\) is calculated by\n\\[\rJump_{t_i} = \\log\\frac{S(t_i)}{S(t_{i-1})}×I(\\mathcal{L(i)} - G^{-1}(1-\\alpha)S_{n} + C_{n})\\tag{6}\r\\]\nwhere \\(I(⋅)\\) is an Indicator function that returns 1 if the content is greater than 0 and 0 otherwise.\n\r1. Loading data\rSo now that we know how to estimate, let’s load the Tick data first. The data is csv from QuantDataManager and saved in a working directory.\nlibrary(magrittr)\r# Read Tick data\rstrPath \u0026lt;- r\u0026quot;(C:\\Users\\hogehoge\\JPYUSD_Tick_2011.csv)\u0026quot;\rJPYUSD \u0026lt;- readr::read_csv(strPath)\rOn an unrelated note, I recently upgraded R to 4.0.2, and I’m quite happy to say that with 4.0 and above, you can escape the strings made by Python, which relieves some of the stress I’ve been experiencing.\nThe data looks like the following: in addition to the date, the Bid value, Ask value and the volume of transactions are stored. Here, we use the 2011 tick. The reason for this is to cover the dollar/yen at the time of the Great East Japan Earthquake.\nsummary(JPYUSD)\r## DateTime Bid Ask Volume ## Min. :2011-01-03 07:00:00 Min. :75.57 Min. :75.58 Min. : 1.00 ## 1st Qu.:2011-03-30 15:09:23 1st Qu.:77.43 1st Qu.:77.44 1st Qu.: 2.00 ## Median :2011-06-15 14:00:09 Median :80.40 Median :80.42 Median : 2.00 ## Mean :2011-06-22 05:43:11 Mean :79.91 Mean :79.92 Mean : 2.55 ## 3rd Qu.:2011-09-09 13:54:51 3rd Qu.:81.93 3rd Qu.:81.94 3rd Qu.: 3.00 ## Max. :2011-12-30 06:59:59 Max. :85.52 Max. :85.54 Max. :90.00\rBy the way, DateTime includes the period from 07:00:00 on 2011/1/3 to 06:59:59 on 2011-12-30 (16:59:59 on 2011-12-30) in Japan by UTC. The sample size is approximately 12 million entries.\nNROW(JPYUSD)\r## [1] 11946621\r\r2. Preprocessing\rThen calculate the median value from Bid and Ask and take the logarithm to calculate the return later.\n# Calculate the median value of Ask and Bid and make it logarithmic (for calculating the logarithmic return).\rJPYUSD \u0026lt;- JPYUSD %\u0026gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %\u0026gt;% dplyr::mutate(logMid = log(Mid))\rI format the currently irregularly arranged trading data into 5-minute increments of returns. The way to do it is:\nCreate a POSIXct vector with 1 year chopped every 5 minutes.\rcreate a function that calculates the logarithmic return from the first and last sample in the window of 5min in turn, if you pass 1. as an argument.\rexecute. This is the plan. First, create the vector of 1.\r\r# Create POSIX vector to calculate returns in 5min increments (288 x days)\rstart \u0026lt;- as.POSIXct(\u0026quot;2011-01-02 22:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)\rend \u0026lt;- as.POSIXct(\u0026quot;2011-12-31 21:55:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)\rfrom \u0026lt;- seq(from=start,to=end,by=5*60)\rThen, let’s move on to 2. If you have 12 million data, even if you use purrr::map or apply with R, it takes a long time to call a function and it’s quite inefficient. I tried to use sapply, but it didn’t complete the process and it was forced to terminate. RCCp is useful in such a case. Although R has many very useful functions for graphs and statistics, it is not very good at large repetition, including calls to user-defined functions (because it is a scripting language, rather than a compiling language, I mean). So, I write the part of repetitive process in C++ and compile it as R function using Rcpp and execute it. It is very efficient to compile and visualize the results and write them in R. Also, Rccp helps you to write C++ in a similar way to R with less sense of discomfort. I think the following will give you a good idea of the details. It’s pretty well organized and is God, to say the least.\nRcpp for everyone\nNow, let’s write the code for the second step. In coding, I used articles on the net for reference. C++ has a longer history and more users than R, so you can find information you want to know.\n#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;algorithm\u0026gt;\rusing namespace Rcpp;\r//[[Rcpp::plugins(cpp11)]]\r// [[Rcpp::export]]\rDataFrame Rolling_r_cpp(\rDataFrame input, // Data frame of (measurement time, measured value data)\rnewDatetimeVector from, // The starting point vector for the timing of the calculation\rdouble time_window = 5*60) // Calculated window width (in seconds)\r{ // Extract the measurement time and value as a vector\rnewDatetimeVector time = input[\u0026quot;DateTime\u0026quot;]; // This time is assumed to be sorted in ascending order.\rNumericVector data = input[\u0026quot;logMid\u0026quot;];\r// The endpoint vector of the timing to be calculated\rnewDatetimeVector to = from + time_window;\r// Number to calculate\rR_xlen_t N = from.length();\r// vector for storage NumericVector value(N);\r// An object representing the position of a vector element\rnewDatetimeVector::iterator begin = time.begin();\rnewDatetimeVector::iterator end = time.end();\rnewDatetimeVector::iterator p1 = begin;\rnewDatetimeVector::iterator p2 = begin;\r// Loop for window i\rfor(R_xlen_t i = 0; i \u0026lt; N; ++i){\r// Rcout \u0026lt;\u0026lt; \u0026quot;i=\u0026quot; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;;\rdouble f = from[i]; // Time of the window\u0026#39;s start point\rdouble t = f + time_window; // The time of the window\u0026#39;s endpoint\r// If the endpoint of the window is before the first measurement time, it is NA or\r// If the starting point of the window is after the last measurement time, NA\rif(t \u0026lt;= *begin || f \u0026gt; *(end-1)){ value[i] = NA_REAL;\rcontinue;// Go to the next loop\r}\r// Vector time from position p1 and subsequent elements x\r// Let p1 be the position of the first element whose time is at the start point f \u0026quot;after\u0026quot; the window\rp1 = std::find_if(p1, end, [\u0026amp;f](double x){return f\u0026lt;=x;});\r// p1 = std::lower_bound(p1, end, f); //Same as above\r// Vector time from position p1 and subsequent elements x\r// Let p2 be the position of the last element whose time is \u0026quot;before\u0026quot; the endpoint t of the window\r// (In the below, this is accomplished by making the time one position before the \u0026#39;first element\u0026#39;, where the time is the window\u0026#39;s endpoint t \u0026#39;after\u0026#39;)\rp2 = std::find_if(p1, end, [\u0026amp;t](double x){return t\u0026lt;=x;}) - 1 ;\r// p2 = std::lower_bound(p1, end, t) - 1 ;//Same as above\r// Convert the position p1,p2 of an element to the element numbers i1, i2\rR_xlen_t i1 = p1 - begin;\rR_xlen_t i2 = p2 - begin; // Checking the element number\r// C++ starts with the element number 0, so I\u0026#39;m adding 1 to match the R\r// Rcout \u0026lt;\u0026lt; \u0026quot;i1 = \u0026quot; \u0026lt;\u0026lt; i1+1 \u0026lt;\u0026lt; \u0026quot; i2 = \u0026quot; \u0026lt;\u0026lt; i2+1 \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;;\r// Calculate the data in the relevant range\rif(i1\u0026gt;i2) {\rvalue[i] = NA_REAL; // When there is no data in the window\r} else { value[i] = data[i2] - data[i1];\r}\r// ↑You can create various window functions by changing above\r}\r// Output the calculated time and the value as a data frame.\rDataFrame out =\rDataFrame::create(\rNamed(\u0026quot;from\u0026quot;, from),\rNamed(\u0026quot;r\u0026quot;, value*100));\rreturn out;\r}\rIf you compile the program with Rcpp::sourceCpp, the function of R can be executed as follows.\nsystem.time(results \u0026lt;- Rolling_r_cpp(JPYUSD,from))\r## ユーザ システム 経過 ## 0.04 0.02 0.07\rIt takes less than a second to process 12 million records. So Convenient!!\nsummary(results)\r## from r ## Min. :2011-01-02 22:00:00 Min. :-1.823 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 ## Median :2011-07-03 09:57:30 Median : 0.000 ## Mean :2011-07-03 09:57:30 Mean : 0.000 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 ## Max. :2011-12-31 21:55:00 Max. : 2.880 ## NA\u0026#39;s :29977\rThe return is precisely calculated. The recommended length of the window is 270 in 5 min increments, but we’ll make it flexible as well. And we carefully process the NA.\n#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;cmath\u0026gt;\rusing namespace Rcpp;\r//[[Rcpp::plugins(cpp11)]]\r// [[Rcpp::export]]\rfloat rbv_cpp(\rNumericVector x, // Return vector to calculate rbv\rbool na_rm = true) // If NA is included in x, remove it and calculate it or\r{\r// Get the number of calculations\rR_xlen_t N = x.length();\r// Define variables to contain the results of a calculation\rfloat out = 0;\r// Check for missing x\rLogicalVector lg_NA = is_na(x);\r// If there is a NA in x, whether to exclude that NA and calculate\rif(any(lg_NA).is_true() and na_rm==FALSE){\rout = NA_REAL; // Output NA as a result of the calculation\r} else {\r// Excluding NA\rif (any(lg_NA).is_true() and na_rm==TRUE){\rx[is_na(x)==TRUE] = 0.00; // Fill in the NA with zeros and effectively exclude it from the calculation.\r}\r// Compute the numerator (sum of rbv)\rfor(R_xlen_t i = 1; i \u0026lt; N; ++i){\rout = out + std::abs(x[i])*std::abs(x[i-1]);\r}\r// Calculate the average and take the route.\rlong denomi; //denominator\rif(N-sum(lg_NA)-2\u0026gt;0){\rdenomi = N-sum(lg_NA)-2;\r} else {\rdenomi = 1;\r}\rout = out/denomi;\rout = std::sqrt(out);\r}\rreturn out;\r}\r// [[Rcpp::export]]\rDataFrame Rolling_rbv_cpp(\rDataFrame input, //Data frame of (measurement time, measured value data)\rint K = 270, // Rolling Window width to calculate\rbool na_pad = false, // Returning NA when the window width is insufficient\rbool na_remove = false // If the NA exists in the window width, exclude it from the calculation\r){\r// Extract the return vector and number of samples\rNumericVector data = input[\u0026quot;r\u0026quot;];\rR_xlen_t T = data.length();\r// Prepare a vector to store the results\rNumericVector value(T);\r// Calculate and store RBVs per Windows width\rif(na_pad==TRUE){\rvalue[0] = NA_REAL; // return NA.\rvalue[1] = NA_REAL; // return NA.\rvalue[2] = NA_REAL; // return NA.\r} else {\rvalue[0] = 0; // Return zero.\rvalue[1] = 0; // Return zero.\rvalue[2] = 0; // Return zero.\r}\rfor(R_xlen_t t = 3; t \u0026lt; T; ++t){\r// Bifurcation of the process depending on whether or not there is enough Windows width\rif (t-K\u0026gt;=0){\rvalue[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // Run a normal calculation\r} else if(na_pad==FALSE) {\rvalue[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Run a calculation with an incomplete Widnows width of less than K\r} else {\rvalue[t] = NA_REAL; // return NA.\r}\r}\r// Output the calculated time and value as a data frame.\rDataFrame out =\rDataFrame::create(\rNamed(\u0026quot;from\u0026quot;, input[\u0026quot;from\u0026quot;]),\rNamed(\u0026quot;r\u0026quot;, data),\rNamed(\u0026quot;rbv\u0026quot;,value));\rreturn out;\r}\rNow, compile it and run it with R.\nsystem.time(results \u0026lt;- results %\u0026gt;% Rolling_rbv_cpp(na_remove = FALSE))\r## ユーザ システム 経過 ## 1.00 0.36 1.36\rSo fast!\n\r3. Calculating Jump Statistics\rNow let’s calculate the statistic \\(\\mathcal{L}_{t_i}\\) from the returns and standard deviation we just calculated.\n# Standardize the absolute value of the log return = Jump statistic\rresults \u0026lt;- results %\u0026gt;% dplyr::mutate(J=ifelse(rbv\u0026gt;0,abs(r)/rbv,NA))\rThis is what it looks like now.\nsummary(results)\r## from r rbv J ## Min. :2011-01-02 22:00:00 Min. :-1.823 Min. :0.00 Min. : 0.00 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 1st Qu.:0.02 1st Qu.: 0.28 ## Median :2011-07-03 09:57:30 Median : 0.000 Median :0.02 Median : 0.64 ## Mean :2011-07-03 09:57:30 Mean : 0.000 Mean :0.03 Mean : 0.93 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 3rd Qu.:0.03 3rd Qu.: 1.23 ## Max. :2011-12-31 21:55:00 Max. : 2.880 Max. :0.16 Max. :58.60 ## NA\u0026#39;s :29977 NA\u0026#39;s :44367 NA\u0026#39;s :44423\rNow let’s move on to the Jump test. First, we need to define the useful functions.\n# Preparing Constants \u0026amp; Functions for Calculating Jump Test\rc \u0026lt;- (2/pi)^0.5\rCn \u0026lt;- function(n){\rreturn((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5))\r}\rSn \u0026lt;- function(n){\r1/(c*(2*log(n))^0.5)\r}\rNow we perform the test. Rejected samples return 1 and all others 0.\n# Perform a jump test (10%) (return value is logical)\rN \u0026lt;- NROW(results$J)\rresults \u0026lt;- results %\u0026gt;% dplyr::mutate(Jump = J \u0026gt; 2.25*Sn(N) + Cn(N))\rsummary(results)\r## from r rbv J ## Min. :2011-01-02 22:00:00 Min. :-1.823 Min. :0.00 Min. : 0.00 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 1st Qu.:0.02 1st Qu.: 0.28 ## Median :2011-07-03 09:57:30 Median : 0.000 Median :0.02 Median : 0.64 ## Mean :2011-07-03 09:57:30 Mean : 0.000 Mean :0.03 Mean : 0.93 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 3rd Qu.:0.03 3rd Qu.: 1.23 ## Max. :2011-12-31 21:55:00 Max. : 2.880 Max. :0.16 Max. :58.60 ## NA\u0026#39;s :29977 NA\u0026#39;s :44367 NA\u0026#39;s :44423 ## Jump ## Mode :logical ## FALSE:59864 ## TRUE :257 ## NA\u0026#39;s :44423 ## ## ## \r\r4. Visualization using ggplot2\rNow that the numbers have been calculated, let’s visualize them by plotting the intraday logarithmic return of JPY/USD in 5-minute increments for 2011/03/11 and the jump. By the way, the horizontal axis has been adjusted to Japan time.\n# Plotting about Jump at the time of the 2011/03/11 Great East Japan Earthquake\rresults %\u0026gt;% dplyr::filter(from \u0026gt;= as.POSIXct(\u0026quot;2011-03-11 00:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;),from \u0026lt; as.POSIXct(\u0026quot;2011-03-12 00:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x=from,y=r)) +\rggplot2::geom_path(linetype=3) +\rggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=\u0026quot;red\u0026quot;)) +\rggplot2::scale_x_datetime(date_breaks = \u0026quot;2 hours\u0026quot;, labels = scales::date_format(format=\u0026quot;%H:%M\u0026quot;,tz=\u0026quot;Asia/Tokyo\u0026quot;)) +\rggplot2::ggtitle(\u0026quot;JPY/USD Jumps within Tohoku earthquake on 2011-3-11\u0026quot;)\r## Warning: Removed 36 row(s) containing missing values (geom_path).\r## Warning: Removed 36 row(s) containing missing values (geom_path).\rI’ve spent quite a bit of time writing this, and it’s 23:37 right now, so I’ll refrain from discussing it in depth, but since the earthquake occurred at 14:46:18, you can see that the market reacted to the weakening of the yen immediately after the disaster. After that, for some reason, the yen moved higher and peaked at 19:00. It is said that the yen is a safe asset, but this is the only time it is not safe given the heightened uncertainty.\n\r5. Summary\rI introduced the use of Rcpp to improve the efficiency of R analysis. The C++ is much faster than R even if you write the code in a simple way, so it is hard to make coding mistakes. Also, even if a compile error occurs, RStudio gives you a clue as to where the compile error is occurring, so there is no stress in that respect either, which is why I recommend it.\n\r\rA stochastic process with non-negative, integer, non-decreasing values.↩︎\n\rThe mean is not necessarily zero, depending on the shape of the drift term, but we are assuming a small enough drift term now. In the paper, it is defined more precisely.↩︎\n\r\r\r","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"a2572500921f2507d6076f96260ed768","permalink":"/en/post/post21/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/en/post/post21/","section":"post","summary":"Hi. I'm doing an analysis using currency tick data, and the sample size is 12 million data per year. It's in memory, but it's very inefficient because it takes a lot of time to process each one of them when we try to do complicated processing. This time, I would like to introduce the way to compile functions written in C++ as functions on R using Rcpp package to increase the processing speed.","tags":["R","C++","preprocessing","finance"],"title":"Rcpp to speed up data handling (using Tick data processing as an example)","type":"post"},{"authors":null,"categories":["horse racing"],"content":"Hi. In the last post, I built a model to predict the order of a racehorse using CNN based on its body image. The results were not so good, and we needed to refine our analysis, especially when we analyzed the factors using shap values, we found that the horses responded more to the stables in the background than to their bodies. This time, I would like to use Pytorch\u0026rsquo;s Pre-trained model to cut out the background from the horse photo and re-analyze the photo with only the horse body.\n1. Downloading the pre-trained model The code is adapted from here. First, install the packages.\nimport numpy as np import cv2 import matplotlib.pyplot as plt import torch import torchvision from torchvision import transforms import glob from PIL import Image import PIL import os Next, install the pre-trained model.\n#Install a pre-trained model device = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True) model = model.to(device) model.eval() Apparently, all pre-trained models assume a mini-batch of 3-channel RGB images of shape \\((N, 3, H, W)\\) normalized in the same way. Here, \\(N\\) is assumed to be the number of images and \\(H\\) and \\(W\\) are assumed to be at least 224 pixels. The images need to be scaled to a range of [0, 1] and then normalized using the mean value = [0.485, 0.456, 0.406] and the standard value = [0.229, 0.224, 0.225]. So, we define a function that does the preprocessing.\n#preprocessing preprocess = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ]) 2. Executing the Background Deletion Process Now, let\u0026rsquo;s load the images collected by the selenium code in the previous post and remove the background one by one.\n#Specify a folder folders = os.listdir(r\u0026#34;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026#34;) #Reads an image from each folder and converts it to a numpy array of RGB values using the Image function for i, folder in enumerate(folders): files = glob.glob(\u0026#34;C:/Users/aashi/umanalytics/photo/image/\u0026#34; + folder + \u0026#34;/*.jpg\u0026#34;) index = i for k, file in enumerate(files): img_array = np.fromfile(file, dtype=np.uint8) img = cv2.imdecode(img_array, cv2.IMREAD_COLOR) h,w,_ = img.shape input_tensor = preprocess(img) input_batch = input_tensor.unsqueeze(0).to(device) with torch.no_grad(): output = model(input_batch)[\u0026#39;out\u0026#39;][0] output_predictions = output.argmax(0) mask_array = output_predictions.byte().cpu().numpy() Image.fromarray(mask_array*255).save(r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\\mask.jpg\u0026#39;) mask = cv2.imread(r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\\mask.jpg\u0026#39;) bg = np.full_like(img,255) img = cv2.multiply(img.astype(float), mask.astype(float)/255) bg = cv2.multiply(bg.astype(float), 1.0 - mask.astype(float)/255) outImage = cv2.add(img, bg) Image.fromarray(outImage.astype(np.uint8)).convert(\u0026#39;L\u0026#39;).save(file) The following mask image is output using the pre-trained model, and the numpy array and the mask image are merged to create a background delete image. The output looks like the following.\nplt.gray() plt.figure(figsize=(20,20)) plt.subplot(1,3,1) plt.imshow(img) plt.subplot(1,3,2) plt.imshow(mask) plt.subplot(1,3,3) plt.imshow(outImage) plt.show() plt.close() Here\u0026rsquo;s what the folders look like. Some of them are handled well and some of them show the trainer. I know there is a way to identify the objects and mask only the horses, but this model doesn\u0026rsquo;t allow for object labeling, so we\u0026rsquo;ll continue with that.\n3. Analysis using CNN Here is the same content as in the previous article. I will only post the results.\n## Test accuracy: 0.0\r## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000002F791188\u0026gt;\rThe results have been disastrous. I have not been able to identify it at all. Aren\u0026rsquo;t there any characteristics in the horse photos that would predict the rankings? Or is it that there is no variation in the photos of the horses in G1 races and it is impossible to identify them? Either way, it seems a bit harsh.\n4. Interpretation of results using Shap values As before, let\u0026rsquo;s see how it fails using the shap value. We will use this image as an example.\nplt.imshow(X_test[4]) plt.show() plt.close() import shap background = X_resampled[np.random.choice(X_resampled.shape[0],100,replace=False)] e = shap.GradientExplainer(model,background) shap_values = e.shap_values(X_test[[4]]) shap.image_plot(shap_values[1],X_test[[4]]) They seem to be evaluating from the paws to the face. Surprisingly, they do not seem to be evaluating the buttocks. I would like to try to visualize which aspect of the image is captured in each layer.\nfrom keras import models layer_outputs = [layer.output for layer in model.layers[:8]] layer_names = [] for layer in model.layers[:8]: layer_names.append(layer.name) images_per_row = 16 activation_model = models.Model(inputs=model.input, outputs=layer_outputs) activations = activation_model.predict(X_train[[0]]) for layer_name, layer_activation in zip(layer_names, activations): n_features = layer_activation.shape[-1] size = layer_activation.shape[1] n_cols = n_features // images_per_row display_grid = np.zeros((size * n_cols, images_per_row * size)) for col in range(n_cols): for row in range(images_per_row): channel_image = layer_activation[0, :, :, col * images_per_row + row] channel_image -= channel_image.mean() channel_image /= channel_image.std() channel_image *= 64 channel_image += 128 channel_image = np.clip(channel_image, 0, 255).astype(\u0026#39;uint8\u0026#39;) display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image scale = 1. / size plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0])) plt.title(layer_name) plt.grid(False) plt.imshow(display_grid, cmap=\u0026#39;viridis\u0026#39;) plt.show() plt.close() I still can\u0026rsquo;t understand this one.\n5.Summary I removed the stable background and rerun it, but the results were the same - it was a good experience using PyTorch and removing the background, but not with any results, so I\u0026rsquo;ll stop with the horse photos for now.\n","date":1597190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597190400,"objectID":"cc3bd2ae9c58d0a37a0f6159c997c020","permalink":"/en/post/post20/","publishdate":"2020-08-12T00:00:00Z","relpermalink":"/en/post/post20/","section":"post","summary":"I cropped it because the background of the stables was in the way of the horse model I made in the previous post.","tags":["Python","machine_learning","preprocessing"],"title":"Automatically cropping the background of a horse body photo with Pytorch's Pre-trained model","type":"post"},{"authors":null,"categories":["single-shot","work-related","statistics"],"content":"\r\r\r1. What is “Backtesting”?\r2. Backtesting overfits.\r3. What is the distribution that the Sharpe ratio follows?\r4. Trying to derive the minimum backtest length\r4. At the end\r\r\r1. What is “Backtesting”?\rBacktesting is an algorithmic historical simulation of an investment strategy. Backtesting uses an algorithm to calculate the gains and losses that would have been incurred if the investment strategy you have drafted had been implemented over a period of time. Common statistics that evaluate the performance of an investment strategy, such as the Sharpe ratio and the information ratio, are used. Investors typically examine these back-testing statistics to determine asset allocation to the best performing investment (management) strategies, so asset managers do trial-and-error back-testing of a bloody number of times for good performance and present customer by that materials.\nFrom an investor’s perspective, it is important to distinguish between in-sample (IS) and out-of-sample (OOS) performance of a back-tested investment strategy; IS performance is defined as the sample used to design the investment strategy (referred to in the machine learning literature as the “training period” or “training set”) It is what is called “OOS”) simulated in a sample (aka “test set”). OOS performance, on the other hand, is simulated on a sample (aka “test set”) that was not used to design the investment strategy. Since backtesting predicts the effectiveness of an investment strategy with its performance, it can be guaranteed to be reproducible and realistic if the IS performance matches the OOS performance. However, it is difficult to judge whether the backtest is reliable when you receive it because the results of the out-sample are future results. It cannot be called a pure out-sample as long as the results of the OOS can be fed back to improve the strategy.\nSo, when you receive a backtest with good results from a fund manager, it is very important that you manage to assess how realistic the simulation is. It is also important for fund managers to understand the uncertainty of their own backtesting results. In this post, we will look at how to evaluate the realism of a backtesting simulation and what to look out for in order to ensure a repeatable backtest.\n\r2. Backtesting overfits.\rBailey, Borwein, López de Prado and Zhu (2015) argue that it is (relatively) easy to overfit (overlearn) a backtesting simulation for any financial time series. Here, overfitting is a machine learning concept that describes a situation in which a model focuses on a particular set of observed data (IS data) rather than on a general structure.\nBailey et. al. (2015) cite a situation where the backtesting results of a stock strategy are not good as an example of this claim. As the name implies, backtesting uses historical data, so it is possible to identify specific stocks that are experiencing losses and design the trading system to improve performance by adding some parameters to remove recommendations for those stocks ( (a technique known as “data snooping”). With a few repetitions of the simulation, you can derive “optimal parameters” that benefit from characteristics that are present in a particular sample but may be rare in the population.\nThere is a vast accumulation of research in the machine learning literature to address the problem of overfitting. But Bailey et. al. (2015) argue that the methods proposed in the context of machine learning are generally not applicable to multiple investment problems. The reasons for this seem to be the following four points.\nMachine learning methods to prevent overfitting require explicit point estimates and confidence intervals in the domain in which the event is defined in order to assess the explanatory power and quality of the prediction, because few investment strategies make such explicit predictions.\n\rFor example, it is not often said that “the E-mini S\u0026amp;P 500 is expected to be around 1,600 at 5 points per standard deviation as of Friday’s close,” but rather qualitative recommendations such as “buy” or “strong buy” are usually provided. Moreover, these forecasts do not specify the expiration date of the forecast and are subject to change when some unexpected event occurs.\r\rEven if a particular investment strategy relies on a prediction formula, other components of the investment strategy may be overfitted.\n\rIn other words, there are many ways to overfit an investment strategy other than simply adjusting the prediction equation.\r\rThe methods of regression overfitting are parametric and involve many assumptions about data that are unobservable in the case of finance.\n\rSome methods do not control for the number of trials.\n\r\rBailey et. al. (2015) show that a comparatively low number of trials is needed to identify investment strategies with relatively poor backtesting performance. Think of the number of trials here as the number of trials and errors. It also calculates the minimum backtest length (MinBTL), which is the length of the backtest required for the number of trials. In this paper, the Sharpe ratio is always used to evaluate performance, but it can be applied to other performance measures as well. Let’s take a look at what it does.\n\r3. What is the distribution that the Sharpe ratio follows?\rTo derive the MinBTL, we first derive the (asymptotic) distribution of the Sharpe ratio. To begin with, the design of an investment strategy usually starts with the prior knowledge or belief that a particular pattern may help to predict the future value of a financial variable. For example, if you are aware of the lead-lag effect between bonds of different maturities, you can design a strategy that bets on a return to the equilibrium value if the yield curve rises. This model could take the form of a cointegration equation, a vector error correction model, or a system of stochastic differential equations.\nThe number of such model configurations (or trials) is vast, and fund managers naturally want to choose the one that maximizes the performance of their strategy, and to do so they conduct historical simulations (back testing) (see above). Backtesting evaluates the optimal sample size, frequency of signal updates, risk sizing, stop loss, maximum holding period, etc., in combination with other variables.\nThe Sharpe ratio used as a measure of performance assessment in this paper is a statistic that assesses the performance of a strategy based on a sample of historical returns, defined as the average excess return/standard deviation (risk) to BM. It is usually interpreted as “return to risk 1 standard deviation” and, depending on the asset class, if it is greater than 1, it can be considered a very good strategy. In the following, we will assume that the excess return \\(r_t\\) of a strategy is a random variable of i.i.d. and follows a normal distribution. That is, we assume that the distribution of \\(r_t\\) is independent of \\(r_s(t\\neq s)\\). It is not a very realistic assumption, though.\n\\[\rr_t \\sim \\mathcal{N}(\\mu,\\sigma^2)\r\\]\rwhere \\(\\mathcal{N}\\) is the normal distribution of the mean \\(\\mu\\) and the variance \\(\\sigma^2\\). Now, the excess return \\(r_{t}(q)\\) at time t~t-q+1 is defined (I’m ignoring the compounding part.)\n\\[\rr_{t}(q) \\equiv r_{t} + r_{t-1} + ... + r_{t-q+1}\r\\]\rThen, the annualized Sharpe ratio is represented\n\\[\r\\begin{eqnarray}\rSR(q) \u0026amp;=\u0026amp; \\frac{E[r_{t}(q)]}{\\sqrt{Var(r_{t}(q))}}\\\\\r\u0026amp;=\u0026amp; \\frac{q\\mu}{\\sqrt{q}\\sigma}\\\\\r\u0026amp;=\u0026amp; \\frac{\\mu}{\\sigma}\\sqrt{q}\r\\end{eqnarray}\r\\]\nWe can express this as where \\(q\\) is the number (frequency) of returns per year. For example, \\(q=365\\) for daily returns (excluding leap years).\nThe true value of \\(SR\\) cannot be known because \\(\\mu\\) and \\(\\sigma\\) are generally unknown. So, let \\(R_t\\) be the sample return and the risk-free rate \\(R^f\\)(constant), we calculate the estimated Sharpe ratio by the sample mean of \\(\\hat{\\mu}=1/T\\sum_{t=1}^T R_{t}-R^f\\) and the sample standard deviation of \\(\\hat{\\sigma}=\\sqrt{1/T\\sum_{t=1}^{T}(R_{t}-\\hat{\\mu})}\\) (where \\(T\\) is the sample size to be back-tested).\n\\[\r\\hat{SR}(q) = \\frac{\\hat{\\mu}}{\\hat{\\sigma}}\\sqrt{q}\r\\]\rAs an inevitable consequence, the calculation of \\(SR\\) is likely to be accompanied by considerable estimation errors. Now, let us derive the main topic of this section, the asymptotic distribution of \\(\\hat{SR}\\). First, since the asymptotic distributions of \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}^2\\) are finite and i.i.d., applying the central limit theorem derives\n\\[\r\\sqrt{T}\\hat{\\mu}\\sim^{a}\\mathcal{N}(\\mu,\\sigma^2), \\\\\r\\sqrt{T}\\hat{\\sigma}^2\\sim^a\\mathcal{N}(\\sigma^2,2\\sigma^4)\r\\]\rSince the Sharpe ratio is a random variable which is calculated from this \\(\\hat{\\mu}\\) and \\(\\hat{\\sigma}^2\\), let us denote this function as \\(g(\\hat{{\\boldsymbol \\theta}})\\) where \\(\\hat{{\\boldsymbol \\theta}}=(\\hat{\\mu},\\hat{\\sigma}^2)\\). Now, since it is i.i.d., \\(\\hat{{\\boldsymbol \\theta}}\\) is independent of each other, and, from the above discussion, the asymptotic joint distribution is written as\n\\[\r\\sqrt{T}\\hat{{\\boldsymbol \\theta}} \\sim^a \\mathcal{N}({\\boldsymbol \\theta},{\\boldsymbol V_{\\boldsymbol \\theta}})\r\\]\rwhere \\({\\boldsymbol V_{\\boldsymbol \\theta}}\\) is\n\\[\r{\\boldsymbol V_{\\boldsymbol \\theta}} = \\left( \\begin{array}{cccc}\r\\sigma^2 \u0026amp; 0\\\\\r0 \u0026amp; 2\\sigma^4\\\\\r\\end{array}\r\\right)\r\\]\rSince the sharp ratio estimates are now only a function of \\(g(\\hat{{\\boldsymbol \\theta}})\\) and \\(\\hat{{\\boldsymbol \\theta}}\\), from the delta method, the following\n\\[\r\\hat{SR} = g(\\hat{{\\boldsymbol \\theta}}) \\sim^a \\mathcal{N}(g({\\boldsymbol \\theta}),\\boldsymbol V_g)\r\\]\rasymptotically follows a normal distribution where \\(\\boldsymbol V_g\\) is\n\\[\r\\boldsymbol V_g=\\frac{\\partial g}{\\partial{\\boldsymbol \\theta}}{\\boldsymbol V_{\\boldsymbol \\theta}}\\frac{\\partial g}{\\partial{\\boldsymbol \\theta}\u0026#39;}\r\\]\rSince \\(g({\\boldsymbol \\theta})=\\mu/\\sigma\\), then\n\\[\r\\frac{\\partial g}{\\partial{\\boldsymbol \\theta}\u0026#39;} = \\left[ \\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\\\\r\\frac{\\partial g}{\\partial \\sigma^2}\\\\\r\\end{array}\r\\right]\r= \\left[ \\begin{array}{cccc}\r\\frac{1}{\\sigma}\\\\\r-\\frac{\\mu}{2\\sigma^3}\\\\\r\\end{array}\r\\right]\r\\]\rTherefore, you can derive\n\\[\r\\begin{eqnarray}\r\\boldsymbol V_g \u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}, \\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right)\r\\left( \\begin{array}{cccc}\r\\sigma^2 \u0026amp; 0\\\\\r0 \u0026amp; 2\\sigma^4\\\\\r\\end{array}\r\\right)\r\\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\\\\r\\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right) \\\\\r\u0026amp;=\u0026amp; \\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\sigma^2, \\frac{\\partial g}{\\partial \\sigma}2\\sigma^4\\\\\r\\end{array}\r\\right)\r\\left(\r\\begin{array}{cccc}\r\\frac{\\partial g}{\\partial \\mu}\\\\\r\\frac{\\partial g}{\\partial \\sigma}\\\\\r\\end{array}\r\\right) \\\\\r\u0026amp;=\u0026amp; (\\frac{\\partial g}{\\partial \\mu})^2\\sigma^2 + (\\frac{\\partial g}{\\partial \\sigma})^2\\sigma^4 \\\\\r\u0026amp;=\u0026amp; 1 + \\frac{\\mu^2}{2\\sigma^2} \\\\\r\u0026amp;=\u0026amp; 1 + \\frac{1}{2}SR^2\r\\end{eqnarray}\r\\]\rYou may need to be careful when you see good performance because the variance tends to be exponentially larger as the absolute value of the Sharpe ratio increases. Here’s the distribution that the annualized Sharpe ratio estimate \\(\\hat{SR}(q)\\) follows\n\\[\r\\hat{SR}(q)\\sim^a \\mathcal{N}(\\sqrt{q}SR,\\frac{V(q)}{T}) \\\\\rV(q) = q{\\boldsymbol V}_g = q(1 + \\frac{1}{2}SR^2)\r\\]\rNow, if \\(y\\) is the number of years of backtesting, we can write \\(T=yq\\) and use this to rewrite the above equation as follows (for a 3-year measurement with daily returns, the sample size \\(T\\) is \\(T=3×365=1095\\)).\n\\[\r\\hat{SR}(q)\\sim^a \\mathcal{N}(\\sqrt{q}SR,\\frac{1+\\frac{1}{2}SR^2}{y}) \\tag{1}\r\\]\rThe frequency \\(q\\) affects the mean of the Sharpe ratio, but not the variance. We can now derive an asymptotic distribution of the Sharpe ratio estimates. Now, what do we wanted to do with this? We were thinking about the reliability of the backtest. In other words, what is the probability that a backtest of \\(N\\) investment strategy ideas that FM twists and turns to develop a new product will produce a very high (good) value even though the true value of all of those Sharpe ratios is zero. In Bailey et. al. (2015), it was described as follows\nHow high is the expected maximum Sharpe ratio IS among a set of strategy configurations where the true Sharpe ratio is zero?\nWe also want to know how long you should be backtesting for in order to reduce the value of the expected maximum Sharpe ratio.\n\r4. Trying to derive the minimum backtest length\rThe situation we are considering now is that let \\(\\mu=0\\) and \\(y\\) 1 year for simplicity, then from equation (1) \\(\\hat{SR}(q)\\) follows the standard normal distribution \\(\\mathcal{N}(0,1)\\). Now, we will consider the expected value of the maximum value \\(\\max[\\hat{SR}]_N\\) of \\(\\hat{SR}_n(n=1,2,...N)\\), but as those with good instincts will have noticed, the discussion goes into the context of extreme value statistics. Since \\(\\hat{SR}_n\\sim\\mathcal{N}(0,1)\\) is i.i.d., the extreme value distribution of that maximum statistic becomes Gumbel distribution from the Fisher-Tippett-Gnedenko theorem (sorry, I haven’t been able to follow the proof).\n\\[\r\\lim_{N\\rightarrow\\infty}prob[\\frac{\\max[\\hat{SR}]_N-\\alpha}{\\beta}\\leq x] = G(x) = e^{-e^{-x}}\r\\]\rwhere \\(\\alpha=Z(x)^{-1}[1-1/N], \\beta=Z(x)^{-1}[1-1/Ne^{-1}]-\\alpha\\), and \\(Z(x)\\) represents the cumulative distribution function of the standard normal distribution. The moment generating function of the Gumbel distribution \\(M_x(t)\\) is written as\n\\[\r\\begin{eqnarray}\rM_x(t) \u0026amp;=\u0026amp; E[e^{tx}] = \\int_{-\\infty}^\\infty e^{tx}e^{-x}e^{-e^{-x}}dx \\\\\r\\end{eqnarray}\r\\]\rConverting variables with \\(x=-\\log(y)\\) gives \\(dx/dy=-1/y=-(e^{-x})^{-1}\\), so\n\\[\r\\begin{eqnarray}\rM_x(t) \u0026amp;=\u0026amp; \\int_{\\infty}^0-e^{-t\\log(y)}e^{-y}dy \\\\\r\u0026amp;=\u0026amp; \\int_{0}^\\infty y^{-t}e^{-y}dy \\\\\r\u0026amp;=\u0026amp; \\Gamma(1-t)\r\\end{eqnarray}\r\\]\rwhere \\(\\Gamma(x)\\) is a gamma function. From here, the expected value (mean) of the standardized maximum statistic is\n\\[\r\\begin{eqnarray}\r\\lim_{N\\rightarrow\\infty} E[\\frac{\\max[\\hat{SR}]_N-\\alpha}{\\beta}] \u0026amp;=\u0026amp; M_x\u0026#39;(t)|_{t=0} \\\\\r\u0026amp;=\u0026amp; (-1)\\Gamma\u0026#39;(1) \\\\\r\u0026amp;=\u0026amp; (-1)(-\\gamma) = \\gamma\r\\end{eqnarray}\r\\]\rHere, $… $ is the Euler-Mascheroni constant. Thus, when \\(N\\) is large, the expected value of the maximum statistic of the standard normal distribution of i.i.d. is approximated as (\\(N\u0026gt;1\\))\n\\[\rE[\\max[\\hat{SR}]] \\approx \\alpha + \\gamma\\beta = (1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}] \\tag{2}\r\\]\rThis is Proposition 1 of Bailey et. al. (2015). We plot \\(E[\\max[\\hat{SR}]]\\) as a function of the number of strategies (number of trials and errors) \\(N\\), which is shown below.\nlibrary(ggplot2)\r## Warning: パッケージ \u0026#39;ggplot2\u0026#39; はバージョン 4.0.3 の R の下で造られました\rExMaxSR = function(N){\rgamma_ct = -digamma(1)\rZ = qnorm(0.99)\rreturn((1-gamma_ct)*Z*(1-1/N) + gamma_ct*Z*(1-1/N*exp(1)^{-1}))\r}\rN = list(0:100)\rresult = purrr::map(N,ExMaxSR)\rggplot2::ggplot(data.frame(ExpMaxSR = unlist(result),N = unlist(N)),aes(x=N,y=ExpMaxSR)) +\rgeom_line(size=1) + ylim(0,3)\rYou can see that the expected value of \\(\\max[\\hat{SR}]\\) is rapidly increasing for the small \\(N\\). At \\(N=10\\), we expect to find at least one apparently quite good performing strategy, even though the true value of the Sharpe ratio of all strategies is zero. In finance, hold-out back-testing is often used, but this method does not take into account the number of trials, which is why it does not return reliable results when \\(N\\) is large. Don’t you think it’s very risky to do a lot of simulations in order to improve the backtest results? In the end, only the best performing of the \\(N\\) strategies will show up in the presentation material, so even if you consider 10 strategies, as in this example, any of them will have a Sharpe ratio distributed around 1.87. Of course, we don’t include the number of trials and errors in our materials, so it’s very misleading. When evaluating these materials, you might want to suspect false positives first.\nSo, as for what to do, Bailey et. al. (2015) calculate the Minimum Backtest Length. In short, they caution that as the number of trials (errors) \\(N\\) is increased, the number of years of backtesting \\(y\\) should also increase. Let’s show the relationship between \\(N\\) and the Minimum Backtest Length. As before, we will assume that \\(\\mu=0\\), but consider the case with \\(y\\neq 1\\). The expected value of the maximum statistic of the annualized Sharpe ratio is from equation (2)\n\\[\rE[\\max[\\hat{SR}(q)]_N] \\approx y^{-1/2}((1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}])\r\\]\rBy solving this for \\(y\\), you can get MinBTL.\n\\[\rMinBTL \\approx (\\frac{(1-\\gamma)Z^{-1}[1-\\frac{1}{N}]+\\gamma Z^{-1}[1-\\frac{1}{N}e^{-1}]}{\\bar{E[\\max[\\hat{SR}(q)]_N]}})^2\r\\]\rHere, \\(\\bar{E[\\max[\\hat{SR}(q)]_N]}\\) is the upper limit of \\(E[\\max[\\hat{SR}(q)]_N]\\), and the \\(N\\) strategy, where the true value of the Sharpe ratio is zero, suppresses the value that the maximum Sharpe ratio statistic can take. In doing so, the required backtesting years \\(y\\) are derived as MinBTL. Plotting the MinBTL as a function of \\(N\\), with \\(\\bar{E[\\max[\\hat{SR}(q)]_N]}=1\\) is shown below.\nMinBTL \u0026lt;- function(N,MaxSR){\rreturn((ExMaxSR(N)/MaxSR)^2)\r}\rN = list(1:100)\rresult = purrr::map2(N,1,MinBTL)\rggplot2::ggplot(data.frame(MinBTL = unlist(result),N = unlist(N)),aes(x=N,y=MinBTL)) +\rgeom_line(size=1) + ylim(0,6)\rsimSR \u0026lt;- function(T1){\rr = rnorm(T1)\rreturn(mean(r)/sd(r))\r}\rIf your backtesting period is less than 3 years, the number of trials (or errors) \\(N\\) should be limited to one. It is important to note that even if you are backtesting within the MinBTL, it is still possible to overfit. In other words, the MinBTL is a necessary condition, not a sufficient condition.\n\r4. At the end\rLópez de Prado (2018) lists the following as generic means of preventing overfitting\nDevelop models for entire asset classes or investment universes, rather than for specific securities. Investors diversify, hence they do not make mistake X only on security Y. If you find mistake X only on security Y, no matter how apparently profitable, it is likely a false discovery.\n\rApply bagging as a means to both prevent overfitting and reduce the variance of the forecasting error. If bagging deteriorates the performance of a strategy, it was likely overfit to a small number of observations or outliers.\n\rDo not backtest until all your research is complete.\n\rRecord every backtest conducted on a dataset so that the probability of backtest overfitting may be estimated on the final selected result (see Bailey, Borwein, López de Prado and Zhu(2017)), and the Sharpe ratio may be properly deflated by the number of trials carried out (Bailey and López de Prado(2014.b)).\n\rSimulate scenarios rather than history. A standard backtest is a historical simulation, which can be easily overfit. History is just the random path that was realized, and it could have been entirely different. Your strategy should be profitable under a wide range of scenarios, not just the anecdotal historical path. It is harder to overfit the outcome of thousands of “what if” scenarios.\n\rDo not research under the influence of a backtest. If the backtest fails to identify a profitable strategy,\rstart from scratch. Resist the temptation of reusing those results.\n\r\rI think 3 and 6 are relevant contexts for today’s post. There is an accumulation of other research in this area, so if you’re going to do backtesting on the job, it’s good to study techniques, but I recommend learning about the proper way to operate backtesting as a primer in the first place.\nIn this post, I’ve decided to tackle a slightly different topic from the usual one. I often see the results of backtests in my own work, and I often do hold-out backtests on this blog. I will continue to follow my research on this topic so that I can understand and evaluate the uncertainty of the results obtained.\n\r","date":1594166400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594166400,"objectID":"ffd693614e217f006fe4ab18e5859efb","permalink":"/en/post/post19/","publishdate":"2020-07-08T00:00:00Z","relpermalink":"/en/post/post19/","section":"post","summary":"I'm sure that if you are a quants and are planning a new strategy, you may want to backtest to see its performance. In this post, I've looked at the impact of backtesting overfitting on out-of-sample performance and compiled a reminder.","tags":["machine-learning","finance","time-series_analysis"],"title":"Is that backtest really reproducible?","type":"post"},{"authors":null,"categories":["horse_racing"],"content":"Hi. This time I would like to write an article about predicting horse racing. In the last post, I created a prediction model for horse racing rankings using table data obtained from yahoo horse racing using LightGBM. I used structural data last time, but anyone can do this kind of analysis in these days. So this time, I developed a Convolutional Neural Network (CNN) which extracts features from a horse\u0026rsquo;s body image and predicts its ranking. This is the second time I\u0026rsquo;ve used Earth Engine to analyze satellite images, and the first time I\u0026rsquo;ve used deep learning in this blog. The code is written in Python.\n1. Crawling for data collection The first step is to collect images of the horse\u0026rsquo;s body from the internet; the best thing to do would be to use pictures of the paddock on race day. However, as far as I\u0026rsquo;ve been able to find, there are no sites that post photos of the paddock in a cohesive format. It may be interesting to use it as a clipped image or to apply it as a video to the Encoder-Decoder model of CNN to RNN, because it may be that a horse racing fan may have a paddock video on Youtube. However, I don\u0026rsquo;t have the ability to do that much.\nWhere to get the data from So in this case, the data is taken from Daley\u0026rsquo;s Web site. Here you can find pre-race photos of horses running in the last 1 year? You can find pre-race photos of horses running in G1 races in the past year? Horse bettors who can\u0026rsquo;t go to the actual racecourse can look at these pictures and analyze the condition of the horses.\nPlease note that this site does not include body photos of all the horses that are entered in the race. Also, since this is only a limited number of G1 races, it\u0026rsquo;s entirely possible that all the horses are finished to begin with, and it\u0026rsquo;s entirely possible that you won\u0026rsquo;t be able to tell the difference. However, I\u0026rsquo;ll make it a priority to try and do it quickly and use this data for this one.\nRunning crawling by selenium For crawling, we\u0026rsquo;ll use selenium. I won\u0026rsquo;t go into web crawling as I\u0026rsquo;m mainly using CNN in this article. The code I used is as follows.\n[Note] If you use the following codes, please do so at your own risk.\nfrom selenium import webdriver from selenium.webdriver.chrome.options import Options from selenium.webdriver.support.select import Select from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.alert import Alert from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.common.exceptions import TimeoutException from selenium.webdriver.common.action_chains import ActionChains from time import sleep from urllib import request import random # selenium option settings (spell) options = Options() options.add_argument(\u0026#39;--disable-gpu\u0026#39;); options.add_argument(\u0026#39;--disable-extensions\u0026#39;); options.add_argument(\u0026#39;--proxy-server=\u0026#34;direct://\u0026#34;\u0026#39;); options.add_argument(\u0026#39;--proxy-bypass-list=*\u0026#39;); options.add_argument(\u0026#39;--start-maximized\u0026#39;); # driver specification DRIVER_PATH = r\u0026#39;C:/Users/aashi/Desktop/chromedriver_win32/chromedriver.exe\u0026#39; driver = webdriver.Chrome(executable_path=DRIVER_PATH, chrome_options=options) # Pass the url and go to the site url = \u0026#39;https://www.daily.co.jp/horse/horsecheck/photo/\u0026#39; driver.get(url) driver.implicitly_wait(15) # Maximum time to wait for an object to load, and if this is exceeded, an error sleep(5) # 1 second sleep as the web page transition is performed # Image data is saved for each race. selector0 = \u0026#34;body \u0026gt; div \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; div \u0026gt; section \u0026gt; a\u0026#34; elements = driver.find_elements_by_css_selector(selector0) for i in range(0,len(elements)): elements = driver.find_elements_by_css_selector(selector0) element = elements[i] element.click() sleep(5) # 5 seconds sleep as the web page transition is performed target = driver.find_element_by_link_text(\u0026#39;Ｇ１馬体診断写真集のTOP\u0026#39;) actions = ActionChains(driver) actions.move_to_element(target) actions.perform() sleep(5) # 5 seconds sleep as the web page transition is performed selector = \u0026#34;body \u0026gt; div.wrapper.horse.is-fixedHeader.is-fixedAnimation \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; article \u0026gt; div.photoDetail-wrapper \u0026gt; section \u0026gt; div \u0026gt; figure\u0026#34; figures = driver.find_elements_by_css_selector(selector) download_dir = r\u0026#39;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026#39; selector = \u0026#34;body \u0026gt; div \u0026gt; main \u0026gt; div \u0026gt; div.primaryContents \u0026gt; article \u0026gt; article \u0026gt; div.photoDetail-wrapper \u0026gt; section \u0026gt; h1\u0026#34; race_name = driver.find_element_by_css_selector(selector).text for figure in figures: img_name = figure.find_element_by_tag_name(\u0026#39;figcaption\u0026#39;).text horse_src = figure.find_element_by_tag_name(\u0026#39;img\u0026#39;).get_attribute(\u0026#34;src\u0026#34;) save_name = download_dir + \u0026#39;/\u0026#39; + race_name + \u0026#39;_\u0026#39; + img_name + \u0026#39;.jpg\u0026#39; request.urlretrieve(horse_src,save_name) driver.back() The saved images were cross-checked with the actual race results and manually divided into the top three groups and the rest of the groups. The images are saved as follows.\nThis completes the collection of the original data.\n2. Training CNN using Keras What is Keras? Now, let\u0026rsquo;s train CNN using Keras. Keras is one of the Neural Network libraries that runs on Tensorflow and Theano. Keras is one of the Neural Network libraries that runs on Tensorflow and Theano. Keras is characterized by its ability to build models with relatively short code and its many learning algorithms.\nWhat is CNN? CNN is a type of (Deep) Neural Network often used in image analysis, and as its name suggests, it is an additional convolution. Convolution is a process like the following.\nThe input here is the image data. Image analysis recognizes and analyzes images as numerical values. The image on the computer is represented by the RGB value, which is a numerical value from 0 to 255 of three colors, red (Red), green (Green) and blue (Blue). There are three layers of vectors in the form of 255 red, 0 green, 0 blue, and so on, and in this case a perfect red is represented. In the case above, you can think of a, b, c, etc. as representing one of the RGB values of each pixel. Convolution calculates the features of an image by taking the inner product of these RGB values with a matrix called the kernel. The following video(Japanese) is a good example of what the convolution layer means.\n\rBy learning the kernel to successfully get the distinctive parts of that image, it is possible to identify the image. I think the convolutional layer is the most important part of the CNN.\nAs shown in the above figure, CNN has not only convolutional layers but also input and output layers as well as usual Neural Network layers. If you want to know about the MaxPooling layer, see the following movie(Japanese).\n\rAlthough the gradient method is known as the most orthodox training method for deep learning, various extension algorithms such as Adam have been proposed. Basically, Adam or momentum is often used.\nCoding Now, let\u0026rsquo;s get to the coding.\nfrom keras.utils import np_utils ## Using TensorFlow backend.\rfrom keras.models import Sequential from keras.layers.convolutional import MaxPooling2D from keras.layers import Activation, Conv2D, Flatten, Dense,Dropout from sklearn.model_selection import train_test_split from keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop, Nadam from PIL import Image import numpy as np import glob import matplotlib.pyplot as plt import time import os The first step is to convert the collected image data to numerical data to create the training data. The directory structure is as follows, with the top image and other images being stored in separate directories. When we read in the images from each directory, we give a category variable of 1 for the top image and 0 for others.\nphotograph of a horse\n superior (in rank) Other  #Specify a folder folders = os.listdir(r\u0026#34;C:\\Users\\aashi\\umanalytics\\photo\\image\u0026#34;) #Specify the total number of strokes (50 x 50 x 3 in this case). image_size = 300 dense_size = len(folders) X = [] Y = [] #Reads an image from each folder and converts it to a numpy array of RGB values using the Image function for i, folder in enumerate(folders): files = glob.glob(\u0026#34;C:/Users/aashi/umanalytics/photo/image/\u0026#34; + folder + \u0026#34;/*.jpg\u0026#34;) index = i for k, file in enumerate(files): image = Image.open(file) image = image.convert(\u0026#34;L\u0026#34;).convert(\u0026#34;RGB\u0026#34;) image = image.resize((image_size, image_size)) #I\u0026#39;m dropping the number of pixels. data = np.asarray(image) X.append(data) Y.append(index) X = np.array(X) Y = np.array(Y) X = X.astype(\u0026#39;float32\u0026#39;) X = X / 255.0 # Conversion to 0~1 X.shape Y = np_utils.to_categorical(Y, dense_size) #splitting training data and test data X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20) I\u0026rsquo;ve been able to split the training data and the test data. What I\u0026rsquo;m considering now is a binary classification of \u0026ldquo;top\u0026rdquo; and \u0026ldquo;other\u0026rdquo;, but I defined \u0026ldquo;top\u0026rdquo; as the top 3, so the data is unbalanced (about 5 times as much other data as the top data). In this case, if we train on the data as it is, it is easier to predict the label with the larger sample size (in this case, \u0026ldquo;other\u0026rdquo;), and the model will have a bias. Therefore, it is necessary to adjust the training data so that the sample size is the same for each of the two classes.\nindex_zero = np.random.choice(np.array(np.where(y_train[:,1]==0))[0,],np.count_nonzero(y_train[:,1]==1),replace=False) index_one = np.array(np.where(y_train[:,1]==1))[0] y_resampled = y_train[np.hstack((index_one,index_zero))] X_resampled = X_train[np.hstack((index_one,index_zero))] We will use this y_resampled and X_resampled for the training data. Next, we will build the CNN. In Keras, a model is defined by specifying a sequential model and adding a layer by add method.\nmodel = Sequential() model.add(Conv2D(32, (3, 3), padding=\u0026#39;same\u0026#39;,input_shape=X_train.shape[1:])) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(Conv2D(32, (3, 3))) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Conv2D(64, (3, 3), padding=\u0026#39;same\u0026#39;)) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(Conv2D(64, (3, 3))) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512)) model.add(Activation(\u0026#39;relu\u0026#39;)) model.add(Dropout(0.5)) model.add(Dense(dense_size)) model.add(Activation(\u0026#39;softmax\u0026#39;)) model.summary() ## Model: \u0026quot;sequential_1\u0026quot;\r## _________________________________________________________________\r## Layer (type) Output Shape Param # ## =================================================================\r## conv2d_1 (Conv2D) (None, 300, 300, 32) 896 ## _________________________________________________________________\r## activation_1 (Activation) (None, 300, 300, 32) 0 ## _________________________________________________________________\r## conv2d_2 (Conv2D) (None, 298, 298, 32) 9248 ## _________________________________________________________________\r## activation_2 (Activation) (None, 298, 298, 32) 0 ## _________________________________________________________________\r## max_pooling2d_1 (MaxPooling2 (None, 149, 149, 32) 0 ## _________________________________________________________________\r## dropout_1 (Dropout) (None, 149, 149, 32) 0 ## _________________________________________________________________\r## conv2d_3 (Conv2D) (None, 149, 149, 64) 18496 ## _________________________________________________________________\r## activation_3 (Activation) (None, 149, 149, 64) 0 ## _________________________________________________________________\r## conv2d_4 (Conv2D) (None, 147, 147, 64) 36928 ## _________________________________________________________________\r## activation_4 (Activation) (None, 147, 147, 64) 0 ## _________________________________________________________________\r## max_pooling2d_2 (MaxPooling2 (None, 73, 73, 64) 0 ## _________________________________________________________________\r## dropout_2 (Dropout) (None, 73, 73, 64) 0 ## _________________________________________________________________\r## flatten_1 (Flatten) (None, 341056) 0 ## _________________________________________________________________\r## dense_1 (Dense) (None, 512) 174621184 ## _________________________________________________________________\r## activation_5 (Activation) (None, 512) 0 ## _________________________________________________________________\r## dropout_3 (Dropout) (None, 512) 0 ## _________________________________________________________________\r## dense_2 (Dense) (None, 2) 1026 ## _________________________________________________________________\r## activation_6 (Activation) (None, 2) 0 ## =================================================================\r## Total params: 174,687,778\r## Trainable params: 174,687,778\r## Non-trainable params: 0\r## _________________________________________________________________\rNow, let\u0026rsquo;s get to the learning part. We\u0026rsquo;ll use Adadelta for the algorithm. I don\u0026rsquo;t really understand it.\noptimizers =\u0026#34;Adadelta\u0026#34; results = {} epochs = 50 model.compile(loss=\u0026#39;categorical_crossentropy\u0026#39;, optimizer=optimizers, metrics=[\u0026#39;accuracy\u0026#39;]) results = model.fit(X_resampled, y_resampled, validation_split=0.2, epochs=epochs) Undersampling for unbalanced data adjustment From here, we perform binary classification with Test data, but since we are undersampling the training data, we have an undersampled sample selection bias when calculating the prediction probability. The paper is available here.\nTherefore, I would like to formulate this part of the problem here, although a correction is needed. I will describe the current binary classification problem as the problem of predicting the objective variable \\(Y\\), which takes a binary value from the explanatory thousand \\(X\\). Let \\((X,Y)\\) be a dataset where the positive example is considerably less than the negative example and the sample size of the negative example is matched to the positive example as \\((X_s,Y_s)\\). We define a categorical variable \\(s\\) that takes 1 if the \\((X,Y)\\) sample set is also included in \\((X_s,Y_s)\\) and 0 if it is not. Given an explanatory variable \\(x\\) to the model constructed using the dataset \\((X,Y)\\), the positive example and the conditional probability of predicting can be expressed as \\(P(y=1|x)\\). On the other hand, the conditional probability of predicting a positive example in a model constructed using \\((X_s,Y_s)\\) can be expressed as \\(P(y=1|x)\\) using Bayes' theorem and the categorical variable \\(s\\).\n$$ P(y=1|x,s=1) = \\frac{P(s=1|y=1)P(y=1|x)}{P(s=1|y=1)P(y=1|x) + P(s=1|y=0)P(y=0|x)} $$ It can be written as. Since \\((X_s,Y_s)\\) matches the sample size of the negative example to the positive example, \\(P(s=1,y=1)=1\\), the above formula is rewritten as\n$$ P(y=1|x,s=1) = \\frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)P(y=0|x)} = \\frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)(1-P(y=1|x))} $$ It is self-evident from the definition of \\((X_s,Y_s)\\) that \\(P(s=1|y=0)\\neq0\\) (0 would result in unbalanced data with only positive examples). Thus, as long as \\(P(y=0,x) \\neq0\\), the probability that the undersampling model will be rejected as a positive example is positively biased against the probability that the original data set will produce. What we want to find is \\(P(y=1|x)\\) with no bias, so \\(P=P(y=1|x),P_s=P(y|x,s=1),\\beta=P(s=1,y=0)\\), then we can get\n$$ P = \\frac{\\beta P_s}{\\beta P_s-P_s+1} $$ and can use this relationship formula to correct for bias. Let\u0026rsquo;s define what we\u0026rsquo;ve just identified as a function.\ndef calibration(y_proba, beta): return y_proba / (y_proba + (1 - y_proba) / beta) sampling_rate = sum(y_train[:,1]) / sum(1-y_train[:,1]) y_proba_calib = calibration(model.predict(X_test), sampling_rate) y_pred = np_utils.to_categorical(np.argmax(y_proba_calib,axis=1), dense_size) from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score score = accuracy_score(y_test, y_pred) print(\u0026#39;Test accuracy:\u0026#39;, score) ## Test accuracy: 0.3220338983050847\rIt\u0026rsquo;s not good at all. I ran the ConfusionMatrix and found out that it doesn\u0026rsquo;t work.\nConfusionMatrixDisplay(confusion_matrix(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))).plot() ## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x000000004A54CC88\u0026gt;\rplt.show() plt.close() I did a bias correction for the imbalance data, but the model is still very predictive of negative values. This doesn\u0026rsquo;t work.\n3. Interpretation of results using Shap values I would like to consider the shap value of the model we just learned and interpret the results. I\u0026rsquo;ll add an explanation of the shap value when I have time. Simply put, the visualization captures which parts of the image the CNN captured features and predicted the horse to be at the top. We\u0026rsquo;ll be analyzing this horse.\nplt.imshow(X_test[0]) plt.show() plt.close() import shap background = X_train[np.random.choice(X_train.shape[0],100,replace=False)] e = shap.GradientExplainer(model,background) shap_values = e.shap_values(X_test[[0]]) shap.image_plot(shap_values[1],X_test[[0]]) It\u0026rsquo;s very subtle, but it looks like you\u0026rsquo;re appreciating the legs and buttocks, etc. It needs to be cropped to take out the horse\u0026rsquo;s body only, since it seems to be responding to the background. I think I need to build a model for object detection. I\u0026rsquo;ll think about this another time.\n4. Finally To be honest, it hasn\u0026rsquo;t worked out at all. Is it still difficult to predict rankings from the horse\u0026rsquo;s body? Does multiplying it with other variables change the results? I don\u0026rsquo;t think I\u0026rsquo;m able to extract good features from the horses as it is. Do I need to get to the point where I can get a paddock video from Youtube and analyze it with the Encoder-Decoder model to make it work? I\u0026rsquo;d love to do it when I\u0026rsquo;m good enough to do it (I don\u0026rsquo;t know when that will be). Until then, I need to improve my PC specs. Maybe I\u0026rsquo;ll use the cash handout.\n","date":1593907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593907200,"objectID":"6dc680f46f0d2b9d9526697571eabba2","permalink":"/en/post/post18/","publishdate":"2020-07-05T00:00:00Z","relpermalink":"/en/post/post18/","section":"post","summary":"We created a horse racing prediction model from a new perspective of horse body photography.","tags":["Python","Web_scraping","machine_learning"],"title":"I predicted the standings based on horse photos using CNN.","type":"post"},{"authors":null,"categories":["horse_racing"],"content":"Hi. It\u0026rsquo;s been quite a while, but I\u0026rsquo;d like to create a model to predict the outcome of a race based on race result data previously collected from yahoo.keiba in order to study Python.\n1.Data Import First, I get the race result data saved from sqlite to the pandas data frame.\nconn = sqlite3.connect(r\u0026#39;C:\\hogehoge\\horse_data.db\u0026#39;) sql = r\u0026#39;SELECT * FROM race_result\u0026#39; df = pd.read_sql(con=conn,sql=sql) Let\u0026rsquo;s check the contents of the data. The columns are as follows, with order being the order of arrival.\ndf.columns ## Index(['order', 'frame_number', 'horse_number', 'trainer', 'passing_rank',\r## 'last_3F', 'time', 'margin', 'horse_name', 'horse_age', 'horse_sex',\r## 'horse_weight', 'horse_weight_change', 'brinker', 'jockey',\r## 'jockey_weight', 'jockey_weight_change', 'odds', 'popularity',\r## 'race_date', 'race_course', 'race_name', 'race_distance', 'type',\r## 'race_turn', 'race_condition', 'race_weather', 'colour', 'owner',\r## 'farm', 'locality', 'horse_birthday', 'father', 'mother', 'prize',\r## 'http'],\r## dtype='object')\rChecking the contents of the order, you\u0026rsquo;ll see that many of the orders have parentheses () and that they are recognized by the letter type because of the presence of cancellation, abort and disqualification. By the way, the order in parentheses is the order of entry, which means that the horse has been disqualified for interfering with another horse\u0026rsquo;s running (http://www.jra.go.jp/judge/).\ndf.loc[:,\u0026#39;order\u0026#39;].unique() ## array(['1', '7', '2', '8', '5', '15', '6', '12', '11', '14', '3', '13',\r## '4', '16', '9', '10', '取消', '中止', '除外', '17', '18', '4(3)', '2(1)',\r## '3(2)', '6(4)', '失格', '9(8)', '16(6)', '12(12)', '13(9)', '6(3)',\r## '10(7)', '6(5)', '9(3)', '11(8)', '13(2)', '12(9)', '14(7)',\r## '10(1)', '16(8)', '14(6)', '10(3)', '12(1)', '13(6)', '7(1)',\r## '12(6)', '6(2)', '11(2)', '15(6)', '13(10)', '14(4)', '7(5)',\r## '17(4)', '9(7)', '16(14)', '12(11)', '14(2)', '8(2)', '9(5)',\r## '11(5)', '12(7)', '11(1)', '12(8)', '7(4)', '5(4)', '13(12)',\r## '14(3)', '10(2)', '11(10)', '18(3)', '10(4)', '15(8)', '8(3)',\r## '5(1)', '10(5)', '7(3)', '5(2)', '9(1)', '13(3)', '16(11)',\r## '11(3)', '18(15)', '11(6)', '10(6)', '14(12)', '12(5)', '15(14)',\r## '17(8)', '18(6)', '4(2)', '18(10)', '16(7)', '13(1)', '16(10)',\r## '15(7)', '9(4)', '15(5)', '12(3)', '8(7)', '15(2)', '12(10)',\r## '14(9)', '3(1)', '6(1)', '14(5)', '15(4)', '11(4)', '12(4)',\r## '16(4)', '9(2)', '13(5)', '12(2)', '15(1)', '4(1)', '14(13)',\r## '14(1)', '13(7)', '5(3)', '8(6)', '15(13)', '7(2)', '15(11)',\r## '10(9)', '11(9)', '8(4)', '15(3)', '13(4)', '16(12)', '16(5)',\r## '18(11)', '10(8)', '18(8)', '14(8)', '16(9)', '8(5)', '8(1)',\r## '14(11)', '9(6)', '16(13)', '16(15)', '11(11)', '15(10)', '7(6)'],\r## dtype=object)\rLet\u0026rsquo;s fix this first. Remove the parentheses and change the type to int, and add the arrival order as a new column arriving order.\ndf[\u0026#39;arriving order\u0026#39;] = df[df.order.str.contains(r\u0026#39;\\d*\\(\\d*\\)\u0026#39;,regex=True)][\u0026#39;order\u0026#39;].replace(r\u0026#39;\\d+\\(\u0026#39;,r\u0026#39;\u0026#39;,regex=True).replace(r\u0026#39;\\)\u0026#39;,r\u0026#39;\u0026#39;,regex=True).astype(\u0026#39;float64\u0026#39;) df[\u0026#39;arriving order\u0026#39;].unique() ## array([nan, 3., 1., 2., 4., 8., 6., 12., 9., 7., 5., 10., 14.,\r## 11., 15., 13.])\rdf[\u0026#39;order\u0026#39;] = df[\u0026#39;order\u0026#39;].replace(r\u0026#39;\\(\\d+\\)\u0026#39;,r\u0026#39;\u0026#39;,regex=True) df = df[lambda df: ~df.order.str.contains(r\u0026#39;(取消|中止|除外|失格)\u0026#39;,regex=True)] ## C:\\Users\\aashi\\ANACON~1\\lib\\site-packages\\pandas\\core\\strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\r## return func(self, *args, **kwargs)\rdf[\u0026#39;order\u0026#39;] = df[\u0026#39;order\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;order\u0026#39;].unique() ## array([ 1., 7., 2., 8., 5., 15., 6., 12., 11., 14., 3., 13., 4.,\r## 16., 9., 10., 17., 18.])\rWe were able to process it into a clean float type. Now let\u0026rsquo;s move on to preprocessing the last three furlongs' times. We use the last three furlongs' time of the last race for our prediction.\nimport numpy as np df[\u0026#39;last_3F\u0026#39;] = df[\u0026#39;last_3F\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False).astype(\u0026#39;float64\u0026#39;) df[\u0026#39;last_3F\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;last_3F\u0026#39;].shift(-1) Also include the previous race and rankings and any additional positions in the dataset.\ndf[\u0026#39;prerace\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;race_name\u0026#39;].shift(-1) df[\u0026#39;preorder\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;order\u0026#39;].shift(-1) df[\u0026#39;prepassing\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;passing_rank\u0026#39;].shift(-1) The accumulated prize money earned at the time of running will also be added.\ndf[\u0026#39;preprize\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;prize\u0026#39;].shift(-1) df[\u0026#39;preprize\u0026#39;] = df[\u0026#39;preprize\u0026#39;].fillna(0) df[\u0026#39;margin\u0026#39;] = df.groupby(\u0026#39;horse_name\u0026#39;)[\u0026#39;margin\u0026#39;].shift(-1) We also add missing values, data type fixes, and label encoding for categorical data.\ndf[\u0026#39;horse_weight\u0026#39;] = df[\u0026#39;horse_weight\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;margin\u0026#39;] = df[\u0026#39;margin\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False) df[\u0026#39;horse_age\u0026#39;] = df[\u0026#39;horse_age\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;horse_weight_change\u0026#39;] = df[\u0026#39;horse_weight_change\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;jockey_weight\u0026#39;] = df[\u0026#39;jockey_weight\u0026#39;].astype(\u0026#39;float64\u0026#39;) df[\u0026#39;race_distance\u0026#39;] = df[\u0026#39;race_distance\u0026#39;].replace(r\u0026#39;m\u0026#39;,r\u0026#39;\u0026#39;,regex=True).astype(\u0026#39;float64\u0026#39;) df[\u0026#39;race_turn\u0026#39;] = df[\u0026#39;race_turn\u0026#39;].replace(r\u0026#39;character(0)\u0026#39;,np.nan,regex=False) df.loc[df[\u0026#39;order\u0026#39;]!=1,\u0026#39;order\u0026#39;] = 0 df[\u0026#39;race_turn\u0026#39;] = df[\u0026#39;race_turn\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;colour\u0026#39;] = df[\u0026#39;colour\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;prepassing\u0026#39;] = df[\u0026#39;prepassing\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;prerace\u0026#39;] = df[\u0026#39;prerace\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;father\u0026#39;] = df[\u0026#39;father\u0026#39;].fillna(\u0026#39;missing\u0026#39;) df[\u0026#39;mother\u0026#39;] = df[\u0026#39;mother\u0026#39;].fillna(\u0026#39;missing\u0026#39;) from sklearn import preprocessing cat_list = [\u0026#39;trainer\u0026#39;, \u0026#39;horse_name\u0026#39;, \u0026#39;horse_sex\u0026#39;, \u0026#39;brinker\u0026#39;, \u0026#39;jockey\u0026#39;, \u0026#39;race_course\u0026#39;, \u0026#39;race_name\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;race_turn\u0026#39;, \u0026#39;race_condition\u0026#39;, \u0026#39;race_weather\u0026#39;, \u0026#39;colour\u0026#39;, \u0026#39;father\u0026#39;, \u0026#39;mother\u0026#39;, \u0026#39;prerace\u0026#39;, \u0026#39;prepassing\u0026#39;] for column in cat_list: target_column = df[column] le = preprocessing.LabelEncoder() le.fit(target_column) label_encoded_column = le.transform(target_column) df[column] = pd.Series(label_encoded_column).astype(\u0026#39;category\u0026#39;) ## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r## LabelEncoder()\r2. Creating a Model Now, let\u0026rsquo;s try to build a prediction model with LightGBM. The optuna LightGBM is used to perform hyperparameter tuning and calculate the confusion matrix, as well as the correctness rate of test data calculated with the trained model.\nimport optuna.integration.lightgbm as lgb from sklearn.model_selection import train_test_split y = df[\u0026#39;order\u0026#39;] x = df.drop([\u0026#39;order\u0026#39;,\u0026#39;passing_rank\u0026#39;,\u0026#39;time\u0026#39;,\u0026#39;odds\u0026#39;,\u0026#39;popularity\u0026#39;,\u0026#39;owner\u0026#39;,\u0026#39;farm\u0026#39;,\u0026#39;locality\u0026#39;,\u0026#39;horse_birthday\u0026#39;,\u0026#39;http\u0026#39;,\u0026#39;prize\u0026#39;,\u0026#39;race_date\u0026#39;,\u0026#39;margin\u0026#39;],axis=1) X_train, X_test, y_train, y_test = train_test_split(x, y) X_train, x_val, y_train, y_val = train_test_split(X_train, y_train) lgb_train = lgb.Dataset(X_train, y_train) lgb_eval = lgb.Dataset(x_val, y_val) lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train) lgbm_params = { \u0026#39;objective\u0026#39;: \u0026#39;binary\u0026#39;, \u0026#39;boost_from_average\u0026#39;: False } model = lgb.train(lgbm_params, lgb_train, categorical_feature = cat_list, valid_sets = lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=False) def calibration(y_proba, beta): return y_proba / (y_proba + (1 - y_proba) / beta) sampling_rate = y_train.sum() / len(y_train) y_proba = model.predict(X_test, num_iteration=model.best_iteration) y_proba_calib = calibration(y_proba, sampling_rate) y_pred = np.vectorize(lambda x: 1 if x \u0026gt; 0.49 else 0)(y_proba_calib) Visualization part.\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc import matplotlib.pyplot as plt import seaborn as sns # Calculating the AUC (Area Under the Curve) fpr, tpr, thresholds = roc_curve(y_test, y_pred) auc = auc(fpr, tpr) # Plot the ROC curve plt.plot(fpr, tpr, label=\u0026#39;ROC curve (area = %.2f)\u0026#39;%auc) plt.legend() plt.title(\u0026#39;ROC curve\u0026#39;) plt.xlabel(\u0026#39;False Positive Rate\u0026#39;) plt.ylabel(\u0026#39;True Positive Rate\u0026#39;) plt.grid(True) plt.show() plt.close() # Generate a Confusion Matrix ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot() ## \u0026lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x00000000C690E288\u0026gt;\rplt.show() plt.close() accuracy_score(y_test, y_pred) ## 0.9300689387764461\rprecision_score(y_test, y_pred) ## 0.9185022026431718\rThe accuracy_score (prediction accuracy) is over 90% and the precision_Score (the percentage of data correct that predicted positive = 1) is good.\nrecall_score(y_test, y_pred) ## 0.012291457878912929\rf1_score(y_test, y_pred) ## 0.02425828970331588\rOn the other hand, we can see that the recall_score (percentage of sample that was predicted to be positive and actually true) is low and the false negative is high. As a result, the F1 value is also low. In the case of the horse racing prediction model, high false negatives are better than high false positives, but we have to work harder to reduce the false negatives to increase the return rate. This is an issue for the future. In the next section, I will use the shapley value to do a factorization.\n3. Interpreting results in shap import shap shap.initjs() ## \u0026lt;IPython.core.display.HTML object\u0026gt;\rexplainer = shap.TreeExplainer(model) ## Setting feature_perturbation = \u0026quot;tree_path_dependent\u0026quot; because no background data was given.\rshap_values = explainer.shap_values(X_test) ## LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\rFirst, we\u0026rsquo;ll see how important each feature is. The summary_plot method is used.\nshap.summary_plot(shap_values, X_test) The horizontal axis represents the average importance of each feature (absolute value of the shap value), and we can see that preprize (amount of money won up to the last race), horse_age, and preorder (order of finish in the last race) are all important in predicting the winner of the race. The same is true for horse_age. However, it is not possible to evaluate it qualitatively just because it is important. For example, if the relationship between a higher preprize and a higher probability of being first is confirmed, that can be important information. Then you can check it. The summary_plot method is used.\nshap.summary_plot(shap_values[1], X_test) The above figure also shows the importance of each feature (not absolute values in this case). In this case, the importance of each feature is shown in the violin plot and is colored according to the size of the feature value. For example, in the case of preprize, the red distribution occurs only where the horizontal axis is greater than 0, and this is where the feature value of preprize is large. This means that we can take the obvious interpretation that the probability of finishing first increases on average with the amount of money won up to the previous race. Other factors such as horse_age, preorder, and last_3F seem to increase the probability of finishing first as the feature value becomes smaller, while horse_weight and jokey_weight seem to increase the probability of finishing first as the feature value becomes larger. On the other hand, there is no qualitative relationship between the two variables.\nNext, let\u0026rsquo;s look at the relationship between feature value and probability in more detail. We saw earlier that preprize increases the probability of being the first one to arrive as the feature value increases. But we don\u0026rsquo;t know if the increase is linear, exponential, or diminishing as in dependence on \\(log x\\). Let\u0026rsquo;s find out with the dependence_plot.\nshap.dependence_plot(ind=\u0026#34;preprize\u0026#34;, shap_values=shap_values[1], features=X_test) The above figure plots the approximate form of the learned LightGBM as a function of preprize. As we saw earlier, the probability of being the first one to be placed increases as the feature value increases. However, the increase is gradual and diminishing, and it almost reaches its peak at over 20 million yen. Also, in the figure above, we have color-coded by HORSE_AGE, so you can see the relationship with PREPRIZE. As you might expect, the probability of horses with high preprize is higher for the youngest horses to win the race.\nLet\u0026rsquo;s also check the dependence_plot of the preorder.\nshap.dependence_plot(ind=\u0026#34;preorder\u0026#34;, shap_values=shap_values[1], features=X_test) As expected, the higher the order of the last race, the higher the probability of finishing first. I also checked the relationship with the time of last_3F, but it doesn\u0026rsquo;t seem to be very relevant here.\n4. Summary I made a prediction model of horse racing using LightGBM. As you would expect of Light GBM, the prediction accuracy is very high. Also, the shap value was successfully used to detect important features. This will help us understand how LightGBM feels and improve our modeling accuracy as we continue to find better features.\n","date":1582934400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582934400,"objectID":"ab5059d2582cbfad9b39f58a8285b701","permalink":"/en/post/post16/","publishdate":"2020-02-29T00:00:00Z","relpermalink":"/en/post/post16/","section":"post","summary":"I used Kaggler's favorite LightGBM to predict the horse race standings.","tags":["Python","preprocessing","machine_learning"],"title":"Predicting Horse Racing Results Using LightGBM","type":"post"},{"authors":null,"categories":["macroeconomics"],"content":"Hi. I wanted to do a fitting of the 10 year long term interest rate for a certain reason. So, we will use US data to analyze the results. First, let\u0026rsquo;s collect the data. We will use the quantmod package to drop the data from FRED. The command getsymbols(key,from=start date,src=\u0026quot;FRED\u0026quot;, auto.assign=TRUE) is easy to use. You can find the key on the FRED website.\n1. Data collection library(quantmod) # data name collected symbols.name \u0026lt;- c(\u0026#34;10-Year Treasury Constant Maturity Rate\u0026#34;,\u0026#34;Effective Federal Funds Rate\u0026#34;,\u0026#34; Consumer Price Index for All Urban Consumers: All Items\u0026#34;,\u0026#34;Civilian Unemployment Rate\u0026#34;,\u0026#34;3-Month Treasury Bill: Secondary Market Rate\u0026#34;,\u0026#34;Industrial Production Index\u0026#34;,\u0026#34; 10-Year Breakeven Inflation Rate\u0026#34;,\u0026#34;Trade Weighted U.S. Dollar Index: Broad, Goods\u0026#34;,\u0026#34; Smoothed U.S. Recession Probabilities\u0026#34;,\u0026#34;Moody\u0026#39;s Seasoned Baa Corporate Bond Yield\u0026#34;,\u0026#34;5-Year, 5-Year Forward Inflation Expectation Rate\u0026#34;,\u0026#34;Personal Consumption Expenditures\u0026#34;) # Collect economic data symbols \u0026lt;- c(\u0026#34;GS10\u0026#34;,\u0026#34;FEDFUNDS\u0026#34;,\u0026#34;CPIAUCSL\u0026#34;,\u0026#34;UNRATE\u0026#34;,\u0026#34;TB3MS\u0026#34;,\u0026#34;INDPRO\u0026#34;,\u0026#34;T10YIEM\u0026#34;,\u0026#34;TWEXBMTH\u0026#34;,\u0026#34;RECPROUSM156N\u0026#34;,\u0026#34;BAA\u0026#34;,\u0026#34;T5YIFRM\u0026#34;,\u0026#34;PCE\u0026#34;) getSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, src = \u0026#34;FRED\u0026#34;, auto.assign = TRUE) ## [1] \u0026quot;GS10\u0026quot; \u0026quot;FEDFUNDS\u0026quot; \u0026quot;CPIAUCSL\u0026quot; \u0026quot;UNRATE\u0026quot; ## [5] \u0026quot;TB3MS\u0026quot; \u0026quot;INDPRO\u0026quot; \u0026quot;T10YIEM\u0026quot; \u0026quot;TWEXBMTH\u0026quot; ## [9] \u0026quot;RECPROUSM156N\u0026quot; \u0026quot;BAA\u0026quot; \u0026quot;T5YIFRM\u0026quot; \u0026quot;PCE\u0026quot;\rmacro_indicator \u0026lt;- merge(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE) rm(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE,USEPUINDXD) 2. Monthly Analysis Part The data are here. We will create a dataset for the estimation. The dependent variable is the 10-Year Treasury Constant Maturity Rate(GS10). The explanatory variables are as follows\n   explanatory variable key proxy variable     Federal Funds Rate FEDFUNDS Short term rate   Consumer Price Index CPIAUCSL Price   Unemployment Rate UNRATE Employment   3-Month Treasury Bill TB3MS Short term rate   Industrial Production Index INDPRO Business conditions   Breakeven Inflation Rate T10YIEM Price   Trade Weighted Dollar Index TWEXBMTH Exchange rates   Recession Probabilities RECPROUSM156N Business condition   Moody\u0026rsquo;s Seasoned Baa Corporate Bond Yield BAA Risk premium   Inflation Expectation Rate T5YIFRM Price   Personal Consumption Expenditures PCE Business condition   Economic Policy Uncertainty Index USEPUINDXD Politics    It\u0026rsquo;s a pretty appropriate choice of variables, but in many cases, we haven\u0026rsquo;t done it properly in terms of how to model long-term interest rates from a macro modeling perspective\u0026hellip; In DSGE, we formulate the path of short-term interest rates linked to the path of short-term interest rates up to 10 years into the future according to the efficient market hypothesis and long-term interest rates equal to the path of short-term interest rates when I was a graduate student It was modeling (which seems to be done properly in macro finance circles). That\u0026rsquo;s why I\u0026rsquo;ve added short-term interest rates as an explanatory variable. And I also added three indicators for prices that would have an impact on the short-term interest rate. In addition, I added data on the economy because it is well known that it is highly correlated with the economy. This is due to the fact that in the first place, it is common in macro models to model short-term interest rates as following the Taylor rule.\n$$ r_t = \\rho r_{t-1} + \\alpha \\pi_{t} + \\beta y_{t} $$\nwhere \\(r_t\\) is the policy interest rate (short-term interest rate), \\(\\pi_t\\) is the inflation rate, and \\(y_t\\) is the output. The $R_rho, \\\\alpha, and \\(y_t\\) are called deep parameters, which represent inertia, the sensitivity of the interest rate to inflation, and the sensitivity of the interest rate to output, respectively. It is well known as the \u0026ldquo;Taylor\u0026rsquo;s Principle\u0026rdquo; that when $\\rho=0, \\(\\beta=0\\), a reasonably expected equilibrium solution can only be obtained when \\(\\alpha\u0026gt;=1\\). Other explanatory variables include Moody's Seasoned Baa Corporate Bond Yield, which may also have an arbitrage relationship with corporate bonds. Also, we would like to add the VIX index and an index related to finances if we wanted to. The fiscal index is either Quatery or Annualy and cannot be used for monthly estimation. This is the most difficult part. I will re-estimate if I come up with something.\nNow, let\u0026rsquo;s get into the estimation. Since there are many explanatory variables in this case, we want to do a lasso regression to narrow down the valid variables. We will also do an OLS for comparison. The explanatory variables will be the values of the dependent variable one period ago. Probably, even one period ago, depending on when the data are published, it may not be in time for the next month\u0026rsquo;s estimates, but I\u0026rsquo;ll do this anyway.\n# make dataset traindata \u0026lt;- na.omit(merge(macro_indicator[\u0026#34;2003-01-01::2015-12-31\u0026#34;][,1],stats::lag(macro_indicator[\u0026#34;2003-01-01::2015-12-31\u0026#34;][,-1],1))) testdata \u0026lt;- na.omit(merge(macro_indicator[\u0026#34;2016-01-01::\u0026#34;][,1],stats::lag(macro_indicator[\u0026#34;2016-01-01::\u0026#34;][,-1],1))) # fitting OLS trial1 \u0026lt;- lm(GS10~.,data = traindata) summary(trial1) ## ## Call:\r## lm(formula = GS10 ~ ., data = traindata)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.7593 -0.2182 0.0041 0.2143 0.7051 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 15.0576773 4.3419245 3.468 0.000693 ***\r## FEDFUNDS -0.2075752 0.1413832 -1.468 0.144253 ## CPIAUCSL -0.0750111 0.0204871 -3.661 0.000352 ***\r## UNRATE -0.2183796 0.0784608 -2.783 0.006109 ** ## TB3MS 0.3031085 0.1393904 2.175 0.031310 * ## INDPRO -0.0705997 0.0263855 -2.676 0.008328 ** ## T10YIEM 1.1476564 0.1758964 6.525 1.10e-09 ***\r## TWEXBMTH -0.0313911 0.0117338 -2.675 0.008338 ** ## RECPROUSM156N -0.0103854 0.0021233 -4.891 2.66e-06 ***\r## BAA 0.7802368 0.0858538 9.088 7.60e-16 ***\r## T5YIFRM -0.4529529 0.1881510 -2.407 0.017342 * ## PCE 0.0009815 0.0002477 3.963 0.000116 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.2953 on 143 degrees of freedom\r## Multiple R-squared: 0.9218,\tAdjusted R-squared: 0.9158 ## F-statistic: 153.2 on 11 and 143 DF, p-value: \u0026lt; 2.2e-16\rIt\u0026rsquo;s a higher degree of freedom-adjusted coefficient of determination; we use the model through 12/31/2015 to predict the out-sample data (01/01/2016~) and calculate the mean squared error.\nest.OLS.Y \u0026lt;- predict(trial1,testdata[,-1]) Y \u0026lt;- as.matrix(testdata[,1]) mse.OLS \u0026lt;- sum((Y - est.OLS.Y)^2) / length(Y) mse.OLS ## [1] 0.2422673\rThe next step is the lasso regression, using the cv.glmnet function of the glmnet package to perform Cross Validation and determine \\(\\lambda\\).\n# fitting lasso regression library(glmnet) trial2 \u0026lt;- cv.glmnet(as.matrix(traindata[,-1]),as.matrix(traindata[,1]),family=\u0026#34;gaussian\u0026#34;,alpha=1) plot(trial2) trial2$lambda.min ## [1] 0.001106745\rcoef(trial2,s=trial2$lambda.min) ## 12 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## s1\r## (Intercept) 9.560938684\r## FEDFUNDS -0.007306457\r## CPIAUCSL -0.045386980\r## UNRATE -0.174775320\r## TB3MS 0.126669996\r## INDPRO -0.059539752\r## T10YIEM 1.188103243\r## TWEXBMTH -0.015425929\r## RECPROUSM156N -0.009488571\r## BAA 0.743677181\r## T5YIFRM -0.459542956\r## PCE 0.000602374\rUnemployment Rate, 3-Month Treasury Bill, Breakeven Inflation Rate, Moody's Seasoned Baa Corporate Bond Yield and Inflation Expectation Rate. That\u0026rsquo;s the result of a larger regression coefficient. Other than the unemployment rate, the results are within expectations. However, the correlation with the economy seems to be low as far as this result is concerned (does it only work in the opposite direction?). Calculate the MSE.\nest.lasso.Y \u0026lt;- predict(trial2, newx = as.matrix(testdata[,-1]), s = trial2$lambda.min, type = \u0026#39;response\u0026#39;) mse.lasso \u0026lt;- sum((Y - est.lasso.Y)^2) / length(Y) mse.lasso ## [1] 0.1842137\rThe lasso regression gives better results. Let\u0026rsquo;s plot the predicted and actual values from the lasso regression as a time series.\nlibrary(tidyverse) ggplot(gather(data.frame(actual=Y[,1],lasso_prediction=est.lasso.Y[,1],OLS_prediction=est.OLS.Y,date=as.POSIXct(rownames(Y))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) + geom_line(size=1.5) + scale_x_datetime(breaks = \u0026#34;6 month\u0026#34;,date_labels = \u0026#34;%Y-%m\u0026#34;) + scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,3.5)) The sense of direction is good. On the other hand, I am not predicting a sharp decline in interest rates from January 2016 or after December 2018. It looks like we\u0026rsquo;ll have to try to do one of the following to improve the accuracy of this part of the projections: consider some variables OR run a rolling estimate.\n3. Daily Analysis Part In addition to monthly analysis, I would like to do daily analysis. In the case of daily data, the jagged edge problem is unlikely to occur because the data is often released after the market closes. We will start by collecting daily data.\n# data name collected symbols.name \u0026lt;- c(\u0026#34;10-Year Treasury Constant Maturity Rate\u0026#34;,\u0026#34;Effective Federal Funds Rate\u0026#34;, \u0026#34;based on U.S. Dollar\u0026#34;,\u0026#34;NASDAQ Composite Index\u0026#34;,\u0026#34;3-Month Treasury Bill: Secondary Market Rate\u0026#34;,\u0026#34;Economic Policy Uncertainty Index for United States\u0026#34;,\u0026#34; 10-Year Breakeven Inflation Rate\u0026#34;,\u0026#34;Trade Weighted U.S. Dollar Index: Broad, Goods\u0026#34;,\u0026#34;Moody\u0026#39;s Seasoned Baa Corporate Bond Yield\u0026#34;,\u0026#34;5-Year, 5-Year Forward Inflation Expectation Rate\u0026#34;) # Collect economic data symbols \u0026lt;- c(\u0026#34;DGS10\u0026#34;,\u0026#34;DFF\u0026#34;,\u0026#34;NASDAQCOM\u0026#34;,\u0026#34;DTB3\u0026#34;,\u0026#34;USEPUINDXD\u0026#34;,\u0026#34;T10YIE\u0026#34;,\u0026#34;DTWEXB\u0026#34;,\u0026#34;DBAA\u0026#34;,\u0026#34;T5YIFR\u0026#34;) getSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, src = \u0026#34;FRED\u0026#34;, auto.assign = TRUE) ## [1] \u0026quot;DGS10\u0026quot; \u0026quot;DFF\u0026quot; \u0026quot;NASDAQCOM\u0026quot; \u0026quot;DTB3\u0026quot; \u0026quot;USEPUINDXD\u0026quot;\r## [6] \u0026quot;T10YIE\u0026quot; \u0026quot;DTWEXB\u0026quot; \u0026quot;DBAA\u0026quot; \u0026quot;T5YIFR\u0026quot;\rNASDAQCOM.r \u0026lt;- ROC(na.omit(NASDAQCOM)) macro_indicator.d \u0026lt;- merge(DGS10,DFF,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR) rm(DGS10,DFF,NASDAQCOM,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR) The next step is to build the data set. We separate the data for training and for training. Considering the actual prediction process, we use data from two business days ago as the explanatory variables.\n# make dataset traindata.d \u0026lt;- na.omit(merge(macro_indicator.d[\u0026#34;1980-01-01::2010-12-31\u0026#34;][,1],stats::lag(macro_indicator.d[\u0026#34;1980-01-01::2010-12-31\u0026#34;][,-1],2))) testdata.d \u0026lt;- na.omit(merge(macro_indicator.d[\u0026#34;2010-01-01::\u0026#34;][,1],stats::lag(macro_indicator.d[\u0026#34;2010-01-01::\u0026#34;][,-1],2))) # fitting OLS trial1.d \u0026lt;- lm(DGS10~.,data = traindata.d) summary(trial1.d) ## ## Call:\r## lm(formula = DGS10 ~ ., data = traindata.d)\r## ## Residuals:\r## Min 1Q Median 3Q Max ## -0.81961 -0.12380 0.00509 0.14514 0.72712 ## ## Coefficients:\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -3.5168438 0.1664198 -21.132 \u0026lt; 2e-16 ***\r## DFF 0.0770736 0.0217208 3.548 0.000403 ***\r## NASDAQCOM -0.4301865 0.4280568 -1.005 0.315121 ## DTB3 0.1137128 0.0238678 4.764 2.14e-06 ***\r## USEPUINDXD -0.0006591 0.0001073 -6.144 1.11e-09 ***\r## T10YIE 0.6957403 0.0351666 19.784 \u0026lt; 2e-16 ***\r## DTWEXB 0.0276208 0.0010896 25.350 \u0026lt; 2e-16 ***\r## DBAA 0.2879946 0.0131335 21.928 \u0026lt; 2e-16 ***\r## T5YIFR 0.3433321 0.0376058 9.130 \u0026lt; 2e-16 ***\r## ---\r## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r## ## Residual standard error: 0.2167 on 1148 degrees of freedom\r## Multiple R-squared: 0.8902,\tAdjusted R-squared: 0.8894 ## F-statistic: 1163 on 8 and 1148 DF, p-value: \u0026lt; 2.2e-16\rThe coefficient of determination remains high.\nest.OLS.Y.d \u0026lt;- predict(trial1.d,testdata.d[,-1]) Y.d \u0026lt;- as.matrix(testdata.d[,1]) mse.OLS.d \u0026lt;- sum((Y.d - est.OLS.Y.d)^2) / length(Y.d) mse.OLS.d ## [1] 0.8350614\rNext is the lasso regression. Determine \\(lambda\\) in CV.\n# fitting lasso regression trial2.d \u0026lt;- cv.glmnet(as.matrix(traindata.d[,-1]),as.matrix(traindata.d[,1]),family=\u0026#34;gaussian\u0026#34;,alpha=1) plot(trial2.d) trial2.d$lambda.min ## [1] 0.001339007\rcoef(trial2.d,s=trial2.d$lambda.min) ## 9 x 1 sparse Matrix of class \u0026quot;dgCMatrix\u0026quot;\r## s1\r## (Intercept) -3.4297225594\r## DFF 0.0707532975\r## NASDAQCOM -0.3512778327\r## DTB3 0.1204390843\r## USEPUINDXD -0.0006453775\r## T10YIE 0.6894098221\r## DTWEXB 0.0272773903\r## DBAA 0.2831306290\r## T5YIFR 0.3411269206\rThe coefficient of libor became zero, and the MSE of OLS was higher than that of libor.\nest.lasso.Y.d \u0026lt;- predict(trial2.d, newx = as.matrix(testdata.d[,-1]), s = trial2.d$lambda.min, type = \u0026#39;response\u0026#39;) mse.lasso.d \u0026lt;- sum((Y.d - est.lasso.Y.d)^2) / length(Y.d) mse.lasso.d ## [1] 0.8492769\rPlot the predictions.\nggplot(gather(data.frame(actual=Y.d[,1],lasso_prediction=est.lasso.Y.d[,1],OLS_prediction=est.OLS.Y.d,date=as.POSIXct(rownames(Y.d))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) + geom_line(size=1.5) + scale_x_datetime(breaks = \u0026#34;2 year\u0026#34;,date_labels = \u0026#34;%Y-%m\u0026#34;) + scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,5)) As with the monthly, there is very little difference between the predicted values for OLS and lasso. We have been able to capture the fluctuations quite nicely, but we have not been able to capture the interest rate decline caused by the United States federal government credit-rating downgrades in 2011 or the interest rate increase associated with the economic recovery in 2013. The only daily economic indicator I have is POS data, but I might be able to use the recently used nightlight satellite imagery data, if I had to say so. I\u0026rsquo;ll try it if I have time. For now, I\u0026rsquo;d like to end this article for now. Thank you for reading this article.\n","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563235200,"objectID":"8bd91b4fb1550ed802c88f7dcf9db6b7","permalink":"/en/post/post14/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/en/post/post14/","section":"post","summary":"Just an idea.","tags":["R","time-series_analysis","finance"],"title":"Fitting the 10-year long-term interest rate","type":"post"},{"authors":null,"categories":["macroeconomics"],"content":"\r\r\r1. Preparing to use the Earth Engine in advance\r2. Obtaining Satellite Image Data Using Python API\r\rImage\rImageCollection\rFeatureCollection…\r\r\r\rHi. Last time, we built a GDP forecasting model using the GPLVM model. However, since we are talking about nowcasting, I would like to perform an analysis using alternative data. I suddenly found the following article.\nFocus: Nowcast’s GDP estimates, including the world’s first increased use of satellite imagery\nHere’s an article by Dr. Tsutomu Watanabe of the University of Tokyo, who developed a GDP forecasting model using satellite images. In the article, the following is written\n\rThe U.S. National Oceanic and Atmospheric Administration (NOAA) purchases images of the Suomi NPP meteorological satellite as it passes over Japan at 1:30 a.m. each day and measures the brightness of each square in a 720-meter vertical and horizontal grid. Even with the same brightness, the magnitude of economic activity varies depending on the use of the land, such as agricultural land, commercial land, industrial land, etc., so refer to the Land Use Survey of the Geographical Survey Institute. The correlation between land use and economic activity indicated by brightness is played out, and the magnitude of economic activity is estimated from the brightness, taking this result into account.\rWatanabe calls this the “democratization of statistics” and predicts that it will become a global trend, because anyone can analyze publicly available data like satellite images, regardless of whether it is government or private sector.\n\rand, I was intrigued by the analysis using satellite images.\rIs satellite photography available to everyone? But then I found out that Google is offering a service called Earth Engine.\nhttps://earthengine.google.com/\n\rGoogle Earth Engine combines a multi-petabyte catalog of satellite imagery and geospatial datasets with planetary-scale analysis capabilities and makes it available for scientists, researchers, and developers to detect changes, map trends, and quantify differences on the Earth’s surface.\n\rIf it’s for research, education, or non-commercial purposes, what a great way to analyze satellite photo data for free. Watch the video below to see what exactly it can do.\n\rIn this post, I will use Eath Engine’s Python API to acquire and analyze satellite image data.\n1. Preparing to use the Earth Engine in advance\rIn order to use Earth Engine, you need to apply using your Google Account. You can do this from “Sign Up” in the upper right corner of the image above. After you apply for it, you will receive an email in your Gmail as shown below.\nとりあえずというのはWEB上のEarth Engine コードエディタは使用できるということです。コードエディタというのは以下のようなもので、ブラウザ上でデータを取得したり、解析をしたり、解析結果をMAPに投影したりすることができる便利ツールです。Earth Engineの本体はむしろこいつで、APIは副次的なものと考えています。\nYou can type the code into the code editor in the middle, but the language is javascript (the API is both python and javascript). It’s quite useful because you can project the analysis results to MAP, refer to the reference (left), and check the data spit out to Console. However, if you want to do advanced analysis after dropping the data, you should use Python, which I am familiar with, so I use the API in this case.\rI digress. Now, once you get the Earth Engine approval, install the earthengine-api with pip. Then, type earthengine authenticate on the command prompt. Then, the browser will open by itself and you will see a screen for python api authentication as shown below.\n次に以下のような画面にいきますので、そのまま承認します。これでauthenticationの完成です。pythonからAPIが使えます。\n\r2. Obtaining Satellite Image Data Using Python API\rWe are now ready to use the Python API. From here, we will retrieve the satellite image data. As you can see below, there are many datasets in the Earth Engine.\nhttps://developers.google.com/earth-engine/datasets/\nWe will use the VIIRS Stray Light Corrected Nighttime Day/Night Band Composites Version 1 dataset. This dataset provides monthly averages of nighttime light intensity around the world. The sample period is 2014-01~present.\nThe Earth Engine has several unique data types. You should remember the following three.\nImage\rThe raster data at a single point in time. An image object is composed of several bands. This band varies from data to data, but roughly each band may represent an RGB value. These are the most basic data for using the Earth Engine.\n\rImageCollection\rThe object of Image objects in chronological order. In this case, we will use this data for time-series analysis.\n\rFeatureCollection…\rThe GeoJSON Feature. It contains Geometry objects for geographic information and their properties (e.g. country names). This time, this feature is used to get location information of Japan.\nLet’s start with the coding. The first step is to get the FeatureCollection object for Japan. Geographical information is stored in the Fusion Tables and we extract the data whose country is Japan by ID. FeatureCollection(), we can get it easily by passing the ID as an argument to ee.\nimport ee\rfrom dateutil.parser import parse\ree.Initialize()\r# get Japan geometory as FeatureCollection from fusion table\rjapan = ee.FeatureCollection(\u0026#39;ft:1tdSwUL7MVpOauSgRzqVTOwdfy17KDbw-1d9omPw\u0026#39;).filter(ee.Filter.eq(\u0026#39;Country\u0026#39;, \u0026#39;Japan\u0026#39;))\rNext, let’s get a nighttime satellite image. ImageCollection() is also used to obtain a nighttime satellite image. ImageCollection(). Here, band is extracted to avg_rad, which is monthly average light intensity.\n# get night-light data from earth engine from 2014-01-01 to 2019-01-01\rdataset = ee.ImageCollection(\u0026#39;NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG\u0026#39;).filter(ee.Filter.date(\u0026#39;2014-01-01\u0026#39;,\u0026#39;2019-01-01\u0026#39;)).select(\u0026#39;avg_rad\u0026#39;)\rLet’s cut out a satellite image to the area around Japan and output it as an image file. You can use the image object to create an image file (otherwise, you will get a lot of images…). . Since what you just got is an ImageCollection object, you need to compress it into an Image object (top is an ImageCollection object, bottom is a compressed Image object).\nHere, I would like to show you the average image of the sample period from the average value of Image objects in an ImageCollection object. You can do this with ImageCollection.mean(). Also, I used .visualize({min:0.5}) to filter the image if the pixel value is more than 0.5. If you don’t do this, you can’t see what you think are clouds or garbage? You’ll get something like this. Next, the method .getDownloadURL is used to get the URL to download the processed image. (If the scale is too small, an error occurs and the image cannot be processed.\ndataset.mean().visualize(min=0.5).getDownloadURL(dict(name=\u0026#39;thumbnail\u0026#39;,region=[[[120.3345348936478, 46.853488838010854],[119.8071911436478, 24.598157870729043],[148.6353161436478, 24.75788466523463],[149.3384411436478, 46.61252884462868]]],scale=5000))\rHere’s the image we got.\nAs expected, light levels are high in the Kanto area centered on Tokyo, the Kansai area centered on Osaka, Aichi, Fukuoka and Hokkaido (Sapporo and its vicinity), indicating that economic activity is active. It also shows that there are areas with higher light levels in the coastal areas than on land. This may not seem to be a phenomenon directly related to economic activity. It is striking that the northern part of the image becomes completely dark after 38 degrees north latitude, which is outside the scope of this analysis. Needless to say, this is the borderline between North and South Korea, so the difference in the level of economic activity between the two countries must be visually contrasted. The dataset we used here is from 2014, but some other datasets allow us to get data from the 1990s (although we can’t get more recent data instead). It would be interesting to use them to observe the economic development of the Korean peninsula and China.\nNow that we have the image, we can’t analyze it at this point. We will try to get the data of the nightlight mapped to pixel values and analyze it numerically. However, the procedure to acquire the data is a little different from the previous one. But the procedure is a little different from the previous one, because this time you need to aggregate** all the different values of nighttime light in Japan, pixel by pixel, into a single proxy value. Once you have the pixel-by-pixel values, you have too much to analyze. The image looks like this (taken from the Earth Engine site)\nThe left image is a satellite image at one point in time of the ImageCollection acquired earlier. The Earth Engine API provides the method .reduceRegions(), so you can use it. The arguments are: reducer=aggregation method (here, the total value), collection=region (a FeatureCollection object), and scale=resolution. Here, the first Image object in an ImageCollection (dataset) is called with the .reduceRegions() method.\n# initialize output box\rtime0 = dataset.first().get(\u0026#39;system:time_start\u0026#39;);\rfirst = dataset.first().reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(\u0026#39;time_start\u0026#39;, time0)\rSince we want time series data, we need to do the same thing for each Image in the ImageCollection. processing. Here, the function myfunc is defined to merge the Computed Object processed by the reduceRegions method with the previously processed one in the Image object, and it is passed to iterate. Finally, the generated data is downloaded using the getDownloadURL method as before (file format is csv).\n# define reduceRegions function for iteration\rdef myfunc(image,first):\radded = image.reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(\u0026#39;time_start\u0026#39;, image.get(\u0026#39;system:time_start\u0026#39;))\rreturn ee.FeatureCollection(first).merge(added)\r# implement iteration\rnightjp = dataset.filter(ee.Filter.date(\u0026#39;2014-02-01\u0026#39;,\u0026#39;2019-01-01\u0026#39;)).iterate(myfunc,first)\r# get url to download\ree.FeatureCollection(nightjp).getDownloadURL(filetype=\u0026#39;csv\u0026#39;,selectors=ee.FeatureCollection(nightjp).first().propertyNames().getInfo())\rI was able to get the url to the CSV file. I’m going to plot this time series to end today.\rHere’s what it looks like when you load the data.\nimport pandas as pd\rimport matplotlib.pyplot as plt\rimport os\ros.environ[\u0026#39;QT_QPA_PLATFORM_PLUGIN_PATH\u0026#39;] = \u0026#39;C:/Users/aashi/Anaconda3/Library/plugins/platforms\u0026#39;\rplt.style.use(\u0026#39;ggplot\u0026#39;)\rnightjp_csv.head()\r## system:index sum Country Unnamed: 3 Unnamed: 4\r## 0 2014/1/1 881512.4572 Japan NaN NaN\r## 1 2014/2/1 827345.3551 Japan NaN NaN\r## 2 2014/3/1 729110.4619 Japan NaN NaN\r## 3 2014/4/1 612665.8866 Japan NaN NaN\r## 4 2014/5/1 661434.5027 Japan NaN NaN\rplt.plot(pd.to_datetime(nightjp_csv[\u0026#39;system:index\u0026#39;]),nightjp_csv[\u0026#39;sum\u0026#39;])\rIt’s quite seasonal. It seems that the amount of light seems to increase in the winter because there are less hours of daylight. Nevertheless, it is a rapid increase. Next time, I would like to perform a statistical analysis based on this data and economic statistics, which is a proxy variable for business confidence. Please stay tuned.\n\r\r","date":1563235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563235200,"objectID":"59f787335065eeaf96823fbe7b34c9e7","permalink":"/en/post/post12/","publishdate":"2019-07-16T00:00:00Z","relpermalink":"/en/post/post12/","section":"post","summary":"I did an analysis using alternative data.","tags":["Python","preprocessing","Earth Engine"],"title":"Obtaining satellite image data with the Google Earth Engine API and do nowcasting on business conditions","type":"post"},{"authors":null,"categories":["競馬"],"content":"For the second time, I\u0026rsquo;d like to drop past race results in rvest again. If you haven\u0026rsquo;t seen the past articles, I suggest you look at that one first.\nThe reason I decided to take back the data this time is because I wanted to add more items to the explanatory variables when analyzing horse racing. So this time I created the program as an addition to the previous R script. The 14 new data items I added are as follows\n grass or dirt. right-handed or left-handed. race conditions (good or slight) weather the color of the horse\u0026rsquo;s coat (chestnut, deer hair, etc.) horse owner producers place of origin date of birth the father\u0026rsquo;s horse the mother\u0026rsquo;s horse winnings up to that race (available from 2003) jockey\u0026rsquo;s weight. increase or decrease in the weight of the jockey.  Actually, I haven\u0026rsquo;t finished collecting data yet, and the R program has been running for a long time (I\u0026rsquo;ve been running it for about 3 days). However, the program itself is running tightly and I\u0026rsquo;ll try to introduce the script. Maybe I\u0026rsquo;ll write the results in a postscript.\n1. Script The first step is to call the package.\n# Web scraping of horse racing data by rvest #install.packages(\u0026#34;rvest\u0026#34;) #if (!require(\u0026#34;pacman\u0026#34;)) install.packages(\u0026#34;pacman\u0026#34;) #install.packages(\u0026#34;beepr\u0026#34;) #install.packages(\u0026#34;RSQLite\u0026#34;) pacman::p_load(qdapRegex) library(rvest) library(stringr) library(dplyr) library(beepr) library(RSQLite) I\u0026rsquo;m pretty much warnning out, so I\u0026rsquo;m banning it and connecting to SQLite\n# warning prohibition options(warn=-1) # Connecting to SQLite con = dbConnect(SQLite(), \u0026#34;horse_data.db\u0026#34;, synchronous=\u0026#34;off\u0026#34;) Since the odds are only available since 1994, the data is taken from 1994 up to the most recent date. yahoo horse racing has races organized by month, so the data is taken using that as a variable. Basically, you go to List of race results for the relevant year and month and then go to Timetable for each day of the race for each day on that page. Since there are roughly a dozen or so races at each individual track, get the link and go to the race results page for each race. Then you will get the race results. The first step is to get a link to the timetable for each individual racecourse for each day.\nfor(year in 1994:2019){ start.time \u0026lt;- Sys.time() # calculate time # Retrieve the race results page on Yahoo! for (k in 1:12){ # K is for the month. tryCatch( { keiba.yahoo \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp/schedule/list/\u0026#34;, year,\u0026#34;/?month=\u0026#34;,k)) # Access to the list of race results for a given year and month Sys.sleep(2) race_lists \u0026lt;- keiba.yahoo %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) # Retrieve all URLs # Get the list of races for each day by track race_lists \u0026lt;- race_lists[str_detect(race_lists, pattern=\u0026#34;race/list/\\\\d+/\u0026#34;)==1] # Extracts URLs containing \u0026#34;results\u0026#34;. } , error = function(e){signal \u0026lt;- 1} ) Here, we only extract the links we get that contain the word result in the url. In essence, it is a link to the race tables of each racecourse. From here, we use the links to the race tables of the tracks to access that page and get the links to the pages that contain the results of all 12 races.\nfor (j in 1:length(race_lists)){ # where j is the link to the race table for the year in question. tryCatch( { race_list \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,race_lists[j])) race_url \u0026lt;- race_list %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) # 全urlを取得 # Get the URL of the race results race_url \u0026lt;- race_url[str_detect(race_url, pattern=\u0026#34;result\u0026#34;)==1] # Extracts URLs containing \u0026#34;results\u0026#34;. } , error = function(e){signal \u0026lt;- 1} ) Now that we have the links to the results of each race, it\u0026rsquo;s time to get the race results and the formatting part of the code. It\u0026rsquo;s quite a long and complicated code. The race results are stored in the following table attributes, so we\u0026rsquo;ll simply pull them first.\nfor (i in 1:length(race_url)){ # where i is the race that was held at the racecourse in question print(str_c(year, \u0026#34;/\u0026#34;, k, \u0026#34;/\u0026#34;,j, \u0026#34; group \u0026#34;, i,\u0026#34; order\u0026#34;)) tryCatch( { race1 \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,race_url[i])) # Get the URL of the race results signal \u0026lt;- 0 Sys.sleep(2) } , error = function(e){signal \u0026lt;- 1} ) # If the race is aborted or there are no errors in the process so far, run the process if (identical(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@class = \u0026#39;resultAtt mgnBL fntSS\u0026#39;]\u0026#34;) %\u0026gt;% html_text(),character(0)) == TRUE \u0026amp;\u0026amp; signal == 0){ # Scraping the race results race_result \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//table[@id = \u0026#39;raceScore\u0026#39;]\u0026#34;) %\u0026gt;% html_table() race_result \u0026lt;- do.call(\u0026#34;data.frame\u0026#34;,race_result) # Change the list to a data frame. colnames(race_result) \u0026lt;- c(\u0026#34;order\u0026#34;,\u0026#34;frame_number\u0026#34;,\u0026#34;horse_number\u0026#34;,\u0026#34;horse_name/age\u0026#34;,\u0026#34;time/margin\u0026#34;,\u0026#34;passing_rank/last_3F\u0026#34;,\u0026#34;jockey/weight\u0026#34;,\u0026#34;popularity/odds\u0026#34;,\u0026#34;trainer\u0026#34;) #　column renaming If you just get a table, it will not be useful for analysis because of the following \\n or multiple information in one cell. So, we need to mold the data into a form.\n# Passing order and time for 3 furlongs uphill race_result \u0026lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026#34;(\\\\d{2}-\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2}-\\\\d{2})|(\\\\d{2}-\\\\d{2})\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,\u0026#34;\\\\d{2}\\\\.\\\\d\u0026#34;))) race_result \u0026lt;- race_result[-6] # Time and Difference race_result \u0026lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,\u0026#34;\\\\d\\\\.\\\\d{2}\\\\.\\\\d|\\\\d{2}\\\\.\\\\d\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,\u0026#34;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+\u0026#34;))) race_result$margin[race_result$order==1] \u0026lt;- \u0026#34;トップ\u0026#34; race_result$margin[race_result$margin==\u0026#34;character(0)\u0026#34;] \u0026lt;- \u0026#34;大差\u0026#34; race_result$margin[race_result$order==0] \u0026lt;- NA race_result \u0026lt;- race_result[-5] # Horse\u0026#39;s name, age and weight race_result \u0026lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;[ァ-ヴー・]+\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;牡\\\\d+|牝\\\\d+|せん\\\\d+\u0026#34;))) race_result$horse_sex \u0026lt;- str_extract(race_result$horse_age, pattern = \u0026#34;牡|牝|せん\u0026#34;) race_result$horse_age \u0026lt;- str_extract(race_result$horse_age, pattern = \u0026#34;\\\\d\u0026#34;) race_result \u0026lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;\\\\d{3}\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;\\\\([\\\\+|\\\\-]\\\\d+\\\\)|\\\\([\\\\d+]\\\\)\u0026#34;))) race_result$horse_weight_change \u0026lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=\u0026#34;\u0026#34;) race_result \u0026lt;- dplyr::mutate(race_result,brinker=as.character(str_extract_all(race_result$`horse_name/age`,\u0026#34;B\u0026#34;))) race_result$brinker[race_result$brinker!=\u0026#34;B\u0026#34;] \u0026lt;- \u0026#34;N\u0026#34; race_result \u0026lt;- race_result[-4] # jockey race_result \u0026lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,\u0026#34;[ぁ-ん一-龠]+\\\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,jockey_weight=as.character(str_extract_all(race_result$`jockey/weight`,\u0026#34;\\\\d{2}\u0026#34;))) race_result$jockey_weight_change \u0026lt;- 0 race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026#34;☆\u0026#34;)==1] \u0026lt;- 1 race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026#34;△\u0026#34;)==1] \u0026lt;- 2 race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,\u0026#34;△\u0026#34;)==1] \u0026lt;- 3 race_result \u0026lt;- race_result[-4] # Odds and popularity race_result \u0026lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,\u0026#34;\\\\(.+\\\\)\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,\u0026#34;\\\\d+[^(\\\\d+.\\\\d)]\u0026#34;))) race_result$odds \u0026lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=\u0026#34;\u0026#34;) race_result \u0026lt;- race_result[-4] Next, we\u0026rsquo;ll import information other than the table we just retrieved. Specifically, race names, weather conditions, track conditions, dates, racecourses, etc. These information are listed at the top of the race results page.\n# Race Information race_date \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;raceTitName\u0026#39;]/p[@id = \u0026#39;raceTitDay\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_name \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;raceTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_distance \u0026lt;- race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//p[@id = \u0026#39;raceTitMeta\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_result \u0026lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,\u0026#34;\\\\d+年\\\\d+月\\\\d+日\u0026#34;))) race_result$race_date \u0026lt;- str_replace_all(race_result$race_date,\u0026#34;年\u0026#34;,\u0026#34;/\u0026#34;) race_result$race_date \u0026lt;- str_replace_all(race_result$race_date,\u0026#34;月\u0026#34;,\u0026#34;/\u0026#34;) race_result$race_date \u0026lt;- as.Date(race_result$race_date) race_course \u0026lt;- as.character(str_extract_all(race_date,pattern = \u0026#34;札幌|函館|福島|新潟|東京|中山|中京|京都|阪神|小倉\u0026#34;)) race_result$race_course \u0026lt;- race_course race_result \u0026lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,\u0026#34;\\\\s\u0026#34;,\u0026#34;\u0026#34;))) race_result \u0026lt;- dplyr::mutate(race_result,race_distance=as.character(str_extract_all(race_distance,\u0026#34;\\\\d+m\u0026#34;))) race_type=as.character(str_extract_all(race_distance,pattern = \u0026#34;芝|ダート\u0026#34;)) race_result$type \u0026lt;- race_type race_turn \u0026lt;- as.character(str_extract_all(race_distance,pattern = \u0026#34;右|左\u0026#34;)) race_result$race_turn \u0026lt;- race_turn if(length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg ryou\u0026#39;]\u0026#34;)) == 1){ race_result$race_condition \u0026lt;- \u0026#34;良\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg yayaomo\u0026#39;]\u0026#34;)) == 1){ race_result$race_condition \u0026lt;- \u0026#34;稍重\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg omo\u0026#39;]\u0026#34;)) == 1){ race_result$race_condition \u0026lt;- \u0026#34;重\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg furyou\u0026#39;]\u0026#34;)) == 1){ race_result$race_condition \u0026lt;- \u0026#34;不良\u0026#34; } else race_result$race_condition \u0026lt;- \u0026#34;NA\u0026#34; if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg hare\u0026#39;]\u0026#34;)) == 1){ race_result$race_weather \u0026lt;- \u0026#34;晴れ\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg ame\u0026#39;]\u0026#34;)) == 1){ race_result$race_weather \u0026lt;- \u0026#34;曇り\u0026#34; } else if (length(race1 %\u0026gt;% html_nodes(xpath = \u0026#34;//img[@class = \u0026#39;spBg kumori\u0026#39;]\u0026#34;)) == 1){ race_result$race_weather \u0026lt;- \u0026#34;雨\u0026#34; } else race_result$race_weather \u0026lt;- \u0026#34;その他\u0026#34; The next step is to get information about each horse. In fact, the horse\u0026rsquo;s name in the table we got earlier is a link, and if you follow the link, you can get [information about each horse] (https://keiba.yahoo.co.jp/directory/horse/2015105508/) (such as coat color and date of birth).\nhorse_url \u0026lt;- race1 %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) horse_url \u0026lt;- horse_url[str_detect(horse_url, pattern=\u0026#34;directory/horse\u0026#34;)==1] # Extract only links to horse information. for (l in 1:length(horse_url)){ tryCatch( { horse1 \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,horse_url[l])) Sys.sleep(0.5) horse_name \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;dirTitName\u0026#39;]/h1[@class = \u0026#39;fntB\u0026#39;]\u0026#34;) %\u0026gt;% html_text() horse \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026#34;//div[@id = \u0026#39;dirTitName\u0026#39;]/ul\u0026#34;) %\u0026gt;% html_text() race_result$colour[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;毛色：.+\u0026#34;)) race_result$owner[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;馬主：.+\u0026#34;)) race_result$farm[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;生産者：.+\u0026#34;)) race_result$locality[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;産地：.+\u0026#34;)) race_result$horse_birthday[race_result$horse_name==horse_name] \u0026lt;- as.character(str_extract_all(horse,\u0026#34;\\\\d+年\\\\d+月\\\\d+日\u0026#34;)) race_result$father[race_result$horse_name==horse_name] \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026#34;//td[@class = \u0026#39;bloodM\u0026#39;][@rowspan = \u0026#39;4\u0026#39;]\u0026#34;) %\u0026gt;% html_text() race_result$mother[race_result$horse_name==horse_name] \u0026lt;- horse1 %\u0026gt;% html_nodes(xpath = \u0026#34;//td[@class = \u0026#39;bloodF\u0026#39;][@rowspan = \u0026#39;4\u0026#39;]\u0026#34;) %\u0026gt;% html_text() } , error = function(e){ race_result$colour[race_result$horse_name==horse_name] \u0026lt;- NA race_result$owner[race_result$horse_name==horse_name] \u0026lt;- NA race_result$farm[race_result$horse_name==horse_name] \u0026lt;- NA race_result$locality[race_result$horse_name==horse_name] \u0026lt;- NA race_result$horse_birthday[race_result$horse_name==horse_name] \u0026lt;- NA race_result$father[race_result$horse_name==horse_name] \u0026lt;- NA race_result$mother[race_result$horse_name==horse_name] \u0026lt;- NA } ) } race_result$colour \u0026lt;- str_replace_all(race_result$colour,\u0026#34;毛色：\u0026#34;,\u0026#34;\u0026#34;) race_result$owner \u0026lt;- str_replace_all(race_result$owner,\u0026#34;馬主：\u0026#34;,\u0026#34;\u0026#34;) race_result$farm \u0026lt;- str_replace_all(race_result$farm,\u0026#34;生産者：\u0026#34;,\u0026#34;\u0026#34;) race_result$locality \u0026lt;- str_replace_all(race_result$locality,\u0026#34;産地：\u0026#34;,\u0026#34;\u0026#34;) #race_result$horse_birthday \u0026lt;- str_replace_all(race_result$horse_birthday,\u0026#34;年\u0026#34;,\u0026#34;/\u0026#34;) #race_result$horse_birthday \u0026lt;- str_replace_all(race_result$horse_birthday,\u0026#34;月\u0026#34;,\u0026#34;/\u0026#34;) #race_result$horse_birthday \u0026lt;- as.Date(race_result$horse_birthday) race_result \u0026lt;- dplyr::arrange(race_result,horse_number) # arrange in order of precedence The next step is to go and drop the amount of money you have won up to that race. You can access this by following the link marked [Runoffs] (https://keiba.yahoo.co.jp/race/denma/1802010601/) on the race results page. This is where you will find the prize money and you will get it.\nyosou_url \u0026lt;- race1 %\u0026gt;% html_nodes(\u0026#34;a\u0026#34;) %\u0026gt;% html_attr(\u0026#34;href\u0026#34;) yosou_url \u0026lt;- yosou_url[str_detect(yosou_url, pattern=\u0026#34;denma\u0026#34;)==1] if (length(yosou_url)==1){ yosou1 \u0026lt;- read_html(str_c(\u0026#34;https://keiba.yahoo.co.jp\u0026#34;,yosou_url)) Sys.sleep(2) yosou \u0026lt;- yosou1 %\u0026gt;% html_nodes(xpath = \u0026#34;//td[@class = \u0026#39;txC\u0026#39;]\u0026#34;) %\u0026gt;% as.character() prize \u0026lt;- yosou[grepl(\u0026#34;万\u0026#34;,yosou)==TRUE] %\u0026gt;% str_extract_all(\u0026#34;\\\\d+万\u0026#34;) prize \u0026lt;- t(do.call(\u0026#34;data.frame\u0026#34;,prize)) %\u0026gt;% as.character() race_result$prize \u0026lt;- prize race_result$prize \u0026lt;- str_replace_all(race_result$prize,\u0026#34;万\u0026#34;,\u0026#34;\u0026#34;) %\u0026gt;% as.numeric() } else race_result$prize \u0026lt;- NA We create a data frame called dataset to store the results of each race and store the data.\n## file storage if (k == 1 \u0026amp;\u0026amp; i == 1 \u0026amp;\u0026amp; j == 1){ dataset \u0026lt;- race_result } else { dataset \u0026lt;- rbind(dataset,race_result) } }else { print(\u0026#34;We couldn\u0026#39;t save it.\u0026#34;) } } } } beep(3) write.csv(dataset,\u0026#34;race_result2.csv\u0026#34;, row.names = FALSE) if (year == 1994){ dbWriteTable(con, \u0026#34;race_result\u0026#34;, dataset) } else { dbWriteTable(con, \u0026#34;temp\u0026#34;, dataset) dbSendQuery(con, \u0026#34;INSERT INTO race_result select * from temp\u0026#34;) dbSendQuery(con, \u0026#34;DROP TABLE temp\u0026#34;) } } end.time \u0026lt;- Sys.time() print(str_c(\u0026#34;処理時間は\u0026#34;,end.time-start.time,\u0026#34;です。\u0026#34;)) beep(5) options(warn = 1) dbDisconnect(con) That\u0026rsquo;s all. The data taken is as follows\nhead(race_result) ## order frame_number horse_number trainer passing_rank last_3F time\r## 1 10 1 1 田中 剛 09-09 39.0 1.14.3\r## 2 16 1 2 天間 昭一 11-11 40.3 1.15.7\r## 3 15 2 3 田中 清隆 14-14 39.4 1.15.1\r## 4 9 2 4 中舘 英二 08-08 39.1 1.14.3\r## 5 12 3 5 根本 康広 11-11 39.0 1.14.4\r## 6 4 3 6 杉浦 宏昭 04-04 38.4 1.13.2\r## margin horse_name horse_age horse_sex horse_weight\r## 1 アタマ サトノジョニー 3 牡 512\r## 2 3 1/2馬身 ツギノイッテ 3 牡 464\r## 3 3馬身 ギュウホ 3 牡 444\r## 4 2 1/2馬身 セイウンメラビリア 3 牝 466\r## 5 クビ サバイバルトリック 3 牝 450\r## 6 アタマ ステイホット 3 牝 474\r## horse_weight_change brinker jockey jockey_weight jockey_weight_change\r## 1 +30 N 松岡 正海 56 0\r## 2 +8 N 西田 雄一郎 56 0\r## 3 +8 N 杉原 誠人 56 0\r## 4 +10 N 村田 一誠 54 0\r## 5 -2 N 野中 悠太郎 51 0\r## 6 -2 N 大野 拓弥 54 0\r## odds popularity race_date race_course race_name race_distance type\r## 1 40.3 9 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 2 340.9 16 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 3 283.1 14 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 4 299.7 15 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 5 26.7 8 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## 6 2.4 1 2019-01-05 中山 サラ系3歳未勝利 1200m ダート\r## race_turn race_condition race_weather colour owner\r## 1 右 良 晴れ 栗毛 株式会社 サトミホースカンパニー\r## 2 右 良 晴れ 黒鹿毛 西村 新一郎\r## 3 右 良 晴れ 鹿毛 有限会社 ミルファーム\r## 4 右 良 晴れ 青鹿毛 西山 茂行\r## 5 右 良 晴れ 黒鹿毛 福田 光博\r## 6 右 良 晴れ 栗毛 小林 善一\r## farm locality horse_birthday father\r## 1 千代田牧場 新ひだか町 2016年1月29日 オルフェーヴル\r## 2 織笠 時男 青森県 2016年4月17日 スクワートルスクワート\r## 3 神垣 道弘 新ひだか町 2016年4月19日 ジャングルポケット\r## 4 石郷岡 雅樹 新冠町 2016年4月21日 キンシャサノキセキ\r## 5 原田牧場 日高町 2016年4月30日 リーチザクラウン\r## 6 社台ファーム 千歳市 2016年3月13日 キャプテントゥーレ\r## mother prize\r## 1 スパークルジュエル 0\r## 2 エプソムアイリス 0\r## 3 デライトシーン 0\r## 4 ドリームシップ 0\r## 5 フリーダムガール 180\r## 6 ステイアライヴ 455\r","date":1562976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562976000,"objectID":"290827b2bff5fc58ceb8d3e3c7aa86ac","permalink":"/en/post/post11/","publishdate":"2019-07-13T00:00:00Z","relpermalink":"/en/post/post11/","section":"post","summary":"I found some data missing from the last data collection, so I scraped it again.","tags":["R","Web_scraping","preprocessing","SQL"],"title":"Scraping past race results on yahoo horse racing on rvest (for the second time)","type":"post"},{"authors":null,"categories":["Work-related"],"content":"Hi. I attended a workshop on asset allocation at my place of work, so I\u0026rsquo;d like to run some simulations here, even though this field is completely outside my expertise. In this article, I would like to do an exercise on how to estimate the variance-covariance matrix (predictions) of a minimum variance portfolio as the base portfolio, referring to previous studies. The reference research is in the following paper (in the Journal of Operations Research).\nAsset Allocation with Correlation: A Composite Trade-Off\n1. Minimum Variance Portfolio I won\u0026rsquo;t go into a detailed explanation of the minimum variance portfolio here, but it is a portfolio that calculates the average return and variance of each asset (domestic stocks, foreign stocks, domestic bonds, foreign bonds, and alternatives), plots them on a quadratic plane of average return vertical and variance horizontal axes, calculates the range of possible investments, and then calculates the smallest variance in the set (see the chart below).\nIn an earlier study, Carroll et. al. (2017), the following is stated\n this paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.\n Even at present, it is difficult to estimate expected returns, and a minimum variance portfolio, which does not require it, is a useful and practical technique. The objective function of a minimum variance portfolio is, as the name implies, to minimize diversification. If we now denote the vector of collected returns for each asset by \\(r\\), the holding weight of each asset by \\(\\theta\\), and the portfolio return by \\(R_{p}\\), then the overall portfolio variance \\(var(R_{p})\\) can be written as follows.\n$$ var(R_{p}) = var(r^{T}\\theta) = E( (r^{T}\\theta)(r^{T}\\theta)^{T}) = \\theta^{T}\\Sigma\\theta $$\nwhere \\(Sigma\\) is the variance-covariance matrix of \\(r\\). Thus, the minimization problem is as follows\n$$ \\min_{\\theta}(\\theta^{T}\\Sigma\\theta) \\\ns.t 1^{T}\\theta = 1 $$\nHere we have added full investment as a constraint. Let\u0026rsquo;s use the Lagrangian to solve this problem. The Lagrange function \\(L\\) is as follows\n$$ L = \\theta^{T}\\Sigma\\theta + \\lambda(1^{T}\\theta - 1) $$\nThe first condition is\n$$ \\displaystyle\\frac{\\partial L}{\\partial \\theta} = 2\\Sigma\\theta + 1\\lambda = 0 \\\n\\displaystyle \\frac{\\partial L}{\\partial \\lambda} = 1^{T} \\theta = 1 $$\nSolving the first equation for \\(\\theta\\), we find that\n$$ \\theta = \\Sigma^{-1}1\\lambda^{*} $$\nwhere \\(\\lambda^{*}=-1/2\\lambda\\). Substitute this into the second formula and solve for \\(\\lambda^{*}\\)\n$$ 1^{T}\\Sigma1\\lambda^{} = 1 \\\n\\displaystyle \\lambda^{} = \\frac{1}{1^{T}\\Sigma^{-1}1} $$\nSince \\(\\theta = \\Sigma^{-1}1\\lambda^{*}\\), erasing \\(\\lambda^{*}\\) will cause it to be\n$$ \\displaystyle \\theta_{gmv} = \\frac{\\Sigma^{-1}1}{1^{T}\\Sigma^{-1}1} $$\nand we can now find the optimal weight. For now, I\u0026rsquo;ll implement this in R.\ngmv \u0026lt;- function(r_dat,r_cov){ library(MASS) i \u0026lt;- matrix(1,NCOL(r_dat),1) r_weight \u0026lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i) wr_dat \u0026lt;- r_dat*as.numeric(r_weight) portfolio \u0026lt;- apply(wr_dat,1,sum) pr_dat \u0026lt;- data.frame(wr_dat,portfolio) sd \u0026lt;- sd(portfolio) result \u0026lt;- list(r_weight,pr_dat,sd) names(result) \u0026lt;- c(\u0026#34;weight\u0026#34;,\u0026#34;return\u0026#34;,\u0026#34;portfolio risk\u0026#34;) return(result) } nlgmv \u0026lt;- function(r_dat,r_cov){ qp.out \u0026lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))), bvec=c(1,rep(0,NCOL(r_dat))),meq=1) r_weight \u0026lt;- qp.out$solution wr_dat \u0026lt;- r_dat*r_weight portfolio \u0026lt;- apply(wr_dat,1,sum) pr_dat \u0026lt;- data.frame(wr_dat,portfolio) sd \u0026lt;- sd(portfolio) result \u0026lt;- list(r_weight,pr_dat,sd) names(result) \u0026lt;- c(\u0026#34;weight\u0026#34;,\u0026#34;return\u0026#34;,\u0026#34;portfolio risk\u0026#34;) return(result) } The input is a return and variance-covariance matrix for each asset. The output is the weight, return and risk. nlgmv is a short-sale constrained version of the minimum variance portfolio. Since we cannot get an analytical solution, we are trying to get a numerical solution.\n2. How to predict the variance-covariance matrix We have found the formula for the minimum variance portfolio. The next step is to analyze how to find the input, the variance-covariance matrix. I think the most primitive approach would be the historical approach of finding the covariance matrix for a sample of return data available before that point in time, and then finding the minimum variance portfolio by fixing the value of the covariance matrix (i.e., the weights are also fixed). However, since this is just using historical averages for future projections, I\u0026rsquo;m sure there will be all sorts of problems. As a non-specialist, I can think of a situation where the return on asset A declined significantly the day before, but the variance of this asset is expected to remain high tomorrow, and the effect of yesterday is diluted by using the average. And I feel that not changing the weights from the start would also move away from the optimal point over time. However, there seems to be some trial and error in this area as to how to estimate it then. Also, in Carroll et. al. (2017), it is stated that,\n The estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.\n The estimates in this paper are based on the following models. All of them are unique in that the variance-covariance matrix is time-varying.\nA. Constant conditional correlation (CCC) model The original paper is here.\nFirst, from the relationship between the variance-covariance matrix and the correlation matrix, we have \\(\\Sigma_{t} = D_{t}R_{t}D_{t}\\), where \\(R_{t}\\) is the variance-covariance matrix and \\(D_{t}\\) is \\(diag(\\sigma_{1,t},...,\\sigma_{N,t})\\), a matrix whose diagonal components are the standard deviation of each asset in \\(tt\\) period. From here, we estimate \\(D_{t}\\) and \\(R_{t}\\) separately. First, \\(D_{t}\\) is estimated using the following multivariate GARCH model (1,1).\n$$ r_{t} = \\mu + u_{t} \\\nu_{t} = \\sigma_{t}\\epsilon \\\n\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}u_{t-1}^{2} + \\alpha_{2}\\sigma_{t-1}^{2} \\\n\\epsilon_{t} = NID(0,1) \\\nE(u_{t}|u_{t-1}) = 0 $$ where \\(\\mu\\) is the sample mean of the returns. \\(\\alpha_{i}\\) is the parameter to be estimated. Since we are estimating \\(D_{t}\\) with GARCH, we can say that we are modeling a relationship where the distribution of returns follows a thicker base distribution than the normal distribution and where the change in returns is not constant but depends on the variance of the previous day. We can say that we have estimated \\(D_{t}\\) in this way. The next step in the estimation of \\(R_{t}\\) is to take a historical approach to the estimation of \\(R_{t}\\), in which the returns are sampled. In other words, \\(R_{t}\\) is a constant. Thus, we are making the assumption that the magnitude of return variation varies with time, but the relative relationship of each asset is invariant.\nB. Dynamic Conditional Correlation (DCC) model The original paper is here。\nこちらのモデルでは、$D_{t}$を求めるところまでは①と同じですが、[$R_{t}$の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、$tex:t$期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列$R_{t}$は、\nThis model is the same as (1) up to the point of finding \\(D_{t}\\), but the way of finding \\(R_{t}\\) is different, and we use ARMA(1,1) to estimate it. Since the correlation matrix is still not a constant, we are going to use the available returns up to the \\(t\\) period to make the estimate. The correlation matrix \\(R_{t}\\) in this model is\n$$ R_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2} $$\nwhere \\(Q_{t}\\) is a conditional variance-covariance matrix in the \\(t\\) period, formulated as follows,\n$$ Q_{t} = \\bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\\epsilon_{i,t-1}\\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1} $$\nwhere \\(\\bar{Q}\\) is the variance-covariance matrix calculated in a historical way and \\(a,b\\) is the parameter. This method differs from the previous one in that it assumes that not only does the magnitude of return variation vary with time, but also the relative relationship of each asset changes over time. Since we have observed some events in which the returns of all assets fall and the correlation of each asset becomes positive during a financial crisis, this formulation may be attractive.\nC. Dynamic Equicorrelation (DECO) model The original paper is here。\nI haven\u0026rsquo;t read this paper exactly yet, but from the definition of the correlation matrix \\(R_{t}\\) becomes\n$$ R_{t} = (1-\\rho_{t})I_{N} + \\rho_{t}1 $$\nHere, \\(\\rho_{t}\\) is a scalar and a coefficient for the degree of equicorrelation, and I understand that equicorrelation is an average pair-wise correlation. In other words, if there are no missing values, it\u0026rsquo;s no different than a normal correlation. However, it seems to be a good estimator in that respect, because as assets increase, such problems need to be addressed. We can calculate \\(\\rho_{t}\\) as follows.\n$$ \\displaystyle \\rho_{t} = \\frac{1}{N(N-1)}(\\iota^{T}R_{t}^{DCC}\\iota - N) = \\frac{2}{N(N-1)}\\sum_{i\u0026gt;j}\\frac{q_{ij,t}}{\\sqrt{q_{ii,t} q_{jj,t}}} $$\nwhere \\(\\iota\\) is an \\(N×1\\) vector with all elements being 1. And \\(q_{ij,t}\\) is the \\(i,j\\) element of \\(Q_{t}\\).\nNow that we\u0026rsquo;ve modeled the variance-covariance matrix, we\u0026rsquo;ll implement it so far in R.\ncarroll \u0026lt;- function(r_dat,FLG){ library(rmgarch) if(FLG == \u0026#34;benchmark\u0026#34;){ H \u0026lt;- cov(r_dat) }else{ #1. define variables N \u0026lt;- NCOL(r_dat) # the number of assets #2. estimate covariance matrix basic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = \u0026#39;sGARCH\u0026#39;), distribution.model = \u0026#39;norm\u0026#39;) multi_garch = multispec(replicate(N, basic_garch)) dcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = \u0026#34;mvnorm\u0026#34;,model = \u0026#34;DCC\u0026#34;) fit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE)) forecast_dcc_garch \u0026lt;- dccforecast(fit_dcc_garch) if (FLG == \u0026#34;CCC\u0026#34;){ #Constant conditional correlation (CCC) model D \u0026lt;- sigma(forecast_dcc_garch) R_ccc \u0026lt;- cor(r_dat) H \u0026lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1]) colnames(H) \u0026lt;- colnames(r_dat) rownames(H) \u0026lt;- colnames(r_dat) } else{ #Dynamic Conditional Correlation (DCC) model H \u0026lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1]) if (FLG == \u0026#34;DECO\u0026#34;){ #Dynamic Equicorrelation (DECO) model one \u0026lt;- matrix(1,N,N) iota \u0026lt;- rep(1,N) Q_dcc \u0026lt;- rcor(forecast_dcc_garch,type=\u0026#34;Q\u0026#34;)[[1]][,,1] rho \u0026lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N)) D \u0026lt;- sigma(forecast_dcc_garch) R_deco \u0026lt;- (1-rho)*diag(1,N,N) + rho*one H \u0026lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1]) colnames(H) \u0026lt;- colnames(r_dat) rownames(H) \u0026lt;- colnames(r_dat) } } } return(H) } I shouldn\u0026rsquo;t normally use the package, but since it\u0026rsquo;s an exercise today I\u0026rsquo;m only going to pursue the results of the estimates, and I\u0026rsquo;ll be writing a post about GARCH in the next week or so. Now we\u0026rsquo;re ready to go. We can now put the return data into this function to calculate the variance-covariance matrix and use it to calculate the minimum variance portfolio.\n3. Collection of data for testing The data was based on the following article.\n(Introduction to Asset Allocation)[https://www.r-bloggers.com/introduction-to-asset-allocation/]\nWe used NAV data for an ETF (iShares) that is linked to the following index\n S\u0026amp;P500 NASDAQ100 MSCI Emerging Markets Russell 2000 MSCI EAFE US 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index) U.S. Real Estate(the Dow Jones US Real Estate Index) gold bullion market  The first step is to collect data.\nlibrary(quantmod) #************************** # ★8 ASSETS SIMULATION # SPY - S\u0026amp;P 500  # QQQ - Nasdaq 100 # EEM - Emerging Markets # IWM - Russell 2000 # EFA - EAFE # TLT - 20 Year Treasury # IYR - U.S. Real Estate # GLD - Gold #************************** # load historical prices from Yahoo Finance symbol.names = c(\u0026#34;S\u0026amp;P 500\u0026#34;,\u0026#34;Nasdaq 100\u0026#34;,\u0026#34;Emerging Markets\u0026#34;,\u0026#34;Russell 2000\u0026#34;,\u0026#34;EAFE\u0026#34;,\u0026#34;20 Year Treasury\u0026#34;,\u0026#34;U.S. Real Estate\u0026#34;,\u0026#34;Gold\u0026#34;) symbols = c(\u0026#34;SPY\u0026#34;,\u0026#34;QQQ\u0026#34;,\u0026#34;EEM\u0026#34;,\u0026#34;IWM\u0026#34;,\u0026#34;EFA\u0026#34;,\u0026#34;TLT\u0026#34;,\u0026#34;IYR\u0026#34;,\u0026#34;GLD\u0026#34;) getSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, auto.assign = TRUE) #gn dates for all symbols \u0026amp; convert to monthly hist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD) month.ends = endpoints(hist.prices, \u0026#39;day\u0026#39;) hist.prices = Cl(hist.prices)[month.ends, ] colnames(hist.prices) = symbols # remove any missing data hist.prices = na.omit(hist.prices[\u0026#39;1995::\u0026#39;]) # compute simple returns hist.returns = na.omit( ROC(hist.prices, type = \u0026#39;discrete\u0026#39;) ) # compute historical returns, risk, and correlation ia = list() ia$expected.return = apply(hist.returns, 2, mean, na.rm = T) ia$risk = apply(hist.returns, 2, sd, na.rm = T) ia$correlation = cor(hist.returns, use = \u0026#39;complete.obs\u0026#39;, method = \u0026#39;pearson\u0026#39;) ia$symbols = symbols ia$symbol.names = symbol.names ia$n = length(symbols) ia$hist.returns = hist.returns # convert to annual, year = 12 months annual.factor = 12 ia$expected.return = annual.factor * ia$expected.return ia$risk = sqrt(annual.factor) * ia$risk rm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD) Here\u0026rsquo;s how the returns are plotted.\nPerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = \u0026#34;Performance summary\u0026#34;) Next, we\u0026rsquo;ll code the backtest. We\u0026rsquo;ll publish the code all at once.\n# BACK TEST backtest \u0026lt;- function(r_dat,FLG,start_date,span,learning_term,port){ #----------------------------------------- # BACKTEST # r_dat - return data(xts object)  # FLG - flag(CCC,DCC,DECO) # start_date - start date for backtest # span - rebalance frequency # learning_term - learning term (days) # port - method of portfolio optimization #----------------------------------------- library(stringi) initial_dat \u0026lt;- r_dat[stri_c(as.Date(start_date)-learning_term,\u0026#34;::\u0026#34;,as.Date(start_date))] for (i in NROW(initial_dat):NROW(r_dat)) { if (i == NROW(initial_dat)){ H \u0026lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG) if (port == \u0026#34;nlgmv\u0026#34;){ result \u0026lt;- nlgmv(initial_dat,H) }else if (port == \u0026#34;risk parity\u0026#34;){ result \u0026lt;- risk_parity(initial_dat,H) } weight \u0026lt;- t(result$weight) colnames(weight) \u0026lt;- colnames(initial_dat) p_return \u0026lt;- initial_dat[NROW(initial_dat),]*result$weight } else { if (i %in% endpoints(r_dat,span)){ H \u0026lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG) if (port == \u0026#34;nlgmv\u0026#34;){ result \u0026lt;- nlgmv(test_dat,H) }else if (port == \u0026#34;risk parity\u0026#34;){ result \u0026lt;- risk_parity(test_dat,H) } } weight \u0026lt;- rbind(weight,t(result$weight)) p_return \u0026lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight) } if (i != NROW(r_dat)){ term \u0026lt;- stri_c(index(r_dat[i+1,])-learning_term,\u0026#34;::\u0026#34;,index(r_dat[i+1,])) test_dat \u0026lt;- r_dat[term] } } p_return$portfolio \u0026lt;- xts(apply(p_return,1,sum),order.by = index(p_return)) weight.xts \u0026lt;- xts(weight,order.by = index(p_return)) result \u0026lt;- list(p_return,weight.xts) names(result) \u0026lt;- c(\u0026#34;return\u0026#34;,\u0026#34;weight\u0026#34;) return(result) } CCC \u0026lt;- backtest(hist.returns,\u0026#34;CCC\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) DCC \u0026lt;- backtest(hist.returns,\u0026#34;DCC\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) DECO \u0026lt;- backtest(hist.returns,\u0026#34;DECO\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) benchmark \u0026lt;- backtest(hist.returns,\u0026#34;benchmark\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) result \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio) colnames(result) \u0026lt;- c(\u0026#34;CCC\u0026#34;,\u0026#34;DCC\u0026#34;,\u0026#34;DECO\u0026#34;,\u0026#34;benchmark\u0026#34;) Here\u0026rsquo;s a graph of the calculation results.\nPerformanceAnalytics::charts.PerformanceSummary(result,main = \u0026#34;BACKTEST\u0026#34;) I did not use the minimum variance portfolio defined above because I imposed a short sale constraint. Apparently it\u0026rsquo;s difficult to solve analytically with this alone, so I\u0026rsquo;m going to solve numerically. I used a weekly rebalancing period, so it took me a while to calculate the results on my own PC, but I was able to calculate the results.\nSince Lehman, it appears to have outperformed the benchmark equal weighted portfolio. In particular, DECO is looking good. I would like to rethink the meaning of Equicorrelation. The change in each incorporation ratio is as follows.\n# plot allocation weighting d_allocation \u0026lt;- function(ggweight,title){ #install.packages(\u0026#34;tidyverse\u0026#34;) library(tidyverse) ggweight \u0026lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method) ggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) + geom_area(colour=\u0026#34;black\u0026#34;,size=.1) + scale_y_continuous(limits = c(0,1)) + labs(title=title) + facet_grid(method~.) } gmv_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026#34;CCC\u0026#34;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026#34;DCC\u0026#34;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026#34;DECO\u0026#34;,Date=index(DECO$weight))) # plot allocation weighting d_allocation(gmv_weight,\u0026#34;GMV Asset Allocation\u0026#34;) They seem to have increased their allocation to TLT, or U.S. Treasuries, during Lehman, while CCC and DCC have a high allocation to U.S. Treasuries in other areas as well, and the often-cited problem of a minimally diversified portfolio seems to be occurring here as well. On the other hand, DECO has a unique mix ratio, and I think we need to go back and read the paper again.\nPS（2019/3/3） So far, I\u0026rsquo;ve been doing the analysis with a minimum variance portfolio, but I also wanted to see the results of risk parity, so I wrote the code for that as well.\nrisk_parity \u0026lt;- function(r_dat,r_cov){ fn \u0026lt;- function(weight, r_cov) { N \u0026lt;- NROW(r_cov) risks \u0026lt;- weight * (r_cov %*% weight) g \u0026lt;- rep(risks, times = N) - rep(risks, each = N) return(sum(g^2)) } dfn \u0026lt;- function(weight,r_cov){ out \u0026lt;- weight for (i in 0:length(weight)) { up \u0026lt;- dn \u0026lt;- weight up[i] \u0026lt;- up[i]+.0001 dn[i] \u0026lt;- dn[i]-.0001 out[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002 } return(out) } std \u0026lt;- sqrt(diag(r_cov)) x0 \u0026lt;- 1/std/sum(1/std) res \u0026lt;- nloptr::nloptr(x0=x0, eval_f=fn, eval_grad_f=dfn, eval_g_eq=function(weight,r_cov) { sum(weight) - 1 }, eval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) }, lb=rep(0,length(std)),ub=rep(1,length(std)), opts = list(\u0026#34;algorithm\u0026#34;=\u0026#34;NLOPT_LD_SLSQP\u0026#34;,\u0026#34;print_level\u0026#34; = 0,\u0026#34;xtol_rel\u0026#34;=1.0e-8,\u0026#34;maxeval\u0026#34; = 1000), r_cov = r_cov) r_weight \u0026lt;- res$solution names(r_weight) \u0026lt;- colnames(r_cov) wr_dat \u0026lt;- r_dat*r_weight portfolio \u0026lt;- apply(wr_dat,1,sum) pr_dat \u0026lt;- data.frame(wr_dat,portfolio) sd \u0026lt;- sd(portfolio) result \u0026lt;- list(r_weight,pr_dat,sd) names(result) \u0026lt;- c(\u0026#34;weight\u0026#34;,\u0026#34;return\u0026#34;,\u0026#34;portfolio risk\u0026#34;) return(result) } CCC \u0026lt;- backtest(hist.returns,\u0026#34;CCC\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) DCC \u0026lt;- backtest(hist.returns,\u0026#34;DCC\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) DECO \u0026lt;- backtest(hist.returns,\u0026#34;DECO\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) benchmark \u0026lt;- backtest(hist.returns,\u0026#34;benchmark\u0026#34;,\u0026#34;2007-01-04\u0026#34;,\u0026#34;months\u0026#34;,365,\u0026#34;risk parity\u0026#34;) result \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio) colnames(result) \u0026lt;- c(\u0026#34;CCC\u0026#34;,\u0026#34;DCC\u0026#34;,\u0026#34;DECO\u0026#34;,\u0026#34;benchmark\u0026#34;) Here\u0026rsquo;s the result.\nPerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026#34;BACKTEST COMPARISON\u0026#34;) library(plotly) # plot allocation weighting riskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026#34;CCC\u0026#34;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026#34;DCC\u0026#34;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026#34;DECO\u0026#34;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026#34;benchmark\u0026#34;,Date=index(benchmark$weight))) PerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026#34;BACKTEST COMPARISON\u0026#34;) riskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026#34;CCC\u0026#34;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026#34;DCC\u0026#34;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026#34;DECO\u0026#34;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026#34;benchmark\u0026#34;,Date=index(benchmark$weight))) # plot allocation weighting d_allocation(riskparity_weight, \u0026#34;Risk Parity Asset Allocation\u0026#34;) The results are positive, with all methods outperforming benchmark. As expected, the estimation of the variance-covariance matrix seems to be performing well. The good performance of DECO may also be due to the fact that it uses the average of the correlation coefficients of each asset pair in the correlation matrix, which resulted in a greater inclusion of risk assets than the other methods. The weights are as follows\nThat\u0026rsquo;s it for today, for now.\n","date":1550361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551571200,"objectID":"0c6f6cbe96ac9b1239a629e09477d6d1","permalink":"/en/post/post2/","publishdate":"2019-02-17T00:00:00Z","relpermalink":"/en/post/post2/","section":"post","summary":"We had an in-house workshop and spent the holidays locked up in the Library of Congress, fishing for prior research, which allowed us to build a subtle asset allocation model.","tags":["R"],"title":"I built the Asset Allocation Model in R.","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/en/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/en/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["macroeconomics"],"content":"　Hi. Yesterday, I wrote an article about Bayesian Vector Autoregression. In the article, the topic of hyperparameter tuning came up, and looking for some efficient way to tune it, I found Bayesian Optimization. Since I am planning to use machine learning methods in daily GDP, I thought that Bayesian Optimization could be quite useful, and I spent all night yesterday to understand it.\nI will implement it here, but Bayesian Optimization uses Gaussian Pocess Regression (GPR), and my motivation for writing this entry was to implement it first. I will write about the implementation of Bayesian Optimization after this entry.\n1. What is GPR? 　The GRP is, simply put, a type of nonlinear regression method using Bayesian estimation. Although the model itself is linear, it is characterized by its ability to estimate infinite nonlinear transformations of input variables using a kernel trick as explanatory variables (depending on what you choose for the kernel). The GPR assumes that \\(N\\) input and teacher data are available for training, and the \\(N+1\\) of input data are also available. From this situation, we can predict the \\(N+1\\)th teacher data.\nThe data contains noise and follows the following probability model. $$ t_{i} = y_{i} + \\epsilon_{i} $$ where \\(t_{i}\\) is the \\(i\\)th observable teacher data (scalar), \\(y_{i}\\) is the unobservable output data (scalar), and \\(\\beta_{i}\\) follows a normal distribution \\(N(0, \\beta^{-1})\\) with measurement error. \\(y_{i}\\) follows the following probability model.\n$$ \\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i}) $$\nwhere \\(x_{i}\\) is the ith input data vector, \\(\\phi(・)\\) is the non-linear function and \\(\\bf{w}^{T}\\) is the weight coefficient (regression coefficient) vector for each input data. As a nonlinear function, I assume \\(\\psi(x_{i}) = (x_{1,i}, x_{1,i}^{2},... ,x_{1,i}x_{2,i},...)\\). ($x_{1,i}$ is the first variable in the \\(i\\)th input data \\(x_{i}\\)). The conditional probability of obtaining \\(t_{i}\\) from the probabilistic model of the teacher data, with the \\(i\\)th output data \\(y_{i}\\) obtained, is\n$$ p(t_{i}|y_{i}) = N(t_{i}|y_{i},\\beta^{-1}) $$\n\\(\\displaystyle \\textbf{t} = (t_{1},... ,t_{n})^{T}\\) and \\(\\displaystyle \\textbf{y} = (y_{1},... ,y_{n})^{T}\\), then by extending the above equation, we have\n$$ \\displaystyle p(\\textbf{t}|\\textbf{y}) = N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N}) $$\nWe assume that the expected value of \\(\\textbf{w}\\) as a prior distribution is 0, and all variances are \\(\\alpha\\). We also assume that \\(\\displaystyle \\textbf{y}\\) follows a Gaussian process. A Gaussian process is one where the simultaneous distribution of \\(\\displaystyle \\textbf{y}\\) follows a multivariate Gaussian distribution. In code, it looks like this\n\\normalsize\n# Define Kernel function Kernel_Mat \u0026lt;- function(X,sigma,beta){ N \u0026lt;- NROW(X) K \u0026lt;- matrix(0,N,N) for (i in 1:N) { for (k in 1:N) { if(i==k) kdelta = 1 else kdelta = 0 K[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta } } return(K) } N \u0026lt;- 10 # max value of X M \u0026lt;- 1000 # sample size X \u0026lt;- matrix(seq(1,N,length=M),M,1) # create X testK \u0026lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix library(MASS) P \u0026lt;- 6 # num of sample path Y \u0026lt;- matrix(0,M,P) # define Y for(i in 1:P){ Y[,i] \u0026lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y } # Plot matplot(x=X,y=Y,type = \u0026#34;l\u0026#34;,lwd = 2) \\normalsize　The covariance matrix \\(K\\) between the elements of \\(\\displaystyle \\textbf{y}\\), \\(\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\\) is calculated using the kernel method from the input \\(x\\). Then, from this \\(K\\) and average 0, we generate six series of multivariate normal random numbers and plot them.As these series are computed from a covariance matrix, we model that the more positive the covariance of each element, the more likely they are to be the same. Also, as you can see in the graphs, the graphs are very smooth and very flexible in their representation. The code samples and plots 1000 input points, limiting the input to 0 to 10 due to computational cost, but in principle, \\(x\\) is defined in the real number space, so \\(p(\\textbf{y})\\) follows an infinite dimensional multivariate normal distribution.\nAs described above, since \\(\\displaystyle \\textbf{y}\\) is assumed to follow a Gaussian process, \\(p(\\textbf{y})\\) follows a multivariate normal distribution \\(N(\\textbf{y}|0,K)\\) with simultaneous probability \\(p(\\textbf{y})\\) averaging 0 and the variance covariance matrix \\(K\\). Each element \\(K_{i,j}\\) of \\(K\\) is\n$$ \\begin{eqnarray}\rK_{i,j} \u0026amp;=\u0026amp; cov[y_{i},y_{j}] = cov[\\textbf{w}\\phi(x_{i}),\\textbf{w}\\phi(x_{j})] \\\\\r\u0026amp;=\u0026amp;\\phi(x_{i})\\phi(x_{j})cov[\\textbf{w},\\textbf{w}]=\\phi(x_{i})\\phi(x_{j})\\alpha\r\\end{eqnarray} $$\nHere, the \\(\\phi(x_{i})\\phi(x_{j})\\alpha\\) is more expensive as the dimensionality of the \\(\\phi(x_{i})\\) is increased (i.e., the more non-linear transformation is applied, the less the calculation is completed). However, when the kernel function \\(k(x,x')\\) is used, the computational complexity is higher in the dimensions of the sample size of the input data \\(x_{i},x_{j}\\), so the computation becomes easier. There are several types of kernel functions, but the following Gaussian kernels are commonly used.\n$$ k(x,x') = a \\exp(-b(x-x')^{2}) $$\nNow that we have defined the concurrent probability of \\(\\displaystyle \\textbf{y}\\), we can find the joint probability of \\(\\displaystyle \\textbf{t}\\).\n$$ \\begin{eqnarray}\r\\displaystyle p(\\textbf{t}) \u0026amp;=\u0026amp; \\int p(\\textbf{t}|\\textbf{y})p(\\textbf{y}) d\\textbf{y} \\\\\r\\displaystyle \u0026amp;=\u0026amp; \\int N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N})N(\\textbf{y}|0,K)d\\textbf{y} \\\\\r\u0026amp;=\u0026amp; N(\\textbf{y}|0,\\textbf{C}_{N})\r\\end{eqnarray} $$\nwhere \\(\\textbf{C}_{N} = K + \\beta^{-1}\\beta^{I}_{N}\\). Note that the last expression expansion uses the regenerative nature of the normal distribution (the proof can be easily derived from the moment generating function of the normal distribution). The point is just to say that the covariance is the sum of the covariances of the two distributions, since they are independent. Personally, I imagine that \\(p(\\textbf{y})\\) is the prior distribution of the Gaussian process I just described, \\(p(\\textbf{t}|\\textbf{y})\\) is the likelihood function, and \\(p(\\textbf{t})\\) is the posterior distribution. The only constraint on the prior distribution \\(p(\\textbf{y})\\) is that it is smooth with a loosely constrained distribution. The joint probability of \\(N\\) observable teacher data \\(\\textbf{t}\\) and \\(t_{N+1}\\) is\n$$ p(\\textbf{t},t_{N+1}) = N(\\textbf{t},t_{N+1}|0,\\textbf{C}_{N+1}) $$\nwhere \\(\\textbf{C}_{N+1}\\) is\n$$ \\textbf{C}{N+1} = \\left( \\begin{array}{cccc} \\textbf{C}{N} \u0026amp; \\textbf{k} \\\n\\textbf{k}^{T} \u0026amp; c \\\n\\end{array} \\right) $$\nwhere \\(\\textbf{k} = (k(x_{1},x_{N+1}),...,k(x_{N},x_{N+1}))\\) and \\(c = k(x_{N+1},x_{N+1})\\). The conditional distribution \\(p(t_{N+1}|\\textbf{t})\\) can be obtained from the joint distribution of \\(\\textbf{t}\\) and \\(t_{N+1}\\).\n$$ p(t_{N+1}|\\textbf{t}) = N(t_{N+1}|\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{t},c-\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{k}) $$\nIn calculating the conditional distribution, we use Properties of the conditional multivariate normal distribution. As you can see from the above equation, the conditional distribution \\(p(t_{N+1}|\\textbf{t})\\) can be calculated if \\(N+1\\) input data, \\(N\\) teacher data, and parameters \\(a,b\\) of the kernel function are known, so if any point is given as input data, it is possible to approximate the Generating Process. The nice thing about the GPR is that it gives predictions without the direct estimation of the above defined probabilistic model \\(\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\\). The stochastic model has \\(\\phi(x_{i})\\), which converts the input data to a high-dimensional vector through a nonlinear transformation. Therefore, the higher the dimensionality, the larger the computational complexity of the \\(\\phi(x_{i})\\phi(x_{j})\\alpha\\) will be, but the GPR uses a kernel trick, so the computational complexity of the sample size dimension of the input data vector will be sufficient.\n2. Implementation of the `GPR' 　For now, let\u0026rsquo;s implement this in R, which I\u0026rsquo;ve implemented in PRML test data, so I tweaked it. \\normalsize\nlibrary(ggplot2) library(grid) # 1.Gaussian Process Regression # PRML\u0026#39;s synthetic data set curve_fitting \u0026lt;- data.frame( x=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000), t=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502)) f \u0026lt;- function(beta, sigma, xmin, xmax, input, train) { kernel \u0026lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function K \u0026lt;- outer(input, input, kernel); # calc gram matrix C_N \u0026lt;- K + diag(length(input))/beta m \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean  m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance x \u0026lt;- seq(xmin,xmax,length=100) output \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)), tx=input,ty=train), aes(x=x1,y=m)) + geom_line() + geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) + geom_point(aes(x=tx,y=ty)) return(output) } grid.newpage() # make a palet pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2 print(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) print(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) print(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1)) print(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) \\normalsizeThe \\(beta^{-1}\\) represents the measurement error. The higher the value of $\\beta$ (i.e., the smaller the measurement error), the easier it is to overfit, since the error of the predictions is less than that of the data already available. This is the case in the top left corner of the figure above. The top left corner is \\(\\beta=400\\), which means that it overfits the current data available. Conversely, a small value of \\(\\beta\\) will produce predictions that ignore the errors with the teacher data, but may improve the generalization performance. The top right figure shows this. For \\(beta=4\\), the average barely passes through the data points we have, and \\(b\\) is currently available. \\(b\\) represents the magnitude of the effect of the data we have at the moment on the surroundings. If \\(b\\) is small, the adjacent points will interact strongly with each other, which may reduce the accuracy but increase the generalization performance. Conversely, if \\(b\\) is large, the result will be unnatural, fitting only individual points. This is illustrated in the figure below right ($b=\\frac{1}{0.03}, \\beta=25$). As you can see, the graph is overfitting because of the large \\(\\beta\\) and because \\(b\\) is also large, so it fits only individual points, resulting in an absurdly large graph. The bottom left graph is the best. It has \\(b=\\frac{1}{0.3}\\), and \\(b=2\\). Let\u0026rsquo;s try extending the x interval of this graph to [0,2]. Then we get the following graph.\n\\normalsize\ngrid.newpage() # make a palet pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2 print(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1)) print(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) \\normalsizeAs you can see, all the graphs except the bottom left one have a band of 95% confidence intervals that immediately widen and are completely useless where there are no data points. On the other hand, the lower left graph has a decent band up to 1.3 to 1.4, and the average value seems to pass through a point that is consistent with our intuitive understanding of the function. You can also see that if you are too far away from the observable data points, you will get a normal distribution with a mean of 0 and a variance of 1 no matter what you give to the parameters. Now that we have shown that the accuracy of the prediction of the out-sample varies depending on the value of the parameters, the question here is how to estimate these hyperparameters. This is done by using the gradient method to find the hyperparameters that maximize the log-likelihood function \\(\\ln p(\\bf{t}|a,b)\\) (($\\beta$ seems to be of a slightly different type, and the developmental discussion appears to take other tuning methods. We haven\u0026rsquo;t gotten to that level yet, so we\u0026rsquo;ll calibrate it here). Since \\(p(\\textbf{t}) = N(\\textbf{y}|0, \\textbf{C}_{N})\\), the log-likelihood function is\n$$ \\displaystyle \\ln p(\\textbf{t}|a,b,\\beta) = -\\frac{1}{2}\\ln|\\textbf{C}{N}| - \\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}{N}^{-1}\\textbf{k} $$\nAfter that, we can differentiate this with the parameters and solve the obtained simultaneous equations to get the maximum likelihood estimator. Now let\u0026rsquo;s get the derivatives.\n$$ \\displaystyle \\frac{\\partial}{\\partial \\theta_{i}} \\ln p(\\textbf{t}|\\theta) = -\\frac{1}{2}Tr(\\textbf{C}_{N}^{-1}\\frac{\\partial \\textbf{C}_{N}}{\\partial \\theta_{i}}) + \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1} \\frac{\\partial\\textbf{C}_{N}}{\\partial\\theta_{i}}\\textbf{C}_{N}^{-1}\\textbf{t} $$\nwhere \\(theta\\) is the parameter set and \\(theta_{i}\\) represents the \\(i\\)th parameter. If you don\u0026rsquo;t understand this derivative [here](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning %20-%20Springer%20Springer%20%202006.pdf) in the supplement to (C.21) and (C.22) equations. Since we are using the Gaussian kernel in this case, we get\n$$ \\displaystyle \\frac{\\partial k(x,x')}{\\partial a} = \\exp(-b(x-x')^{2}) \\\n\\displaystyle \\frac{\\partial k(x,x')}{\\partial b} = -a(x-x')^{2}\\exp(-b(x-x')^{2}) $$\nfrom the above formula. However, this time we will use the gradient method to find the best parameters. Here\u0026rsquo;s the code for the implementation (it\u0026rsquo;s pretty much a lost cause).\n\\normalsize\ng \u0026lt;- function(xmin, xmax, input, train){ # initial value beta = 100 b = 1 a = 1 learning_rate = 0.1 itermax \u0026lt;- 1000 if (class(input) == \u0026#34;numeric\u0026#34;){ N \u0026lt;- length(input) } else { N \u0026lt;- NROW(input) } kernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel derivative_a \u0026lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2) derivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2) dloglik_a \u0026lt;- function(C_N,y,x1,x2) { -sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y } dloglik_b \u0026lt;- function(C_N,y,x1,x2) { -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y } # loglikelihood function likelihood \u0026lt;- function(b,a,x,y){ kernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2) K \u0026lt;- outer(x, x, kernel) C_N \u0026lt;- K + diag(N)/beta itermax \u0026lt;- 1000 l \u0026lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y return(l) } K \u0026lt;- outer(input, input, kernel) C_N \u0026lt;- K + diag(N)/beta for (i in 1:itermax){ kernel \u0026lt;- function(x1, x2) a*exp(-b*(x1-x2)^2) derivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2) dloglik_b \u0026lt;- function(C_N,y,x1,x2) { -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y } K \u0026lt;- outer(input, input, kernel) # calc gram matrix C_N \u0026lt;- K + diag(N)/beta l \u0026lt;- 0 if(abs(l-likelihood(b,a,input,train))\u0026lt;0.0001\u0026amp;i\u0026gt;2){ break }else{ a \u0026lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input)) b \u0026lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input)) } l \u0026lt;- likelihood(b,a,input,train) } K \u0026lt;- outer(input, input, kernel) C_N \u0026lt;- K + diag(length(input))/beta m \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) x \u0026lt;- seq(xmin,xmax,length=100) output \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)), tx=input,ty=train), aes(x=x1,y=m)) + geom_line() + geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) + geom_point(aes(x=tx,y=ty)) return(output) } print(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) \\normalsizeYes, it does sound like good (lol). That\u0026rsquo;s it for today, for now.\n","date":1543708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543708800,"objectID":"fe859c6266641600992fd6a0d6f11947","permalink":"/en/post/post1/","publishdate":"2018-12-02T00:00:00Z","relpermalink":"/en/post/post1/","section":"post","summary":"I implemented the much talked about Gauss regression, which is too versatile to be fun in reverse.","tags":["R"],"title":"Implementing Gaussian regression.","type":"post"},{"authors":null,"categories":[],"content":"\r\r\rIt’s nice to meet you.\rMy name is Ayato Ashihara, and I’m a first-year graduate of an asset management company in Tokyo.\nI’d like to introduce myself briefly.\rI’m 24 years old and graduated from a graduate school.\rI majored in macroeconomics, especially DSGE models, state space models, Kalman filters, Bayesian estimation, and MCMC.\rI also did some natural language processing as research support for my advisor.\rI originally wanted to be an academic researcher, but due to financial problems, I had to find a job.\rNow I’m doing menial work in sales support.\nI think this blog will be a memorandum of my research hobby as I cannot give up my interest in research.\nI am currently working on the following two research projects.\nhorse betting version of the factor model\rquarterly GDP projection model\r\rThe first question is whether it is possible to create a model for constructing a horse betting portfolio that aims for a collection rate of over 100%, from the perspective that the horse betting market is more speculatively attractive than the stock market. For stocks, there is a factor model like Pharma French, but I would like to see if we can apply it to create a horse-trading version of the factor model.\nSecondly, we are aware of the problem of the low accuracy of the recent preliminary quarterly GDP report, so we wondered if it would be possible to construct a new, highly accurate forecasting model. In particular, I would like to use machine learning to create a model that incorporates data that is not normally used in macroeconomic research, rather than a model based on macroeconomic theory.\nWe’ve only just begun our research.\rI’ll do my best to keep you interested…\nBest regards.\n","date":1526601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526601600,"objectID":"ceb64c07c657dc4dad3f41bf7df6c579","permalink":"/en/post/post4/","publishdate":"2018-05-18T00:00:00Z","relpermalink":"/en/post/post4/","section":"post","summary":"Nice to meet you. I'd like to start this blog by introducing myself.","tags":[],"title":"It's nice to meet you.","type":"post"},{"authors":["Ayato Ashihara"],"categories":null,"content":"\rClick the Cite button above to get Bibtex and Cite ME !!\r\r\r","date":1522454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522454400,"objectID":"8eb0b83c007669e1fcaec626e70937a6","permalink":"/en/publication/%E7%B4%80%E8%A6%812019/","publishdate":"2018-03-31T00:00:00Z","relpermalink":"/en/publication/%E7%B4%80%E8%A6%812019/","section":"publication","summary":"Japan  has  experienced  a  long-lasting  stagnation  since  the  early  1990s.  According  to  the  Reference Dates of Business Cycle, the Cabinet Office of Japan, there are four recession periods between 1987 and 2010,  and  three  of  them  are  considered  to  be  financially-related.  This  implies  that  the  stagnation  wastriggered by financial factors. Nonetheless, many studies using Dynamic Stochastic General Equilibrium models claim that a decline in Total Factor Productivity is the main driver of the stagnation. To resolve this contradiction,  this study estimates the Japanese economy by a New-Keynesian  DSGE  model augmented with  financial  friction  used  in  Christiano,  Motto  and  Rostagno  (2014),  where “risk  shock”  is newly incorporated into the model that refers to uncertainty in the financial market. According to our estimationresults, the estimated risk shock can explain the overall fluctuations of GDP and investment, and thus it isconsidered to be the main driver of the stagnation. We also find that it is highly correlated with the Business condition  Diffusion  Index,  the  Financial  Position  Diffusion  Index  and  the  Lending  Attitude  Index  of  Financial Institutions in Tankan released by the Bank of Japan. Therefore, we conclude that the estimated risk  shock  can  be  interpreted  as  the  firms’   distrust  toward  their  business  conditions,  and  it  delayed  theirinvestment decisions, then causing the prolonged economic contraction.","tags":"","title":"Does Financial Risk Explain Japan’s Great Stagnation?","type":"publication"},{"authors":["Ayato Ashihara"],"categories":null,"content":"\rClick the Cite button above to get Bibtex and Cite ME !!\r\r\r","date":1488844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488844800,"objectID":"e1ffa3282e6f7fc6edad66690c0b404c","permalink":"/en/publication/ael2018/","publishdate":"2017-03-07T00:00:00Z","relpermalink":"/en/publication/ael2018/","section":"publication","summary":"Using Japanese financial data that provide enough observations under the good and bad regimes of financial conditions, we find that fiscal multipliers are smaller in the bad regime than in the good regime.","tags":[""],"title":"Is fiscal expansion more effective in a financial crisis?","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/en/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/en/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]