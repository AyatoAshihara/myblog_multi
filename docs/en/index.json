[{"authors":null,"categories":null,"content":"I\u0026rsquo;m 26 years old, from Nara, Japan and currently in my third year since completing graduate studies. I majored in macroeconomics (mainly Dynamic Stochastic General Equilibruim models), and also experimented with time series analysis and text mining. Recently, I\u0026rsquo;m interested in data wrangling and am studying for certification to advance my career as an (infrastructure) engineer. Currently, I am working in the Investment Planning Group at an asset management company.\n","date":1522454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1522454400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/en/author/ayato-ashihara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/author/ayato-ashihara/","section":"authors","summary":"I\u0026rsquo;m 26 years old, from Nara, Japan and currently in my third year since completing graduate studies. I majored in macroeconomics (mainly Dynamic Stochastic General Equilibruim models), and also experimented with time series analysis and text mining.","tags":null,"title":"Ayato Ashihara","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/en/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/en/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/en/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/en/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/en/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/en/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/en/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":"","date":1603888504,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603888504,"objectID":"e368e80244db4f7a9c74bc3236074d9e","permalink":"/en/post/","publishdate":"2020-10-28T21:35:04+09:00","relpermalink":"/en/post/","section":"post","summary":"","tags":[],"title":"Post","type":"post"},{"authors":null,"categories":["macroeconomics","single-shot"],"content":"\r\r\r1.OECD.Stat Web API\r2.pandasdmx\r3.implementation\r4. Another matter…\r\r\rHi. There are several ways to collect macroeconomic data, but collecting data for each country can be a challenge. However, you can automate the tedious process of collecting data from the OECD via API. Today, I will introduce the method.\n1.OECD.Stat Web API\rOECD.org offers a service called OECD.Stat, which provides a variety of economic data for OECD and certain non-member countries. You can also download the csv data manually by going to the website. Since OECD provides a web API, you only need to use Python or R to do this.\nBelow is a list of implementation details for specific OECD REST SDMX interfaces at this time.\n\rOnly anonymous queries are supported and there is no authentication.\n\rEach response is limited to 1,000,000 observations.\n\rThe maximum length of the request URL is 1000 characters.\n\rCross-origin requests are supported in the CORS header (see here for more information about CORS).\n\rErrors are not returned in the results, but HTTP status codes and messages are set according to the Web Service Guidelines.\n\rIf a nonexistent dataset is requested, 401 Unauthorized is returned.\n\rThe source (or Agency ID) parameter of the REST query is required, but the ALL keyword is supported.\n\rVersioning is not supported: the latest implementation version is always used.\n\rSorting of data is not supported.\n\rThe lastNObservations parameter is not supported.\n\rEven when dimensionAtObservation=AllDimensions is used, the observations follow a chronological (or import-specific) order.\n\rSearching for reference metadata is not supported at this time.\n\r\r\r2.pandasdmx\rThe Web API is provided in the form of sdmx-json. There is a useful package for using it in Python, which is called pandasdmx**. Here’s how to download the data.\nImport pandasdmx, pass OECD to Request method as an argument and create api.Request object.\rPass the query condition to the data method of the api.Request object, and download the data of sdmx-json format from OECD.org.\rFormat the downloaded data into a pandas data frame with the method to_pandas().\r\r\r3.implementation\rLet’s do this in practice. What we’ll get is the **Revisions Analysis Dataset -- Infra-annual Economic Indicators**, one of the OECD datasets, the Monthly Ecnomic Indicator (MEI). We have access to all data, including revisions to the preliminary data on key economic variables (such as gross domestic product and its expenditure items, industrial production and construction output indices, balance of payments, composite key indicators, consumer price index, retail trade volume, unemployment rate, number of workers, hourly wages, money supply, and trade statistics), as first published You can see everything from data to confirmed data with corrections. The dataset provides a snapshot of data that were previously available for analysis in the Leading Economic Indicators database at monthly intervals beginning in February 1999. In other words, the dataset allows us to build predictive models based on the data available at each point in time. The most recent data is useful, but it is preliminary and therefore subject to uncertainty. The problem is that this situation cannot be replicated when backtesting, and the analysis is often done under a better environment than the actual operation. This is the so-called Jagged Edge problem. In this dataset, we think it is very useful because we can reproduce the situation of actual operation. This time, you will get the following data items.\n\r\rIndicators\rStatistical ID\rFrequency\r\r\r\rGross Domestic Product\r101\rQuarterly\r\rIndex of Industrial Production\r201\rMonthly\r\rRetail Trade Volume\r202\rMonthly\r\rMonetary Aggregates\r601\rMonthly\r\rInternational Trade in Goods\r702+703\rMonthly\r\rBalance of Payments\r701\rQuarterly\r\rEmployment\r502\rMonthly\r\rHarmonised Unemployment Rates\r501\rMonthly\r\rHourly Earnings in Manufacturing\r503\rMonthly\r\rEarly Estimates of Unit Labor Cost\r504\rQuarterly\r\rProduction of Construction\r203\rMonthly\r\r\r\rFirst, we define the functions. The arguments are database ID, other IDs (country IDs and statistical IDs), start point and end point.\nimport pandasdmx as sdmx\r## C:\\Users\\aashi\\Anaconda3\\lib\\site-packages\\pandasdmx\\remote.py:13: RuntimeWarning: optional dependency requests_cache is not installed; cache options to Session() have no effect\r## RuntimeWarning,\roecd = sdmx.Request(\u0026#39;OECD\u0026#39;)\rdef resp_OECD(dsname,dimensions,start,end):\rdim_args = [\u0026#39;+\u0026#39;.join(d) for d in dimensions]\rdim_str = \u0026#39;.\u0026#39;.join(dim_args)\rresp = oecd.data(resource_id=dsname, key=dim_str + \u0026quot;/all?startTime=\u0026quot; + start + \u0026quot;\u0026amp;endTime=\u0026quot; + end)\rdf = resp.to_pandas().reset_index()\rreturn(df)\rSpecify the dimension from which the data will be obtained. Below, (1) country, (2) statistical items, (3) time of acquisition, and (4) frequency are specified with a tuple.\ndimensions = ((\u0026#39;USA\u0026#39;,\u0026#39;JPN\u0026#39;,\u0026#39;GBR\u0026#39;,\u0026#39;FRA\u0026#39;,\u0026#39;DEU\u0026#39;,\u0026#39;ITA\u0026#39;,\u0026#39;CAN\u0026#39;,\u0026#39;NLD\u0026#39;,\u0026#39;BEL\u0026#39;,\u0026#39;SWE\u0026#39;,\u0026#39;CHE\u0026#39;),(\u0026#39;201\u0026#39;,\u0026#39;202\u0026#39;,\u0026#39;601\u0026#39;,\u0026#39;702\u0026#39;,\u0026#39;703\u0026#39;,\u0026#39;701\u0026#39;,\u0026#39;502\u0026#39;,\u0026#39;503\u0026#39;,\u0026#39;504\u0026#39;,\u0026#39;203\u0026#39;),(\u0026quot;202001\u0026quot;,\u0026quot;202002\u0026quot;,\u0026quot;202003\u0026quot;,\u0026quot;202004\u0026quot;,\u0026quot;202005\u0026quot;,\u0026quot;202006\u0026quot;,\u0026quot;202007\u0026quot;,\u0026quot;202008\u0026quot;),(\u0026quot;M\u0026quot;,\u0026quot;Q\u0026quot;))\rLet’s execute the function.\nresult = resp_OECD(\u0026#39;MEI_ARCHIVE\u0026#39;,dimensions,\u0026#39;2019-Q1\u0026#39;,\u0026#39;2020-Q2\u0026#39;)\rresult.count()\r## LOCATION 8266\r## VAR 8266\r## EDI 8266\r## FREQUENCY 8266\r## TIME_PERIOD 8266\r## value 8266\r## dtype: int64\rLet’s look at the first few cases of data.\nresult.head()\r## LOCATION VAR EDI FREQUENCY TIME_PERIOD value\r## 0 BEL 201 202001 M 2019-01 112.5\r## 1 BEL 201 202001 M 2019-02 111.8\r## 2 BEL 201 202001 M 2019-03 109.9\r## 3 BEL 201 202001 M 2019-04 113.5\r## 4 BEL 201 202001 M 2019-05 112.1\rYou can see that the data is stored in tidy form (long type). The most right value is stored as a value, and the other indexes are\n\rLOCATION - Country\r\r\rVAR - Items\n\rEDI - At the time of acquisition (in the case of MEI_ARCHIVE)\n\rFREQUENCY - Frequency (monthly, quarterly, etc.)\n\rTIME_PERIOD - Reference point\n\r\rTherefore, the same ` exists in rows with different EDIs. For example, above you can see the data for 2019-01~2019-05 available as of 2020/01 for the Belgian (BEL) Industrial Production Index (201). This is very much appreciated as it is provided in Long format, which is also easy to visualize and regress. Here’s a visualization of the industrial production index as it is updated.\nimport seaborn as sns\rimport matplotlib.pyplot as plt\rimport pandas as pd\rresult = result[result[\u0026#39;FREQUENCY\u0026#39;]==\u0026#39;M\u0026#39;]\rresult[\u0026#39;TIME_PERIOD\u0026#39;] = pd.to_datetime(result[\u0026#39;TIME_PERIOD\u0026#39;],format=\u0026#39;%Y-%m\u0026#39;)\rsns.relplot(data=result[lambda df: (df.VAR==\u0026#39;201\u0026#39;) \u0026amp; (pd.to_numeric(df.EDI) \u0026gt; 202004)],x=\u0026#39;TIME_PERIOD\u0026#39;,y=\u0026#39;value\u0026#39;,hue=\u0026#39;LOCATION\u0026#39;,kind=\u0026#39;line\u0026#39;,col=\u0026#39;EDI\u0026#39;)\r## \u0026lt;seaborn.axisgrid.FacetGrid object at 0x00000000316BC3C8\u0026gt;\rplt.show()\rWhile we can see that the line graphs are depressed as the economic damage from the corona increases, there have been subtle but significant revisions to the historical values from preliminary to confirmed. We can also see that there is a lag in the release of statistical data by country. Belgium seems to be the slowest to release the data. When I have time, I would like to add a simple analysis of the forecasting model using this data.\n\r4. Another matter…\rPython 3 エンジニア認定データ分析試験に合格しました。合格率70%だけあって、かなり簡単でしたがPythonを基礎から見返すいい機会になりました。今やっている業務ではデータ分析はおろかPythonやRを使う機会すらないので、転職も含めた可能性を考えています。とりあえず、以下の資格を今年度中に取得する予定で、金融にこだわらずにスキルを活かせるポストを探していこうと思います。ダイエットと同じで宣言して自分を追い込まないと。。。\nI passed the Python 3 Engineer Certification Data Analysis exam. It was pretty easy, with only a 70% pass rate, but it was a good opportunity to revisit the basics of Python. I haven’t even had the opportunity to use Python or R, let alone data analysis, in the work I’m doing now, so I’m considering the possibility of a career change. In the meantime, I plan to get the following qualifications by the end of this year, and I’ll be looking for a post where I can use my skills without focusing on finance. Like a diet, I need to declare and push myself.\nG Test\rOracle Database Master Silver SQL\rLinuc level 1\rFundamental Information Technology Engineer Examination\rAWS Certified Solutions Architect - Associate\r\rI will report on the status of my acceptance on my blog each time.\n\r","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603065600,"objectID":"3fd599a45600780ad64608f7f529e396","permalink":"/en/post/post22/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/en/post/post22/","section":"post","summary":"I used the OECD.org API to obtain macroeconomic data.","tags":["Python","Gaussian-regression","preprocessing","Web_scraping","API"],"title":"Get macro panel data from OECD.org via API","type":"post"},{"authors":null,"categories":["single-shot","statistics","programming"],"content":"\r\r0. What I want to do\rWhat I’m going to show you is pre-processing (and analysis) using the Tick data of the exchange rate, as I mentioned above. I won’t go into the details of this article because my main focus is to improve efficiency using Rcpp, but I will briefly describe what I want to do first. I will not go into details, but I will show what we want to do in a nutshell first.\nWhat we want to do is to detect a Jump from the five-minute incremental return of the JPY/USD rate. Here, a jump is the point at which the exchange rate has risen (fallen) sharply compared to the previous rate. During the day, the exchange rate moves in small increments, but when there is an event, it rises (or falls) significantly. It is very interesting to see what kind of event causes the jump. In order to test this, we first need to detect jumps. The following paper is a reference point.\nSuzanne S. Lee \u0026amp; Per A. Mykland, 2008. “Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,” Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.\nIt is a very well-respected paper with 204 Citation. We will scratch out the estimation method. First, let the continuous compound return be \\(d\\log S(t)\\) for \\(t\u0026gt;0\\). where \\(S(t)\\) is the price of the asset at \\(t\\). If there are no jumps in the market, \\(S(t)\\) is assumed to follow the following stochastic process\n\\[\rd\\log S(t) = \\mu(t)dt + \\sigma(t)dW(t) \\tag{1}\r\\]\nwhere \\(W(t)\\) is the standard Brownian motion, \\(\\mu(t)\\) is the drift term, and \\(\\sigma(t)\\) is the spot volatility. Also, when there is a Jump, \\(S(t)\\) is assumed to follow the following stochastic process\n\\[\rd\\log S(t) = \\mu(t)dt + \\sigma(t)dW(t) + Y(t)dJ(t) \\tag{2}\r\\]\nwhere \\(J(t)\\) is a counting process independent of \\(W(t)\\). 1 \\(Y(t)\\) represents the size of the jump and is a predictable process.\nNext, consider the logarithmic return of \\(S(t)\\). That is, \\(\\log S(t_i)/S(t_{i-1})\\), which follows a normal distribution \\(N(0, \\sigma(t_i))\\). 2Now, we define the statistic \\(\\mathcal{L(i)}\\) at \\(t_i\\) when there is a jump from \\(t_{i-1}\\) to \\(t_{i}\\) as follows.\n\\[\r\\mathcal{L(i)} \\equiv \\frac{|\\log S(t_i)/S(t_{i-1})|}{\\hat{\\sigma}_{t_i}} \\tag{3}\r\\]\nIt is a simple standardization of the absolute value of the log return, but uses the “Realized Bipower Variation” defined below as an estimator of the standard deviation.\n\\[\r\\hat{\\sigma}_{t_i} = \\frac{1}{K-2}\\sum_{j=i-K+2}^{i-2}|\\log S(t_j)/\\log S(t_{j-1})||\\log S(t_{j-1})/\\log S(t_{j-2})| \\tag{4}\r\\]\n\\(K\\) is the number of sample sizes contained in the Window. If we use a return in 5-minute increments and the jump occurs at 10:00 on 9/10/2020, and \\(K=270\\), then we will calculate using samples from the previous day, 9/9/2020 11:30 to 9/11/2020 09:55. What we’re doing is adding up the absolute value of the return multiplied by the absolute value of the return, which seems to make it difficult for the estimate of the next instant after the jump occurs (i.e., \\(t_{i+1}\\) and so on) to be affected by the jump. Incidentally, \\(K=270\\) is introduced in another paper as a recommended value for returns in 5-minute increments.\nLet us move on to how the calculated jump statistic \\(\\mathcal{L(i)}\\) is used in the statistical test to detect jumps. This is done by considering the maximum value of the random variable \\(\\mathcal{L(i)}\\) (also a random variable), and if it deviates significantly from that distribution (like 95% of points), the return of the statistic is the jump.\nIf there is no Jump in the period \\([t_{i-1},t_{i}]\\), then with the length of this period \\(\\Delta=t_{i}-t_{i-1}\\) approached to \\(0\\), i.e., \\(\\Delta\\rightarrow0\\), the sample maximum of the absolute value of the standard normal variable converges to the Gumbel distribution. Thus, Jump can reject and detect the null hypothesis when the following conditions are met\n\\[\r\\mathcal{L(i)} \u0026gt; G^{-1}(1-\\alpha)S_{n} + C_{n} \\tag{5}\r\\]\nwhere \\(G^{-1}(1-\\alpha)\\) is the \\((1-\\alpha)\\) quantile function of the standard Gumbell distribution. If \\(\\alpha=10%\\), \\(G^{-1}(1-\\alpha)=2.25\\). Note that (We won’t derive it, but we can prove it using equations 1 and 2)\n\\[\rS_{n} = \\frac{1}{c(2\\log n)^{0.5}} \\\\\rC_{n} = \\frac{(2\\log n)^{0.5}}{c}-\\frac{\\log \\pi+\\log(\\log n)}{2c(2\\log n)^{0.5}}\r\\]\nwhere \\(c=(2/\\pi)^{0.5}\\) and \\(n\\) is the total sample size used for estimation.\rFinally, \\(Jump_{t_i}\\) is calculated by\n\\[\rJump_{t_i} = \\log\\frac{S(t_i)}{S(t_{i-1})}×I(\\mathcal{L(i)} - G^{-1}(1-\\alpha)S_{n} + C_{n})\\tag{6}\r\\]\nwhere \\(I(⋅)\\) is an Indicator function that returns 1 if the content is greater than 0 and 0 otherwise.\n\r1. Loading data\rSo now that we know how to estimate, let’s load the Tick data first. The data is csv from QuantDataManager and saved in a working directory.\nlibrary(magrittr)\r# Read Tick data\rstrPath \u0026lt;- r\u0026quot;(C:\\Users\\hogehoge\\JPYUSD_Tick_2011.csv)\u0026quot;\rJPYUSD \u0026lt;- readr::read_csv(strPath)\rOn an unrelated note, I recently upgraded R to 4.0.2, and I’m quite happy to say that with 4.0 and above, you can escape the strings made by Python, which relieves some of the stress I’ve been experiencing.\nThe data looks like the following: in addition to the date, the Bid value, Ask value and the volume of transactions are stored. Here, we use the 2011 tick. The reason for this is to cover the dollar/yen at the time of the Great East Japan Earthquake.\nsummary(JPYUSD)\r## DateTime Bid Ask Volume ## Min. :2011-01-03 07:00:00 Min. :75.57 Min. :75.58 Min. : 1.00 ## 1st Qu.:2011-03-30 15:09:23 1st Qu.:77.43 1st Qu.:77.44 1st Qu.: 2.00 ## Median :2011-06-15 14:00:09 Median :80.40 Median :80.42 Median : 2.00 ## Mean :2011-06-22 05:43:11 Mean :79.91 Mean :79.92 Mean : 2.55 ## 3rd Qu.:2011-09-09 13:54:51 3rd Qu.:81.93 3rd Qu.:81.94 3rd Qu.: 3.00 ## Max. :2011-12-30 06:59:59 Max. :85.52 Max. :85.54 Max. :90.00\rBy the way, DateTime includes the period from 07:00:00 on 2011/1/3 to 06:59:59 on 2011-12-30 (16:59:59 on 2011-12-30) in Japan by UTC. The sample size is approximately 12 million entries.\nNROW(JPYUSD)\r## [1] 11946621\r\r2. Preprocessing\rThen calculate the median value from Bid and Ask and take the logarithm to calculate the return later.\n# Calculate the median value of Ask and Bid and make it logarithmic (for calculating the logarithmic return).\rJPYUSD \u0026lt;- JPYUSD %\u0026gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %\u0026gt;% dplyr::mutate(logMid = log(Mid))\rI format the currently irregularly arranged trading data into 5-minute increments of returns. The way to do it is:\nCreate a POSIXct vector with 1 year chopped every 5 minutes.\rcreate a function that calculates the logarithmic return from the first and last sample in the window of 5min in turn, if you pass 1. as an argument.\rexecute. This is the plan. First, create the vector of 1.\r\r# Create POSIX vector to calculate returns in 5min increments (288 x days)\rstart \u0026lt;- as.POSIXct(\u0026quot;2011-01-02 22:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)\rend \u0026lt;- as.POSIXct(\u0026quot;2011-12-31 21:55:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)\rfrom \u0026lt;- seq(from=start,to=end,by=5*60)\rThen, let’s move on to 2. If you have 12 million data, even if you use purrr::map or apply with R, it takes a long time to call a function and it’s quite inefficient. I tried to use sapply, but it didn’t complete the process and it was forced to terminate. RCCp is useful in such a case. Although R has many very useful functions for graphs and statistics, it is not very good at large repetition, including calls to user-defined functions (because it is a scripting language, rather than a compiling language, I mean). So, I write the part of repetitive process in C++ and compile it as R function using Rcpp and execute it. It is very efficient to compile and visualize the results and write them in R. Also, Rccp helps you to write C++ in a similar way to R with less sense of discomfort. I think the following will give you a good idea of the details. It’s pretty well organized and is God, to say the least.\nRcpp for everyone\nNow, let’s write the code for the second step. In coding, I used articles on the net for reference. C++ has a longer history and more users than R, so you can find information you want to know.\n#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;algorithm\u0026gt;\rusing namespace Rcpp;\r//[[Rcpp::plugins(cpp11)]]\r// [[Rcpp::export]]\rDataFrame Rolling_r_cpp(\rDataFrame input, // Data frame of (measurement time, measured value data)\rnewDatetimeVector from, // The starting point vector for the timing of the calculation\rdouble time_window = 5*60) // Calculated window width (in seconds)\r{ // Extract the measurement time and value as a vector\rnewDatetimeVector time = input[\u0026quot;DateTime\u0026quot;]; // This time is assumed to be sorted in ascending order.\rNumericVector data = input[\u0026quot;logMid\u0026quot;];\r// The endpoint vector of the timing to be calculated\rnewDatetimeVector to = from + time_window;\r// Number to calculate\rR_xlen_t N = from.length();\r// vector for storage NumericVector value(N);\r// An object representing the position of a vector element\rnewDatetimeVector::iterator begin = time.begin();\rnewDatetimeVector::iterator end = time.end();\rnewDatetimeVector::iterator p1 = begin;\rnewDatetimeVector::iterator p2 = begin;\r// Loop for window i\rfor(R_xlen_t i = 0; i \u0026lt; N; ++i){\r// Rcout \u0026lt;\u0026lt; \u0026quot;i=\u0026quot; \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;;\rdouble f = from[i]; // Time of the window\u0026#39;s start point\rdouble t = f + time_window; // The time of the window\u0026#39;s endpoint\r// If the endpoint of the window is before the first measurement time, it is NA or\r// If the starting point of the window is after the last measurement time, NA\rif(t \u0026lt;= *begin || f \u0026gt; *(end-1)){ value[i] = NA_REAL;\rcontinue;// Go to the next loop\r}\r// Vector time from position p1 and subsequent elements x\r// Let p1 be the position of the first element whose time is at the start point f \u0026quot;after\u0026quot; the window\rp1 = std::find_if(p1, end, [\u0026amp;f](double x){return f\u0026lt;=x;});\r// p1 = std::lower_bound(p1, end, f); //Same as above\r// Vector time from position p1 and subsequent elements x\r// Let p2 be the position of the last element whose time is \u0026quot;before\u0026quot; the endpoint t of the window\r// (In the below, this is accomplished by making the time one position before the \u0026#39;first element\u0026#39;, where the time is the window\u0026#39;s endpoint t \u0026#39;after\u0026#39;)\rp2 = std::find_if(p1, end, [\u0026amp;t](double x){return t\u0026lt;=x;}) - 1 ;\r// p2 = std::lower_bound(p1, end, t) - 1 ;//Same as above\r// Convert the position p1,p2 of an element to the element numbers i1, i2\rR_xlen_t i1 = p1 - begin;\rR_xlen_t i2 = p2 - begin; // Checking the element number\r// C++ starts with the element number 0, so I\u0026#39;m adding 1 to match the R\r// Rcout \u0026lt;\u0026lt; \u0026quot;i1 = \u0026quot; \u0026lt;\u0026lt; i1+1 \u0026lt;\u0026lt; \u0026quot; i2 = \u0026quot; \u0026lt;\u0026lt; i2+1 \u0026lt;\u0026lt; \u0026quot;\\n\u0026quot;;\r// Calculate the data in the relevant range\rif(i1\u0026gt;i2) {\rvalue[i] = NA_REAL; // When there is no data in the window\r} else { value[i] = data[i2] - data[i1];\r}\r// ↑You can create various window functions by changing above\r}\r// Output the calculated time and the value as a data frame.\rDataFrame out =\rDataFrame::create(\rNamed(\u0026quot;from\u0026quot;, from),\rNamed(\u0026quot;r\u0026quot;, value*100));\rreturn out;\r}\rIf you compile the program with Rcpp::sourceCpp, the function of R can be executed as follows.\nsystem.time(results \u0026lt;- Rolling_r_cpp(JPYUSD,from))\r## ユーザ システム 経過 ## 0.02 0.00 0.02\rIt takes less than a second to process 12 million records. So Convenient!!\nsummary(results)\r## from r ## Min. :2011-01-02 22:00:00 Min. :-1.823 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 ## Median :2011-07-03 09:57:30 Median : 0.000 ## Mean :2011-07-03 09:57:30 Mean : 0.000 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 ## Max. :2011-12-31 21:55:00 Max. : 2.880 ## NA\u0026#39;s :29977\rThe return is precisely calculated. The recommended length of the window is 270 in 5 min increments, but we’ll make it flexible as well. And we carefully process the NA.\n#include \u0026lt;Rcpp.h\u0026gt;\r#include \u0026lt;cmath\u0026gt;\rusing namespace Rcpp;\r//[[Rcpp::plugins(cpp11)]]\r// [[Rcpp::export]]\rfloat rbv_cpp(\rNumericVector x, // Return vector to calculate rbv\rbool na_rm = true) // If NA is included in x, remove it and calculate it or\r{\r// Get the number of calculations\rR_xlen_t N = x.length();\r// Define variables to contain the results of a calculation\rfloat out = 0;\r// Check for missing x\rLogicalVector lg_NA = is_na(x);\r// If there is a NA in x, whether to exclude that NA and calculate\rif(any(lg_NA).is_true() and na_rm==FALSE){\rout = NA_REAL; // Output NA as a result of the calculation\r} else {\r// Excluding NA\rif (any(lg_NA).is_true() and na_rm==TRUE){\rx[is_na(x)==TRUE] = 0.00; // Fill in the NA with zeros and effectively exclude it from the calculation.\r}\r// Compute the numerator (sum of rbv)\rfor(R_xlen_t i = 1; i \u0026lt; N; ++i){\rout = out + std::abs(x[i])*std::abs(x[i-1]);\r}\r// Calculate the average and take the route.\rlong denomi; //denominator\rif(N-sum(lg_NA)-2\u0026gt;0){\rdenomi = N-sum(lg_NA)-2;\r} else {\rdenomi = 1;\r}\rout = out/denomi;\rout = std::sqrt(out);\r}\rreturn out;\r}\r// [[Rcpp::export]]\rDataFrame Rolling_rbv_cpp(\rDataFrame input, //Data frame of (measurement time, measured value data)\rint K = 270, // Rolling Window width to calculate\rbool na_pad = false, // Returning NA when the window width is insufficient\rbool na_remove = false // If the NA exists in the window width, exclude it from the calculation\r){\r// Extract the return vector and number of samples\rNumericVector data = input[\u0026quot;r\u0026quot;];\rR_xlen_t T = data.length();\r// Prepare a vector to store the results\rNumericVector value(T);\r// Calculate and store RBVs per Windows width\rif(na_pad==TRUE){\rvalue[0] = NA_REAL; // return NA.\rvalue[1] = NA_REAL; // return NA.\rvalue[2] = NA_REAL; // return NA.\r} else {\rvalue[0] = 0; // Return zero.\rvalue[1] = 0; // Return zero.\rvalue[2] = 0; // Return zero.\r}\rfor(R_xlen_t t = 3; t \u0026lt; T; ++t){\r// Bifurcation of the process depending on whether or not there is enough Windows width\rif (t-K\u0026gt;=0){\rvalue[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // Run a normal calculation\r} else if(na_pad==FALSE) {\rvalue[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Run a calculation with an incomplete Widnows width of less than K\r} else {\rvalue[t] = NA_REAL; // return NA.\r}\r}\r// Output the calculated time and value as a data frame.\rDataFrame out =\rDataFrame::create(\rNamed(\u0026quot;from\u0026quot;, input[\u0026quot;from\u0026quot;]),\rNamed(\u0026quot;r\u0026quot;, data),\rNamed(\u0026quot;rbv\u0026quot;,value));\rreturn out;\r}\rNow, compile it and run it with R.\nsystem.time(results \u0026lt;- results %\u0026gt;% Rolling_rbv_cpp(na_remove = FALSE))\r## ユーザ システム 経過 ## 0.24 0.09 0.33\rSo fast!\n\r3. Calculating Jump Statistics\rNow let’s calculate the statistic \\(\\mathcal{L}_{t_i}\\) from the returns and standard deviation we just calculated.\n# Standardize the absolute value of the log return = Jump statistic\rresults \u0026lt;- results %\u0026gt;% dplyr::mutate(J=ifelse(rbv\u0026gt;0,abs(r)/rbv,NA))\rThis is what it looks like now.\nsummary(results)\r## from r rbv J ## Min. :2011-01-02 22:00:00 Min. :-1.823 Min. :0.00 Min. : 0.00 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 1st Qu.:0.02 1st Qu.: 0.28 ## Median :2011-07-03 09:57:30 Median : 0.000 Median :0.02 Median : 0.64 ## Mean :2011-07-03 09:57:30 Mean : 0.000 Mean :0.03 Mean : 0.93 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 3rd Qu.:0.03 3rd Qu.: 1.23 ## Max. :2011-12-31 21:55:00 Max. : 2.880 Max. :0.16 Max. :58.60 ## NA\u0026#39;s :29977 NA\u0026#39;s :44367 NA\u0026#39;s :44423\rNow let’s move on to the Jump test. First, we need to define the useful functions.\n# Preparing Constants \u0026amp; Functions for Calculating Jump Test\rc \u0026lt;- (2/pi)^0.5\rCn \u0026lt;- function(n){\rreturn((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5))\r}\rSn \u0026lt;- function(n){\r1/(c*(2*log(n))^0.5)\r}\rNow we perform the test. Rejected samples return 1 and all others 0.\n# Perform a jump test (10%) (return value is logical)\rN \u0026lt;- NROW(results$J)\rresults \u0026lt;- results %\u0026gt;% dplyr::mutate(Jump = J \u0026gt; 2.25*Sn(N) + Cn(N))\rsummary(results)\r## from r rbv J ## Min. :2011-01-02 22:00:00 Min. :-1.823 Min. :0.00 Min. : 0.00 ## 1st Qu.:2011-04-03 15:58:45 1st Qu.:-0.014 1st Qu.:0.02 1st Qu.: 0.28 ## Median :2011-07-03 09:57:30 Median : 0.000 Median :0.02 Median : 0.64 ## Mean :2011-07-03 09:57:30 Mean : 0.000 Mean :0.03 Mean : 0.93 ## 3rd Qu.:2011-10-02 03:56:15 3rd Qu.: 0.015 3rd Qu.:0.03 3rd Qu.: 1.23 ## Max. :2011-12-31 21:55:00 Max. : 2.880 Max. :0.16 Max. :58.60 ## NA\u0026#39;s :29977 NA\u0026#39;s :44367 NA\u0026#39;s :44423 ## Jump ## Mode :logical ## FALSE:59864 ## TRUE :257 ## NA\u0026#39;s :44423 ## ## ## \r\r4. Visualization using ggplot2\rNow that the numbers have been calculated, let’s visualize them by plotting the intraday logarithmic return of JPY/USD in 5-minute increments for 2011/03/11 and the jump. By the way, the horizontal axis has been adjusted to Japan time.\n# Plotting about Jump at the time of the 2011/03/11 Great East Japan Earthquake\rresults %\u0026gt;% dplyr::filter(from \u0026gt;= as.POSIXct(\u0026quot;2011-03-11 00:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;),from \u0026lt; as.POSIXct(\u0026quot;2011-03-12 00:00:00\u0026quot;,tz=\u0026quot;UTC\u0026quot;)) %\u0026gt;% ggplot2::ggplot(ggplot2::aes(x=from,y=r)) +\rggplot2::geom_path(linetype=3) +\rggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=\u0026quot;red\u0026quot;)) +\rggplot2::scale_x_datetime(date_breaks = \u0026quot;2 hours\u0026quot;, labels = scales::date_format(format=\u0026quot;%H:%M\u0026quot;,tz=\u0026quot;Asia/Tokyo\u0026quot;)) +\rggplot2::ggtitle(\u0026quot;JPY/USD Jumps within Tohoku earthquake on 2011-3-11\u0026quot;)\r## Warning: Removed 36 row(s) containing missing values (geom_path).\r## Warning: Removed 36 row(s) containing missing values (geom_path).\rI’ve spent quite a bit of time writing this, and it’s 23:37 right now, so I’ll refrain from discussing it in depth, but since the earthquake occurred at 14:46:18, you can see that the market reacted to the weakening of the yen immediately after the disaster. After that, for some reason, the yen moved higher and peaked at 19:00. It is said that the yen is a safe asset, but this is the only time it is not safe given the heightened uncertainty.\n\r5. Summary\rI introduced the use of Rcpp to improve the efficiency of R analysis. The C++ is much faster than R even if you write the code honestly, so it is hard to make coding mistakes. The C++ is much faster than R even if you write the code in a simple way. Also, even if a compile error occurs, RStudio gives you a clue as to where the compile error is occurring, so there is no stress in that respect either, which is why I recommend it.\n\r\rA stochastic process with non-negative, integer, non-decreasing values.↩︎\n\rThe mean is not necessarily zero, depending on the shape of the drift term, but we are assuming a small enough drift term now. In the paper, it is defined more precisely.↩︎\n\r\r\r","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"a2572500921f2507d6076f96260ed768","permalink":"/en/post/post21/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/en/post/post21/","section":"post","summary":"Hi. I'm doing an analysis using currency tick data, and the sample size is 12 million data per year. It's in memory, but it's very inefficient because it takes a lot of time to process each one of them when we try to do complicated processing. This time, I would like to introduce the way to compile functions written in C++ as functions on R using Rcpp package to increase the processing speed.","tags":["R","C++","preprocessing","finance"],"title":"Rcpp to speed up data handling (using Tick data processing as an example)","type":"post"},{"authors":null,"categories":["Work-related"],"content":"\r\r\r\r1. Minimum Variance Portfolio\r2. How to predict the variance-covariance matrix\r\rA. Constant conditional correlation (CCC) model\rB. Dynamic Conditional Correlation (DCC) model\rC. Dynamic Equicorrelation (DECO) model\r\r3. Collection of data for testing\r\r\rHi. I attended a workshop on asset allocation at my place of work, so I’d like to run some simulations here, even though this field is completely outside my expertise. In this article, I would like to do an exercise on how to estimate the variance-covariance matrix (predictions) of a minimum variance portfolio as the base portfolio, referring to previous studies. The reference research is in the following paper (in the Journal of Operations Research).\nAsset Allocation with Correlation: A Composite Trade-Off\n1. Minimum Variance Portfolio\rI won’t go into a detailed explanation of the minimum variance portfolio here, but it is a portfolio that calculates the average return and variance of each asset (domestic stocks, foreign stocks, domestic bonds, foreign bonds, and alternatives), plots them on a quadratic plane of average return vertical and variance horizontal axes, calculates the range of possible investments, and then calculates the smallest variance in the set (see the chart below).\nminimum variance portfolio\n\rIn an earlier study, Carroll et. al. (2017), the following is stated\n\rthis paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.\n\rEven at present, it is difficult to estimate expected returns, and a minimum variance portfolio, which does not require it, is a useful and practical technique. The objective function of a minimum variance portfolio is, as the name implies, to minimize diversification. If we now denote the vector of collected returns for each asset by \\(r\\), the holding weight of each asset by \\(\\theta\\), and the portfolio return by \\(R_{p}\\), then the overall portfolio variance \\(var(R_{p})\\) can be written as follows.\n\\[\rvar(R_{p}) = var(r^{T}\\theta) = E( (r^{T}\\theta)(r^{T}\\theta)^{T}) = \\theta^{T}\\Sigma\\theta\r\\]\nwhere \\(Sigma\\) is the variance-covariance matrix of \\(r\\). Thus, the minimization problem is as follows\n\\[\r\\min_{\\theta}(\\theta^{T}\\Sigma\\theta) \\\\\rs.t 1^{T}\\theta = 1\r\\]\nHere we have added full investment as a constraint. Let’s use the Lagrangian to solve this problem. The Lagrange function \\(L\\) is as follows\n\\[\rL = \\theta^{T}\\Sigma\\theta + \\lambda(1^{T}\\theta - 1)\r\\]\nThe first condition is\n\\[\r\\displaystyle\\frac{\\partial L}{\\partial \\theta} = 2\\Sigma\\theta + 1\\lambda = 0 \\\\\r\\displaystyle \\frac{\\partial L}{\\partial \\lambda} = 1^{T} \\theta = 1\r\\]\nSolving the first equation for \\(\\theta\\), we find that\n\\[\r\\theta = \\Sigma^{-1}1\\lambda^{*}\r\\]\nwhere \\(\\lambda^{*}=-1/2\\lambda\\). Substitute this into the second formula and solve for \\(\\lambda^{*}\\)\n\\[\r1^{T}\\Sigma1\\lambda^{*} = 1 \\\\\r\\displaystyle \\lambda^{*} = \\frac{1}{1^{T}\\Sigma^{-1}1}\r\\]\nSince \\(\\theta = \\Sigma^{-1}1\\lambda^{*}\\), erasing \\(\\lambda^{*}\\) will cause it to be\n\\[\r\\displaystyle \\theta_{gmv} = \\frac{\\Sigma^{-1}1}{1^{T}\\Sigma^{-1}1}\r\\]\nand we can now find the optimal weight. For now, I’ll implement this in R.\ngmv \u0026lt;- function(r_dat,r_cov){\rlibrary(MASS)\ri \u0026lt;- matrix(1,NCOL(r_dat),1)\rr_weight \u0026lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i)\rwr_dat \u0026lt;- r_dat*as.numeric(r_weight)\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;) return(result)\r}\rnlgmv \u0026lt;- function(r_dat,r_cov){\rqp.out \u0026lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))),\rbvec=c(1,rep(0,NCOL(r_dat))),meq=1)\rr_weight \u0026lt;- qp.out$solution\rwr_dat \u0026lt;- r_dat*r_weight\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;)\rreturn(result)\r}\rThe input is a return and variance-covariance matrix for each asset. The output is the weight, return and risk. nlgmv is a short-sale constrained version of the minimum variance portfolio. Since we cannot get an analytical solution, we are trying to get a numerical solution.\n\r2. How to predict the variance-covariance matrix\rWe have found the formula for the minimum variance portfolio. The next step is to analyze how to find the input, the variance-covariance matrix. I think the most primitive approach would be the historical approach of finding the covariance matrix for a sample of return data available before that point in time, and then finding the minimum variance portfolio by fixing the value of the covariance matrix (i.e., the weights are also fixed). However, since this is just using historical averages for future projections, I’m sure there will be all sorts of problems. As a non-specialist, I can think of a situation where the return on asset A declined significantly the day before, but the variance of this asset is expected to remain high tomorrow, and the effect of yesterday is diluted by using the average. And I feel that not changing the weights from the start would also move away from the optimal point over time. However, there seems to be some trial and error in this area as to how to estimate it then. Also, in Carroll et. al. (2017), it is stated that,\n\rThe estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.\n\rThe estimates in this paper are based on the following models. All of them are unique in that the variance-covariance matrix is time-varying.\nA. Constant conditional correlation (CCC) model\rThe original paper is here.\nFirst, from the relationship between the variance-covariance matrix and the correlation matrix, we have \\(\\Sigma_{t} = D_{t}R_{t}D_{t}\\), where \\(R_{t}\\) is the variance-covariance matrix and \\(D_{t}\\) is \\(diag(\\sigma_{1,t},...,\\sigma_{N,t})\\), a matrix whose diagonal components are the standard deviation of each asset in \\(tt\\) period. From here, we estimate \\(D_{t}\\) and \\(R_{t}\\) separately. First, \\(D_{t}\\) is estimated using the following multivariate GARCH model (1,1).\n\\[\rr_{t} = \\mu + u_{t} \\\\\ru_{t} = \\sigma_{t}\\epsilon \\\\\r\\sigma_{t}^{2} = \\alpha_{0} + \\alpha_{1}u_{t-1}^{2} + \\alpha_{2}\\sigma_{t-1}^{2} \\\\\r\\epsilon_{t} = NID(0,1) \\\\\rE(u_{t}|u_{t-1}) = 0\r\\]\rwhere \\(\\mu\\) is the sample mean of the returns. \\(\\alpha_{i}\\) is the parameter to be estimated. Since we are estimating \\(D_{t}\\) with GARCH, we can say that we are modeling a relationship where the distribution of returns follows a thicker base distribution than the normal distribution and where the change in returns is not constant but depends on the variance of the previous day. We can say that we have estimated \\(D_{t}\\) in this way. The next step in the estimation of \\(R_{t}\\) is to take a historical approach to the estimation of \\(R_{t}\\), in which the returns are sampled. In other words, \\(R_{t}\\) is a constant. Thus, we are making the assumption that the magnitude of return variation varies with time, but the relative relationship of each asset is invariant.\n\rB. Dynamic Conditional Correlation (DCC) model\rThe original paper is here。\nこちらのモデルでは、\\(D_{t}\\)を求めるところまでは①と同じですが、[\\(R_{t}\\)の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、\\(tex:t\\)期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列\\(R_{t}\\)は、\nThis model is the same as (1) up to the point of finding \\(D_{t}\\), but the way of finding \\(R_{t}\\) is different, and we use ARMA(1,1) to estimate it. Since the correlation matrix is still not a constant, we are going to use the available returns up to the \\(t\\) period to make the estimate. The correlation matrix \\(R_{t}\\) in this model is\n\\[\rR_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2}\r\\]\nwhere \\(Q_{t}\\) is a conditional variance-covariance matrix in the \\(t\\) period, formulated as follows,\n\\[\rQ_{t} = \\bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\\epsilon_{i,t-1}\\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1}\r\\]\nwhere \\(\\bar{Q}\\) is the variance-covariance matrix calculated in a historical way and \\(a,b\\) is the parameter. This method differs from the previous one in that it assumes that not only does the magnitude of return variation vary with time, but also the relative relationship of each asset changes over time. Since we have observed some events in which the returns of all assets fall and the correlation of each asset becomes positive during a financial crisis, this formulation may be attractive.\n\rC. Dynamic Equicorrelation (DECO) model\rThe original paper is here。\nI haven’t read this paper exactly yet, but from the definition of the correlation matrix \\(R_{t}\\) becomes\n\\[\rR_{t} = (1-\\rho_{t})I_{N} + \\rho_{t}1\r\\]\nHere, \\(\\rho_{t}\\) is a scalar and a coefficient for the degree of equicorrelation, and I understand that equicorrelation is an average pair-wise correlation. In other words, if there are no missing values, it’s no different than a normal correlation. However, it seems to be a good estimator in that respect, because as assets increase, such problems need to be addressed. We can calculate \\(\\rho_{t}\\) as follows.\n\\[\r\\displaystyle \\rho_{t} = \\frac{1}{N(N-1)}(\\iota^{T}R_{t}^{DCC}\\iota - N) = \\frac{2}{N(N-1)}\\sum_{i\u0026gt;j}\\frac{q_{ij,t}}{\\sqrt{q_{ii,t} q_{jj,t}}}\r\\]\nwhere \\(\\iota\\) is an \\(N×1\\) vector with all elements being 1. And \\(q_{ij,t}\\) is the \\(i,j\\) element of \\(Q_{t}\\).\nNow that we’ve modeled the variance-covariance matrix, we’ll implement it so far in R.\ncarroll \u0026lt;- function(r_dat,FLG){\rlibrary(rmgarch)\rif(FLG == \u0026quot;benchmark\u0026quot;){\rH \u0026lt;- cov(r_dat)\r}else{\r#1. define variables\rN \u0026lt;- NCOL(r_dat) # the number of assets\r#2. estimate covariance matrix\rbasic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = \u0026#39;sGARCH\u0026#39;), distribution.model = \u0026#39;norm\u0026#39;)\rmulti_garch = multispec(replicate(N, basic_garch))\rdcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = \u0026quot;mvnorm\u0026quot;,model = \u0026quot;DCC\u0026quot;)\rfit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE))\rforecast_dcc_garch \u0026lt;- dccforecast(fit_dcc_garch)\rif (FLG == \u0026quot;CCC\u0026quot;){\r#Constant conditional correlation (CCC) model\rD \u0026lt;- sigma(forecast_dcc_garch)\rR_ccc \u0026lt;- cor(r_dat)\rH \u0026lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1])\rcolnames(H) \u0026lt;- colnames(r_dat)\rrownames(H) \u0026lt;- colnames(r_dat)\r}\relse{\r#Dynamic Conditional Correlation (DCC) model\rH \u0026lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1])\rif (FLG == \u0026quot;DECO\u0026quot;){\r#Dynamic Equicorrelation (DECO) model\rone \u0026lt;- matrix(1,N,N)\riota \u0026lt;- rep(1,N)\rQ_dcc \u0026lt;- rcor(forecast_dcc_garch,type=\u0026quot;Q\u0026quot;)[[1]][,,1]\rrho \u0026lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N))\rD \u0026lt;- sigma(forecast_dcc_garch)\rR_deco \u0026lt;- (1-rho)*diag(1,N,N) + rho*one\rH \u0026lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1])\rcolnames(H) \u0026lt;- colnames(r_dat)\rrownames(H) \u0026lt;- colnames(r_dat)\r}\r}\r}\rreturn(H)\r}\rI shouldn’t normally use the package, but since it’s an exercise today I’m only going to pursue the results of the estimates, and I’ll be writing a post about GARCH in the next week or so.\rNow we’re ready to go. We can now put the return data into this function to calculate the variance-covariance matrix and use it to calculate the minimum variance portfolio.\n\r\r3. Collection of data for testing\rThe data was based on the following article.\n(Introduction to Asset Allocation)[https://www.r-bloggers.com/introduction-to-asset-allocation/]\nWe used NAV data for an ETF (iShares) that is linked to the following index\n\rS\u0026amp;P500\rNASDAQ100\rMSCI Emerging Markets\rRussell 2000\rMSCI EAFE\rUS 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index)\rU.S. Real Estate(the Dow Jones US Real Estate Index)\rgold bullion market\r\rThe first step is to collect data.\nlibrary(quantmod)\r#**************************\r# ★8 ASSETS SIMULATION\r# SPY - S\u0026amp;P 500 # QQQ - Nasdaq 100\r# EEM - Emerging Markets\r# IWM - Russell 2000\r# EFA - EAFE\r# TLT - 20 Year Treasury\r# IYR - U.S. Real Estate\r# GLD - Gold\r#**************************\r# load historical prices from Yahoo Finance\rsymbol.names = c(\u0026quot;S\u0026amp;P 500\u0026quot;,\u0026quot;Nasdaq 100\u0026quot;,\u0026quot;Emerging Markets\u0026quot;,\u0026quot;Russell 2000\u0026quot;,\u0026quot;EAFE\u0026quot;,\u0026quot;20 Year Treasury\u0026quot;,\u0026quot;U.S. Real Estate\u0026quot;,\u0026quot;Gold\u0026quot;)\rsymbols = c(\u0026quot;SPY\u0026quot;,\u0026quot;QQQ\u0026quot;,\u0026quot;EEM\u0026quot;,\u0026quot;IWM\u0026quot;,\u0026quot;EFA\u0026quot;,\u0026quot;TLT\u0026quot;,\u0026quot;IYR\u0026quot;,\u0026quot;GLD\u0026quot;)\rgetSymbols(symbols, from = \u0026#39;1980-01-01\u0026#39;, auto.assign = TRUE)\r#gn dates for all symbols \u0026amp; convert to monthly\rhist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)\rmonth.ends = endpoints(hist.prices, \u0026#39;day\u0026#39;)\rhist.prices = Cl(hist.prices)[month.ends, ]\rcolnames(hist.prices) = symbols\r# remove any missing data\rhist.prices = na.omit(hist.prices[\u0026#39;1995::\u0026#39;])\r# compute simple returns\rhist.returns = na.omit( ROC(hist.prices, type = \u0026#39;discrete\u0026#39;) )\r# compute historical returns, risk, and correlation\ria = list()\ria$expected.return = apply(hist.returns, 2, mean, na.rm = T)\ria$risk = apply(hist.returns, 2, sd, na.rm = T)\ria$correlation = cor(hist.returns, use = \u0026#39;complete.obs\u0026#39;, method = \u0026#39;pearson\u0026#39;)\ria$symbols = symbols\ria$symbol.names = symbol.names\ria$n = length(symbols)\ria$hist.returns = hist.returns\r# convert to annual, year = 12 months\rannual.factor = 12\ria$expected.return = annual.factor * ia$expected.return\ria$risk = sqrt(annual.factor) * ia$risk\rrm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)\rHere’s how the returns are plotted.\nPerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = \u0026quot;Performance summary\u0026quot;)\rNext, we’ll code the backtest. We’ll publish the code all at once.\n# BACK TEST\rbacktest \u0026lt;- function(r_dat,FLG,start_date,span,learning_term,port){\r#-----------------------------------------\r# BACKTEST\r# r_dat - return data(xts object) # FLG - flag(CCC,DCC,DECO)\r# start_date - start date for backtest\r# span - rebalance frequency\r# learning_term - learning term (days)\r# port - method of portfolio optimization\r#-----------------------------------------\rlibrary(stringi)\rinitial_dat \u0026lt;- r_dat[stri_c(as.Date(start_date)-learning_term,\u0026quot;::\u0026quot;,as.Date(start_date))]\rfor (i in NROW(initial_dat):NROW(r_dat)) {\rif (i == NROW(initial_dat)){\rH \u0026lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG)\rif (port == \u0026quot;nlgmv\u0026quot;){\rresult \u0026lt;- nlgmv(initial_dat,H)\r}else if (port == \u0026quot;risk parity\u0026quot;){\rresult \u0026lt;- risk_parity(initial_dat,H)\r}\rweight \u0026lt;- t(result$weight)\rcolnames(weight) \u0026lt;- colnames(initial_dat)\rp_return \u0026lt;- initial_dat[NROW(initial_dat),]*result$weight\r} else {\rif (i %in% endpoints(r_dat,span)){\rH \u0026lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG)\rif (port == \u0026quot;nlgmv\u0026quot;){\rresult \u0026lt;- nlgmv(test_dat,H)\r}else if (port == \u0026quot;risk parity\u0026quot;){\rresult \u0026lt;- risk_parity(test_dat,H)\r}\r}\rweight \u0026lt;- rbind(weight,t(result$weight))\rp_return \u0026lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight)\r}\rif (i != NROW(r_dat)){\rterm \u0026lt;- stri_c(index(r_dat[i+1,])-learning_term,\u0026quot;::\u0026quot;,index(r_dat[i+1,])) test_dat \u0026lt;- r_dat[term]\r}\r}\rp_return$portfolio \u0026lt;- xts(apply(p_return,1,sum),order.by = index(p_return))\rweight.xts \u0026lt;- xts(weight,order.by = index(p_return))\rresult \u0026lt;- list(p_return,weight.xts)\rnames(result) \u0026lt;- c(\u0026quot;return\u0026quot;,\u0026quot;weight\u0026quot;)\rreturn(result)\r}\rCCC \u0026lt;- backtest(hist.returns,\u0026quot;CCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDCC \u0026lt;- backtest(hist.returns,\u0026quot;DCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDECO \u0026lt;- backtest(hist.returns,\u0026quot;DECO\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rbenchmark \u0026lt;- backtest(hist.returns,\u0026quot;benchmark\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rresult \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)\rcolnames(result) \u0026lt;- c(\u0026quot;CCC\u0026quot;,\u0026quot;DCC\u0026quot;,\u0026quot;DECO\u0026quot;,\u0026quot;benchmark\u0026quot;)\rHere’s a graph of the calculation results.\nPerformanceAnalytics::charts.PerformanceSummary(result,main = \u0026quot;BACKTEST\u0026quot;)\rI did not use the minimum variance portfolio defined above because I imposed a short sale constraint. Apparently it’s difficult to solve analytically with this alone, so I’m going to solve numerically. I used a weekly rebalancing period, so it took me a while to calculate the results on my own PC, but I was able to calculate the results.\nSince Lehman, it appears to have outperformed the benchmark equal weighted portfolio. In particular, DECO is looking good. I would like to rethink the meaning of Equicorrelation. The change in each incorporation ratio is as follows.\n# plot allocation weighting\rd_allocation \u0026lt;- function(ggweight,title){\r#install.packages(\u0026quot;tidyverse\u0026quot;)\rlibrary(tidyverse)\rggweight \u0026lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method)\rggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) +\rgeom_area(colour=\u0026quot;black\u0026quot;,size=.1) +\rscale_y_continuous(limits = c(0,1)) +\rlabs(title=title) + facet_grid(method~.)\r}\rgmv_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)))\r# plot allocation weighting\rd_allocation(gmv_weight,\u0026quot;GMV Asset Allocation\u0026quot;)\rThey seem to have increased their allocation to TLT, or U.S. Treasuries, during Lehman, while CCC and DCC have a high allocation to U.S. Treasuries in other areas as well, and the often-cited problem of a minimally diversified portfolio seems to be occurring here as well. On the other hand, DECO has a unique mix ratio, and I think we need to go back and read the paper again.\nPS（2019/3/3）\rSo far, I’ve been doing the analysis with a minimum variance portfolio, but I also wanted to see the results of risk parity, so I wrote the code for that as well.\nrisk_parity \u0026lt;- function(r_dat,r_cov){\rfn \u0026lt;- function(weight, r_cov) {\rN \u0026lt;- NROW(r_cov)\rrisks \u0026lt;- weight * (r_cov %*% weight)\rg \u0026lt;- rep(risks, times = N) - rep(risks, each = N)\rreturn(sum(g^2))\r}\rdfn \u0026lt;- function(weight,r_cov){\rout \u0026lt;- weight\rfor (i in 0:length(weight)) {\rup \u0026lt;- dn \u0026lt;- weight\rup[i] \u0026lt;- up[i]+.0001\rdn[i] \u0026lt;- dn[i]-.0001\rout[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002\r}\rreturn(out)\r}\rstd \u0026lt;- sqrt(diag(r_cov)) x0 \u0026lt;- 1/std/sum(1/std)\rres \u0026lt;- nloptr::nloptr(x0=x0,\reval_f=fn,\reval_grad_f=dfn,\reval_g_eq=function(weight,r_cov) { sum(weight) - 1 },\reval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) },\rlb=rep(0,length(std)),ub=rep(1,length(std)),\ropts = list(\u0026quot;algorithm\u0026quot;=\u0026quot;NLOPT_LD_SLSQP\u0026quot;,\u0026quot;print_level\u0026quot; = 0,\u0026quot;xtol_rel\u0026quot;=1.0e-8,\u0026quot;maxeval\u0026quot; = 1000),\rr_cov = r_cov)\rr_weight \u0026lt;- res$solution\rnames(r_weight) \u0026lt;- colnames(r_cov)\rwr_dat \u0026lt;- r_dat*r_weight\rportfolio \u0026lt;- apply(wr_dat,1,sum)\rpr_dat \u0026lt;- data.frame(wr_dat,portfolio)\rsd \u0026lt;- sd(portfolio)\rresult \u0026lt;- list(r_weight,pr_dat,sd)\rnames(result) \u0026lt;- c(\u0026quot;weight\u0026quot;,\u0026quot;return\u0026quot;,\u0026quot;portfolio risk\u0026quot;)\rreturn(result)\r}\rCCC \u0026lt;- backtest(hist.returns,\u0026quot;CCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDCC \u0026lt;- backtest(hist.returns,\u0026quot;DCC\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rDECO \u0026lt;- backtest(hist.returns,\u0026quot;DECO\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rbenchmark \u0026lt;- backtest(hist.returns,\u0026quot;benchmark\u0026quot;,\u0026quot;2007-01-04\u0026quot;,\u0026quot;months\u0026quot;,365,\u0026quot;risk parity\u0026quot;)\rresult \u0026lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)\rcolnames(result) \u0026lt;- c(\u0026quot;CCC\u0026quot;,\u0026quot;DCC\u0026quot;,\u0026quot;DECO\u0026quot;,\u0026quot;benchmark\u0026quot;)\rHere’s the result.\nPerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026quot;BACKTEST COMPARISON\u0026quot;)\rlibrary(plotly)\r# plot allocation weighting\rriskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026quot;benchmark\u0026quot;,Date=index(benchmark$weight)))\rPerformanceAnalytics::charts.PerformanceSummary(result, main = \u0026quot;BACKTEST COMPARISON\u0026quot;)\rriskparity_weight \u0026lt;- rbind(data.frame(CCC$weight,method=\u0026quot;CCC\u0026quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=\u0026quot;DCC\u0026quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=\u0026quot;DECO\u0026quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=\u0026quot;benchmark\u0026quot;,Date=index(benchmark$weight)))\r# plot allocation weighting\rd_allocation(riskparity_weight, \u0026quot;Risk Parity Asset Allocation\u0026quot;)\rThe results are positive, with all methods outperforming benchmark.\rAs expected, the estimation of the variance-covariance matrix seems to be performing well. The good performance of DECO may also be due to the fact that it uses the average of the correlation coefficients of each asset pair in the correlation matrix, which resulted in a greater inclusion of risk assets than the other methods. The weights are as follows\nThat’s it for today, for now.\n\r","date":1550361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551571200,"objectID":"bc248846d8caf05f6b02c4f3564abd29","permalink":"/en/post/post2/","publishdate":"2019-02-17T00:00:00Z","relpermalink":"/en/post/post2/","section":"post","summary":"We had an in-house workshop and spent the holidays locked up in the Library of Congress, fishing for prior research, which allowed us to build a subtle asset allocation model.","tags":["R"],"title":"I built the Asset Allocation Model in R.","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/en/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/en/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["macroeconomics"],"content":"\r\r\r\r1. What is GPR?\r2. Implementation of the `GPR’\r\r\r　Hi. Yesterday, I wrote an article about Bayesian Vector Autoregression.\rIn the article, the topic of hyperparameter tuning came up, and looking for some efficient way to tune it, I found Bayesian Optimization. Since I am planning to use machine learning methods in daily GDP, I thought that Bayesian Optimization could be quite useful, and I spent all night yesterday to understand it.\nI will implement it here, but Bayesian Optimization uses Gaussian Pocess Regression (GPR), and my motivation for writing this entry was to implement it first. I will write about the implementation of Bayesian Optimization after this entry.\n1. What is GPR?\r　The GRP is, simply put, a type of nonlinear regression method using Bayesian estimation. Although the model itself is linear, it is characterized by its ability to estimate infinite nonlinear transformations of input variables using a kernel trick as explanatory variables (depending on what you choose for the kernel).\rThe GPR assumes that \\(N\\) input and teacher data are available for training, and the \\(N+1\\) of input data are also available. From this situation, we can predict the \\(N+1\\)th teacher data.\nThe data contains noise and follows the following probability model.\r\\[\rt_{i} = y_{i} + \\epsilon_{i}\r\\]\rwhere \\(t_{i}\\) is the \\(i\\)th observable teacher data (scalar), \\(y_{i}\\) is the unobservable output data (scalar), and \\(\\beta_{i}\\) follows a normal distribution \\(N(0, \\beta^{-1})\\) with measurement error. \\(y_{i}\\) follows the following probability model.\n\\[\r\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\r\\]\nwhere \\(x_{i}\\) is the ith input data vector, \\(\\phi(・)\\) is the non-linear function and \\(\\bf{w}^{T}\\) is the weight coefficient (regression coefficient) vector for each input data. As a nonlinear function, I assume \\(\\psi(x_{i}) = (x_{1,i}, x_{1,i}^{2},... ,x_{1,i}x_{2,i},...)\\). (\\(x_{1,i}\\) is the first variable in the \\(i\\)th input data \\(x_{i}\\)). The conditional probability of obtaining \\(t_{i}\\) from the probabilistic model of the teacher data, with the \\(i\\)th output data \\(y_{i}\\) obtained, is\n\\[\rp(t_{i}|y_{i}) = N(t_{i}|y_{i},\\beta^{-1})\r\\]\n\\(\\displaystyle \\textbf{t} = (t_{1},... ,t_{n})^{T}\\) and \\(\\displaystyle \\textbf{y} = (y_{1},... ,y_{n})^{T}\\), then by extending the above equation, we have\n\\[\r\\displaystyle p(\\textbf{t}|\\textbf{y}) = N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N})\r\\]\nWe assume that the expected value of \\(\\textbf{w}\\) as a prior distribution is 0, and all variances are \\(\\alpha\\). We also assume that \\(\\displaystyle \\textbf{y}\\) follows a Gaussian process. A Gaussian process is one where the simultaneous distribution of \\(\\displaystyle \\textbf{y}\\) follows a multivariate Gaussian distribution. In code, it looks like this\n# Define Kernel function\rKernel_Mat \u0026lt;- function(X,sigma,beta){\rN \u0026lt;- NROW(X)\rK \u0026lt;- matrix(0,N,N)\rfor (i in 1:N) {\rfor (k in 1:N) {\rif(i==k) kdelta = 1 else kdelta = 0\rK[i,k] \u0026lt;- K[k,i] \u0026lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta\r}\r}\rreturn(K)\r}\rN \u0026lt;- 10 # max value of X\rM \u0026lt;- 1000 # sample size\rX \u0026lt;- matrix(seq(1,N,length=M),M,1) # create X\rtestK \u0026lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix\rlibrary(MASS)\rP \u0026lt;- 6 # num of sample path\rY \u0026lt;- matrix(0,M,P) # define Y\rfor(i in 1:P){\rY[,i] \u0026lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y\r}\r# Plot\rmatplot(x=X,y=Y,type = \u0026quot;l\u0026quot;,lwd = 2)\r　The covariance matrix \\(K\\) between the elements of \\(\\displaystyle \\textbf{y}\\), \\(\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\\) is calculated using the kernel method from the input \\(x\\). Then, from this \\(K\\) and average 0, we generate six series of multivariate normal random numbers and plot them.As these series are computed from a covariance matrix, we model that the more positive the covariance of each element, the more likely they are to be the same. Also, as you can see in the graphs, the graphs are very smooth and very flexible in their representation. The code samples and plots 1000 input points, limiting the input to 0 to 10 due to computational cost, but in principle, \\(x\\) is defined in the real number space, so \\(p(\\textbf{y})\\) follows an infinite dimensional multivariate normal distribution.\nAs described above, since \\(\\displaystyle \\textbf{y}\\) is assumed to follow a Gaussian process, \\(p(\\textbf{y})\\) follows a multivariate normal distribution \\(N(\\textbf{y}|0,K)\\) with simultaneous probability \\(p(\\textbf{y})\\) averaging 0 and the variance covariance matrix \\(K\\). Each element \\(K_{i,j}\\) of \\(K\\) is\n\\[\r\\begin{eqnarray}\rK_{i,j} \u0026amp;=\u0026amp; cov[y_{i},y_{j}] = cov[\\textbf{w}\\phi(x_{i}),\\textbf{w}\\phi(x_{j})] \\\\\r\u0026amp;=\u0026amp;\\phi(x_{i})\\phi(x_{j})cov[\\textbf{w},\\textbf{w}]=\\phi(x_{i})\\phi(x_{j})\\alpha\r\\end{eqnarray}\r\\]\nHere, the \\(\\phi(x_{i})\\phi(x_{j})\\alpha\\) is more expensive as the dimensionality of the \\(\\phi(x_{i})\\) is increased (i.e., the more non-linear transformation is applied, the less the calculation is completed). However, when the kernel function \\(k(x,x\u0026#39;)\\) is used, the computational complexity is higher in the dimensions of the sample size of the input data \\(x_{i},x_{j}\\), so the computation becomes easier. There are several types of kernel functions, but the following Gaussian kernels are commonly used.\n\\[\rk(x,x\u0026#39;) = a \\exp(-b(x-x\u0026#39;)^{2})\r\\]\nNow that we have defined the concurrent probability of \\(\\displaystyle \\textbf{y}\\), we can find the joint probability of \\(\\displaystyle \\textbf{t}\\).\n\\[\r\\begin{eqnarray}\r\\displaystyle p(\\textbf{t}) \u0026amp;=\u0026amp; \\int p(\\textbf{t}|\\textbf{y})p(\\textbf{y}) d\\textbf{y} \\\\\r\\displaystyle \u0026amp;=\u0026amp; \\int N(\\textbf{t}|\\textbf{y},\\beta^{-1}\\textbf{I}_{N})N(\\textbf{y}|0,K)d\\textbf{y} \\\\\r\u0026amp;=\u0026amp; N(\\textbf{y}|0,\\textbf{C}_{N})\r\\end{eqnarray}\r\\]\nwhere \\(\\textbf{C}_{N} = K + \\beta^{-1}\\beta^{I}_{N}\\). Note that the last expression expansion uses the regenerative nature of the normal distribution (the proof can be easily derived from the moment generating function of the normal distribution). The point is just to say that the covariance is the sum of the covariances of the two distributions, since they are independent. Personally, I imagine that \\(p(\\textbf{y})\\) is the prior distribution of the Gaussian process I just described, \\(p(\\textbf{t}|\\textbf{y})\\) is the likelihood function, and \\(p(\\textbf{t})\\) is the posterior distribution. The only constraint on the prior distribution \\(p(\\textbf{y})\\) is that it is smooth with a loosely constrained distribution.\rThe joint probability of \\(N\\) observable teacher data \\(\\textbf{t}\\) and \\(t_{N+1}\\) is\n\\[\rp(\\textbf{t},t_{N+1}) = N(\\textbf{t},t_{N+1}|0,\\textbf{C}_{N+1})\r\\]\nwhere \\(\\textbf{C}_{N+1}\\) is\n\\[\r\\textbf{C}_{N+1} = \\left(\r\\begin{array}{cccc}\r\\textbf{C}_{N} \u0026amp; \\textbf{k} \\\\\r\\textbf{k}^{T} \u0026amp; c \\\\\r\\end{array}\r\\right)\r\\]\nwhere \\(\\textbf{k} = (k(x_{1},x_{N+1}),...,k(x_{N},x_{N+1}))\\) and \\(c = k(x_{N+1},x_{N+1})\\). The conditional distribution \\(p(t_{N+1}|\\textbf{t})\\) can be obtained from the joint distribution of \\(\\textbf{t}\\) and \\(t_{N+1}\\).\n\\[\rp(t_{N+1}|\\textbf{t}) = N(t_{N+1}|\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{t},c-\\textbf{k}^{T}\\textbf{C}_{N+1}^{-1}\\textbf{k})\r\\]\nIn calculating the conditional distribution, we use Properties of the conditional multivariate normal distribution. As you can see from the above equation, the conditional distribution \\(p(t_{N+1}|\\textbf{t})\\) can be calculated if \\(N+1\\) input data, \\(N\\) teacher data, and parameters \\(a,b\\) of the kernel function are known, so if any point is given as input data, it is possible to approximate the Generating Process. The nice thing about the GPR is that it gives predictions without the direct estimation of the above defined probabilistic model \\(\\displaystyle y_{i} = \\textbf{w}^{T}\\phi(x_{i})\\). The stochastic model has \\(\\phi(x_{i})\\), which converts the input data to a high-dimensional vector through a nonlinear transformation. Therefore, the higher the dimensionality, the larger the computational complexity of the \\(\\phi(x_{i})\\phi(x_{j})\\alpha\\) will be, but the GPR uses a kernel trick, so the computational complexity of the sample size dimension of the input data vector will be sufficient.\n\r2. Implementation of the `GPR’\r　For now, let’s implement this in R, which I’ve implemented in PRML test data, so I tweaked it.\rlibrary(ggplot2)\rlibrary(grid)\r# 1.Gaussian Process Regression\r# PRML\u0026#39;s synthetic data set\rcurve_fitting \u0026lt;- data.frame(\rx=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000),\rt=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502))\rf \u0026lt;- function(beta, sigma, xmin, xmax, input, train) {\rkernel \u0026lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function\rK \u0026lt;- outer(input, input, kernel); # calc gram matrix\rC_N \u0026lt;- K + diag(length(input))/beta\rm \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance\rx \u0026lt;- seq(xmin,xmax,length=100)\routput \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),\rtx=input,ty=train),\raes(x=x1,y=m)) + geom_line() +\rgeom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +\rgeom_point(aes(x=tx,y=ty))\rreturn(output)\r}\rgrid.newpage() # make a palet\rpushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2\rprint(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))\rprint(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2))\rprint(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))\rprint(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) \rThe \\(beta^{-1}\\) represents the measurement error. The higher the value of \\(\\beta\\) (i.e., the smaller the measurement error), the easier it is to overfit, since the error of the predictions is less than that of the data already available. This is the case in the top left corner of the figure above. The top left corner is \\(\\beta=400\\), which means that it overfits the current data available. Conversely, a small value of \\(\\beta\\) will produce predictions that ignore the errors with the teacher data, but may improve the generalization performance. The top right figure shows this. For \\(beta=4\\), the average barely passes through the data points we have, and \\(b\\) is currently available. \\(b\\) represents the magnitude of the effect of the data we have at the moment on the surroundings. If \\(b\\) is small, the adjacent points will interact strongly with each other, which may reduce the accuracy but increase the generalization performance. Conversely, if \\(b\\) is large, the result will be unnatural, fitting only individual points. This is illustrated in the figure below right (\\(b=\\frac{1}{0.03}, \\beta=25\\)). As you can see, the graph is overfitting because of the large \\(\\beta\\) and because \\(b\\) is also large, so it fits only individual points, resulting in an absurdly large graph. The bottom left graph is the best. It has \\(b=\\frac{1}{0.3}\\), and \\(b=2\\). Let’s try extending the x interval of this graph to [0,2]. Then we get the following graph.\ngrid.newpage() # make a palet\rpushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2\rprint(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))\rprint(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) \rAs you can see, all the graphs except the bottom left one have a band of 95% confidence intervals that immediately widen and are completely useless where there are no data points. On the other hand, the lower left graph has a decent band up to 1.3 to 1.4, and the average value seems to pass through a point that is consistent with our intuitive understanding of the function. You can also see that if you are too far away from the observable data points, you will get a normal distribution with a mean of 0 and a variance of 1 no matter what you give to the parameters.\rNow that we have shown that the accuracy of the prediction of the out-sample varies depending on the value of the parameters, the question here is how to estimate these hyperparameters. This is done by using the gradient method to find the hyperparameters that maximize the log-likelihood function \\(\\ln p(\\bf{t}|a,b)\\) ((\\(\\beta\\) seems to be of a slightly different type, and the developmental discussion appears to take other tuning methods. We haven’t gotten to that level yet, so we’ll calibrate it here). Since \\(p(\\textbf{t}) = N(\\textbf{y}|0, \\textbf{C}_{N})\\), the log-likelihood function is\n\\[\r\\displaystyle \\ln p(\\textbf{t}|a,b,\\beta) = -\\frac{1}{2}\\ln|\\textbf{C}_{N}| - \\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1}\\textbf{k}\r\\]\nAfter that, we can differentiate this with the parameters and solve the obtained simultaneous equations to get the maximum likelihood estimator. Now let’s get the derivatives.\n\\[\r\\displaystyle \\frac{\\partial}{\\partial \\theta_{i}} \\ln p(\\textbf{t}|\\theta) = -\\frac{1}{2}Tr(\\textbf{C}_{N}^{-1}\\frac{\\partial \\textbf{C}_{N}}{\\partial \\theta_{i}}) + \\frac{1}{2}\\textbf{t}^{T}\\textbf{C}_{N}^{-1}\r\\frac{\\partial\\textbf{C}_{N}}{\\partial\\theta_{i}}\\textbf{C}_{N}^{-1}\\textbf{t}\r\\]\nwhere \\(theta\\) is the parameter set and \\(theta_{i}\\) represents the \\(i\\)th parameter. If you don’t understand this derivative here in the supplement to (C.21) and (C.22) equations. Since we are using the Gaussian kernel in this case, we get\n\\[\r\\displaystyle \\frac{\\partial k(x,x\u0026#39;)}{\\partial a} = \\exp(-b(x-x\u0026#39;)^{2}) \\\\\r\\displaystyle \\frac{\\partial k(x,x\u0026#39;)}{\\partial b} = -a(x-x\u0026#39;)^{2}\\exp(-b(x-x\u0026#39;)^{2})\r\\]\nfrom the above formula. However, this time we will use the gradient method to find the best parameters. Here’s the code for the implementation (it’s pretty much a lost cause).\ng \u0026lt;- function(xmin, xmax, input, train){\r# initial value\rbeta = 100\rb = 1\ra = 1\rlearning_rate = 0.1\ritermax \u0026lt;- 1000\rif (class(input) == \u0026quot;numeric\u0026quot;){\rN \u0026lt;- length(input)\r} else\r{\rN \u0026lt;- NROW(input)\r}\rkernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel\rderivative_a \u0026lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2)\rderivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)\rdloglik_a \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y }\rdloglik_b \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y }\r# loglikelihood function\rlikelihood \u0026lt;- function(b,a,x,y){\rkernel \u0026lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2)\rK \u0026lt;- outer(x, x, kernel)\rC_N \u0026lt;- K + diag(N)/beta\ritermax \u0026lt;- 1000\rl \u0026lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y\rreturn(l)\r}\rK \u0026lt;- outer(input, input, kernel) C_N \u0026lt;- K + diag(N)/beta\rfor (i in 1:itermax){\rkernel \u0026lt;- function(x1, x2) a*exp(-b*(x1-x2)^2)\rderivative_b \u0026lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)\rdloglik_b \u0026lt;- function(C_N,y,x1,x2) {\r-sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y }\rK \u0026lt;- outer(input, input, kernel) # calc gram matrix\rC_N \u0026lt;- K + diag(N)/beta\rl \u0026lt;- 0\rif(abs(l-likelihood(b,a,input,train))\u0026lt;0.0001\u0026amp;i\u0026gt;2){\rbreak\r}else{\ra \u0026lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input))\rb \u0026lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input))\r}\rl \u0026lt;- likelihood(b,a,input,train)\r}\rK \u0026lt;- outer(input, input, kernel)\rC_N \u0026lt;- K + diag(length(input))/beta\rm \u0026lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) m_sig \u0026lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel))))\rx \u0026lt;- seq(xmin,xmax,length=100)\routput \u0026lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),\rtx=input,ty=train),\raes(x=x1,y=m)) + geom_line() +\rgeom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +\rgeom_point(aes(x=tx,y=ty))\rreturn(output)\r}\rprint(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))\rYes, it does sound like good (lol).\rThat’s it for today, for now.\n\r","date":1543708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543708800,"objectID":"eb19f4601e090b622a80cdb28ae21cf8","permalink":"/en/post/post1/","publishdate":"2018-12-02T00:00:00Z","relpermalink":"/en/post/post1/","section":"post","summary":"I implemented the much talked about Gauss regression, which is too versatile to be fun in reverse.","tags":["R"],"title":"Implementing Gaussian regression.","type":"post"},{"authors":null,"categories":[],"content":"\r\r\rIt’s nice to meet you.\rMy name is Ayato Ashihara, and I’m a first-year graduate of an asset management company in Tokyo.\nI’d like to introduce myself briefly.\rI’m 24 years old and graduated from a graduate school.\rI majored in macroeconomics, especially DSGE models, state space models, Kalman filters, Bayesian estimation, and MCMC.\rI also did some natural language processing as research support for my advisor.\rI originally wanted to be an academic researcher, but due to financial problems, I had to find a job.\rNow I’m doing menial work in sales support.\nI think this blog will be a memorandum of my research hobby as I cannot give up my interest in research.\nI am currently working on the following two research projects.\nhorse betting version of the factor model\rquarterly GDP projection model\r\rThe first question is whether it is possible to create a model for constructing a horse betting portfolio that aims for a collection rate of over 100%, from the perspective that the horse betting market is more speculatively attractive than the stock market. For stocks, there is a factor model like Pharma French, but I would like to see if we can apply it to create a horse-trading version of the factor model.\nSecondly, we are aware of the problem of the low accuracy of the recent preliminary quarterly GDP report, so we wondered if it would be possible to construct a new, highly accurate forecasting model. In particular, I would like to use machine learning to create a model that incorporates data that is not normally used in macroeconomic research, rather than a model based on macroeconomic theory.\nWe’ve only just begun our research.\rI’ll do my best to keep you interested…\nBest regards.\n","date":1526601600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526601600,"objectID":"ceb64c07c657dc4dad3f41bf7df6c579","permalink":"/en/post/post4/","publishdate":"2018-05-18T00:00:00Z","relpermalink":"/en/post/post4/","section":"post","summary":"Nice to meet you. I'd like to start this blog by introducing myself.","tags":[],"title":"It's nice to meet you.","type":"post"},{"authors":["Ayato Ashihara"],"categories":null,"content":" Click the Cite button above to get Bibtex and Cite ME !!   ","date":1522454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522454400,"objectID":"8eb0b83c007669e1fcaec626e70937a6","permalink":"/en/publication/%E7%B4%80%E8%A6%812019/","publishdate":"2018-03-31T00:00:00Z","relpermalink":"/en/publication/%E7%B4%80%E8%A6%812019/","section":"publication","summary":"Japan  has  experienced  a  long-lasting  stagnation  since  the  early  1990s.  According  to  the  Reference Dates of Business Cycle, the Cabinet Office of Japan, there are four recession periods between 1987 and 2010,  and  three  of  them  are  considered  to  be  financially-related.  This  implies  that  the  stagnation  wastriggered by financial factors. Nonetheless, many studies using Dynamic Stochastic General Equilibrium models claim that a decline in Total Factor Productivity is the main driver of the stagnation. To resolve this contradiction,  this study estimates the Japanese economy by a New-Keynesian  DSGE  model augmented with  financial  friction  used  in  Christiano,  Motto  and  Rostagno  (2014),  where “risk  shock”  is newly incorporated into the model that refers to uncertainty in the financial market. According to our estimationresults, the estimated risk shock can explain the overall fluctuations of GDP and investment, and thus it isconsidered to be the main driver of the stagnation. We also find that it is highly correlated with the Business condition  Diffusion  Index,  the  Financial  Position  Diffusion  Index  and  the  Lending  Attitude  Index  of  Financial Institutions in Tankan released by the Bank of Japan. Therefore, we conclude that the estimated risk  shock  can  be  interpreted  as  the  firms’   distrust  toward  their  business  conditions,  and  it  delayed  theirinvestment decisions, then causing the prolonged economic contraction.","tags":"","title":"Does Financial Risk Explain Japan’s Great Stagnation?","type":"publication"},{"authors":["Ayato Ashihara"],"categories":null,"content":" Click the Cite button above to get Bibtex and Cite ME !!   ","date":1488844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488844800,"objectID":"e1ffa3282e6f7fc6edad66690c0b404c","permalink":"/en/publication/ael2018/","publishdate":"2017-03-07T00:00:00Z","relpermalink":"/en/publication/ael2018/","section":"publication","summary":"Using Japanese financial data that provide enough observations under the good and bad regimes of financial conditions, we find that fiscal multipliers are smaller in the bad regime than in the good regime.","tags":[""],"title":"Is fiscal expansion more effective in a financial crisis?","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/en/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/en/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/en/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]