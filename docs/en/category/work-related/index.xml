<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>work-related | 東京の資産運用会社で働く社会人が研究に没頭するブログ</title>
    <link>/en/category/work-related/</link>
      <atom:link href="/en/category/work-related/index.xml" rel="self" type="application/rss+xml" />
    <description>work-related</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 08 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>work-related</title>
      <link>/en/category/work-related/</link>
    </image>
    
    <item>
      <title>Is that backtest really reproducible?</title>
      <link>/en/post/post19/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post19/</guid>
      <description>
&lt;script src=&#34;../../../en/post/post19/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-backtesting&#34;&gt;1. What is “Backtesting”?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backtesting-overfits.&#34;&gt;2. Backtesting overfits.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-the-distribution-that-the-sharpe-ratio-follows&#34;&gt;3. What is the distribution that the Sharpe ratio follows?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trying-to-derive-the-minimum-backtest-length&#34;&gt;4. Trying to derive &lt;code&gt;the minimum backtest length&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#at-the-end&#34;&gt;4. At the end&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-is-backtesting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. What is “Backtesting”?&lt;/h2&gt;
&lt;p&gt;Backtesting is an algorithmic historical simulation of an investment strategy. Backtesting uses an algorithm to calculate the gains and losses that would have been incurred if the investment strategy you have drafted had been implemented over a period of time. Common statistics that evaluate the performance of an investment strategy, such as the Sharpe ratio and the information ratio, are used. Investors typically examine these back-testing statistics to determine asset allocation to the best performing investment (management) strategies, so asset managers do trial-and-error back-testing of a bloody number of times for good performance and present customer by that materials.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stat.ameba.jp/user_images/20190212/22/nash210/51/5f/j/o0705061514355131242.jpg&#34; alt=&#34;Backtest&#34; /&gt;
From an investor’s perspective, it is important to distinguish between in-sample (IS) and out-of-sample (OOS) performance of a back-tested investment strategy; IS performance is defined as the sample used to design the investment strategy (referred to in the machine learning literature as the “training period” or “training set”) It is what is called “OOS”) simulated in a sample (aka “test set”). OOS performance, on the other hand, is simulated on a sample (aka “test set”) that was not used to design the investment strategy. Since backtesting predicts the effectiveness of an investment strategy with its performance, it can be guaranteed to be reproducible and realistic if the IS performance matches the OOS performance. However, it is difficult to judge whether the backtest is reliable when you receive it because the results of the out-sample are future results. It cannot be called a pure out-sample as long as the results of the OOS can be fed back to improve the strategy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.triton.biz/blog1/wp-content/uploads/2018/04/pic001.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, when you receive a backtest with good results from a fund manager, it is very important that you manage to assess how realistic the simulation is. It is also important for fund managers to understand the uncertainty of their own backtesting results. In this post, we will look at how to evaluate the realism of a backtesting simulation and what to look out for in order to ensure a repeatable backtest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backtesting-overfits.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Backtesting overfits.&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2308659&#34;&gt;Bailey, Borwein, López de Prado and Zhu (2015)&lt;/a&gt; argue that it is (relatively) easy to overfit (overlearn) a backtesting simulation for any financial time series. Here, overfitting is a machine learning concept that describes a situation in which a model focuses on a particular set of observed data (IS data) rather than on a general structure.&lt;/p&gt;
&lt;p&gt;Bailey et. al. (2015) cite a situation where the backtesting results of a stock strategy are not good as an example of this claim. As the name implies, backtesting uses historical data, so it is possible to identify specific stocks that are experiencing losses and design the trading system to improve performance by adding some parameters to remove recommendations for those stocks ( (a technique known as “data snooping”). With a few repetitions of the simulation, you can derive “optimal parameters” that benefit from characteristics that are present in a particular sample but may be rare in the population.&lt;/p&gt;
&lt;p&gt;There is a vast accumulation of research in the machine learning literature to address the problem of overfitting. But Bailey et. al. (2015) argue that the methods proposed in the context of machine learning are generally not applicable to multiple investment problems. The reasons for this seem to be the following four points.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Machine learning methods to prevent overfitting require explicit point estimates and confidence intervals in the domain in which the event is defined in order to assess the explanatory power and quality of the prediction, because few investment strategies make such explicit predictions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For example, it is not often said that “the E-mini S&amp;amp;P 500 is expected to be around 1,600 at 5 points per standard deviation as of Friday’s close,” but rather qualitative recommendations such as “buy” or “strong buy” are usually provided. Moreover, these forecasts do not specify the expiration date of the forecast and are subject to change when some unexpected event occurs.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even if a particular investment strategy relies on a prediction formula, other components of the investment strategy may be overfitted.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In other words, there are many ways to overfit an investment strategy other than simply adjusting the prediction equation.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The methods of regression overfitting are parametric and involve many assumptions about data that are unobservable in the case of finance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some methods do not control for the number of trials.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bailey et. al. (2015) show that a &lt;strong&gt;comparatively low number of trials&lt;/strong&gt; is needed to identify investment strategies with relatively poor backtesting performance. Think of the number of trials here as the number of trials and errors. It also calculates &lt;code&gt;the minimum backtest length&lt;/code&gt; (MinBTL), which is the length of the backtest required for the number of trials. In this paper, the Sharpe ratio is always used to evaluate performance, but it can be applied to other performance measures as well. Let’s take a look at what it does.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-distribution-that-the-sharpe-ratio-follows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. What is the distribution that the Sharpe ratio follows?&lt;/h2&gt;
&lt;p&gt;To derive the MinBTL, we first derive the (asymptotic) distribution of the Sharpe ratio. To begin with, the design of an investment strategy usually starts with the prior knowledge or belief that a particular pattern may help to predict the future value of a financial variable. For example, if you are aware of the lead-lag effect between bonds of different maturities, you can design a strategy that bets on a return to the equilibrium value if the yield curve rises. This model could take the form of a cointegration equation, a vector error correction model, or a system of stochastic differential equations.&lt;/p&gt;
&lt;p&gt;The number of such model configurations (or trials) is vast, and fund managers naturally want to choose the one that maximizes the performance of their strategy, and to do so they conduct historical simulations (back testing) (see above). Backtesting evaluates the optimal sample size, frequency of signal updates, risk sizing, stop loss, maximum holding period, etc., in combination with other variables.&lt;/p&gt;
&lt;p&gt;The Sharpe ratio used as a measure of performance assessment in this paper is a statistic that assesses the performance of a strategy based on a sample of historical returns, defined as the average excess return/standard deviation (risk) to BM. It is usually interpreted as “return to risk 1 standard deviation” and, depending on the asset class, if it is greater than 1, it can be considered a very good strategy. In the following, we will assume that the excess return &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt; of a strategy is a random variable of i.i.d. and follows a normal distribution. That is, we assume that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt; is independent of &lt;span class=&#34;math inline&#34;&gt;\(r_s(t\neq s)\)&lt;/span&gt;. It is not a very realistic assumption, though.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_t \sim \mathcal{N}(\mu,\sigma^2)
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt; is the normal distribution of the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Now, the excess return &lt;span class=&#34;math inline&#34;&gt;\(r_{t}(q)\)&lt;/span&gt; at time t~t-q+1 is defined (I’m ignoring the compounding part.)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_{t}(q) \equiv r_{t} + r_{t-1} + ... + r_{t-q+1}
\]&lt;/span&gt;
Then, the annualized Sharpe ratio is represented&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
SR(q) &amp;amp;=&amp;amp; \frac{E[r_{t}(q)]}{\sqrt{Var(r_{t}(q))}}\\
&amp;amp;=&amp;amp; \frac{q\mu}{\sqrt{q}\sigma}\\
&amp;amp;=&amp;amp; \frac{\mu}{\sigma}\sqrt{q}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can express this as where &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the number (frequency) of returns per year. For example, &lt;span class=&#34;math inline&#34;&gt;\(q=365\)&lt;/span&gt; for daily returns (excluding leap years).&lt;/p&gt;
&lt;p&gt;The true value of &lt;span class=&#34;math inline&#34;&gt;\(SR\)&lt;/span&gt; cannot be known because &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; are generally unknown. So, let &lt;span class=&#34;math inline&#34;&gt;\(R_t\)&lt;/span&gt; be the sample return and the risk-free rate &lt;span class=&#34;math inline&#34;&gt;\(R^f\)&lt;/span&gt;(constant), we calculate the estimated Sharpe ratio by the sample mean of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=1/T\sum_{t=1}^T R_{t}-R^f\)&lt;/span&gt; and the sample standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}=\sqrt{1/T\sum_{t=1}^{T}(R_{t}-\hat{\mu})}\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is the sample size to be back-tested).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q) = \frac{\hat{\mu}}{\hat{\sigma}}\sqrt{q}
\]&lt;/span&gt;
As an inevitable consequence, the calculation of &lt;span class=&#34;math inline&#34;&gt;\(SR\)&lt;/span&gt; is likely to be accompanied by considerable estimation errors. Now, let us derive the main topic of this section, the asymptotic distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}\)&lt;/span&gt;. First, since the asymptotic distributions of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt; are finite and i.i.d., applying the central limit theorem derives&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{T}\hat{\mu}\sim^{a}\mathcal{N}(\mu,\sigma^2), \\
\sqrt{T}\hat{\sigma}^2\sim^a\mathcal{N}(\sigma^2,2\sigma^4)
\]&lt;/span&gt;
Since the Sharpe ratio is a random variable which is calculated from this &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt;, let us denote this function as &lt;span class=&#34;math inline&#34;&gt;\(g(\hat{{\boldsymbol \theta}})\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}=(\hat{\mu},\hat{\sigma}^2)\)&lt;/span&gt;. Now, since it is i.i.d., &lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}\)&lt;/span&gt; is independent of each other, and, from the above discussion, the asymptotic joint distribution is written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{T}\hat{{\boldsymbol \theta}} \sim^a \mathcal{N}({\boldsymbol \theta},{\boldsymbol V_{\boldsymbol \theta}})
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol V_{\boldsymbol \theta}}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
{\boldsymbol V_{\boldsymbol \theta}} = \left( 
    \begin{array}{cccc}
      \sigma^2 &amp;amp; 0\\
      0 &amp;amp; 2\sigma^4\\
    \end{array}
  \right)
\]&lt;/span&gt;
Since the sharp ratio estimates are now only a function of &lt;span class=&#34;math inline&#34;&gt;\(g(\hat{{\boldsymbol \theta}})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}\)&lt;/span&gt;, from the delta method, the following&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR} = g(\hat{{\boldsymbol \theta}}) \sim^a \mathcal{N}(g({\boldsymbol \theta}),\boldsymbol V_g)
\]&lt;/span&gt;
asymptotically follows a normal distribution where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol V_g\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol V_g=\frac{\partial g}{\partial{\boldsymbol \theta}}{\boldsymbol V_{\boldsymbol \theta}}\frac{\partial g}{\partial{\boldsymbol \theta}&amp;#39;}
\]&lt;/span&gt;
Since &lt;span class=&#34;math inline&#34;&gt;\(g({\boldsymbol \theta})=\mu/\sigma\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial g}{\partial{\boldsymbol \theta}&amp;#39;} = \left[ 
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma^2}\\
    \end{array}
  \right]
  = \left[ 
    \begin{array}{cccc}
      \frac{1}{\sigma}\\
      -\frac{\mu}{2\sigma^3}\\
    \end{array}
  \right]
\]&lt;/span&gt;
Therefore, you can derive&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\boldsymbol V_g &amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}, \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right)
  \left( 
    \begin{array}{cccc}
      \sigma^2 &amp;amp; 0\\
      0 &amp;amp; 2\sigma^4\\
    \end{array}
  \right)
  \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right) \\
  &amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\sigma^2, \frac{\partial g}{\partial \sigma}2\sigma^4\\
    \end{array}
  \right)
    \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right) \\
  &amp;amp;=&amp;amp; (\frac{\partial g}{\partial \mu})^2\sigma^2 + (\frac{\partial g}{\partial \sigma})^2\sigma^4 \\
  &amp;amp;=&amp;amp; 1 + \frac{\mu^2}{2\sigma^2} \\
  &amp;amp;=&amp;amp; 1 + \frac{1}{2}SR^2
\end{eqnarray}
\]&lt;/span&gt;
You may need to be careful when you see good performance because the variance tends to be exponentially larger as the absolute value of the Sharpe ratio increases. Here’s the distribution that the annualized Sharpe ratio estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}(q)\)&lt;/span&gt; follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q)\sim^a \mathcal{N}(\sqrt{q}SR,\frac{V(q)}{T}) \\
V(q) = q{\boldsymbol V}_g = q(1 + \frac{1}{2}SR^2)
\]&lt;/span&gt;
Now, if &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the number of years of backtesting, we can write &lt;span class=&#34;math inline&#34;&gt;\(T=yq\)&lt;/span&gt; and use this to rewrite the above equation as follows (for a 3-year measurement with daily returns, the sample size &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(T=3×365=1095\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q)\sim^a \mathcal{N}(\sqrt{q}SR,\frac{1+\frac{1}{2}SR^2}{y}) \tag{1}
\]&lt;/span&gt;
The frequency &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; affects the mean of the Sharpe ratio, but not the variance. We can now derive an asymptotic distribution of the Sharpe ratio estimates. Now, what do we wanted to do with this? We were thinking about the reliability of the backtest. In other words, what is the probability that a backtest of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; investment strategy ideas that FM twists and turns to develop a new product will produce a very high (good) value even though the true value of all of those Sharpe ratios is zero. In Bailey et. al. (2015), it was described as follows&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How high is the expected maximum Sharpe ratio IS among a set of strategy configurations where the true Sharpe ratio is zero?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We also want to know how long you should be backtesting for in order to reduce the value of the expected maximum Sharpe ratio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trying-to-derive-the-minimum-backtest-length&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Trying to derive &lt;code&gt;the minimum backtest length&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The situation we are considering now is that let &lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 1 year for simplicity, then from equation (1) &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}(q)\)&lt;/span&gt; follows the standard normal distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(0,1)\)&lt;/span&gt;. Now, we will consider the expected value of the maximum value &lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]_N\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}_n(n=1,2,...N)\)&lt;/span&gt;, but as those with good instincts will have noticed, the discussion goes into the context of extreme value statistics. Since &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}_n\sim\mathcal{N}(0,1)\)&lt;/span&gt; is i.i.d., the extreme value distribution of that maximum statistic becomes Gumbel distribution from the Fisher-Tippett-Gnedenko theorem (sorry, I haven’t been able to follow the proof).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\lim_{N\rightarrow\infty}prob[\frac{\max[\hat{SR}]_N-\alpha}{\beta}\leq x] = G(x) = e^{-e^{-x}}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\alpha=Z(x)^{-1}[1-1/N], \beta=Z(x)^{-1}[1-1/Ne^{-1}]-\alpha\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Z(x)\)&lt;/span&gt; represents the cumulative distribution function of the standard normal distribution. The moment generating function of the Gumbel distribution &lt;span class=&#34;math inline&#34;&gt;\(M_x(t)\)&lt;/span&gt; is written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
M_x(t) &amp;amp;=&amp;amp; E[e^{tx}] = \int_{-\infty}^\infty e^{tx}e^{-x}e^{-e^{-x}}dx \\
\end{eqnarray}
\]&lt;/span&gt;
Converting variables with &lt;span class=&#34;math inline&#34;&gt;\(x=-\log(y)\)&lt;/span&gt; gives &lt;span class=&#34;math inline&#34;&gt;\(dx/dy=-1/y=-(e^{-x})^{-1}\)&lt;/span&gt;, so&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
M_x(t) &amp;amp;=&amp;amp; \int_{\infty}^0-e^{-t\log(y)}e^{-y}dy \\
&amp;amp;=&amp;amp; \int_{0}^\infty y^{-t}e^{-y}dy \\
&amp;amp;=&amp;amp; \Gamma(1-t)
\end{eqnarray}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x)\)&lt;/span&gt; is a gamma function. From here, the expected value (mean) of the standardized maximum statistic is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\lim_{N\rightarrow\infty} E[\frac{\max[\hat{SR}]_N-\alpha}{\beta}] &amp;amp;=&amp;amp; M_x&amp;#39;(t)|_{t=0} \\
&amp;amp;=&amp;amp; (-1)\Gamma&amp;#39;(1) \\
&amp;amp;=&amp;amp; (-1)(-\gamma) = \gamma
\end{eqnarray}
\]&lt;/span&gt;
Here, $… $ is the Euler-Mascheroni constant. Thus, when &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is large, the expected value of the maximum statistic of the standard normal distribution of i.i.d. is approximated as (&lt;span class=&#34;math inline&#34;&gt;\(N&amp;gt;1\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[\max[\hat{SR}]] \approx \alpha + \gamma\beta = (1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}] \tag{2}
\]&lt;/span&gt;
This is Proposition 1 of Bailey et. al. (2015). We plot &lt;span class=&#34;math inline&#34;&gt;\(E[\max[\hat{SR}]]\)&lt;/span&gt; as a function of the number of strategies (number of trials and errors) &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, which is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: パッケージ &amp;#39;ggplot2&amp;#39; はバージョン 4.0.3 の R の下で造られました&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ExMaxSR = function(N){
  gamma_ct = -digamma(1)
  Z = qnorm(0.99)
  return((1-gamma_ct)*Z*(1-1/N) + gamma_ct*Z*(1-1/N*exp(1)^{-1}))
}
N = list(0:100)
result = purrr::map(N,ExMaxSR)
ggplot2::ggplot(data.frame(ExpMaxSR = unlist(result),N = unlist(N)),aes(x=N,y=ExpMaxSR)) +
  geom_line(size=1) + ylim(0,3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../../en/post/post19/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]\)&lt;/span&gt; is rapidly increasing for the small &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. At &lt;span class=&#34;math inline&#34;&gt;\(N=10\)&lt;/span&gt;, we expect to find at least one apparently quite good performing strategy, even though the true value of the Sharpe ratio of all strategies is zero. In finance, hold-out back-testing is often used, but this method does not take into account the number of trials, which is why it does not return reliable results when &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is large. Don’t you think it’s very risky to do a lot of simulations in order to improve the backtest results? In the end, only the best performing of the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; strategies will show up in the presentation material, so even if you consider 10 strategies, as in this example, any of them will have a Sharpe ratio distributed around 1.87. Of course, we don’t include the number of trials and errors in our materials, so it’s very misleading. When evaluating these materials, you might want to suspect false positives first.&lt;/p&gt;
&lt;p&gt;So, as for what to do, Bailey et. al. (2015) calculate the Minimum Backtest Length. In short, they caution that as the number of trials (errors) &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is increased, the number of years of backtesting &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; should also increase. Let’s show the relationship between &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and the Minimum Backtest Length. As before, we will assume that &lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt;, but consider the case with &lt;span class=&#34;math inline&#34;&gt;\(y\neq 1\)&lt;/span&gt;. The expected value of the maximum statistic of the annualized Sharpe ratio is from equation (2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[\max[\hat{SR}(q)]_N] \approx y^{-1/2}((1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}])
\]&lt;/span&gt;
By solving this for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, you can get MinBTL.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MinBTL \approx (\frac{(1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}]}{\bar{E[\max[\hat{SR}(q)]_N]}})^2
\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\bar{E[\max[\hat{SR}(q)]_N]}\)&lt;/span&gt; is the upper limit of &lt;span class=&#34;math inline&#34;&gt;\(E[\max[\hat{SR}(q)]_N]\)&lt;/span&gt;, and the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; strategy, where the true value of the Sharpe ratio is zero, suppresses the value that the maximum Sharpe ratio statistic can take. In doing so, the required backtesting years &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are derived as MinBTL. Plotting the MinBTL as a function of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\bar{E[\max[\hat{SR}(q)]_N]}=1\)&lt;/span&gt; is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MinBTL &amp;lt;- function(N,MaxSR){
  return((ExMaxSR(N)/MaxSR)^2)
}
N = list(1:100)
result = purrr::map2(N,1,MinBTL)
ggplot2::ggplot(data.frame(MinBTL = unlist(result),N = unlist(N)),aes(x=N,y=MinBTL)) +
  geom_line(size=1) + ylim(0,6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../../en/post/post19/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simSR &amp;lt;- function(T1){
    r = rnorm(T1)
    return(mean(r)/sd(r))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your backtesting period is less than 3 years, the number of trials (or errors) &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; should be limited to one. It is important to note that even if you are backtesting within the MinBTL, it is still possible to overfit. In other words, the MinBTL is a necessary condition, not a sufficient condition.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;at-the-end&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. At the end&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257497&#34;&gt;López de Prado (2018)&lt;/a&gt; lists the following as generic means of preventing overfitting&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Develop models for entire asset classes or investment universes, rather than for specific securities. Investors diversify, hence they do not make mistake X only on security Y. If you find mistake X only on security Y, no matter how apparently profitable, it is likely a false discovery.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apply bagging as a means to both prevent overfitting and reduce the variance of the forecasting error. If bagging deteriorates the performance of a strategy, it was likely overfit to a small number of observations or outliers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Do not backtest until all your research is complete.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Record every backtest conducted on a dataset so that the probability of backtest overfitting may be estimated on the final selected result (see &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2326253&#34;&gt;Bailey, Borwein, López de Prado and Zhu(2017)&lt;/a&gt;), and the Sharpe ratio may be properly deflated by the number of trials carried out (&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2465675&#34;&gt;Bailey and López de Prado(2014.b)&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simulate scenarios rather than history. A standard backtest is a historical simulation, which can be easily overfit. History is just the random path that was realized, and it could have been entirely different. Your strategy should be profitable under a wide range of scenarios, not just the anecdotal historical path. It is harder to overfit the outcome of thousands of “what if” scenarios.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do not research under the influence of a backtest. If the backtest fails to identify a profitable strategy,
start from scratch. Resist the temptation of reusing those results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think 3 and 6 are relevant contexts for today’s post. There is an accumulation of other research in this area, so if you’re going to do backtesting on the job, it’s good to study techniques, but I recommend learning about the proper way to operate backtesting as a primer in the first place.&lt;br /&gt;
In this post, I’ve decided to tackle a slightly different topic from the usual one. I often see the results of backtests in my own work, and I often do hold-out backtests on this blog. I will continue to follow my research on this topic so that I can understand and evaluate the uncertainty of the results obtained.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I built the Asset Allocation Model in R.</title>
      <link>/en/post/post2/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/post2/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-variance-portfolio&#34;&gt;1. Minimum Variance Portfolio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-predict-the-variance-covariance-matrix&#34;&gt;2. How to predict the variance-covariance matrix&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a.-constant-conditional-correlation-ccc-model&#34;&gt;A. Constant conditional correlation (CCC) model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#b.-dynamic-conditional-correlation-dcc-model&#34;&gt;B. Dynamic Conditional Correlation (DCC) model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#c.-dynamic-equicorrelation-deco-model&#34;&gt;C. Dynamic Equicorrelation (DECO) model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#collection-of-data-for-testing&#34;&gt;3. Collection of data for testing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. I attended a workshop on asset allocation at my place of work, so I’d like to run some simulations here, even though this field is completely outside my expertise. In this article, I would like to do an exercise on how to estimate the variance-covariance matrix (predictions) of a minimum variance portfolio as the base portfolio, referring to previous studies. The reference research is in the following paper (in the Journal of Operations Research).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2947643&#34;&gt;Asset Allocation with Correlation: A Composite Trade-Off&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;minimum-variance-portfolio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Minimum Variance Portfolio&lt;/h2&gt;
&lt;p&gt;I won’t go into a detailed explanation of the minimum variance portfolio here, but it is a portfolio that calculates the average return and variance of each asset (domestic stocks, foreign stocks, domestic bonds, foreign bonds, and alternatives), plots them on a quadratic plane of average return vertical and variance horizontal axes, calculates the range of possible investments, and then calculates the smallest variance in the set (see the chart below).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Minimum_variance_flontier_of_MPT.svg/1280px-Minimum_variance_flontier_of_MPT.svg.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;minimum variance portfolio&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In an earlier study, Carroll et. al. (2017), the following is stated&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;this paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even at present, it is difficult to estimate expected returns, and a minimum variance portfolio, which does not require it, is a useful and practical technique. The objective function of a minimum variance portfolio is, as the name implies, &lt;strong&gt;to minimize diversification&lt;/strong&gt;. If we now denote the vector of collected returns for each asset by &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, the holding weight of each asset by &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and the portfolio return by &lt;span class=&#34;math inline&#34;&gt;\(R_{p}\)&lt;/span&gt;, then the overall portfolio variance &lt;span class=&#34;math inline&#34;&gt;\(var(R_{p})\)&lt;/span&gt; can be written as follows.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
var(R_{p}) = var(r^{T}\theta) = E( (r^{T}\theta)(r^{T}\theta)^{T}) = \theta^{T}\Sigma\theta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Sigma\)&lt;/span&gt; is the variance-covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;. Thus, the minimization problem is as follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\theta}(\theta^{T}\Sigma\theta) \\
s.t 1^{T}\theta = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we have added full investment as a constraint. Let’s use the Lagrangian to solve this problem. The Lagrange function &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is as follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L = \theta^{T}\Sigma\theta + \lambda(1^{T}\theta - 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The first condition is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle\frac{\partial L}{\partial \theta} = 2\Sigma\theta + 1\lambda = 0 \\
\displaystyle \frac{\partial L}{\partial \lambda} = 1^{T} \theta = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Solving the first equation for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we find that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta = \Sigma^{-1}1\lambda^{*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}=-1/2\lambda\)&lt;/span&gt;. Substitute this into the second formula and solve for &lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
1^{T}\Sigma1\lambda^{*} = 1 \\
\displaystyle \lambda^{*} = \frac{1}{1^{T}\Sigma^{-1}1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\theta = \Sigma^{-1}1\lambda^{*}\)&lt;/span&gt;, erasing &lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}\)&lt;/span&gt; will cause it to be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \theta_{gmv} = \frac{\Sigma^{-1}1}{1^{T}\Sigma^{-1}1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we can now find the optimal weight. For now, I’ll implement this in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gmv &amp;lt;- function(r_dat,r_cov){
  library(MASS)
  i &amp;lt;- matrix(1,NCOL(r_dat),1)
  r_weight &amp;lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i)
  wr_dat &amp;lt;- r_dat*as.numeric(r_weight)
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;) 
  return(result)
}

nlgmv &amp;lt;- function(r_dat,r_cov){
  qp.out &amp;lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))),
           bvec=c(1,rep(0,NCOL(r_dat))),meq=1)
  r_weight &amp;lt;- qp.out$solution
  wr_dat &amp;lt;- r_dat*r_weight
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The input is a return and variance-covariance matrix for each asset. The output is the weight, return and risk. &lt;code&gt;nlgmv&lt;/code&gt; is a short-sale constrained version of the minimum variance portfolio. Since we cannot get an analytical solution, we are trying to get a numerical solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-predict-the-variance-covariance-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. How to predict the variance-covariance matrix&lt;/h2&gt;
&lt;p&gt;We have found the formula for the minimum variance portfolio. The next step is to analyze how to find the input, the variance-covariance matrix. I think the most primitive approach would be the historical approach of finding the covariance matrix for a sample of return data available before that point in time, and then finding the minimum variance portfolio by fixing the value of the covariance matrix (i.e., the weights are also fixed). However, since this is just using historical averages for future projections, I’m sure there will be all sorts of problems. As a non-specialist, I can think of a situation where the return on asset A declined significantly the day before, but the variance of this asset is expected to remain high tomorrow, and the effect of yesterday is diluted by using the average. And I feel that not changing the weights from the start would also move away from the optimal point over time. However, there seems to be some trial and error in this area as to how to estimate it then. Also, in Carroll et. al. (2017), it is stated that,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The estimates in this paper are based on the following models. All of them are unique in that the variance-covariance matrix is time-varying.&lt;/p&gt;
&lt;div id=&#34;a.-constant-conditional-correlation-ccc-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A. Constant conditional correlation (CCC) model&lt;/h3&gt;
&lt;p&gt;The original paper is &lt;a href=&#34;https://www.jstor.org/stable/2109358?read-now=1&amp;amp;seq=3#page_scan_tab_contents&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, from the relationship between the variance-covariance matrix and the correlation matrix, we have &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t} = D_{t}R_{t}D_{t}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; is the variance-covariance matrix and &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(diag(\sigma_{1,t},...,\sigma_{N,t})\)&lt;/span&gt;, a matrix whose diagonal components are the standard deviation of each asset in &lt;span class=&#34;math inline&#34;&gt;\(tt\)&lt;/span&gt; period. From here, we estimate &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; separately. First, &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; is estimated using the following multivariate GARCH model (1,1).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_{t} = \mu + u_{t} \\
u_{t} = \sigma_{t}\epsilon \\
\sigma_{t}^{2} = \alpha_{0} + \alpha_{1}u_{t-1}^{2} + \alpha_{2}\sigma_{t-1}^{2} \\
\epsilon_{t} = NID(0,1) \\
E(u_{t}|u_{t-1}) = 0
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the sample mean of the returns. &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{i}\)&lt;/span&gt; is the parameter to be estimated. Since we are estimating &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; with GARCH, we can say that we are modeling a relationship where the distribution of returns follows a thicker base distribution than the normal distribution and where the change in returns is not constant but depends on the variance of the previous day. We can say that we have estimated &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; in this way. The next step in the estimation of &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; is to take a historical approach to the estimation of &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;, in which the returns are sampled. In other words, &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; is a constant. Thus, we are making the assumption that the magnitude of return variation varies with time, but the relative relationship of each asset is invariant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-dynamic-conditional-correlation-dcc-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B. Dynamic Conditional Correlation (DCC) model&lt;/h3&gt;
&lt;p&gt;The original paper is &lt;a href=&#34;http://www.cass.city.ac.uk/__data/assets/pdf_file/0003/78960/Week7Engle_2002.pdf&#34;&gt;here&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;こちらのモデルでは、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;を求めるところまでは①と同じですが、[&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、&lt;span class=&#34;math inline&#34;&gt;\(tex:t\)&lt;/span&gt;期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;p&gt;This model is the same as (1) up to the point of finding &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;, but the way of finding &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; is different, and we use &lt;code&gt;ARMA(1,1)&lt;/code&gt; to estimate it. Since the correlation matrix is still not a constant, we are going to use the available returns up to the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; period to make the estimate. The correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; in this model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Q_{t}\)&lt;/span&gt; is a conditional variance-covariance matrix in the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; period, formulated as follows,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q_{t} = \bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\epsilon_{i,t-1}\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{Q}\)&lt;/span&gt; is the variance-covariance matrix calculated in a historical way and &lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt; is the parameter. This method differs from the previous one in that it assumes that not only does the magnitude of return variation vary with time, but also the relative relationship of each asset changes over time. Since we have observed some events in which the returns of all assets fall and the correlation of each asset becomes positive during a financial crisis, this formulation may be attractive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-dynamic-equicorrelation-deco-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;C. Dynamic Equicorrelation (DECO) model&lt;/h3&gt;
&lt;p&gt;The original paper is &lt;a href=&#34;https://faculty.chicagobooth.edu/bryan.kelly/research/pdf/deco.pdf&#34;&gt;here&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;I haven’t read this paper exactly yet, but from the definition of the correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{t} = (1-\rho_{t})I_{N} + \rho_{t}1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\rho_{t}\)&lt;/span&gt; is a scalar and a coefficient for the degree of equicorrelation, and I understand that equicorrelation is an average pair-wise correlation. In other words, if there are no missing values, it’s no different than a normal correlation. However, it seems to be a good estimator in that respect, because as assets increase, such problems need to be addressed. We can calculate &lt;span class=&#34;math inline&#34;&gt;\(\rho_{t}\)&lt;/span&gt; as follows.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \rho_{t} = \frac{1}{N(N-1)}(\iota^{T}R_{t}^{DCC}\iota - N) = \frac{2}{N(N-1)}\sum_{i&amp;gt;j}\frac{q_{ij,t}}{\sqrt{q_{ii,t} q_{jj,t}}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\iota\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(N×1\)&lt;/span&gt; vector with all elements being 1. And &lt;span class=&#34;math inline&#34;&gt;\(q_{ij,t}\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i,j\)&lt;/span&gt; element of &lt;span class=&#34;math inline&#34;&gt;\(Q_{t}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now that we’ve modeled the variance-covariance matrix, we’ll implement it so far in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carroll &amp;lt;- function(r_dat,FLG){
  
  library(rmgarch)
  
  if(FLG == &amp;quot;benchmark&amp;quot;){
    H &amp;lt;- cov(r_dat)
  }else{
    #1. define variables
    N &amp;lt;- NCOL(r_dat) # the number of assets
    
    #2. estimate covariance matrix
    basic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = &amp;#39;sGARCH&amp;#39;), distribution.model = &amp;#39;norm&amp;#39;)
    multi_garch = multispec(replicate(N, basic_garch))
    dcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = &amp;quot;mvnorm&amp;quot;,model = &amp;quot;DCC&amp;quot;)
    fit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE))
    forecast_dcc_garch &amp;lt;- dccforecast(fit_dcc_garch)
    if (FLG == &amp;quot;CCC&amp;quot;){
      #Constant conditional correlation (CCC) model
      D &amp;lt;- sigma(forecast_dcc_garch)
      R_ccc &amp;lt;- cor(r_dat)
      H &amp;lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1])
      colnames(H) &amp;lt;- colnames(r_dat)
      rownames(H) &amp;lt;- colnames(r_dat)
    }
    else{
      #Dynamic Conditional Correlation (DCC) model
      H &amp;lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1])
      if (FLG == &amp;quot;DECO&amp;quot;){
        #Dynamic Equicorrelation (DECO) model
        one &amp;lt;- matrix(1,N,N)
        iota &amp;lt;- rep(1,N)
        Q_dcc &amp;lt;- rcor(forecast_dcc_garch,type=&amp;quot;Q&amp;quot;)[[1]][,,1]
        rho &amp;lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N))
        D &amp;lt;- sigma(forecast_dcc_garch)
        R_deco &amp;lt;- (1-rho)*diag(1,N,N) + rho*one
        H &amp;lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1])
        colnames(H) &amp;lt;- colnames(r_dat)
        rownames(H) &amp;lt;- colnames(r_dat)
      }
    }
  }
  return(H)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I shouldn’t normally use the package, but since it’s an exercise today I’m only going to pursue the results of the estimates, and I’ll be writing a post about GARCH in the next week or so.
Now we’re ready to go. We can now put the return data into this function to calculate the variance-covariance matrix and use it to calculate the minimum variance portfolio.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;collection-of-data-for-testing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Collection of data for testing&lt;/h2&gt;
&lt;p&gt;The data was based on the following article.&lt;/p&gt;
&lt;p&gt;(Introduction to Asset Allocation)[&lt;a href=&#34;https://www.r-bloggers.com/introduction-to-asset-allocation/&#34; class=&#34;uri&#34;&gt;https://www.r-bloggers.com/introduction-to-asset-allocation/&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;We used NAV data for an ETF (iShares) that is linked to the following index&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S&amp;amp;P500&lt;/li&gt;
&lt;li&gt;NASDAQ100&lt;/li&gt;
&lt;li&gt;MSCI Emerging Markets&lt;/li&gt;
&lt;li&gt;Russell 2000&lt;/li&gt;
&lt;li&gt;MSCI EAFE&lt;/li&gt;
&lt;li&gt;US 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index)&lt;/li&gt;
&lt;li&gt;U.S. Real Estate(the Dow Jones US Real Estate Index)&lt;/li&gt;
&lt;li&gt;gold bullion market&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first step is to collect data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(quantmod)

#**************************
# ★8 ASSETS SIMULATION
# SPY - S&amp;amp;P 500 
# QQQ - Nasdaq 100
# EEM - Emerging Markets
# IWM - Russell 2000
# EFA - EAFE
# TLT - 20 Year Treasury
# IYR - U.S. Real Estate
# GLD - Gold
#**************************

# load historical prices from Yahoo Finance
symbol.names = c(&amp;quot;S&amp;amp;P 500&amp;quot;,&amp;quot;Nasdaq 100&amp;quot;,&amp;quot;Emerging Markets&amp;quot;,&amp;quot;Russell 2000&amp;quot;,&amp;quot;EAFE&amp;quot;,&amp;quot;20 Year Treasury&amp;quot;,&amp;quot;U.S. Real Estate&amp;quot;,&amp;quot;Gold&amp;quot;)
symbols = c(&amp;quot;SPY&amp;quot;,&amp;quot;QQQ&amp;quot;,&amp;quot;EEM&amp;quot;,&amp;quot;IWM&amp;quot;,&amp;quot;EFA&amp;quot;,&amp;quot;TLT&amp;quot;,&amp;quot;IYR&amp;quot;,&amp;quot;GLD&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, auto.assign = TRUE)

#gn dates for all symbols &amp;amp; convert to monthly
hist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)
month.ends = endpoints(hist.prices, &amp;#39;day&amp;#39;)
hist.prices = Cl(hist.prices)[month.ends, ]
colnames(hist.prices) = symbols

# remove any missing data
hist.prices = na.omit(hist.prices[&amp;#39;1995::&amp;#39;])

# compute simple returns
hist.returns = na.omit( ROC(hist.prices, type = &amp;#39;discrete&amp;#39;) )

# compute historical returns, risk, and correlation
ia = list()
ia$expected.return = apply(hist.returns, 2, mean, na.rm = T)
ia$risk = apply(hist.returns, 2, sd, na.rm = T)
ia$correlation = cor(hist.returns, use = &amp;#39;complete.obs&amp;#39;, method = &amp;#39;pearson&amp;#39;)

ia$symbols = symbols
ia$symbol.names = symbol.names
ia$n = length(symbols)
ia$hist.returns = hist.returns

# convert to annual, year = 12 months
annual.factor = 12
ia$expected.return = annual.factor * ia$expected.return
ia$risk = sqrt(annual.factor) * ia$risk

rm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how the returns are plotted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = &amp;quot;Performance summary&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, we’ll code the backtest. We’ll publish the code all at once.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# BACK TEST
backtest &amp;lt;- function(r_dat,FLG,start_date,span,learning_term,port){
  #-----------------------------------------
  # BACKTEST
  # r_dat - return data(xts object) 
  # FLG - flag(CCC,DCC,DECO)
  # start_date - start date for backtest
  # span - rebalance frequency
  # learning_term - learning term (days)
  # port - method of portfolio optimization
  #-----------------------------------------
  
  library(stringi)

  initial_dat &amp;lt;- r_dat[stri_c(as.Date(start_date)-learning_term,&amp;quot;::&amp;quot;,as.Date(start_date))]
  for (i in NROW(initial_dat):NROW(r_dat)) {
    if (i == NROW(initial_dat)){
      H &amp;lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG)
      if (port == &amp;quot;nlgmv&amp;quot;){
        result &amp;lt;- nlgmv(initial_dat,H)
      }else if (port == &amp;quot;risk parity&amp;quot;){
        result &amp;lt;- risk_parity(initial_dat,H)
      }
      weight &amp;lt;- t(result$weight)
      colnames(weight) &amp;lt;- colnames(initial_dat)
      p_return &amp;lt;- initial_dat[NROW(initial_dat),]*result$weight
    } else {
      if (i %in% endpoints(r_dat,span)){
        H &amp;lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG)
        if (port == &amp;quot;nlgmv&amp;quot;){
          result &amp;lt;- nlgmv(test_dat,H)
        }else if (port == &amp;quot;risk parity&amp;quot;){
          result &amp;lt;- risk_parity(test_dat,H)
        }
        
      }
      weight &amp;lt;- rbind(weight,t(result$weight))
      p_return &amp;lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight)
    }
    if (i != NROW(r_dat)){
      term &amp;lt;- stri_c(index(r_dat[i+1,])-learning_term,&amp;quot;::&amp;quot;,index(r_dat[i+1,])) 
      test_dat &amp;lt;- r_dat[term]
    }
  }
  p_return$portfolio &amp;lt;- xts(apply(p_return,1,sum),order.by = index(p_return))
  weight.xts &amp;lt;- xts(weight,order.by = index(p_return))

  result &amp;lt;- list(p_return,weight.xts)
  names(result) &amp;lt;- c(&amp;quot;return&amp;quot;,&amp;quot;weight&amp;quot;)
  return(result)
}

CCC &amp;lt;- backtest(hist.returns,&amp;quot;CCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DCC &amp;lt;- backtest(hist.returns,&amp;quot;DCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DECO &amp;lt;- backtest(hist.returns,&amp;quot;DECO&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
benchmark &amp;lt;- backtest(hist.returns,&amp;quot;benchmark&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)

result &amp;lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)
colnames(result) &amp;lt;- c(&amp;quot;CCC&amp;quot;,&amp;quot;DCC&amp;quot;,&amp;quot;DECO&amp;quot;,&amp;quot;benchmark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a graph of the calculation results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result,main = &amp;quot;BACKTEST&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I did not use the minimum variance portfolio defined above because I imposed a short sale constraint. Apparently it’s difficult to solve analytically with this alone, so I’m going to solve numerically. I used a weekly rebalancing period, so it took me a while to calculate the results on my own PC, but I was able to calculate the results.&lt;/p&gt;
&lt;p&gt;Since Lehman, it appears to have outperformed the benchmark equal weighted portfolio. In particular, DECO is looking good. I would like to rethink the meaning of Equicorrelation. The change in each incorporation ratio is as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot allocation weighting
d_allocation &amp;lt;- function(ggweight,title){
  #install.packages(&amp;quot;tidyverse&amp;quot;)
  library(tidyverse)
  ggweight &amp;lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method)
  ggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) +
    geom_area(colour=&amp;quot;black&amp;quot;,size=.1) +
    scale_y_continuous(limits = c(0,1)) +
    labs(title=title) + facet_grid(method~.)
}

gmv_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)))

# plot allocation weighting
d_allocation(gmv_weight,&amp;quot;GMV Asset Allocation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;They seem to have increased their allocation to TLT, or U.S. Treasuries, during Lehman, while CCC and DCC have a high allocation to U.S. Treasuries in other areas as well, and the often-cited problem of a minimally diversified portfolio seems to be occurring here as well. On the other hand, DECO has a unique mix ratio, and I think we need to go back and read the paper again.&lt;/p&gt;
&lt;p&gt;PS（2019/3/3）
So far, I’ve been doing the analysis with a minimum variance portfolio, but I also wanted to see the results of risk parity, so I wrote the code for that as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;risk_parity &amp;lt;- function(r_dat,r_cov){
  fn &amp;lt;- function(weight, r_cov) {
    N &amp;lt;- NROW(r_cov)
    risks &amp;lt;-  weight * (r_cov %*% weight)
    g &amp;lt;- rep(risks, times = N) - rep(risks, each = N)
  return(sum(g^2))
  }
  dfn &amp;lt;- function(weight,r_cov){
    out &amp;lt;- weight
    for (i in 0:length(weight)) {
      up &amp;lt;- dn &amp;lt;- weight
      up[i] &amp;lt;- up[i]+.0001
      dn[i] &amp;lt;- dn[i]-.0001
      out[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002
    }
    return(out)
  }
  std &amp;lt;- sqrt(diag(r_cov)) 
  x0 &amp;lt;- 1/std/sum(1/std)
  res &amp;lt;- nloptr::nloptr(x0=x0,
                 eval_f=fn,
                 eval_grad_f=dfn,
                 eval_g_eq=function(weight,r_cov) { sum(weight) - 1 },
                 eval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) },
                 lb=rep(0,length(std)),ub=rep(1,length(std)),
                 opts = list(&amp;quot;algorithm&amp;quot;=&amp;quot;NLOPT_LD_SLSQP&amp;quot;,&amp;quot;print_level&amp;quot; = 0,&amp;quot;xtol_rel&amp;quot;=1.0e-8,&amp;quot;maxeval&amp;quot; = 1000),
                 r_cov = r_cov)
  r_weight &amp;lt;- res$solution
  names(r_weight) &amp;lt;- colnames(r_cov)
  wr_dat &amp;lt;- r_dat*r_weight
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;)
  return(result)
  }

CCC &amp;lt;- backtest(hist.returns,&amp;quot;CCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DCC &amp;lt;- backtest(hist.returns,&amp;quot;DCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DECO &amp;lt;- backtest(hist.returns,&amp;quot;DECO&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
benchmark &amp;lt;- backtest(hist.returns,&amp;quot;benchmark&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)

result &amp;lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)
colnames(result) &amp;lt;- c(&amp;quot;CCC&amp;quot;,&amp;quot;DCC&amp;quot;,&amp;quot;DECO&amp;quot;,&amp;quot;benchmark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the result.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result, main = &amp;quot;BACKTEST COMPARISON&amp;quot;)

library(plotly)

# plot allocation weighting
riskparity_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=&amp;quot;benchmark&amp;quot;,Date=index(benchmark$weight)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result, main = &amp;quot;BACKTEST COMPARISON&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;riskparity_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=&amp;quot;benchmark&amp;quot;,Date=index(benchmark$weight)))

# plot allocation weighting
d_allocation(riskparity_weight, &amp;quot;Risk Parity Asset Allocation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are positive, with all methods outperforming benchmark.
As expected, the estimation of the variance-covariance matrix seems to be performing well. The good performance of DECO may also be due to the fact that it uses the average of the correlation coefficients of each asset pair in the correlation matrix, which resulted in a greater inclusion of risk assets than the other methods. The weights are as follows&lt;/p&gt;
&lt;p&gt;That’s it for today, for now.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
