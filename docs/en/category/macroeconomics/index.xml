<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>macroeconomics | 東京の資産運用会社で働く社会人が研究に没頭するブログ</title>
    <link>/en/category/macroeconomics/</link>
      <atom:link href="/en/category/macroeconomics/index.xml" rel="self" type="application/rss+xml" />
    <description>macroeconomics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 19 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>macroeconomics</title>
      <link>/en/category/macroeconomics/</link>
    </image>
    
    <item>
      <title>Get macro panel data from OECD.org via API</title>
      <link>/en/post/post22/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post22/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#oecd.stat-web-api&#34;&gt;1.OECD.Stat Web API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pandasdmx&#34;&gt;2.pandasdmx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;3.implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#another-matter&#34;&gt;4. Another matter…&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. There are several ways to collect macroeconomic data, but collecting data for each country can be a challenge. However, you can automate the tedious process of collecting data from the OECD via API. Today, I will introduce the method.&lt;/p&gt;
&lt;div id=&#34;oecd.stat-web-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.OECD.Stat Web API&lt;/h2&gt;
&lt;p&gt;OECD.org offers a service called &lt;a href=&#34;https://stats.oecd.org/&#34;&gt;OECD.Stat&lt;/a&gt;, which provides a variety of economic data for OECD and certain non-member countries. You can also download the csv data manually by going to the website. Since OECD provides a web API, you only need to use &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;R&lt;/code&gt; to do this.&lt;/p&gt;
&lt;p&gt;&lt;Specifics of the OECD implementation&gt;&lt;/p&gt;
&lt;p&gt;Below is a list of implementation details for specific OECD REST SDMX interfaces at this time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Only anonymous queries are supported and there is no authentication.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each response is limited to 1,000,000 observations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The maximum length of the request URL is 1000 characters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cross-origin requests are supported in the &lt;code&gt;CORS&lt;/code&gt; header (see &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/cors/&#34;&gt;here&lt;/a&gt; for more information about &lt;code&gt;CORS&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Errors are not returned in the results, but HTTP status codes and messages are set according to the Web Service Guidelines.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a nonexistent dataset is requested, &lt;code&gt;401 Unauthorized&lt;/code&gt; is returned.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The source (or Agency ID) parameter of the &lt;code&gt;REST&lt;/code&gt; query is required, but the &lt;code&gt;ALL&lt;/code&gt; keyword is supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Versioning is not supported: the latest implementation version is always used.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sorting of data is not supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;lastNObservations&lt;/code&gt; parameter is not supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even when &lt;code&gt;dimensionAtObservation=AllDimensions&lt;/code&gt; is used, the observations follow a chronological (or import-specific) order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Searching for reference metadata is not supported at this time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pandasdmx&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.pandasdmx&lt;/h2&gt;
&lt;p&gt;The Web API is provided in the form of &lt;code&gt;sdmx-json&lt;/code&gt;. There is a useful package for using it in &lt;code&gt;Python&lt;/code&gt;, which is called &lt;code&gt;pandasdmx**&lt;/code&gt;. Here’s how to download the data.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import &lt;code&gt;pandasdmx&lt;/code&gt;, pass &lt;code&gt;OECD&lt;/code&gt; to &lt;code&gt;Request&lt;/code&gt; method as an argument and create &lt;code&gt;api.Request&lt;/code&gt; object.&lt;/li&gt;
&lt;li&gt;Pass the query condition to the data method of the &lt;code&gt;api.Request&lt;/code&gt; object, and download the data of &lt;code&gt;sdmx-json&lt;/code&gt; format from OECD.org.&lt;/li&gt;
&lt;li&gt;Format the downloaded data into a &lt;code&gt;pandas&lt;/code&gt; data frame with the method &lt;code&gt;to_pandas()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.implementation&lt;/h2&gt;
&lt;p&gt;Let’s do this in practice. What we’ll get is the &lt;code&gt;**Revisions Analysis Dataset -- Infra-annual Economic Indicators**&lt;/code&gt;, one of the OECD datasets, the &lt;code&gt;Monthly Ecnomic Indicator&lt;/code&gt; (MEI). We have access to all data, including revisions to the preliminary data on key economic variables (such as gross domestic product and its expenditure items, industrial production and construction output indices, balance of payments, composite key indicators, consumer price index, retail trade volume, unemployment rate, number of workers, hourly wages, money supply, and trade statistics), as first published You can see everything from data to confirmed data with corrections. The dataset provides a snapshot of data that were previously available for analysis in the Leading Economic Indicators database at monthly intervals beginning in February 1999. In other words, the dataset allows us to build predictive models based on the data available at each point in time. The most recent data is useful, but it is preliminary and therefore subject to uncertainty. The problem is that this situation cannot be replicated when backtesting, and the analysis is often done under a better environment than the actual operation. This is the so-called &lt;code&gt;Jagged Edge&lt;/code&gt; problem. In this dataset, we think it is very useful because we can reproduce the situation of actual operation. This time, you will get the following data items.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Indicators&lt;/th&gt;
&lt;th&gt;Statistical ID&lt;/th&gt;
&lt;th&gt;Frequency&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Gross Domestic Product&lt;/td&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Index of Industrial Production&lt;/td&gt;
&lt;td&gt;201&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Retail Trade Volume&lt;/td&gt;
&lt;td&gt;202&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Monetary Aggregates&lt;/td&gt;
&lt;td&gt;601&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;International Trade in Goods&lt;/td&gt;
&lt;td&gt;702+703&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Balance of Payments&lt;/td&gt;
&lt;td&gt;701&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Employment&lt;/td&gt;
&lt;td&gt;502&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Harmonised Unemployment Rates&lt;/td&gt;
&lt;td&gt;501&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Hourly Earnings in Manufacturing&lt;/td&gt;
&lt;td&gt;503&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Early Estimates of Unit Labor Cost&lt;/td&gt;
&lt;td&gt;504&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Production of Construction&lt;/td&gt;
&lt;td&gt;203&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;First, we define the functions. The arguments are database ID, other IDs (country IDs and statistical IDs), start point and end point.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandasdmx as sdmx&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## C:\Users\aashi\Anaconda3\lib\site-packages\pandasdmx\remote.py:13: RuntimeWarning: optional dependency requests_cache is not installed; cache options to Session() have no effect
##   RuntimeWarning,&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;oecd = sdmx.Request(&amp;#39;OECD&amp;#39;)
def resp_OECD(dsname,dimensions,start,end):
    dim_args = [&amp;#39;+&amp;#39;.join(d) for d in dimensions]
    dim_str = &amp;#39;.&amp;#39;.join(dim_args)
    resp = oecd.data(resource_id=dsname, key=dim_str + &amp;quot;/all?startTime=&amp;quot; + start + &amp;quot;&amp;amp;endTime=&amp;quot; + end)
    df = resp.to_pandas().reset_index()
    return(df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specify the dimension from which the data will be obtained. Below, (1) country, (2) statistical items, (3) time of acquisition, and (4) frequency are specified with a tuple.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dimensions = ((&amp;#39;USA&amp;#39;,&amp;#39;JPN&amp;#39;,&amp;#39;GBR&amp;#39;,&amp;#39;FRA&amp;#39;,&amp;#39;DEU&amp;#39;,&amp;#39;ITA&amp;#39;,&amp;#39;CAN&amp;#39;,&amp;#39;NLD&amp;#39;,&amp;#39;BEL&amp;#39;,&amp;#39;SWE&amp;#39;,&amp;#39;CHE&amp;#39;),(&amp;#39;201&amp;#39;,&amp;#39;202&amp;#39;,&amp;#39;601&amp;#39;,&amp;#39;702&amp;#39;,&amp;#39;703&amp;#39;,&amp;#39;701&amp;#39;,&amp;#39;502&amp;#39;,&amp;#39;503&amp;#39;,&amp;#39;504&amp;#39;,&amp;#39;203&amp;#39;),(&amp;quot;202001&amp;quot;,&amp;quot;202002&amp;quot;,&amp;quot;202003&amp;quot;,&amp;quot;202004&amp;quot;,&amp;quot;202005&amp;quot;,&amp;quot;202006&amp;quot;,&amp;quot;202007&amp;quot;,&amp;quot;202008&amp;quot;),(&amp;quot;M&amp;quot;,&amp;quot;Q&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s execute the function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;result = resp_OECD(&amp;#39;MEI_ARCHIVE&amp;#39;,dimensions,&amp;#39;2019-Q1&amp;#39;,&amp;#39;2020-Q2&amp;#39;)
result.count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOCATION       8266
## VAR            8266
## EDI            8266
## FREQUENCY      8266
## TIME_PERIOD    8266
## value          8266
## dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at the first few cases of data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;result.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   LOCATION  VAR     EDI FREQUENCY TIME_PERIOD  value
## 0      BEL  201  202001         M     2019-01  112.5
## 1      BEL  201  202001         M     2019-02  111.8
## 2      BEL  201  202001         M     2019-03  109.9
## 3      BEL  201  202001         M     2019-04  113.5
## 4      BEL  201  202001         M     2019-05  112.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that the data is stored in tidy form (long type). The most right value is stored as a value, and the other indexes are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LOCATION - Country&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;VAR - Items&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;EDI - At the time of acquisition (in the case of MEI_ARCHIVE)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;FREQUENCY - Frequency (monthly, quarterly, etc.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TIME_PERIOD - Reference point&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the same ` exists in rows with different EDIs. For example, above you can see the data for 2019-01~2019-05 available as of 2020/01 for the Belgian (BEL) Industrial Production Index (201). This is very much appreciated as it is provided in Long format, which is also easy to visualize and regress. Here’s a visualization of the industrial production index as it is updated.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

result = result[result[&amp;#39;FREQUENCY&amp;#39;]==&amp;#39;M&amp;#39;]
result[&amp;#39;TIME_PERIOD&amp;#39;] = pd.to_datetime(result[&amp;#39;TIME_PERIOD&amp;#39;],format=&amp;#39;%Y-%m&amp;#39;)
sns.relplot(data=result[lambda df: (df.VAR==&amp;#39;201&amp;#39;) &amp;amp; (pd.to_numeric(df.EDI) &amp;gt; 202004)],x=&amp;#39;TIME_PERIOD&amp;#39;,y=&amp;#39;value&amp;#39;,hue=&amp;#39;LOCATION&amp;#39;,kind=&amp;#39;line&amp;#39;,col=&amp;#39;EDI&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;seaborn.axisgrid.FacetGrid object at 0x00000000316BC3C8&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;2035&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While we can see that the line graphs are depressed as the economic damage from the corona increases, there have been subtle but significant revisions to the historical values from preliminary to confirmed. We can also see that there is a lag in the release of statistical data by country. Belgium seems to be the slowest to release the data. When I have time, I would like to add a simple analysis of the forecasting model using this data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-matter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Another matter…&lt;/h2&gt;
&lt;p&gt;Python 3 エンジニア認定データ分析試験に合格しました。合格率70%だけあって、かなり簡単でしたが&lt;code&gt;Python&lt;/code&gt;を基礎から見返すいい機会になりました。今やっている業務ではデータ分析はおろか&lt;code&gt;Python&lt;/code&gt;や&lt;code&gt;R&lt;/code&gt;を使う機会すらないので、転職も含めた可能性を考えています。とりあえず、以下の資格を今年度中に取得する予定で、金融にこだわらずにスキルを活かせるポストを探していこうと思います。ダイエットと同じで宣言して自分を追い込まないと。。。&lt;/p&gt;
&lt;p&gt;I passed the Python 3 Engineer Certification Data Analysis exam. It was pretty easy, with only a 70% pass rate, but it was a good opportunity to revisit the basics of &lt;code&gt;Python&lt;/code&gt;. I haven’t even had the opportunity to use &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;R&lt;/code&gt;, let alone data analysis, in the work I’m doing now, so I’m considering the possibility of a career change. In the meantime, I plan to get the following qualifications by the end of this year, and I’ll be looking for a post where I can use my skills without focusing on finance. Like a diet, I need to declare and push myself.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;G Test&lt;/li&gt;
&lt;li&gt;Oracle Database Master Silver SQL&lt;/li&gt;
&lt;li&gt;Linuc level 1&lt;/li&gt;
&lt;li&gt;Fundamental Information Technology Engineer Examination&lt;/li&gt;
&lt;li&gt;AWS Certified Solutions Architect - Associate&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will report on the status of my acceptance on my blog each time.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting the 10-year long-term interest rate</title>
      <link>/en/post/post14/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/post14/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-collection&#34;&gt;1. Data collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-analysis-part&#34;&gt;2. Monthly Analysis Part&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#daily-analysis-part&#34;&gt;3. Daily Analysis Part&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. I wanted to do a fitting of the 10 year long term interest rate for a certain reason. So, we will use US data to analyze the results. First, let’s collect the data. We will use the &lt;code&gt;quantmod&lt;/code&gt; package to drop the data from FRED. The command &lt;code&gt;getsymbols(key,from=start date,src=&#34;FRED&#34;, auto.assign=TRUE)&lt;/code&gt; is easy to use. You can find the key on the FRED website.&lt;/p&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Data collection&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(quantmod)

# data name collected
symbols.name &amp;lt;- c(&amp;quot;10-Year Treasury Constant Maturity Rate&amp;quot;,&amp;quot;Effective Federal Funds Rate&amp;quot;,&amp;quot;
Consumer Price Index for All Urban Consumers: All Items&amp;quot;,&amp;quot;Civilian Unemployment Rate&amp;quot;,&amp;quot;3-Month Treasury Bill: Secondary Market Rate&amp;quot;,&amp;quot;Industrial Production Index&amp;quot;,&amp;quot;
10-Year Breakeven Inflation Rate&amp;quot;,&amp;quot;Trade Weighted U.S. Dollar Index: Broad, Goods&amp;quot;,&amp;quot;
Smoothed U.S. Recession Probabilities&amp;quot;,&amp;quot;Moody&amp;#39;s Seasoned Baa Corporate Bond Yield&amp;quot;,&amp;quot;5-Year, 5-Year Forward Inflation Expectation Rate&amp;quot;,&amp;quot;Personal Consumption Expenditures&amp;quot;)

# Collect economic data
symbols &amp;lt;- c(&amp;quot;GS10&amp;quot;,&amp;quot;FEDFUNDS&amp;quot;,&amp;quot;CPIAUCSL&amp;quot;,&amp;quot;UNRATE&amp;quot;,&amp;quot;TB3MS&amp;quot;,&amp;quot;INDPRO&amp;quot;,&amp;quot;T10YIEM&amp;quot;,&amp;quot;TWEXBMTH&amp;quot;,&amp;quot;RECPROUSM156N&amp;quot;,&amp;quot;BAA&amp;quot;,&amp;quot;T5YIFRM&amp;quot;,&amp;quot;PCE&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, src = &amp;quot;FRED&amp;quot;, auto.assign = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;GS10&amp;quot;          &amp;quot;FEDFUNDS&amp;quot;      &amp;quot;CPIAUCSL&amp;quot;      &amp;quot;UNRATE&amp;quot;       
##  [5] &amp;quot;TB3MS&amp;quot;         &amp;quot;INDPRO&amp;quot;        &amp;quot;T10YIEM&amp;quot;       &amp;quot;TWEXBMTH&amp;quot;     
##  [9] &amp;quot;RECPROUSM156N&amp;quot; &amp;quot;BAA&amp;quot;           &amp;quot;T5YIFRM&amp;quot;       &amp;quot;PCE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;macro_indicator &amp;lt;- merge(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE)
rm(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE,USEPUINDXD)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;monthly-analysis-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Monthly Analysis Part&lt;/h2&gt;
&lt;p&gt;The data are &lt;a href=&#34;htmlwidget/macro_indicator.html&#34;&gt;here&lt;/a&gt;. We will create a dataset for the estimation. The dependent variable is the &lt;code&gt;10-Year Treasury Constant Maturity Rate(GS10)&lt;/code&gt;. The explanatory variables are as follows&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;explanatory variable&lt;/th&gt;
&lt;th&gt;key&lt;/th&gt;
&lt;th&gt;proxy variable&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Federal Funds Rate&lt;/td&gt;
&lt;td&gt;FEDFUNDS&lt;/td&gt;
&lt;td&gt;Short term rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Consumer Price Index&lt;/td&gt;
&lt;td&gt;CPIAUCSL&lt;/td&gt;
&lt;td&gt;Price&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment Rate&lt;/td&gt;
&lt;td&gt;UNRATE&lt;/td&gt;
&lt;td&gt;Employment&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;3-Month Treasury Bill&lt;/td&gt;
&lt;td&gt;TB3MS&lt;/td&gt;
&lt;td&gt;Short term rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Industrial Production Index&lt;/td&gt;
&lt;td&gt;INDPRO&lt;/td&gt;
&lt;td&gt;Business conditions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Breakeven Inflation Rate&lt;/td&gt;
&lt;td&gt;T10YIEM&lt;/td&gt;
&lt;td&gt;Price&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Trade Weighted Dollar Index&lt;/td&gt;
&lt;td&gt;TWEXBMTH&lt;/td&gt;
&lt;td&gt;Exchange rates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Recession Probabilities&lt;/td&gt;
&lt;td&gt;RECPROUSM156N&lt;/td&gt;
&lt;td&gt;Business condition&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Moody’s Seasoned Baa Corporate Bond Yield&lt;/td&gt;
&lt;td&gt;BAA&lt;/td&gt;
&lt;td&gt;Risk premium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Inflation Expectation Rate&lt;/td&gt;
&lt;td&gt;T5YIFRM&lt;/td&gt;
&lt;td&gt;Price&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Personal Consumption Expenditures&lt;/td&gt;
&lt;td&gt;PCE&lt;/td&gt;
&lt;td&gt;Business condition&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Economic Policy Uncertainty Index&lt;/td&gt;
&lt;td&gt;USEPUINDXD&lt;/td&gt;
&lt;td&gt;Politics&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It’s a pretty appropriate choice of variables, but in many cases, we haven’t done it properly in terms of how to model long-term interest rates from a macro modeling perspective… In DSGE, we formulate the path of short-term interest rates linked to the path of short-term interest rates up to 10 years into the future according to the efficient market hypothesis and long-term interest rates equal to the path of short-term interest rates when I was a graduate student It was modeling (which seems to be done properly in macro finance circles). That’s why I’ve added short-term interest rates as an explanatory variable. And I also added three indicators for prices that would have an impact on the short-term interest rate. In addition, I added data on the economy because it is well known that it is highly correlated with the economy. This is due to the fact that in the first place, it is common in macro models to model short-term interest rates as following the Taylor rule.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_t = \rho r_{t-1} + \alpha \pi_{t} + \beta y_{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt; is the policy interest rate (short-term interest rate), &lt;span class=&#34;math inline&#34;&gt;\(\pi_t\)&lt;/span&gt; is the inflation rate, and &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; is the output. The $R_rho, \\alpha, and &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; are called deep parameters, which represent inertia, the sensitivity of the interest rate to inflation, and the sensitivity of the interest rate to output, respectively. It is well known as the “Taylor’s Principle” that when $, &lt;span class=&#34;math inline&#34;&gt;\(\beta=0\)&lt;/span&gt;, a reasonably expected equilibrium solution can only be obtained when &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;=1\)&lt;/span&gt;. Other explanatory variables include &lt;code&gt;Moody&#39;s Seasoned Baa Corporate Bond Yield&lt;/code&gt;, which may also have an arbitrage relationship with corporate bonds. Also, we would like to add the &lt;code&gt;VIX&lt;/code&gt; index and an index related to finances if we wanted to. The fiscal index is either Quatery or Annualy and cannot be used for monthly estimation. This is the most difficult part. I will re-estimate if I come up with something.&lt;/p&gt;
&lt;p&gt;Now, let’s get into the estimation. Since there are many explanatory variables in this case, we want to do a &lt;code&gt;lasso&lt;/code&gt; regression to narrow down the valid variables. We will also do an &lt;code&gt;OLS&lt;/code&gt; for comparison. The explanatory variables will be the values of the dependent variable one period ago. Probably, even one period ago, depending on when the data are published, it may not be in time for the next month’s estimates, but I’ll do this anyway.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make dataset
traindata &amp;lt;- na.omit(merge(macro_indicator[&amp;quot;2003-01-01::2015-12-31&amp;quot;][,1],stats::lag(macro_indicator[&amp;quot;2003-01-01::2015-12-31&amp;quot;][,-1],1)))
testdata  &amp;lt;- na.omit(merge(macro_indicator[&amp;quot;2016-01-01::&amp;quot;][,1],stats::lag(macro_indicator[&amp;quot;2016-01-01::&amp;quot;][,-1],1)))

# fitting OLS
trial1 &amp;lt;- lm(GS10~.,data = traindata)
summary(trial1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GS10 ~ ., data = traindata)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.76208 -0.21234  0.00187  0.21595  0.70493 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   14.3578405  4.3524691   3.299 0.001226 ** 
## FEDFUNDS      -0.2011132  0.1438774  -1.398 0.164335    
## CPIAUCSL      -0.0702011  0.0207761  -3.379 0.000938 ***
## UNRATE        -0.2093502  0.0796052  -2.630 0.009477 ** 
## TB3MS          0.2970160  0.1413796   2.101 0.037410 *  
## INDPRO        -0.0645376  0.0260343  -2.479 0.014339 *  
## T10YIEM        1.1484487  0.1769925   6.489 1.32e-09 ***
## TWEXBMTH      -0.0317345  0.0118155  -2.686 0.008091 ** 
## RECPROUSM156N -0.0099083  0.0021021  -4.713 5.72e-06 ***
## BAA            0.7793520  0.0868628   8.972 1.49e-15 ***
## T5YIFRM       -0.4551318  0.1897695  -2.398 0.017759 *  
## PCE            0.0009087  0.0002475   3.672 0.000339 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2981 on 143 degrees of freedom
## Multiple R-squared:  0.9203, Adjusted R-squared:  0.9142 
## F-statistic: 150.1 on 11 and 143 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s a higher degree of freedom-adjusted coefficient of determination; we use the model through 12/31/2015 to predict the out-sample data (01/01/2016~) and calculate the mean squared error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.OLS.Y &amp;lt;- predict(trial1,testdata[,-1])
Y &amp;lt;- as.matrix(testdata[,1])
mse.OLS &amp;lt;- sum((Y - est.OLS.Y)^2) / length(Y)
mse.OLS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1431734&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is the &lt;code&gt;lasso&lt;/code&gt; regression, using the &lt;code&gt;cv.glmnet&lt;/code&gt; function of the &lt;code&gt;glmnet&lt;/code&gt; package to perform Cross Validation and determine &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fitting lasso regression
library(glmnet)
trial2 &amp;lt;- cv.glmnet(as.matrix(traindata[,-1]),as.matrix(traindata[,1]),family=&amp;quot;gaussian&amp;quot;,alpha=1)
plot(trial2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trial2$lambda.min&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.001214651&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(trial2,s=trial2$lambda.min)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 12 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                           1
## (Intercept)    8.4885375862
## FEDFUNDS       .           
## CPIAUCSL      -0.0378778837
## UNRATE        -0.1693313660
## TB3MS          0.1224641351
## INDPRO        -0.0545965007
## T10YIEM        1.1926554061
## TWEXBMTH      -0.0140144490
## RECPROUSM156N -0.0090706154
## BAA            0.7389283529
## T5YIFRM       -0.4638923964
## PCE            0.0005014518&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Unemployment Rate&lt;/code&gt;, &lt;code&gt;3-Month Treasury Bill&lt;/code&gt;, &lt;code&gt;Breakeven Inflation Rate&lt;/code&gt;, &lt;code&gt;Moody&#39;s Seasoned Baa Corporate Bond Yield&lt;/code&gt; and &lt;code&gt;Inflation Expectation Rate&lt;/code&gt;. That’s the result of a larger regression coefficient. Other than the unemployment rate, the results are within expectations. However, the correlation with the economy seems to be low as far as this result is concerned (does it only work in the opposite direction?). Calculate the MSE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.lasso.Y &amp;lt;- predict(trial2, newx = as.matrix(testdata[,-1]), s = trial2$lambda.min, type = &amp;#39;response&amp;#39;)
mse.lasso &amp;lt;- sum((Y - est.lasso.Y)^2) / length(Y)
mse.lasso&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1125487&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;lasso&lt;/code&gt; regression gives better results. Let’s plot the predicted and actual values from the &lt;code&gt;lasso&lt;/code&gt; regression as a time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

ggplot(gather(data.frame(actual=Y[,1],lasso_prediction=est.lasso.Y[,1],OLS_prediction=est.OLS.Y,date=as.POSIXct(rownames(Y))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +
  geom_line(size=1.5) +
  scale_x_datetime(breaks = &amp;quot;6 month&amp;quot;,date_labels = &amp;quot;%Y-%m&amp;quot;) +
  scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,3.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The sense of direction is good. On the other hand, I am not predicting a sharp decline in interest rates from January 2016 or after December 2018. It looks like we’ll have to try to do one of the following to improve the accuracy of this part of the projections: consider some variables OR run a rolling estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;daily-analysis-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Daily Analysis Part&lt;/h2&gt;
&lt;p&gt;In addition to monthly analysis, I would like to do daily analysis. In the case of daily data, the &lt;code&gt;jagged edge&lt;/code&gt; problem is unlikely to occur because the data is often released after the market closes. We will start by collecting daily data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data name collected
symbols.name &amp;lt;- c(&amp;quot;10-Year Treasury Constant Maturity Rate&amp;quot;,&amp;quot;Effective Federal Funds Rate&amp;quot;,&amp;quot;
6-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar&amp;quot;,&amp;quot;NASDAQ Composite Index&amp;quot;,&amp;quot;3-Month Treasury Bill: Secondary Market Rate&amp;quot;,&amp;quot;Economic Policy Uncertainty Index for United States&amp;quot;,&amp;quot;
10-Year Breakeven Inflation Rate&amp;quot;,&amp;quot;Trade Weighted U.S. Dollar Index: Broad, Goods&amp;quot;,&amp;quot;Moody&amp;#39;s Seasoned Baa Corporate Bond Yield&amp;quot;,&amp;quot;5-Year, 5-Year Forward Inflation Expectation Rate&amp;quot;)

# Collect economic data
symbols &amp;lt;- c(&amp;quot;DGS10&amp;quot;,&amp;quot;DFF&amp;quot;,&amp;quot;USD6MTD156N&amp;quot;,&amp;quot;NASDAQCOM&amp;quot;,&amp;quot;DTB3&amp;quot;,&amp;quot;USEPUINDXD&amp;quot;,&amp;quot;T10YIE&amp;quot;,&amp;quot;DTWEXB&amp;quot;,&amp;quot;DBAA&amp;quot;,&amp;quot;T5YIFR&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, src = &amp;quot;FRED&amp;quot;, auto.assign = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;DGS10&amp;quot;       &amp;quot;DFF&amp;quot;         &amp;quot;USD6MTD156N&amp;quot; &amp;quot;NASDAQCOM&amp;quot;   &amp;quot;DTB3&amp;quot;       
##  [6] &amp;quot;USEPUINDXD&amp;quot;  &amp;quot;T10YIE&amp;quot;      &amp;quot;DTWEXB&amp;quot;      &amp;quot;DBAA&amp;quot;        &amp;quot;T5YIFR&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NASDAQCOM.r &amp;lt;- ROC(na.omit(NASDAQCOM))
macro_indicator.d &amp;lt;- merge(DGS10,DFF,USD6MTD156N,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)
rm(DGS10,DFF,USD6MTD156N,NASDAQCOM,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to build the data set. We separate the data for training and for training. Considering the actual prediction process, we use data from two business days ago as the explanatory variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make dataset
traindata.d &amp;lt;- na.omit(merge(macro_indicator.d[&amp;quot;1980-01-01::2010-12-31&amp;quot;][,1],stats::lag(macro_indicator.d[&amp;quot;1980-01-01::2010-12-31&amp;quot;][,-1],2)))
testdata.d  &amp;lt;- na.omit(merge(macro_indicator.d[&amp;quot;2010-01-01::&amp;quot;][,1],stats::lag(macro_indicator.d[&amp;quot;2010-01-01::&amp;quot;][,-1],2)))

# fitting OLS
trial1.d &amp;lt;- lm(DGS10~.,data = traindata.d)
summary(trial1.d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = DGS10 ~ ., data = traindata.d)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.82445 -0.12285  0.00469  0.14332  0.73789 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -3.5051208  0.1690503 -20.734  &amp;lt; 2e-16 ***
## DFF          0.0818269  0.0239371   3.418 0.000653 ***
## USD6MTD156N -0.0135771  0.0233948  -0.580 0.561799    
## NASDAQCOM   -0.3880217  0.4367334  -0.888 0.374483    
## DTB3         0.1227984  0.0280283   4.381 1.29e-05 ***
## USEPUINDXD  -0.0006611  0.0001086  -6.087 1.58e-09 ***
## T10YIE       0.6980971  0.0355734  19.624  &amp;lt; 2e-16 ***
## DTWEXB       0.0270128  0.0012781  21.135  &amp;lt; 2e-16 ***
## DBAA         0.2988122  0.0182590  16.365  &amp;lt; 2e-16 ***
## T5YIFR       0.3374944  0.0381111   8.856  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2173 on 1114 degrees of freedom
## Multiple R-squared:  0.8901, Adjusted R-squared:  0.8892 
## F-statistic:  1002 on 9 and 1114 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient of determination remains high.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.OLS.Y.d &amp;lt;- predict(trial1.d,testdata.d[,-1])
Y.d &amp;lt;- as.matrix(testdata.d[,1])
mse.OLS.d &amp;lt;- sum((Y.d - est.OLS.Y.d)^2) / length(Y.d)
mse.OLS.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8003042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next is the &lt;code&gt;lasso&lt;/code&gt; regression. Determine &lt;span class=&#34;math inline&#34;&gt;\(lambda\)&lt;/span&gt; in &lt;code&gt;CV&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fitting lasso regression
trial2.d &amp;lt;- cv.glmnet(as.matrix(traindata.d[,-1]),as.matrix(traindata.d[,1]),family=&amp;quot;gaussian&amp;quot;,alpha=1)
plot(trial2.d)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trial2.d$lambda.min&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.001472377&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(trial2.d,s=trial2.d$lambda.min)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                         1
## (Intercept) -3.4186530904
## DFF          0.0707022021
## USD6MTD156N  .           
## NASDAQCOM   -0.2675513858
## DTB3         0.1204092358
## USEPUINDXD  -0.0006506183
## T10YIE       0.6915446819
## DTWEXB       0.0270389569
## DBAA         0.2861031504
## T5YIFR       0.3376304446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient of &lt;code&gt;libor&lt;/code&gt; became zero, and the MSE of &lt;code&gt;OLS&lt;/code&gt; was higher than that of &lt;code&gt;libor&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.lasso.Y.d &amp;lt;- predict(trial2.d, newx = as.matrix(testdata.d[,-1]), s = trial2.d$lambda.min, type = &amp;#39;response&amp;#39;)
mse.lasso.d &amp;lt;- sum((Y.d - est.lasso.Y.d)^2) / length(Y.d)
mse.lasso.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8378427&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot the predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gather(data.frame(actual=Y.d[,1],lasso_prediction=est.lasso.Y.d[,1],OLS_prediction=est.OLS.Y.d,date=as.POSIXct(rownames(Y.d))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +
  geom_line(size=1.5) +
  scale_x_datetime(breaks = &amp;quot;2 year&amp;quot;,date_labels = &amp;quot;%Y-%m&amp;quot;) +
  scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As with the monthly, there is very little difference between the predicted values for &lt;code&gt;OLS&lt;/code&gt; and &lt;code&gt;lasso&lt;/code&gt;. We have been able to capture the fluctuations quite nicely, but we have not been able to capture the interest rate decline caused by the &lt;code&gt;United States federal government credit-rating downgrades&lt;/code&gt; in 2011 or the interest rate increase associated with the economic recovery in 2013. The only daily economic indicator I have is POS data, but I might be able to use the recently used nightlight satellite imagery data, if I had to say so. I’ll try it if I have time. For now, I’d like to end this article for now. Thank you for reading this article.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Obtaining satellite image data with the Google Earth Engine API and do nowcasting on business conditions</title>
      <link>/en/post/post12/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/post12/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#preparing-to-use-the-earth-engine-in-advance&#34;&gt;1. Preparing to use the Earth Engine in advance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#obtaining-satellite-image-data-using-python-api&#34;&gt;2. Obtaining Satellite Image Data Using Python API&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#image&#34;&gt;Image&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imagecollection&#34;&gt;ImageCollection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#featurecollection&#34;&gt;FeatureCollection…&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. Last time, we built a GDP forecasting model using the GPLVM model. However, since we are talking about nowcasting, I would like to perform an analysis using alternative data. I suddenly found the following article.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://jp.reuters.com/article/gdp-u-tokyo-idJPKBN15M0NH&#34;&gt;Focus: Nowcast’s GDP estimates, including the world’s first increased use of satellite imagery&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here’s an article by Dr. Tsutomu Watanabe of the University of Tokyo, who developed a GDP forecasting model using satellite images. In the article, the following is written&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The U.S. National Oceanic and Atmospheric Administration (NOAA) purchases images of the Suomi NPP meteorological satellite as it passes over Japan at 1:30 a.m. each day and measures the brightness of each square in a 720-meter vertical and horizontal grid. Even with the same brightness, the magnitude of economic activity varies depending on the use of the land, such as agricultural land, commercial land, industrial land, etc., so refer to the Land Use Survey of the Geographical Survey Institute. The correlation between land use and economic activity indicated by brightness is played out, and the magnitude of economic activity is estimated from the brightness, taking this result into account.
Watanabe calls this the “democratization of statistics” and predicts that it will become a global trend, because anyone can analyze publicly available data like satellite images, regardless of whether it is government or private sector.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and, I was intrigued by the analysis using satellite images.
Is satellite photography available to everyone? But then I found out that Google is offering a service called &lt;code&gt;Earth Engine&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine3.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://earthengine.google.com/&#34; class=&#34;uri&#34;&gt;https://earthengine.google.com/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Google Earth Engine combines a multi-petabyte catalog of satellite imagery and geospatial datasets with planetary-scale analysis capabilities and makes it available for scientists, researchers, and developers to detect changes, map trends, and quantify differences on the Earth’s surface.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If it’s for research, education, or non-commercial purposes, what a great way to analyze satellite photo data for &lt;strong&gt;free&lt;/strong&gt;. Watch the video below to see what exactly it can do.&lt;/p&gt;
&lt;iframe src=&#34;//www.youtube.com/embed/gKGOeTFHnKY&#34; width=&#34;100%&#34; height=&#34;500&#34; seamless frameborder=&#34;0&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;In this post, I will use Eath Engine’s Python API to acquire and analyze satellite image data.&lt;/p&gt;
&lt;div id=&#34;preparing-to-use-the-earth-engine-in-advance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Preparing to use the Earth Engine in advance&lt;/h2&gt;
&lt;p&gt;In order to use Earth Engine, you need to apply using your Google Account. You can do this from “Sign Up” in the upper right corner of the image above. After you apply for it, you will receive an email in your Gmail as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine4.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;とりあえずというのはWEB上の&lt;code&gt;Earth Engine&lt;/code&gt; コードエディタは使用できるということです。コードエディタというのは以下のようなもので、ブラウザ上でデータを取得したり、解析をしたり、解析結果をMAPに投影したりすることができる便利ツールです。&lt;code&gt;Earth Engine&lt;/code&gt;の本体はむしろこいつで、APIは副次的なものと考えています。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine5.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can type the code into the code editor in the middle, but the language is javascript (the API is both &lt;code&gt;python&lt;/code&gt; and &lt;code&gt;javascript&lt;/code&gt;). It’s quite useful because you can project the analysis results to MAP, refer to the reference (left), and check the data spit out to Console. However, if you want to do advanced analysis after dropping the data, you should use Python, which I am familiar with, so I use the API in this case.
I digress. Now, once you get the &lt;code&gt;Earth Engine&lt;/code&gt; approval, install the &lt;code&gt;earthengine-api&lt;/code&gt; with &lt;code&gt;pip&lt;/code&gt;. Then, type &lt;code&gt;earthengine authenticate&lt;/code&gt; on the command prompt. Then, the browser will open by itself and you will see a screen for &lt;code&gt;python api&lt;/code&gt; authentication as shown below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine1.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;次に以下のような画面にいきますので、そのまま承認します。これでauthenticationの完成です。&lt;code&gt;python&lt;/code&gt;からAPIが使えます。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine2.jpg&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;obtaining-satellite-image-data-using-python-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Obtaining Satellite Image Data Using Python API&lt;/h2&gt;
&lt;p&gt;We are now ready to use the &lt;code&gt;Python&lt;/code&gt; API. From here, we will retrieve the satellite image data. As you can see below, there are many datasets in the &lt;code&gt;Earth Engine&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://developers.google.com/earth-engine/datasets/&#34; class=&#34;uri&#34;&gt;https://developers.google.com/earth-engine/datasets/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We will use the &lt;code&gt;VIIRS Stray Light Corrected Nighttime Day/Night Band Composites Version 1&lt;/code&gt; dataset. This dataset provides monthly averages of nighttime light intensity around the world. The sample period is 2014-01~present.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Earth Engine&lt;/code&gt; has several unique data types. You should remember the following three.&lt;/p&gt;
&lt;div id=&#34;image&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Image&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;raster&lt;/code&gt; data at a single point in time. An &lt;code&gt;image&lt;/code&gt; object is composed of several &lt;code&gt;bands&lt;/code&gt;. This &lt;code&gt;band&lt;/code&gt; varies from data to data, but roughly each &lt;code&gt;band&lt;/code&gt; may represent an RGB value. These are the most basic data for using the &lt;code&gt;Earth Engine&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;imagecollection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ImageCollection&lt;/h3&gt;
&lt;p&gt;The object of &lt;code&gt;Image&lt;/code&gt; objects in chronological order. In this case, we will use this data for time-series analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;featurecollection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;FeatureCollection…&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;GeoJSON Feature&lt;/code&gt;. It contains &lt;code&gt;Geometry&lt;/code&gt; objects for geographic information and their properties (e.g. country names). This time, this feature is used to get location information of Japan.&lt;/p&gt;
&lt;p&gt;Let’s start with the coding. The first step is to get the &lt;code&gt;FeatureCollection&lt;/code&gt; object for Japan. Geographical information is stored in the &lt;code&gt;Fusion Tables&lt;/code&gt; and we extract the data whose country is Japan by ID. &lt;code&gt;FeatureCollection()&lt;/code&gt;, we can get it easily by passing the ID as an argument to &lt;code&gt;ee&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import ee
from dateutil.parser import parse

ee.Initialize()

# get Japan geometory as FeatureCollection from fusion table
japan = ee.FeatureCollection(&amp;#39;ft:1tdSwUL7MVpOauSgRzqVTOwdfy17KDbw-1d9omPw&amp;#39;).filter(ee.Filter.eq(&amp;#39;Country&amp;#39;, &amp;#39;Japan&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s get a nighttime satellite image. &lt;code&gt;ImageCollection()&lt;/code&gt; is also used to obtain a nighttime satellite image. &lt;code&gt;ImageCollection()&lt;/code&gt;. Here, &lt;code&gt;band&lt;/code&gt; is extracted to &lt;code&gt;avg_rad&lt;/code&gt;, which is monthly average light intensity.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# get night-light data from earth engine from 2014-01-01 to 2019-01-01
dataset = ee.ImageCollection(&amp;#39;NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG&amp;#39;).filter(ee.Filter.date(&amp;#39;2014-01-01&amp;#39;,&amp;#39;2019-01-01&amp;#39;)).select(&amp;#39;avg_rad&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s cut out a satellite image to the area around Japan and output it as an image file. You can use the &lt;code&gt;image&lt;/code&gt; object to create an image file (otherwise, you will get a lot of images…). . Since what you just got is an &lt;code&gt;ImageCollection&lt;/code&gt; object, you need to compress it into an &lt;code&gt;Image&lt;/code&gt; object (top is an &lt;code&gt;ImageCollection&lt;/code&gt; object, bottom is a compressed &lt;code&gt;Image&lt;/code&gt; object).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://developers.google.com/earth-engine/images/Reduce_ImageCollection.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, I would like to show you the average image of the sample period from the average value of &lt;code&gt;Image&lt;/code&gt; objects in an &lt;code&gt;ImageCollection&lt;/code&gt; object. You can do this with &lt;code&gt;ImageCollection.mean()&lt;/code&gt;. Also, I used &lt;code&gt;.visualize({min:0.5})&lt;/code&gt; to filter the image if the pixel value is more than 0.5. If you don’t do this, you can’t see what you think are clouds or garbage? You’ll get something like this. Next, the method &lt;code&gt;.getDownloadURL&lt;/code&gt; is used to get the URL to download the processed image. (If the &lt;code&gt;scale&lt;/code&gt; is too small, an error occurs and the image cannot be processed.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dataset.mean().visualize(min=0.5).getDownloadURL(dict(name=&amp;#39;thumbnail&amp;#39;,region=[[[120.3345348936478, 46.853488838010854],[119.8071911436478, 24.598157870729043],[148.6353161436478, 24.75788466523463],[149.3384411436478, 46.61252884462868]]],scale=5000))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the image we got.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;earthengine6.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, light levels are high in the Kanto area centered on Tokyo, the Kansai area centered on Osaka, Aichi, Fukuoka and Hokkaido (Sapporo and its vicinity), indicating that economic activity is active. It also shows that there are areas with higher light levels in the coastal areas than on land. This may not seem to be a phenomenon directly related to economic activity. It is striking that the northern part of the image becomes completely dark after 38 degrees north latitude, which is outside the scope of this analysis. Needless to say, this is the borderline between North and South Korea, so the difference in the level of economic activity between the two countries must be visually contrasted. The dataset we used here is from 2014, but some other datasets allow us to get data from the 1990s (although we can’t get more recent data instead). It would be interesting to use them to observe the economic development of the Korean peninsula and China.&lt;/p&gt;
&lt;p&gt;Now that we have the image, we can’t analyze it at this point. We will try to get the data of the nightlight mapped to pixel values and analyze it numerically. However, the procedure to acquire the data is a little different from the previous one. But the procedure is a little different from the previous one, because this time you need to aggregate** all the different values of nighttime light in Japan, pixel by pixel, into a single proxy value. Once you have the pixel-by-pixel values, you have too much to analyze. The image looks like this (taken from the Earth Engine site)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://developers.google.com/earth-engine/images/Reduce_region_diagram.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The left image is a satellite image at one point in time of the &lt;code&gt;ImageCollection&lt;/code&gt; acquired earlier. The Earth Engine API provides the method &lt;code&gt;.reduceRegions()&lt;/code&gt;, so you can use it. The arguments are: &lt;code&gt;reducer&lt;/code&gt;=aggregation method (here, the total value), &lt;code&gt;collection&lt;/code&gt;=region (a &lt;code&gt;FeatureCollection&lt;/code&gt; object), and &lt;code&gt;scale&lt;/code&gt;=resolution. Here, the first &lt;code&gt;Image&lt;/code&gt; object in an &lt;code&gt;ImageCollection&lt;/code&gt; (dataset) is called with the &lt;code&gt;.reduceRegions()&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# initialize output box
time0 = dataset.first().get(&amp;#39;system:time_start&amp;#39;);
first = dataset.first().reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(&amp;#39;time_start&amp;#39;, time0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we want time series data, we need to do the same thing for each &lt;code&gt;Image&lt;/code&gt; in the &lt;code&gt;ImageCollection&lt;/code&gt;. processing. Here, the function &lt;code&gt;myfunc&lt;/code&gt; is defined to merge the &lt;code&gt;Computed Object&lt;/code&gt; processed by the &lt;code&gt;reduceRegions&lt;/code&gt; method with the previously processed one in the &lt;code&gt;Image&lt;/code&gt; object, and it is passed to &lt;code&gt;iterate&lt;/code&gt;. Finally, the generated data is downloaded using the &lt;code&gt;getDownloadURL&lt;/code&gt; method as before (file format is csv).&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# define reduceRegions function for iteration
def myfunc(image,first):
  added = image.reduceRegions(reducer=ee.Reducer.sum(),collection=japan,scale=1000).set(&amp;#39;time_start&amp;#39;, image.get(&amp;#39;system:time_start&amp;#39;))
  return ee.FeatureCollection(first).merge(added)

# implement iteration
nightjp = dataset.filter(ee.Filter.date(&amp;#39;2014-02-01&amp;#39;,&amp;#39;2019-01-01&amp;#39;)).iterate(myfunc,first)

# get url to download
ee.FeatureCollection(nightjp).getDownloadURL(filetype=&amp;#39;csv&amp;#39;,selectors=ee.FeatureCollection(nightjp).first().propertyNames().getInfo())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I was able to get the url to the CSV file. I’m going to plot this time series to end today.
Here’s what it looks like when you load the data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
import matplotlib.pyplot as plt
import os

os.environ[&amp;#39;QT_QPA_PLATFORM_PLUGIN_PATH&amp;#39;] = &amp;#39;C:/Users/aashi/Anaconda3/Library/plugins/platforms&amp;#39;

plt.style.use(&amp;#39;ggplot&amp;#39;)

nightjp_csv.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   system:index          sum Country  Unnamed: 3  Unnamed: 4
## 0     2014/1/1  881512.4572   Japan         NaN         NaN
## 1     2014/2/1  827345.3551   Japan         NaN         NaN
## 2     2014/3/1  729110.4619   Japan         NaN         NaN
## 3     2014/4/1  612665.8866   Japan         NaN         NaN
## 4     2014/5/1  661434.5027   Japan         NaN         NaN&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.plot(pd.to_datetime(nightjp_csv[&amp;#39;system:index&amp;#39;]),nightjp_csv[&amp;#39;sum&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s quite seasonal. It seems that the amount of light seems to increase in the winter because there are less hours of daylight. Nevertheless, it is a rapid increase. Next time, I would like to perform a statistical analysis based on this data and economic statistics, which is a proxy variable for business confidence. Please stay tuned.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Gaussian regression.</title>
      <link>/en/post/post1/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/en/post/post1/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-gpr&#34;&gt;1. What is &lt;code&gt;GPR&lt;/code&gt;?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-of-the-gpr&#34;&gt;2. Implementation of the `GPR’&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;　Hi. Yesterday, I wrote an article about &lt;code&gt;Bayesian Vector Autoregression&lt;/code&gt;.
　In the article, the topic of hyperparameter tuning came up, and looking for some efficient way to tune it, I found &lt;code&gt;Bayesian Optimization&lt;/code&gt;. Since I am planning to use machine learning methods in daily GDP, I thought that &lt;code&gt;Bayesian Optimization&lt;/code&gt; could be quite useful, and I spent all night yesterday to understand it.&lt;br /&gt;
　I will implement it here, but &lt;code&gt;Bayesian Optimization&lt;/code&gt; uses &lt;code&gt;Gaussian Pocess Regression&lt;/code&gt; (&lt;code&gt;GPR&lt;/code&gt;), and my motivation for writing this entry was to implement it first. I will write about the implementation of &lt;code&gt;Bayesian Optimization&lt;/code&gt; after this entry.&lt;/p&gt;
&lt;div id=&#34;what-is-gpr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. What is &lt;code&gt;GPR&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;　The &lt;code&gt;GRP&lt;/code&gt; is, simply put, &lt;strong&gt;a type of nonlinear regression method using Bayesian estimation&lt;/strong&gt;. Although the model itself is linear, it is characterized by its ability to estimate &lt;strong&gt;infinite nonlinear transformations of input variables using a kernel trick&lt;/strong&gt; as explanatory variables (depending on what you choose for the kernel).
　The &lt;code&gt;GPR&lt;/code&gt; assumes that &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; input and teacher data are available for training, and the &lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt; of input data are also available. From this situation, we can predict the &lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;th teacher data.&lt;br /&gt;
　The data contains noise and follows the following probability model.
　
&lt;span class=&#34;math display&#34;&gt;\[
t_{i} = y_{i} + \epsilon_{i}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th observable teacher data (scalar), &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; is the unobservable output data (scalar), and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{i}\)&lt;/span&gt; follows a normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(0, \beta^{-1})\)&lt;/span&gt; with measurement error. &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; follows the following probability model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle y_{i}  = \textbf{w}^{T}\phi(x_{i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt; is the ith input data vector, &lt;span class=&#34;math inline&#34;&gt;\(\phi(・)\)&lt;/span&gt; is the non-linear function and &lt;span class=&#34;math inline&#34;&gt;\(\bf{w}^{T}\)&lt;/span&gt; is the weight coefficient (regression coefficient) vector for each input data. As a nonlinear function, I assume &lt;span class=&#34;math inline&#34;&gt;\(\psi(x_{i}) = (x_{1,i}, x_{1,i}^{2},... ,x_{1,i}x_{2,i},...)\)&lt;/span&gt;. (&lt;span class=&#34;math inline&#34;&gt;\(x_{1,i}\)&lt;/span&gt; is the first variable in the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th input data &lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt;). The conditional probability of obtaining &lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt; from the probabilistic model of the teacher data, with the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th output data &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; obtained, is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 p(t_{i}|y_{i}) = N(t_{i}|y_{i},\beta^{-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{t} = (t_{1},... ,t_{n})^{T}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y} = (y_{1},... ,y_{n})^{T}\)&lt;/span&gt;, then by extending the above equation, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{t}|\textbf{y}) = N(\textbf{t}|\textbf{y},\beta^{-1}\textbf{I}_{N})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We assume that the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}\)&lt;/span&gt; as a prior distribution is 0, and all variances are &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. We also assume that &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt; follows a Gaussian process. A Gaussian process is one where the simultaneous distribution of &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt; follows a multivariate Gaussian distribution. In code, it looks like this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define Kernel function
Kernel_Mat &amp;lt;- function(X,sigma,beta){
  N &amp;lt;- NROW(X)
  K &amp;lt;- matrix(0,N,N)
  for (i in 1:N) {
    for (k in 1:N) {
      if(i==k) kdelta = 1 else kdelta = 0
      K[i,k] &amp;lt;- K[k,i] &amp;lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta
    }
  }
  return(K)
}

N &amp;lt;- 10 # max value of X
M &amp;lt;- 1000 # sample size
X &amp;lt;- matrix(seq(1,N,length=M),M,1) # create X
testK &amp;lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix

library(MASS)

P &amp;lt;- 6 # num of sample path
Y &amp;lt;- matrix(0,M,P) # define Y

for(i in 1:P){
  Y[,i] &amp;lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y
}

# Plot
matplot(x=X,y=Y,type = &amp;quot;l&amp;quot;,lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;　The covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; between the elements of &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y_{i} = \textbf{w}^{T}\phi(x_{i})\)&lt;/span&gt; is calculated using the kernel method from the input &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Then, from this &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; and average 0, we generate six series of multivariate normal random numbers and plot them.As these series are computed from a covariance matrix, we model that &lt;strong&gt;the more positive the covariance of each element, the more likely they are to be the same&lt;/strong&gt;. Also, as you can see in the graphs, the graphs are very smooth and very flexible in their representation. The code samples and plots 1000 input points, limiting the input to 0 to 10 due to computational cost, but in principle, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is defined in the real number space, so &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; follows an infinite dimensional multivariate normal distribution.&lt;/p&gt;
&lt;p&gt;As described above, since &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt; is assumed to follow a Gaussian process, &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; follows a multivariate normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(\textbf{y}|0,K)\)&lt;/span&gt; with simultaneous probability &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; averaging 0 and the variance covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. Each element &lt;span class=&#34;math inline&#34;&gt;\(K_{i,j}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
K_{i,j} &amp;amp;=&amp;amp; cov[y_{i},y_{j}] = cov[\textbf{w}\phi(x_{i}),\textbf{w}\phi(x_{j})] \\
&amp;amp;=&amp;amp;\phi(x_{i})\phi(x_{j})cov[\textbf{w},\textbf{w}]=\phi(x_{i})\phi(x_{j})\alpha
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\phi(x_{j})\alpha\)&lt;/span&gt; is more expensive &lt;strong&gt;as the dimensionality of the &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\)&lt;/span&gt; is increased&lt;/strong&gt; (i.e., the more non-linear transformation is applied, the less the calculation is completed). However, when the kernel function &lt;span class=&#34;math inline&#34;&gt;\(k(x,x&amp;#39;)\)&lt;/span&gt; is used, the computational complexity is higher in the dimensions of the sample size of the input data &lt;span class=&#34;math inline&#34;&gt;\(x_{i},x_{j}\)&lt;/span&gt;, so the computation becomes easier. There are several types of kernel functions, but the following Gaussian kernels are commonly used.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
k(x,x&amp;#39;) = a \exp(-b(x-x&amp;#39;)^{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now that we have defined the concurrent probability of &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;, we can find the joint probability of &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{t}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\displaystyle p(\textbf{t}) &amp;amp;=&amp;amp; \int p(\textbf{t}|\textbf{y})p(\textbf{y}) d\textbf{y} \\
 \displaystyle &amp;amp;=&amp;amp; \int N(\textbf{t}|\textbf{y},\beta^{-1}\textbf{I}_{N})N(\textbf{y}|0,K)d\textbf{y} \\
 &amp;amp;=&amp;amp; N(\textbf{y}|0,\textbf{C}_{N})
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\textbf{C}_{N} = K + \beta^{-1}\beta^{I}_{N}\)&lt;/span&gt;. Note that the last expression expansion uses the regenerative nature of the normal distribution (the proof can be easily derived from the moment generating function of the normal distribution). The point is just to say that the covariance is the sum of the covariances of the two distributions, since they are independent. Personally, I imagine that &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; is the prior distribution of the Gaussian process I just described, &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t}|\textbf{y})\)&lt;/span&gt; is the likelihood function, and &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t})\)&lt;/span&gt; is the posterior distribution. The only constraint on the prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; is that it is smooth with a loosely constrained distribution.
The joint probability of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; observable teacher data &lt;span class=&#34;math inline&#34;&gt;\(\textbf{t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_{N+1}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 p(\textbf{t},t_{N+1}) = N(\textbf{t},t_{N+1}|0,\textbf{C}_{N+1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\textbf{C}_{N+1}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \textbf{C}_{N+1} = \left(
    \begin{array}{cccc}
      \textbf{C}_{N} &amp;amp; \textbf{k} \\
      \textbf{k}^{T} &amp;amp; c \\
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\textbf{k} = (k(x_{1},x_{N+1}),...,k(x_{N},x_{N+1}))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = k(x_{N+1},x_{N+1})\)&lt;/span&gt;. The conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(t_{N+1}|\textbf{t})\)&lt;/span&gt; can be obtained from the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_{N+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(t_{N+1}|\textbf{t}) = N(t_{N+1}|\textbf{k}^{T}\textbf{C}_{N+1}^{-1}\textbf{t},c-\textbf{k}^{T}\textbf{C}_{N+1}^{-1}\textbf{k})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In calculating the conditional distribution, we use &lt;a href=&#34;https://qiita.com/kilometer/items/34249479dc2ac3af5706&#34;&gt;Properties of the conditional multivariate normal distribution&lt;/a&gt;. As you can see from the above equation, the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(t_{N+1}|\textbf{t})\)&lt;/span&gt; can be calculated if &lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt; input data, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; teacher data, and parameters &lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt; of the kernel function are known, so if any point is given as input data, it is possible to approximate the Generating Process. The nice thing about the &lt;code&gt;GPR&lt;/code&gt; is that it gives predictions without the direct estimation of the above defined probabilistic model &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y_{i} = \textbf{w}^{T}\phi(x_{i})\)&lt;/span&gt;. The stochastic model has &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\)&lt;/span&gt;, which converts the input data to a high-dimensional vector through a nonlinear transformation. Therefore, the higher the dimensionality, the larger the computational complexity of the &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\phi(x_{j})\alpha\)&lt;/span&gt; will be, but the &lt;code&gt;GPR&lt;/code&gt; uses a kernel trick, so the computational complexity of the sample size dimension of the input data vector will be sufficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-of-the-gpr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Implementation of the `GPR’&lt;/h2&gt;
&lt;p&gt;　For now, let’s implement this in &lt;code&gt;R&lt;/code&gt;, which I’ve implemented in PRML test data, so I tweaked it.
　
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(grid)

# 1.Gaussian Process Regression

# PRML&amp;#39;s synthetic data set
curve_fitting &amp;lt;- data.frame(
  x=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000),
  t=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502))

f &amp;lt;- function(beta, sigma, xmin, xmax, input, train) {
  kernel &amp;lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function
  K &amp;lt;- outer(input, input, kernel); # calc gram matrix
  C_N &amp;lt;- K + diag(length(input))/beta
  m &amp;lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean 
  m_sig &amp;lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance
  x &amp;lt;- seq(xmin,xmax,length=100)
  output &amp;lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),
                              tx=input,ty=train),
                   aes(x=x1,y=m)) + 
    geom_line() +
    geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +
    geom_point(aes(x=tx,y=ty))
  return(output)
}

grid.newpage() # make a palet
pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2
print(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))
print(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2))
print(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))
print(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(beta^{-1}\)&lt;/span&gt; represents the measurement error. The higher the value of &lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (i.e., the smaller the measurement error), the easier it is to overfit, since the error of the predictions is less than that of the data already available.&lt;/strong&gt; This is the case in the top left corner of the figure above. The top left corner is &lt;span class=&#34;math inline&#34;&gt;\(\beta=400\)&lt;/span&gt;, which means that it overfits the current data available. Conversely, a small value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; will produce predictions that ignore the errors with the teacher data, but may improve the generalization performance. The top right figure shows this. For &lt;span class=&#34;math inline&#34;&gt;\(beta=4\)&lt;/span&gt;, the average barely passes through the data points we have, and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is currently available. &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; represents the magnitude of the effect of the data we have at the moment on the surroundings. If &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is small, the adjacent points will interact strongly with each other, which may reduce the accuracy but increase the generalization performance. Conversely, if &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is large, the result will be unnatural, fitting only individual points. This is illustrated in the figure below right (&lt;span class=&#34;math inline&#34;&gt;\(b=\frac{1}{0.03}, \beta=25\)&lt;/span&gt;). As you can see, the graph is overfitting because of the large &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and because &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is also large, so it fits only individual points, resulting in an absurdly large graph. The bottom left graph is the best. It has &lt;span class=&#34;math inline&#34;&gt;\(b=\frac{1}{0.3}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(b=2\)&lt;/span&gt;. Let’s try extending the x interval of this graph to [0,2]. Then we get the following graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid.newpage() # make a palet
pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2
print(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) 
print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) 
print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))
print(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, all the graphs except the bottom left one have a band of 95% confidence intervals that immediately widen and are completely useless where there are no data points. On the other hand, the lower left graph has a decent band up to 1.3 to 1.4, and the average value seems to pass through a point that is consistent with our intuitive understanding of the function. You can also see that if you are too far away from the observable data points, you will get a normal distribution with a mean of 0 and a variance of 1 no matter what you give to the parameters.
Now that we have shown that the accuracy of the prediction of the out-sample varies depending on the value of the parameters, the question here is how to estimate these hyperparameters. This is done by using the gradient method to find the hyperparameters that maximize the log-likelihood function &lt;span class=&#34;math inline&#34;&gt;\(\ln p(\bf{t}|a,b)\)&lt;/span&gt; ((&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; seems to be of a slightly different type, and the developmental discussion appears to take other tuning methods. We haven’t gotten to that level yet, so we’ll calibrate it here). Since &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t}) = N(\textbf{y}|0, \textbf{C}_{N})\)&lt;/span&gt;, the log-likelihood function is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \ln p(\textbf{t}|a,b,\beta) = -\frac{1}{2}\ln|\textbf{C}_{N}| - \frac{N}{2}\ln(2\pi) - \frac{1}{2}\textbf{t}^{T}\textbf{C}_{N}^{-1}\textbf{k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After that, we can differentiate this with the parameters and solve the obtained simultaneous equations to get the maximum likelihood estimator. Now let’s get the derivatives.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial}{\partial \theta_{i}} \ln p(\textbf{t}|\theta) = -\frac{1}{2}Tr(\textbf{C}_{N}^{-1}\frac{\partial \textbf{C}_{N}}{\partial \theta_{i}}) + \frac{1}{2}\textbf{t}^{T}\textbf{C}_{N}^{-1}
\frac{\partial\textbf{C}_{N}}{\partial\theta_{i}}\textbf{C}_{N}^{-1}\textbf{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(theta\)&lt;/span&gt; is the parameter set and &lt;span class=&#34;math inline&#34;&gt;\(theta_{i}\)&lt;/span&gt; represents the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th parameter. If you don’t understand this derivative &lt;a href=&#34;http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20%20-%20Springer%20Springer%20%202006.pdf&#34;&gt;here&lt;/a&gt; in the supplement to (C.21) and (C.22) equations. Since we are using the Gaussian kernel in this case, we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial k(x,x&amp;#39;)}{\partial a} = \exp(-b(x-x&amp;#39;)^{2}) \\
\displaystyle \frac{\partial k(x,x&amp;#39;)}{\partial b} = -a(x-x&amp;#39;)^{2}\exp(-b(x-x&amp;#39;)^{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;from the above formula. However, this time we will use the gradient method to find the best parameters. Here’s the code for the implementation (it’s pretty much a lost cause).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- function(xmin, xmax, input, train){
  # initial value
  beta = 100
  b = 1
  a = 1
  learning_rate = 0.1
  itermax &amp;lt;- 1000
  if (class(input) == &amp;quot;numeric&amp;quot;){
    N &amp;lt;- length(input)
  } else
  {
    N &amp;lt;- NROW(input)
  }
  kernel &amp;lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel
  derivative_a &amp;lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2)
  derivative_b &amp;lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)
  dloglik_a &amp;lt;- function(C_N,y,x1,x2) {
    -sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y 
  }
  dloglik_b &amp;lt;- function(C_N,y,x1,x2) {
    -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y 
  }
  # loglikelihood function
  likelihood &amp;lt;- function(b,a,x,y){
    kernel &amp;lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2)
    K &amp;lt;- outer(x, x, kernel)
    C_N &amp;lt;- K + diag(N)/beta
    itermax &amp;lt;- 1000
    l &amp;lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y
    return(l)
  }
  K &amp;lt;- outer(input, input, kernel) 
  C_N &amp;lt;- K + diag(N)/beta
  for (i in 1:itermax){
    kernel &amp;lt;- function(x1, x2) a*exp(-b*(x1-x2)^2)
    derivative_b &amp;lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)
    dloglik_b &amp;lt;- function(C_N,y,x1,x2) {
      -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y 
    }
    K &amp;lt;- outer(input, input, kernel) # calc gram matrix
    C_N &amp;lt;- K + diag(N)/beta
    l &amp;lt;- 0
    if(abs(l-likelihood(b,a,input,train))&amp;lt;0.0001&amp;amp;i&amp;gt;2){
      break
    }else{
      a &amp;lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input))
      b &amp;lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input))
    }
    l &amp;lt;- likelihood(b,a,input,train)
  }
  K &amp;lt;- outer(input, input, kernel)
  C_N &amp;lt;- K + diag(length(input))/beta
  m &amp;lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train)  
  m_sig &amp;lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel))))
  x &amp;lt;- seq(xmin,xmax,length=100)
  output &amp;lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),
                              tx=input,ty=train),
                   aes(x=x1,y=m)) + 
    geom_line() +
    geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +
    geom_point(aes(x=tx,y=ty))
  return(output)
}

print(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes, it does sound like good (lol).
That’s it for today, for now.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
