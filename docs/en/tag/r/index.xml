<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | 京都の電子部品メーカーで働く社会人が研究に没頭するブログ</title>
    <link>/en/tag/r/</link>
      <atom:link href="/en/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 10 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>/en/tag/r/</link>
    </image>
    
    <item>
      <title>Rcpp to speed up data handling (using Tick data processing as an example)</title>
      <link>/en/post/post21/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post21/</guid>
      <description>
&lt;script src=&#34;../../../en/post/post21/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what-i-want-to-do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;0. What I want to do&lt;/h2&gt;
&lt;p&gt;As mentioned above, what I will show you this time is the pre-processing (and analysis) of currency tick data. I won’t go into details since my main focus is to improve efficiency of analysis using Rcpp, but I will give you a rough idea of what I want to do.&lt;/p&gt;
&lt;p&gt;What we want to do is to detect &lt;strong&gt;jumps&lt;/strong&gt; in the 5-minute returns of the JPY/USD rate. A jump here is a sudden rise (or fall) in the exchange rate compared to the previous point. During the day, exchange rates move in small increments, but when there is an event, they rise (or fall) significantly. It is very interesting to see what kind of event causes a jump. In order to verify this, we first need to detect jumps. The following paper will be used as a reference.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/rfs/article-abstract/21/6/2535/1574138?redirectedFrom=fulltext&#34;&gt;Suzanne S. Lee &amp;amp; Per A. Mykland, 2008. “Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,” Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is a highly regarded paper with a Citation of 204. I will explain the estimation method in brief. First, let the continuous compound return be &lt;span class=&#34;math inline&#34;&gt;\(d\log S(t)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt;. where &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is the price of the asset at &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. If there are no jumps in the market, &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is assumed to follow the following stochastic process&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) \tag{1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt; is the standard Brownian motion, &lt;span class=&#34;math inline&#34;&gt;\(\mu(t)\)&lt;/span&gt; is the drift term, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma(t)\)&lt;/span&gt; is the spot volatility. Also, when there is a Jump, &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is assumed to follow the following stochastic process&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) + Y(t)dJ(t) \tag{2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(J(t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt; are counting processes independent of each other. &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y(t)\)&lt;/span&gt; represents the size of the jump and is a predictable process.&lt;/p&gt;
&lt;p&gt;Next, consider the logarithmic return of &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;. That is, &lt;span class=&#34;math inline&#34;&gt;\(\log S(t_i)/S(t_{i-1})\)&lt;/span&gt;, which follows a normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(0, \sigma(t_i))\)&lt;/span&gt;. &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Now, we define the statistic &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; when there is a jump from &lt;span class=&#34;math inline&#34;&gt;\(t_{i-1}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt; as follows.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} \equiv \frac{|\log S(t_i)/S(t_{i-1})|}{\hat{\sigma}_{t_i}} \tag{3}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is a simple standardization of the absolute value of the log return, but uses the “Realized Bipower Variation” defined below as an estimator of the standard deviation.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\sigma}_{t_i} = \frac{1}{K-2}\sum_{j=i-K+2}^{i-2}|\log S(t_j)/\log S(t_{j-1})||\log S(t_{j-1})/\log S(t_{j-2})| \tag{4}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the number of sample sizes contained in the Window. If we use a return in 5-minute increments and the jump occurs at 10:00 on 9/10/2020, and &lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt;, then we will calculate using samples from the previous day, 9/9/2020 11:30 to 9/11/2020 09:55. What we’re doing is adding up the absolute value of the return multiplied by the absolute value of the return, which seems to make it difficult for the estimate of the next instant after the jump occurs (i.e., &lt;span class=&#34;math inline&#34;&gt;\(t_{i+1}\)&lt;/span&gt; and so on) to be affected by the jump. Incidentally, &lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt; is introduced in another paper as a recommended value for returns in 5-minute increments.&lt;/p&gt;
&lt;p&gt;Let’s move on to how the Jump statistic calculated in this way can be used in a statistical test to detect a Jump. This is done by considering the maximum value of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt;, and when a value deviates greatly from its distribution (such as the 95th percentile), the return is considered to be a Jump.&lt;/p&gt;
&lt;p&gt;If we assume that there is no Jump in period &lt;span class=&#34;math inline&#34;&gt;\([t_{i-1},t_i]\)&lt;/span&gt;, then let the length of this period &lt;span class=&#34;math inline&#34;&gt;\(\Delta=t_i-t_{i-1}\)&lt;/span&gt;
close to 0, that is, &lt;span class=&#34;math inline&#34;&gt;\(\Delta\rightarrow 0\)&lt;/span&gt;, the (absolute) maximum of the standard normal variable converges to the Gumbel distribution. Therefore, Jump can be detected if the null hypothesis is rejected when the following conditions are met.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} &amp;gt; G^{-1}(1-\alpha)S_{n} + C_{n} \tag{5}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(G^{-1}(1-\alpha)\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\((1-\alpha)\)&lt;/span&gt; quantile function of the standard Gumbell distribution. If &lt;span class=&#34;math inline&#34;&gt;\(\alpha=10%\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(G^{-1}(1-\alpha)=2.25\)&lt;/span&gt;. Note that (We won’t derive it, but we can prove it using equations 1 and 2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_{n} = \frac{1}{c(2\log n)^{0.5}},~ \\
C_{n} = \frac{(2\log n)^{0.5}}{c}-\frac{\log \pi+\log(\log n)}{2c(2\log n)^{0.5}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(c=(2/\pi)^{0.5}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the total sample size used for estimation.
Finally, &lt;span class=&#34;math inline&#34;&gt;\(Jump_{t_i}\)&lt;/span&gt; is calculated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Jump_{t_i} = \log\frac{S(t_i)}{S(t_{i-1})}×I(\mathcal{L(i)} - G^{-1}(1-\alpha)S_{n} + C_{n})\tag{6}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(⋅)\)&lt;/span&gt; is an Indicator function that returns 1 if the content is greater than 0 and 0 otherwise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Loading data&lt;/h2&gt;
&lt;p&gt;So now that we know how to estimate, let’s load the Tick data first. The data is csv from &lt;code&gt;QuantDataManager&lt;/code&gt; and saved in a working directory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)

# Read Tick data
strPath &amp;lt;- r&amp;quot;(C:\Users\hogehoge\JPYUSD_Tick_2011.csv)&amp;quot;
JPYUSD &amp;lt;- readr::read_csv(strPath)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On an unrelated note, I recently upgraded R to 4.0.2, and I’m quite happy to say that with 4.0 and above, you can escape the strings made by &lt;code&gt;Python&lt;/code&gt;, which relieves some of the stress I’ve been experiencing.&lt;/p&gt;
&lt;p&gt;The data looks like the following: in addition to the date, the Bid value, Ask value and the volume of transactions are stored. Here, we use the 2011 tick. The reason for this is to cover the dollar/yen at the time of the Great East Japan Earthquake.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     DateTime                        Bid             Ask            Volume     
##  Min.   :2011-01-03 07:00:00   Min.   :75.57   Min.   :75.58   Min.   : 1.00  
##  1st Qu.:2011-03-30 15:09:23   1st Qu.:77.43   1st Qu.:77.44   1st Qu.: 2.00  
##  Median :2011-06-15 14:00:09   Median :80.40   Median :80.42   Median : 2.00  
##  Mean   :2011-06-22 05:43:11   Mean   :79.91   Mean   :79.92   Mean   : 2.55  
##  3rd Qu.:2011-09-09 13:54:51   3rd Qu.:81.93   3rd Qu.:81.94   3rd Qu.: 3.00  
##  Max.   :2011-12-30 06:59:59   Max.   :85.52   Max.   :85.54   Max.   :90.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the way, &lt;code&gt;DateTime&lt;/code&gt; includes the period from 07:00:00 on 2011/1/3 to 06:59:59 on 2011-12-30 (16:59:59 on 2011-12-30) in Japan by UTC. The sample size is approximately 12 million entries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NROW(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11946621&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Preprocessing&lt;/h2&gt;
&lt;p&gt;Then calculate the median value from Bid and Ask and take the logarithm to calculate the return later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate the median value of Ask and Bid and make it logarithmic (for calculating the logarithmic return).
JPYUSD &amp;lt;- JPYUSD %&amp;gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %&amp;gt;% 
                     dplyr::mutate(logMid = log(Mid))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I format the currently irregularly arranged trading data into 5-minute increments of returns. The way to do it is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a &lt;code&gt;POSIXct&lt;/code&gt; vector with 1 year chopped every 5 minutes.&lt;/li&gt;
&lt;li&gt;create a function that calculates the logarithmic return from the first and last sample in the window of 5min in turn, if you pass 1. as an argument.&lt;/li&gt;
&lt;li&gt;execute. This is the plan. First, create the vector of 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create POSIX vector to calculate returns in 5min increments (288 x days)
start &amp;lt;- as.POSIXct(&amp;quot;2011-01-02 22:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
end &amp;lt;- as.POSIXct(&amp;quot;2011-12-31 21:55:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
from &amp;lt;- seq(from=start,to=end,by=5*60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, let’s move on to 2. If you have 12 million data, even if you use &lt;code&gt;purrr::map&lt;/code&gt; or &lt;code&gt;apply&lt;/code&gt; with &lt;code&gt;R&lt;/code&gt;, it takes a long time to call a function and it’s quite inefficient. I tried to use &lt;code&gt;sapply&lt;/code&gt;, but it didn’t complete the process and it was forced to terminate. &lt;code&gt;RCCp&lt;/code&gt; is useful in such a case. Although &lt;code&gt;R&lt;/code&gt; has many very useful functions for graphs and statistics, it is not very good at large repetition, including calls to user-defined functions (because it is a scripting language, rather than a compiling language, I mean). So, I write the part of repetitive process in &lt;code&gt;C++&lt;/code&gt; and compile it as &lt;code&gt;R&lt;/code&gt; function using &lt;code&gt;Rcpp&lt;/code&gt; and execute it. It is very efficient to compile and visualize the results and write them in &lt;code&gt;R&lt;/code&gt;. Also, &lt;code&gt;Rccp&lt;/code&gt; helps you to write &lt;code&gt;C++&lt;/code&gt; in a similar way to &lt;code&gt;R&lt;/code&gt; with less sense of discomfort. I think the following will give you a good idea of the details. It’s pretty well organized and is God, to say the least.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://teuder.github.io/rcpp4everyone_en/&#34;&gt;Rcpp for everyone&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s write the code for the second step. In coding, I used articles on the net for reference. C++ has a longer history and more users than &lt;code&gt;R&lt;/code&gt;, so you can find information you want to know.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;algorithm&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
DataFrame Rolling_r_cpp(
    DataFrame input,               // Data frame of (measurement time, measured value data)
    newDatetimeVector from,        // The starting point vector for the timing of the calculation
    double time_window = 5*60)  // Calculated window width (in seconds)
{ 
  
  // Extract the measurement time and value as a vector
  newDatetimeVector time = input[&amp;quot;DateTime&amp;quot;]; // This time is assumed to be sorted in ascending order.
  NumericVector     data = input[&amp;quot;logMid&amp;quot;];
  
  // The endpoint vector of the timing to be calculated
  newDatetimeVector to = from + time_window;
  
  // Number to calculate
  R_xlen_t N = from.length();
  
  // vector for storage 
  NumericVector value(N);
  
  // An object representing the position of a vector element
  newDatetimeVector::iterator begin = time.begin();
  newDatetimeVector::iterator end   = time.end();
  newDatetimeVector::iterator p1    = begin;
  newDatetimeVector::iterator p2    = begin;
  
  // Loop for window i
  for(R_xlen_t i = 0; i &amp;lt; N; ++i){
    // Rcout &amp;lt;&amp;lt; &amp;quot;i=&amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    double f = from[i];         // Time of the window&amp;#39;s start point
    double t = f + time_window; // The time of the window&amp;#39;s endpoint
    
    // If the endpoint of the window is before the first measurement time, it is NA or
    // If the starting point of the window is after the last measurement time, NA
    if(t &amp;lt;= *begin || f &amp;gt; *(end-1)){ 
      value[i]  = NA_REAL;
      continue;// Go to the next loop
    }
    
    // Vector time from position p1 and subsequent elements x
    // Let p1 be the position of the first element whose time is at the start point f &amp;quot;after&amp;quot; the window
    p1 = std::find_if(p1, end, [&amp;amp;f](double x){return f&amp;lt;=x;});
    // p1 = std::lower_bound(p1, end, f); //Same as above
    
    // Vector time from position p1 and subsequent elements x
    // Let p2 be the position of the last element whose time is &amp;quot;before&amp;quot; the endpoint t of the window
    // (In the below, this is accomplished by making the time one position before the &amp;#39;first element&amp;#39;, where the time is the window&amp;#39;s endpoint t &amp;#39;after&amp;#39;)
    p2 = std::find_if(p1, end, [&amp;amp;t](double x){return t&amp;lt;=x;}) - 1 ;
    // p2 = std::lower_bound(p1, end, t) - 1 ;//Same as above
    
    // Convert the position p1,p2 of an element to the element numbers i1, i2
    R_xlen_t i1 = p1 - begin;
    R_xlen_t i2 = p2 - begin; 
    
    
    // Checking the element number
    // C++ starts with the element number 0, so I&amp;#39;m adding 1 to match the R
    // Rcout &amp;lt;&amp;lt; &amp;quot;i1 = &amp;quot; &amp;lt;&amp;lt; i1+1 &amp;lt;&amp;lt; &amp;quot; i2 = &amp;quot; &amp;lt;&amp;lt; i2+1 &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    
    // Calculate the data in the relevant range
    if(i1&amp;gt;i2) {
      value[i] = NA_REAL; // When there is no data in the window
    } else { 
      value[i] = data[i2] - data[i1];
    }
    // ↑You can create various window functions by changing above
    
  }
  
  // Output the calculated time and the value as a data frame.
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, from),
      Named(&amp;quot;r&amp;quot;, value*100));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you compile the program with &lt;code&gt;Rcpp::sourceCpp&lt;/code&gt;, the function of &lt;code&gt;R&lt;/code&gt; can be executed as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- Rolling_r_cpp(JPYUSD,from))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       0.04       0.02       0.07&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It takes less than a second to process 12 million records. So Convenient!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r         
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014  
##  Median :2011-07-03 09:57:30   Median : 0.000  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880  
##                                NA&amp;#39;s   :29977&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The return is precisely calculated. The recommended length of the window is 270 in 5 min increments, but we’ll make it flexible as well. And we carefully process the &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;cmath&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
float rbv_cpp(
    NumericVector x, // Return vector to calculate rbv
    bool na_rm = true) // If NA is included in x, remove it and calculate it or
{
  
  // Get the number of calculations
  R_xlen_t N = x.length();
  
  // Define variables to contain the results of a calculation
  float out = 0;

  // Check for missing x
  LogicalVector lg_NA = is_na(x);
  
  // If there is a NA in x, whether to exclude that NA and calculate
  if(any(lg_NA).is_true() and na_rm==FALSE){
    out = NA_REAL; // Output NA as a result of the calculation
  } else {
    
    // Excluding NA
    if (any(lg_NA).is_true() and na_rm==TRUE){
      x[is_na(x)==TRUE] = 0.00; // Fill in the NA with zeros and effectively exclude it from the calculation.
    }
    
    // Compute the numerator (sum of rbv)
    for(R_xlen_t i = 1; i &amp;lt; N; ++i){
      out = out + std::abs(x[i])*std::abs(x[i-1]);
    }
    
    // Calculate the average and take the route.
    long denomi; //denominator
    if(N-sum(lg_NA)-2&amp;gt;0){
      denomi = N-sum(lg_NA)-2;
    } else {
      denomi = 1;
    }
    out = out/denomi;
    out = std::sqrt(out);
  }
  
  return out;
}

// [[Rcpp::export]]
DataFrame Rolling_rbv_cpp(
    DataFrame input, //Data frame of (measurement time, measured value data)
    int K = 270, // Rolling Window width to calculate
    bool na_pad = false, // Returning NA when the window width is insufficient
    bool na_remove = false // If the NA exists in the window width, exclude it from the calculation
){
  // Extract the return vector and number of samples
  NumericVector data = input[&amp;quot;r&amp;quot;];
  R_xlen_t T = data.length();
  
  // Prepare a vector to store the results
  NumericVector value(T);
  
  // Calculate and store RBVs per Windows width
  if(na_pad==TRUE){
    value[0] = NA_REAL; // return NA.
    value[1] = NA_REAL; // return NA.
    value[2] = NA_REAL; // return NA.
  } else {
    value[0] = 0; // Return zero.
    value[1] = 0; // Return zero.
    value[2] = 0; // Return zero.
  }
  
  for(R_xlen_t t = 3; t &amp;lt; T; ++t){
    // Bifurcation of the process depending on whether or not there is enough Windows width
    if (t-K&amp;gt;=0){
      value[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // Run a normal calculation
    } else if(na_pad==FALSE) {
      value[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Run a calculation with an incomplete Widnows width of less than K
    } else {
      value[t] = NA_REAL; // return NA.
    }
  }
  
  // Output the calculated time and value as a data frame.
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, input[&amp;quot;from&amp;quot;]),
      Named(&amp;quot;r&amp;quot;, data),
      Named(&amp;quot;rbv&amp;quot;,value));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, compile it and run it with &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- results %&amp;gt;% Rolling_rbv_cpp(na_remove = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       1.00       0.36       1.36&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So fast!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-jump-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Calculating Jump Statistics&lt;/h2&gt;
&lt;p&gt;Now let’s calculate the statistic &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_{t_i}\)&lt;/span&gt; from the returns and standard deviation we just calculated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Standardize the absolute value of the log return = Jump statistic
results &amp;lt;- results %&amp;gt;% dplyr::mutate(J=ifelse(rbv&amp;gt;0,abs(r)/rbv,NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what it looks like now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s move on to the Jump test. First, we need to define the useful functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Preparing Constants &amp;amp; Functions for Calculating Jump Test
c &amp;lt;- (2/pi)^0.5
Cn &amp;lt;- function(n){
  return((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5))
}
Sn &amp;lt;- function(n){
  1/(c*(2*log(n))^0.5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we perform the test. Rejected samples return 1 and all others 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Perform a jump test (10%) (return value is logical)
N &amp;lt;- NROW(results$J)
results &amp;lt;- results %&amp;gt;% dplyr::mutate(Jump = J &amp;gt; 2.25*Sn(N) + Cn(N))
summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423  
##     Jump        
##  Mode :logical  
##  FALSE:59864    
##  TRUE :257      
##  NA&amp;#39;s :44423    
##                 
##                 
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-using-ggplot2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Visualization using ggplot2&lt;/h2&gt;
&lt;p&gt;Now that the numbers have been calculated, let’s visualize them by plotting the intraday logarithmic return of JPY/USD in 5-minute increments for 2011/03/11 and the jump. By the way, the horizontal axis has been adjusted to Japan time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plotting about Jump at the time of the 2011/03/11 Great East Japan Earthquake
results %&amp;gt;% 
  dplyr::filter(from &amp;gt;= as.POSIXct(&amp;quot;2011-03-11 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;),from &amp;lt; as.POSIXct(&amp;quot;2011-03-12 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)) %&amp;gt;% 
  ggplot2::ggplot(ggplot2::aes(x=from,y=r)) +
  ggplot2::geom_path(linetype=3) +
  ggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=&amp;quot;red&amp;quot;)) +
  ggplot2::scale_x_datetime(date_breaks = &amp;quot;2 hours&amp;quot;, labels = scales::date_format(format=&amp;quot;%H:%M&amp;quot;,tz=&amp;quot;Asia/Tokyo&amp;quot;)) +
  ggplot2::ggtitle(&amp;quot;JPY/USD Jumps within Tohoku earthquake on 2011-3-11&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 36 row(s) containing missing values (geom_path).

## Warning: Removed 36 row(s) containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../../en/post/post21/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ve spent quite a bit of time writing this, and it’s 23:37 right now, so I’ll refrain from discussing it in depth, but since the earthquake occurred at 14:46:18, you can see that the market reacted to the weakening of the yen immediately after the disaster. After that, for some reason, the yen moved higher and peaked at 19:00. It is said that the yen is a safe asset, but this is the only time it is not safe given the heightened uncertainty.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Summary&lt;/h2&gt;
&lt;p&gt;I introduced the use of &lt;code&gt;Rcpp&lt;/code&gt; to improve the efficiency of &lt;code&gt;R&lt;/code&gt; analysis. The &lt;code&gt;C++&lt;/code&gt; is much faster than &lt;code&gt;R&lt;/code&gt; even if you write the code in a simple way, so it is hard to make coding mistakes. Also, even if a compile error occurs, RStudio gives you a clue as to where the compile error is occurring, so there is no stress in that respect either, which is why I recommend it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A stochastic process with non-negative, integer, non-decreasing values.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The mean is not necessarily zero, depending on the shape of the drift term, but we are assuming a small enough drift term now. In the paper, it is defined more precisely.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting the 10-year long-term interest rate</title>
      <link>/en/post/post14/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/post14/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-collection&#34;&gt;1. Data collection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#monthly-analysis-part&#34;&gt;2. Monthly Analysis Part&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#daily-analysis-part&#34;&gt;3. Daily Analysis Part&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. I wanted to do a fitting of the 10 year long term interest rate for a certain reason. So, we will use US data to analyze the results. First, let’s collect the data. We will use the &lt;code&gt;quantmod&lt;/code&gt; package to drop the data from FRED. The command &lt;code&gt;getsymbols(key,from=start date,src=&#34;FRED&#34;, auto.assign=TRUE)&lt;/code&gt; is easy to use. You can find the key on the FRED website.&lt;/p&gt;
&lt;div id=&#34;data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Data collection&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(quantmod)

# data name collected
symbols.name &amp;lt;- c(&amp;quot;10-Year Treasury Constant Maturity Rate&amp;quot;,&amp;quot;Effective Federal Funds Rate&amp;quot;,&amp;quot;
Consumer Price Index for All Urban Consumers: All Items&amp;quot;,&amp;quot;Civilian Unemployment Rate&amp;quot;,&amp;quot;3-Month Treasury Bill: Secondary Market Rate&amp;quot;,&amp;quot;Industrial Production Index&amp;quot;,&amp;quot;
10-Year Breakeven Inflation Rate&amp;quot;,&amp;quot;Trade Weighted U.S. Dollar Index: Broad, Goods&amp;quot;,&amp;quot;
Smoothed U.S. Recession Probabilities&amp;quot;,&amp;quot;Moody&amp;#39;s Seasoned Baa Corporate Bond Yield&amp;quot;,&amp;quot;5-Year, 5-Year Forward Inflation Expectation Rate&amp;quot;,&amp;quot;Personal Consumption Expenditures&amp;quot;)

# Collect economic data
symbols &amp;lt;- c(&amp;quot;GS10&amp;quot;,&amp;quot;FEDFUNDS&amp;quot;,&amp;quot;CPIAUCSL&amp;quot;,&amp;quot;UNRATE&amp;quot;,&amp;quot;TB3MS&amp;quot;,&amp;quot;INDPRO&amp;quot;,&amp;quot;T10YIEM&amp;quot;,&amp;quot;TWEXBMTH&amp;quot;,&amp;quot;RECPROUSM156N&amp;quot;,&amp;quot;BAA&amp;quot;,&amp;quot;T5YIFRM&amp;quot;,&amp;quot;PCE&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, src = &amp;quot;FRED&amp;quot;, auto.assign = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;GS10&amp;quot;          &amp;quot;FEDFUNDS&amp;quot;      &amp;quot;CPIAUCSL&amp;quot;      &amp;quot;UNRATE&amp;quot;       
##  [5] &amp;quot;TB3MS&amp;quot;         &amp;quot;INDPRO&amp;quot;        &amp;quot;T10YIEM&amp;quot;       &amp;quot;TWEXBMTH&amp;quot;     
##  [9] &amp;quot;RECPROUSM156N&amp;quot; &amp;quot;BAA&amp;quot;           &amp;quot;T5YIFRM&amp;quot;       &amp;quot;PCE&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;macro_indicator &amp;lt;- merge(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE)
rm(GS10,FEDFUNDS,CPIAUCSL,UNRATE,TB3MS,INDPRO,T10YIEM,TWEXBMTH,RECPROUSM156N,BAA,T5YIFRM,PCE,USEPUINDXD)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;monthly-analysis-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Monthly Analysis Part&lt;/h2&gt;
&lt;p&gt;The data are &lt;a href=&#34;htmlwidget/macro_indicator.html&#34;&gt;here&lt;/a&gt;. We will create a dataset for the estimation. The dependent variable is the &lt;code&gt;10-Year Treasury Constant Maturity Rate(GS10)&lt;/code&gt;. The explanatory variables are as follows&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;explanatory variable&lt;/th&gt;
&lt;th&gt;key&lt;/th&gt;
&lt;th&gt;proxy variable&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Federal Funds Rate&lt;/td&gt;
&lt;td&gt;FEDFUNDS&lt;/td&gt;
&lt;td&gt;Short term rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Consumer Price Index&lt;/td&gt;
&lt;td&gt;CPIAUCSL&lt;/td&gt;
&lt;td&gt;Price&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Unemployment Rate&lt;/td&gt;
&lt;td&gt;UNRATE&lt;/td&gt;
&lt;td&gt;Employment&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;3-Month Treasury Bill&lt;/td&gt;
&lt;td&gt;TB3MS&lt;/td&gt;
&lt;td&gt;Short term rate&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Industrial Production Index&lt;/td&gt;
&lt;td&gt;INDPRO&lt;/td&gt;
&lt;td&gt;Business conditions&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Breakeven Inflation Rate&lt;/td&gt;
&lt;td&gt;T10YIEM&lt;/td&gt;
&lt;td&gt;Price&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Trade Weighted Dollar Index&lt;/td&gt;
&lt;td&gt;TWEXBMTH&lt;/td&gt;
&lt;td&gt;Exchange rates&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Recession Probabilities&lt;/td&gt;
&lt;td&gt;RECPROUSM156N&lt;/td&gt;
&lt;td&gt;Business condition&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Moody’s Seasoned Baa Corporate Bond Yield&lt;/td&gt;
&lt;td&gt;BAA&lt;/td&gt;
&lt;td&gt;Risk premium&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Inflation Expectation Rate&lt;/td&gt;
&lt;td&gt;T5YIFRM&lt;/td&gt;
&lt;td&gt;Price&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Personal Consumption Expenditures&lt;/td&gt;
&lt;td&gt;PCE&lt;/td&gt;
&lt;td&gt;Business condition&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Economic Policy Uncertainty Index&lt;/td&gt;
&lt;td&gt;USEPUINDXD&lt;/td&gt;
&lt;td&gt;Politics&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It’s a pretty appropriate choice of variables, but in many cases, we haven’t done it properly in terms of how to model long-term interest rates from a macro modeling perspective… In DSGE, we formulate the path of short-term interest rates linked to the path of short-term interest rates up to 10 years into the future according to the efficient market hypothesis and long-term interest rates equal to the path of short-term interest rates when I was a graduate student It was modeling (which seems to be done properly in macro finance circles). That’s why I’ve added short-term interest rates as an explanatory variable. And I also added three indicators for prices that would have an impact on the short-term interest rate. In addition, I added data on the economy because it is well known that it is highly correlated with the economy. This is due to the fact that in the first place, it is common in macro models to model short-term interest rates as following the Taylor rule.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_t = \rho r_{t-1} + \alpha \pi_{t} + \beta y_{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt; is the policy interest rate (short-term interest rate), &lt;span class=&#34;math inline&#34;&gt;\(\pi_t\)&lt;/span&gt; is the inflation rate, and &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; is the output. The $R_rho, \\alpha, and &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; are called deep parameters, which represent inertia, the sensitivity of the interest rate to inflation, and the sensitivity of the interest rate to output, respectively. It is well known as the “Taylor’s Principle” that when $, &lt;span class=&#34;math inline&#34;&gt;\(\beta=0\)&lt;/span&gt;, a reasonably expected equilibrium solution can only be obtained when &lt;span class=&#34;math inline&#34;&gt;\(\alpha&amp;gt;=1\)&lt;/span&gt;. Other explanatory variables include &lt;code&gt;Moody&#39;s Seasoned Baa Corporate Bond Yield&lt;/code&gt;, which may also have an arbitrage relationship with corporate bonds. Also, we would like to add the &lt;code&gt;VIX&lt;/code&gt; index and an index related to finances if we wanted to. The fiscal index is either Quatery or Annualy and cannot be used for monthly estimation. This is the most difficult part. I will re-estimate if I come up with something.&lt;/p&gt;
&lt;p&gt;Now, let’s get into the estimation. Since there are many explanatory variables in this case, we want to do a &lt;code&gt;lasso&lt;/code&gt; regression to narrow down the valid variables. We will also do an &lt;code&gt;OLS&lt;/code&gt; for comparison. The explanatory variables will be the values of the dependent variable one period ago. Probably, even one period ago, depending on when the data are published, it may not be in time for the next month’s estimates, but I’ll do this anyway.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make dataset
traindata &amp;lt;- na.omit(merge(macro_indicator[&amp;quot;2003-01-01::2015-12-31&amp;quot;][,1],stats::lag(macro_indicator[&amp;quot;2003-01-01::2015-12-31&amp;quot;][,-1],1)))
testdata  &amp;lt;- na.omit(merge(macro_indicator[&amp;quot;2016-01-01::&amp;quot;][,1],stats::lag(macro_indicator[&amp;quot;2016-01-01::&amp;quot;][,-1],1)))

# fitting OLS
trial1 &amp;lt;- lm(GS10~.,data = traindata)
summary(trial1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = GS10 ~ ., data = traindata)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.76208 -0.21234  0.00187  0.21595  0.70493 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)   14.3578405  4.3524691   3.299 0.001226 ** 
## FEDFUNDS      -0.2011132  0.1438774  -1.398 0.164335    
## CPIAUCSL      -0.0702011  0.0207761  -3.379 0.000938 ***
## UNRATE        -0.2093502  0.0796052  -2.630 0.009477 ** 
## TB3MS          0.2970160  0.1413796   2.101 0.037410 *  
## INDPRO        -0.0645376  0.0260343  -2.479 0.014339 *  
## T10YIEM        1.1484487  0.1769925   6.489 1.32e-09 ***
## TWEXBMTH      -0.0317345  0.0118155  -2.686 0.008091 ** 
## RECPROUSM156N -0.0099083  0.0021021  -4.713 5.72e-06 ***
## BAA            0.7793520  0.0868628   8.972 1.49e-15 ***
## T5YIFRM       -0.4551318  0.1897695  -2.398 0.017759 *  
## PCE            0.0009087  0.0002475   3.672 0.000339 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2981 on 143 degrees of freedom
## Multiple R-squared:  0.9203, Adjusted R-squared:  0.9142 
## F-statistic: 150.1 on 11 and 143 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s a higher degree of freedom-adjusted coefficient of determination; we use the model through 12/31/2015 to predict the out-sample data (01/01/2016~) and calculate the mean squared error.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.OLS.Y &amp;lt;- predict(trial1,testdata[,-1])
Y &amp;lt;- as.matrix(testdata[,1])
mse.OLS &amp;lt;- sum((Y - est.OLS.Y)^2) / length(Y)
mse.OLS&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1431734&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is the &lt;code&gt;lasso&lt;/code&gt; regression, using the &lt;code&gt;cv.glmnet&lt;/code&gt; function of the &lt;code&gt;glmnet&lt;/code&gt; package to perform Cross Validation and determine &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fitting lasso regression
library(glmnet)
trial2 &amp;lt;- cv.glmnet(as.matrix(traindata[,-1]),as.matrix(traindata[,1]),family=&amp;quot;gaussian&amp;quot;,alpha=1)
plot(trial2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trial2$lambda.min&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.001214651&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(trial2,s=trial2$lambda.min)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 12 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                           1
## (Intercept)    8.4885375862
## FEDFUNDS       .           
## CPIAUCSL      -0.0378778837
## UNRATE        -0.1693313660
## TB3MS          0.1224641351
## INDPRO        -0.0545965007
## T10YIEM        1.1926554061
## TWEXBMTH      -0.0140144490
## RECPROUSM156N -0.0090706154
## BAA            0.7389283529
## T5YIFRM       -0.4638923964
## PCE            0.0005014518&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Unemployment Rate&lt;/code&gt;, &lt;code&gt;3-Month Treasury Bill&lt;/code&gt;, &lt;code&gt;Breakeven Inflation Rate&lt;/code&gt;, &lt;code&gt;Moody&#39;s Seasoned Baa Corporate Bond Yield&lt;/code&gt; and &lt;code&gt;Inflation Expectation Rate&lt;/code&gt;. That’s the result of a larger regression coefficient. Other than the unemployment rate, the results are within expectations. However, the correlation with the economy seems to be low as far as this result is concerned (does it only work in the opposite direction?). Calculate the MSE.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.lasso.Y &amp;lt;- predict(trial2, newx = as.matrix(testdata[,-1]), s = trial2$lambda.min, type = &amp;#39;response&amp;#39;)
mse.lasso &amp;lt;- sum((Y - est.lasso.Y)^2) / length(Y)
mse.lasso&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.1125487&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;lasso&lt;/code&gt; regression gives better results. Let’s plot the predicted and actual values from the &lt;code&gt;lasso&lt;/code&gt; regression as a time series.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

ggplot(gather(data.frame(actual=Y[,1],lasso_prediction=est.lasso.Y[,1],OLS_prediction=est.OLS.Y,date=as.POSIXct(rownames(Y))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +
  geom_line(size=1.5) +
  scale_x_datetime(breaks = &amp;quot;6 month&amp;quot;,date_labels = &amp;quot;%Y-%m&amp;quot;) +
  scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,3.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The sense of direction is good. On the other hand, I am not predicting a sharp decline in interest rates from January 2016 or after December 2018. It looks like we’ll have to try to do one of the following to improve the accuracy of this part of the projections: consider some variables OR run a rolling estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;daily-analysis-part&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Daily Analysis Part&lt;/h2&gt;
&lt;p&gt;In addition to monthly analysis, I would like to do daily analysis. In the case of daily data, the &lt;code&gt;jagged edge&lt;/code&gt; problem is unlikely to occur because the data is often released after the market closes. We will start by collecting daily data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data name collected
symbols.name &amp;lt;- c(&amp;quot;10-Year Treasury Constant Maturity Rate&amp;quot;,&amp;quot;Effective Federal Funds Rate&amp;quot;,&amp;quot;
6-Month London Interbank Offered Rate (LIBOR), based on U.S. Dollar&amp;quot;,&amp;quot;NASDAQ Composite Index&amp;quot;,&amp;quot;3-Month Treasury Bill: Secondary Market Rate&amp;quot;,&amp;quot;Economic Policy Uncertainty Index for United States&amp;quot;,&amp;quot;
10-Year Breakeven Inflation Rate&amp;quot;,&amp;quot;Trade Weighted U.S. Dollar Index: Broad, Goods&amp;quot;,&amp;quot;Moody&amp;#39;s Seasoned Baa Corporate Bond Yield&amp;quot;,&amp;quot;5-Year, 5-Year Forward Inflation Expectation Rate&amp;quot;)

# Collect economic data
symbols &amp;lt;- c(&amp;quot;DGS10&amp;quot;,&amp;quot;DFF&amp;quot;,&amp;quot;USD6MTD156N&amp;quot;,&amp;quot;NASDAQCOM&amp;quot;,&amp;quot;DTB3&amp;quot;,&amp;quot;USEPUINDXD&amp;quot;,&amp;quot;T10YIE&amp;quot;,&amp;quot;DTWEXB&amp;quot;,&amp;quot;DBAA&amp;quot;,&amp;quot;T5YIFR&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, src = &amp;quot;FRED&amp;quot;, auto.assign = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;DGS10&amp;quot;       &amp;quot;DFF&amp;quot;         &amp;quot;USD6MTD156N&amp;quot; &amp;quot;NASDAQCOM&amp;quot;   &amp;quot;DTB3&amp;quot;       
##  [6] &amp;quot;USEPUINDXD&amp;quot;  &amp;quot;T10YIE&amp;quot;      &amp;quot;DTWEXB&amp;quot;      &amp;quot;DBAA&amp;quot;        &amp;quot;T5YIFR&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NASDAQCOM.r &amp;lt;- ROC(na.omit(NASDAQCOM))
macro_indicator.d &amp;lt;- merge(DGS10,DFF,USD6MTD156N,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)
rm(DGS10,DFF,USD6MTD156N,NASDAQCOM,NASDAQCOM.r,DTB3,USEPUINDXD,T10YIE,DTWEXB,DBAA,T5YIFR)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to build the data set. We separate the data for training and for training. Considering the actual prediction process, we use data from two business days ago as the explanatory variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make dataset
traindata.d &amp;lt;- na.omit(merge(macro_indicator.d[&amp;quot;1980-01-01::2010-12-31&amp;quot;][,1],stats::lag(macro_indicator.d[&amp;quot;1980-01-01::2010-12-31&amp;quot;][,-1],2)))
testdata.d  &amp;lt;- na.omit(merge(macro_indicator.d[&amp;quot;2010-01-01::&amp;quot;][,1],stats::lag(macro_indicator.d[&amp;quot;2010-01-01::&amp;quot;][,-1],2)))

# fitting OLS
trial1.d &amp;lt;- lm(DGS10~.,data = traindata.d)
summary(trial1.d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = DGS10 ~ ., data = traindata.d)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.82445 -0.12285  0.00469  0.14332  0.73789 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) -3.5051208  0.1690503 -20.734  &amp;lt; 2e-16 ***
## DFF          0.0818269  0.0239371   3.418 0.000653 ***
## USD6MTD156N -0.0135771  0.0233948  -0.580 0.561799    
## NASDAQCOM   -0.3880217  0.4367334  -0.888 0.374483    
## DTB3         0.1227984  0.0280283   4.381 1.29e-05 ***
## USEPUINDXD  -0.0006611  0.0001086  -6.087 1.58e-09 ***
## T10YIE       0.6980971  0.0355734  19.624  &amp;lt; 2e-16 ***
## DTWEXB       0.0270128  0.0012781  21.135  &amp;lt; 2e-16 ***
## DBAA         0.2988122  0.0182590  16.365  &amp;lt; 2e-16 ***
## T5YIFR       0.3374944  0.0381111   8.856  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2173 on 1114 degrees of freedom
## Multiple R-squared:  0.8901, Adjusted R-squared:  0.8892 
## F-statistic:  1002 on 9 and 1114 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient of determination remains high.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.OLS.Y.d &amp;lt;- predict(trial1.d,testdata.d[,-1])
Y.d &amp;lt;- as.matrix(testdata.d[,1])
mse.OLS.d &amp;lt;- sum((Y.d - est.OLS.Y.d)^2) / length(Y.d)
mse.OLS.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8003042&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next is the &lt;code&gt;lasso&lt;/code&gt; regression. Determine &lt;span class=&#34;math inline&#34;&gt;\(lambda\)&lt;/span&gt; in &lt;code&gt;CV&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fitting lasso regression
trial2.d &amp;lt;- cv.glmnet(as.matrix(traindata.d[,-1]),as.matrix(traindata.d[,1]),family=&amp;quot;gaussian&amp;quot;,alpha=1)
plot(trial2.d)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;trial2.d$lambda.min&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.001472377&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coef(trial2.d,s=trial2.d$lambda.min)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 10 x 1 sparse Matrix of class &amp;quot;dgCMatrix&amp;quot;
##                         1
## (Intercept) -3.4186530904
## DFF          0.0707022021
## USD6MTD156N  .           
## NASDAQCOM   -0.2675513858
## DTB3         0.1204092358
## USEPUINDXD  -0.0006506183
## T10YIE       0.6915446819
## DTWEXB       0.0270389569
## DBAA         0.2861031504
## T5YIFR       0.3376304446&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The coefficient of &lt;code&gt;libor&lt;/code&gt; became zero, and the MSE of &lt;code&gt;OLS&lt;/code&gt; was higher than that of &lt;code&gt;libor&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est.lasso.Y.d &amp;lt;- predict(trial2.d, newx = as.matrix(testdata.d[,-1]), s = trial2.d$lambda.min, type = &amp;#39;response&amp;#39;)
mse.lasso.d &amp;lt;- sum((Y.d - est.lasso.Y.d)^2) / length(Y.d)
mse.lasso.d&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.8378427&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Plot the predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(gather(data.frame(actual=Y.d[,1],lasso_prediction=est.lasso.Y.d[,1],OLS_prediction=est.OLS.Y.d,date=as.POSIXct(rownames(Y.d))),key=data,value=rate,-date),aes(x=date,y=rate, colour=data)) +
  geom_line(size=1.5) +
  scale_x_datetime(breaks = &amp;quot;2 year&amp;quot;,date_labels = &amp;quot;%Y-%m&amp;quot;) +
  scale_y_continuous(breaks=c(1,1.5,2,2.5,3,3.5),limits = c(1.25,5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As with the monthly, there is very little difference between the predicted values for &lt;code&gt;OLS&lt;/code&gt; and &lt;code&gt;lasso&lt;/code&gt;. We have been able to capture the fluctuations quite nicely, but we have not been able to capture the interest rate decline caused by the &lt;code&gt;United States federal government credit-rating downgrades&lt;/code&gt; in 2011 or the interest rate increase associated with the economic recovery in 2013. The only daily economic indicator I have is POS data, but I might be able to use the recently used nightlight satellite imagery data, if I had to say so. I’ll try it if I have time. For now, I’d like to end this article for now. Thank you for reading this article.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scraping past race results on yahoo horse racing on rvest (for the second time)</title>
      <link>/en/post/post11/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/post11/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#script&#34;&gt;1. Script&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;For the second time, I’d like to drop past race results in rvest again. If you haven’t seen the past articles, I suggest you look at that one first.&lt;/p&gt;
&lt;p&gt;The reason I decided to take back the data this time is because I wanted to add more items to the explanatory variables when analyzing horse racing. So this time I created the program as an addition to the previous R script. The 14 new data items I added are as follows&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;grass or dirt.&lt;/li&gt;
&lt;li&gt;right-handed or left-handed.&lt;/li&gt;
&lt;li&gt;race conditions (good or slight)&lt;/li&gt;
&lt;li&gt;weather&lt;/li&gt;
&lt;li&gt;the color of the horse’s coat (chestnut, deer hair, etc.)&lt;/li&gt;
&lt;li&gt;horse owner&lt;/li&gt;
&lt;li&gt;producers&lt;/li&gt;
&lt;li&gt;place of origin&lt;/li&gt;
&lt;li&gt;date of birth&lt;/li&gt;
&lt;li&gt;the father’s horse&lt;/li&gt;
&lt;li&gt;the mother’s horse&lt;/li&gt;
&lt;li&gt;winnings up to that race (available from 2003)&lt;/li&gt;
&lt;li&gt;jockey’s weight.&lt;/li&gt;
&lt;li&gt;increase or decrease in the weight of the jockey.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Actually, I haven’t finished collecting data yet, and the R program has been running for a long time (I’ve been running it for about 3 days). However, the program itself is running tightly and I’ll try to introduce the script. Maybe I’ll write the results in a postscript.&lt;/p&gt;
&lt;div id=&#34;script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Script&lt;/h2&gt;
&lt;p&gt;The first step is to call the package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Web scraping of horse racing data by rvest

#install.packages(&amp;quot;rvest&amp;quot;)
#if (!require(&amp;quot;pacman&amp;quot;)) install.packages(&amp;quot;pacman&amp;quot;)
#install.packages(&amp;quot;beepr&amp;quot;)
#install.packages(&amp;quot;RSQLite&amp;quot;)
pacman::p_load(qdapRegex)
library(rvest)
library(stringr)
library(dplyr)
library(beepr)
library(RSQLite)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m pretty much warnning out, so I’m banning it and connecting to SQLite&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# warning prohibition
options(warn=-1)

# Connecting to SQLite
con = dbConnect(SQLite(), &amp;quot;horse_data.db&amp;quot;, synchronous=&amp;quot;off&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the odds are only available since 1994, the data is taken from 1994 up to the most recent date. yahoo horse racing has races organized by month, so the data is taken using that as a variable. Basically, you go to &lt;a href=&#34;https://keiba.yahoo.co.jp/schedule/list/2018/?month=7&#34;&gt;List of race results for the relevant year and month&lt;/a&gt; and then go to &lt;a href=&#34;https://keiba.yahoo.co.jp/race/list/18020106/&#34;&gt;Timetable for each day of the race&lt;/a&gt; for each day on that page. Since there are roughly a dozen or so races at each individual track, get the link and go to the &lt;a href=&#34;https://keiba.yahoo.co.jp/race/result/1802010601/&#34;&gt;race results page&lt;/a&gt; for each race. Then you will get the race results. The first step is to get a link to the timetable for each individual racecourse for each day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(year in 1994:2019){
  start.time &amp;lt;- Sys.time() # calculate time
  # Retrieve the race results page on Yahoo!
  for (k in 1:12){ # K is for the month.
    
    tryCatch(
      {
        keiba.yahoo &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp/schedule/list/&amp;quot;, year,&amp;quot;/?month=&amp;quot;,k)) # Access to the list of race results for a given year and month
        Sys.sleep(2)
        race_lists &amp;lt;- keiba.yahoo %&amp;gt;%
          html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% 
          html_attr(&amp;quot;href&amp;quot;) # Retrieve all URLs
        
        # Get the list of races for each day by track
        race_lists &amp;lt;- race_lists[str_detect(race_lists, pattern=&amp;quot;race/list/\\d+/&amp;quot;)==1] # Extracts URLs containing &amp;quot;results&amp;quot;.
      }
      , error = function(e){signal &amp;lt;- 1}
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we only extract the links we get that contain the word result in the url. In essence, it is a link to the race tables of each racecourse. From here, we use the links to the race tables of the tracks to access that page and get the links to the pages that contain the results of all 12 races.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    for (j in 1:length(race_lists)){ # where j is the link to the race table for the year in question.
      
      tryCatch(
        {
          race_list &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,race_lists[j]))
          race_url &amp;lt;- race_list %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) # 全urlを取得
          
          # Get the URL of the race results
          race_url &amp;lt;- race_url[str_detect(race_url, pattern=&amp;quot;result&amp;quot;)==1] # Extracts URLs containing &amp;quot;results&amp;quot;.
        }
        , error = function(e){signal &amp;lt;- 1}
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the links to the results of each race, it’s time to get the race results and the formatting part of the code. It’s quite a long and complicated code. The race results are stored in the following table attributes, so we’ll simply pull them first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;      for (i in 1:length(race_url)){ # where i is the race that was held at the racecourse in question
        
        print(str_c(year, &amp;quot;/&amp;quot;, k, &amp;quot;/&amp;quot;,j, &amp;quot; group &amp;quot;, i,&amp;quot; order&amp;quot;))
        
        tryCatch(
          {
            race1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,race_url[i])) # Get the URL of the race results
            signal &amp;lt;- 0
            Sys.sleep(2)
          }
          , error = function(e){signal &amp;lt;- 1}
        )
        
        # If the race is aborted or there are no errors in the process so far, run the process
        if (identical(race1 %&amp;gt;%
                      html_nodes(xpath = &amp;quot;//div[@class = &amp;#39;resultAtt mgnBL fntSS&amp;#39;]&amp;quot;) %&amp;gt;%
                      html_text(),character(0)) == TRUE &amp;amp;&amp;amp; signal == 0){
          
          # Scraping the race results
          race_result &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//table[@id = &amp;#39;raceScore&amp;#39;]&amp;quot;) %&amp;gt;%
            html_table()
          race_result &amp;lt;- do.call(&amp;quot;data.frame&amp;quot;,race_result) # Change the list to a data frame.
          
          colnames(race_result) &amp;lt;- c(&amp;quot;order&amp;quot;,&amp;quot;frame_number&amp;quot;,&amp;quot;horse_number&amp;quot;,&amp;quot;horse_name/age&amp;quot;,&amp;quot;time/margin&amp;quot;,&amp;quot;passing_rank/last_3F&amp;quot;,&amp;quot;jockey/weight&amp;quot;,&amp;quot;popularity/odds&amp;quot;,&amp;quot;trainer&amp;quot;) #　column renaming&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you just get a table, it will not be useful for analysis because of the following or multiple information in one cell. So, we need to mold the data into a form.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          # Passing order and time for 3 furlongs uphill
          race_result &amp;lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;(\\d{2}-\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2})&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;\\d{2}\\.\\d&amp;quot;)))
          race_result &amp;lt;- race_result[-6]
          
          # Time and Difference
          race_result &amp;lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;\\d\\.\\d{2}\\.\\d|\\d{2}\\.\\d&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+&amp;quot;)))
          race_result$margin[race_result$order==1] &amp;lt;- &amp;quot;トップ&amp;quot;
          race_result$margin[race_result$margin==&amp;quot;character(0)&amp;quot;] &amp;lt;- &amp;quot;大差&amp;quot;
          race_result$margin[race_result$order==0] &amp;lt;- NA
          race_result &amp;lt;- race_result[-5]
          
          # Horse&amp;#39;s name, age and weight
          race_result &amp;lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;[ァ-ヴー・]+&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;牡\\d+|牝\\d+|せん\\d+&amp;quot;)))
          race_result$horse_sex &amp;lt;- str_extract(race_result$horse_age, pattern = &amp;quot;牡|牝|せん&amp;quot;)
          race_result$horse_age &amp;lt;- str_extract(race_result$horse_age, pattern = &amp;quot;\\d&amp;quot;)
          race_result &amp;lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\d{3}&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\([\\+|\\-]\\d+\\)|\\([\\d+]\\)&amp;quot;)))
          race_result$horse_weight_change &amp;lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
          race_result &amp;lt;- dplyr::mutate(race_result,brinker=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;B&amp;quot;)))
          race_result$brinker[race_result$brinker!=&amp;quot;B&amp;quot;] &amp;lt;- &amp;quot;N&amp;quot;
          race_result &amp;lt;- race_result[-4]
          
          # jockey
          race_result &amp;lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,&amp;quot;[ぁ-ん一-龠]+\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,jockey_weight=as.character(str_extract_all(race_result$`jockey/weight`,&amp;quot;\\d{2}&amp;quot;)))
          race_result$jockey_weight_change &amp;lt;- 0
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;☆&amp;quot;)==1] &amp;lt;- 1
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;△&amp;quot;)==1] &amp;lt;- 2
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;△&amp;quot;)==1] &amp;lt;- 3
          race_result &amp;lt;- race_result[-4]
          
          # Odds and popularity
          race_result &amp;lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\(.+\\)&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\d+[^(\\d+.\\d)]&amp;quot;)))
          race_result$odds &amp;lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
          race_result &amp;lt;- race_result[-4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll import information other than the table we just retrieved. Specifically, race names, weather conditions, track conditions, dates, racecourses, etc. These information are listed at the top of the race results page.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          # Race Information
          race_date &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/p[@id = &amp;#39;raceTitDay&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
          race_name &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/h1[@class = &amp;#39;fntB&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
          race_distance &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//p[@id = &amp;#39;raceTitMeta&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
        
          race_result &amp;lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,&amp;quot;\\d+年\\d+月\\d+日&amp;quot;)))
          race_result$race_date &amp;lt;- str_replace_all(race_result$race_date,&amp;quot;年&amp;quot;,&amp;quot;/&amp;quot;)
          race_result$race_date &amp;lt;- str_replace_all(race_result$race_date,&amp;quot;月&amp;quot;,&amp;quot;/&amp;quot;)
          race_result$race_date &amp;lt;- as.Date(race_result$race_date)
          race_course &amp;lt;- as.character(str_extract_all(race_date,pattern = &amp;quot;札幌|函館|福島|新潟|東京|中山|中京|京都|阪神|小倉&amp;quot;))
          race_result$race_course &amp;lt;- race_course
          race_result &amp;lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,&amp;quot;\\s&amp;quot;,&amp;quot;&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,race_distance=as.character(str_extract_all(race_distance,&amp;quot;\\d+m&amp;quot;)))
          race_type=as.character(str_extract_all(race_distance,pattern = &amp;quot;芝|ダート&amp;quot;))
          race_result$type &amp;lt;- race_type
          race_turn &amp;lt;- as.character(str_extract_all(race_distance,pattern = &amp;quot;右|左&amp;quot;))
          race_result$race_turn &amp;lt;- race_turn
          
          if(length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg ryou&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;良&amp;quot;
          } else if (length(race1 %&amp;gt;% 
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg yayaomo&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;稍重&amp;quot;
          } else if (length(race1 %&amp;gt;%
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg omo&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;重&amp;quot;
          } else if (length(race1 %&amp;gt;% 
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg furyou&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;不良&amp;quot;
          } else race_result$race_condition &amp;lt;- &amp;quot;NA&amp;quot;
          
          if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg hare&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;晴れ&amp;quot;
          } else if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg ame&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;曇り&amp;quot;
          } else if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg kumori&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;雨&amp;quot;
          } else race_result$race_weather &amp;lt;- &amp;quot;その他&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to get information about each horse. In fact, the horse’s name in the table we got earlier is a link, and if you follow the link, you can get [information about each horse] (&lt;a href=&#34;https://keiba.yahoo.co.jp/directory/horse/2015105508/&#34; class=&#34;uri&#34;&gt;https://keiba.yahoo.co.jp/directory/horse/2015105508/&lt;/a&gt;) (such as coat color and date of birth).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          horse_url &amp;lt;- race1 %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) 
          horse_url &amp;lt;- horse_url[str_detect(horse_url, pattern=&amp;quot;directory/horse&amp;quot;)==1] # Extract only links to horse information.
          
          for (l in 1:length(horse_url)){
            tryCatch(
              {
                horse1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,horse_url[l]))
                Sys.sleep(0.5)
                horse_name &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;dirTitName&amp;#39;]/h1[@class = &amp;#39;fntB&amp;#39;]&amp;quot;) %&amp;gt;% 
                  html_text()
                horse &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;dirTitName&amp;#39;]/ul&amp;quot;) %&amp;gt;% 
                  html_text()
                race_result$colour[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;毛色：.+&amp;quot;)) 
                race_result$owner[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;馬主：.+&amp;quot;))
                race_result$farm[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;生産者：.+&amp;quot;))
                race_result$locality[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;産地：.+&amp;quot;))
                race_result$horse_birthday[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;\\d+年\\d+月\\d+日&amp;quot;))
                race_result$father[race_result$horse_name==horse_name] &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;bloodM&amp;#39;][@rowspan = &amp;#39;4&amp;#39;]&amp;quot;) %&amp;gt;% html_text()
                race_result$mother[race_result$horse_name==horse_name] &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;bloodF&amp;#39;][@rowspan = &amp;#39;4&amp;#39;]&amp;quot;) %&amp;gt;% html_text()
              }
              , error = function(e){
                race_result$colour[race_result$horse_name==horse_name] &amp;lt;- NA 
                race_result$owner[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$farm[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$locality[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$horse_birthday[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$father[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$mother[race_result$horse_name==horse_name] &amp;lt;- NA
                }
            )
          }
          
          race_result$colour &amp;lt;- str_replace_all(race_result$colour,&amp;quot;毛色：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$owner &amp;lt;- str_replace_all(race_result$owner,&amp;quot;馬主：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$farm &amp;lt;- str_replace_all(race_result$farm,&amp;quot;生産者：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$locality &amp;lt;- str_replace_all(race_result$locality,&amp;quot;産地：&amp;quot;,&amp;quot;&amp;quot;)
          #race_result$horse_birthday &amp;lt;- str_replace_all(race_result$horse_birthday,&amp;quot;年&amp;quot;,&amp;quot;/&amp;quot;)
          #race_result$horse_birthday &amp;lt;- str_replace_all(race_result$horse_birthday,&amp;quot;月&amp;quot;,&amp;quot;/&amp;quot;)
          #race_result$horse_birthday &amp;lt;- as.Date(race_result$horse_birthday)
          
          race_result &amp;lt;- dplyr::arrange(race_result,horse_number) # arrange in order of precedence&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to go and drop the amount of money you have won up to that race. You can access this by following the link marked [Runoffs] (&lt;a href=&#34;https://keiba.yahoo.co.jp/race/denma/1802010601/&#34; class=&#34;uri&#34;&gt;https://keiba.yahoo.co.jp/race/denma/1802010601/&lt;/a&gt;) on the race results page. This is where you will find the prize money and you will get it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          yosou_url &amp;lt;- race1 %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) 
          yosou_url &amp;lt;- yosou_url[str_detect(yosou_url, pattern=&amp;quot;denma&amp;quot;)==1]
          
          if (length(yosou_url)==1){
          yosou1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,yosou_url)) 
          Sys.sleep(2)
          yosou &amp;lt;- yosou1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;txC&amp;#39;]&amp;quot;) %&amp;gt;% as.character()
          prize &amp;lt;- yosou[grepl(&amp;quot;万&amp;quot;,yosou)==TRUE] %&amp;gt;% str_extract_all(&amp;quot;\\d+万&amp;quot;)
          prize &amp;lt;- t(do.call(&amp;quot;data.frame&amp;quot;,prize)) %&amp;gt;% as.character()
          race_result$prize &amp;lt;- prize
          race_result$prize &amp;lt;- str_replace_all(race_result$prize,&amp;quot;万&amp;quot;,&amp;quot;&amp;quot;) %&amp;gt;% as.numeric()
          } else race_result$prize &amp;lt;- NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a data frame called dataset to store the results of each race and store the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          ## file storage
          if (k == 1 &amp;amp;&amp;amp; i == 1 &amp;amp;&amp;amp; j == 1){
            dataset &amp;lt;- race_result
          } else {
            dataset &amp;lt;- rbind(dataset,race_result)
          }
        }else
        {
          print(&amp;quot;We couldn&amp;#39;t save it.&amp;quot;) 
        }
      }
    }
  }
  beep(3)
  write.csv(dataset,&amp;quot;race_result2.csv&amp;quot;, row.names = FALSE)
  
  if (year == 1994){
    dbWriteTable(con, &amp;quot;race_result&amp;quot;, dataset)
  } else {
    dbWriteTable(con, &amp;quot;temp&amp;quot;, dataset)
    dbSendQuery(con, &amp;quot;INSERT INTO race_result select * from temp&amp;quot;)
    dbSendQuery(con, &amp;quot;DROP TABLE temp&amp;quot;)
  }
}
end.time &amp;lt;- Sys.time()
print(str_c(&amp;quot;処理時間は&amp;quot;,end.time-start.time,&amp;quot;です。&amp;quot;))
beep(5)

options(warn = 1)

dbDisconnect(con)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s all. The data taken is as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(race_result)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   order frame_number horse_number   trainer passing_rank last_3F   time
## 1    10            1            1   田中 剛        09-09    39.0 1.14.3
## 2    16            1            2 天間 昭一        11-11    40.3 1.15.7
## 3    15            2            3 田中 清隆        14-14    39.4 1.15.1
## 4     9            2            4 中舘 英二        08-08    39.1 1.14.3
## 5    12            3            5 根本 康広        11-11    39.0 1.14.4
## 6     4            3            6 杉浦 宏昭        04-04    38.4 1.13.2
##      margin         horse_name horse_age horse_sex horse_weight
## 1    アタマ     サトノジョニー         3        牡          512
## 2 3 1/2馬身       ツギノイッテ         3        牡          464
## 3     3馬身           ギュウホ         3        牡          444
## 4 2 1/2馬身 セイウンメラビリア         3        牝          466
## 5      クビ サバイバルトリック         3        牝          450
## 6    アタマ       ステイホット         3        牝          474
##   horse_weight_change brinker      jockey jockey_weight jockey_weight_change
## 1                 +30       N   松岡 正海            56                    0
## 2                  +8       N 西田 雄一郎            56                    0
## 3                  +8       N   杉原 誠人            56                    0
## 4                 +10       N   村田 一誠            54                    0
## 5                  -2       N 野中 悠太郎            51                    0
## 6                  -2       N   大野 拓弥            54                    0
##    odds popularity  race_date race_course       race_name race_distance   type
## 1  40.3         9  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 2 340.9        16  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 3 283.1        14  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 4 299.7        15  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 5  26.7         8  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 6   2.4         1  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
##   race_turn race_condition race_weather colour                           owner
## 1        右             良         晴れ   栗毛 株式会社 サトミホースカンパニー
## 2        右             良         晴れ 黒鹿毛                     西村 新一郎
## 3        右             良         晴れ   鹿毛           有限会社 ミルファーム
## 4        右             良         晴れ 青鹿毛                       西山 茂行
## 5        右             良         晴れ 黒鹿毛                       福田 光博
## 6        右             良         晴れ   栗毛                       小林 善一
##           farm   locality horse_birthday                 father
## 1   千代田牧場 新ひだか町  2016年1月29日         オルフェーヴル
## 2    織笠 時男     青森県  2016年4月17日 スクワートルスクワート
## 3    神垣 道弘 新ひだか町  2016年4月19日     ジャングルポケット
## 4  石郷岡 雅樹     新冠町  2016年4月21日     キンシャサノキセキ
## 5     原田牧場     日高町  2016年4月30日       リーチザクラウン
## 6 社台ファーム     千歳市  2016年3月13日     キャプテントゥーレ
##               mother prize
## 1 スパークルジュエル     0
## 2   エプソムアイリス     0
## 3     デライトシーン     0
## 4     ドリームシップ     0
## 5   フリーダムガール   180
## 6     ステイアライヴ   455&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I built the Asset Allocation Model in R.</title>
      <link>/en/post/post2/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/post2/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#minimum-variance-portfolio&#34;&gt;1. Minimum Variance Portfolio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-to-predict-the-variance-covariance-matrix&#34;&gt;2. How to predict the variance-covariance matrix&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a.-constant-conditional-correlation-ccc-model&#34;&gt;A. Constant conditional correlation (CCC) model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#b.-dynamic-conditional-correlation-dcc-model&#34;&gt;B. Dynamic Conditional Correlation (DCC) model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#c.-dynamic-equicorrelation-deco-model&#34;&gt;C. Dynamic Equicorrelation (DECO) model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#collection-of-data-for-testing&#34;&gt;3. Collection of data for testing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. I attended a workshop on asset allocation at my place of work, so I’d like to run some simulations here, even though this field is completely outside my expertise. In this article, I would like to do an exercise on how to estimate the variance-covariance matrix (predictions) of a minimum variance portfolio as the base portfolio, referring to previous studies. The reference research is in the following paper (in the Journal of Operations Research).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2947643&#34;&gt;Asset Allocation with Correlation: A Composite Trade-Off&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;minimum-variance-portfolio&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Minimum Variance Portfolio&lt;/h2&gt;
&lt;p&gt;I won’t go into a detailed explanation of the minimum variance portfolio here, but it is a portfolio that calculates the average return and variance of each asset (domestic stocks, foreign stocks, domestic bonds, foreign bonds, and alternatives), plots them on a quadratic plane of average return vertical and variance horizontal axes, calculates the range of possible investments, and then calculates the smallest variance in the set (see the chart below).&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Minimum_variance_flontier_of_MPT.svg/1280px-Minimum_variance_flontier_of_MPT.svg.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;minimum variance portfolio&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;In an earlier study, Carroll et. al. (2017), the following is stated&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;this paper focusses on minimum-variance portfolios requiring only estimates of asset covariance, hence bypassing the well-known problem of estimation error in forecasting expected asset returns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Even at present, it is difficult to estimate expected returns, and a minimum variance portfolio, which does not require it, is a useful and practical technique. The objective function of a minimum variance portfolio is, as the name implies, &lt;strong&gt;to minimize diversification&lt;/strong&gt;. If we now denote the vector of collected returns for each asset by &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, the holding weight of each asset by &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, and the portfolio return by &lt;span class=&#34;math inline&#34;&gt;\(R_{p}\)&lt;/span&gt;, then the overall portfolio variance &lt;span class=&#34;math inline&#34;&gt;\(var(R_{p})\)&lt;/span&gt; can be written as follows.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
var(R_{p}) = var(r^{T}\theta) = E( (r^{T}\theta)(r^{T}\theta)^{T}) = \theta^{T}\Sigma\theta
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Sigma\)&lt;/span&gt; is the variance-covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;. Thus, the minimization problem is as follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\min_{\theta}(\theta^{T}\Sigma\theta) \\
s.t 1^{T}\theta = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here we have added full investment as a constraint. Let’s use the Lagrangian to solve this problem. The Lagrange function &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; is as follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L = \theta^{T}\Sigma\theta + \lambda(1^{T}\theta - 1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The first condition is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle\frac{\partial L}{\partial \theta} = 2\Sigma\theta + 1\lambda = 0 \\
\displaystyle \frac{\partial L}{\partial \lambda} = 1^{T} \theta = 1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Solving the first equation for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we find that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta = \Sigma^{-1}1\lambda^{*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}=-1/2\lambda\)&lt;/span&gt;. Substitute this into the second formula and solve for &lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
1^{T}\Sigma1\lambda^{*} = 1 \\
\displaystyle \lambda^{*} = \frac{1}{1^{T}\Sigma^{-1}1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since &lt;span class=&#34;math inline&#34;&gt;\(\theta = \Sigma^{-1}1\lambda^{*}\)&lt;/span&gt;, erasing &lt;span class=&#34;math inline&#34;&gt;\(\lambda^{*}\)&lt;/span&gt; will cause it to be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \theta_{gmv} = \frac{\Sigma^{-1}1}{1^{T}\Sigma^{-1}1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and we can now find the optimal weight. For now, I’ll implement this in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gmv &amp;lt;- function(r_dat,r_cov){
  library(MASS)
  i &amp;lt;- matrix(1,NCOL(r_dat),1)
  r_weight &amp;lt;- (ginv(r_cov)%*%i)/as.numeric(t(i)%*%ginv(r_cov)%*%i)
  wr_dat &amp;lt;- r_dat*as.numeric(r_weight)
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;) 
  return(result)
}

nlgmv &amp;lt;- function(r_dat,r_cov){
  qp.out &amp;lt;- solve.QP(Dmat=r_cov,dvec=rep(0,NCOL(r_dat)),Amat=cbind(rep(1,NCOL(r_dat)),diag(NCOL(r_dat))),
           bvec=c(1,rep(0,NCOL(r_dat))),meq=1)
  r_weight &amp;lt;- qp.out$solution
  wr_dat &amp;lt;- r_dat*r_weight
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;)
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The input is a return and variance-covariance matrix for each asset. The output is the weight, return and risk. &lt;code&gt;nlgmv&lt;/code&gt; is a short-sale constrained version of the minimum variance portfolio. Since we cannot get an analytical solution, we are trying to get a numerical solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-predict-the-variance-covariance-matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. How to predict the variance-covariance matrix&lt;/h2&gt;
&lt;p&gt;We have found the formula for the minimum variance portfolio. The next step is to analyze how to find the input, the variance-covariance matrix. I think the most primitive approach would be the historical approach of finding the covariance matrix for a sample of return data available before that point in time, and then finding the minimum variance portfolio by fixing the value of the covariance matrix (i.e., the weights are also fixed). However, since this is just using historical averages for future projections, I’m sure there will be all sorts of problems. As a non-specialist, I can think of a situation where the return on asset A declined significantly the day before, but the variance of this asset is expected to remain high tomorrow, and the effect of yesterday is diluted by using the average. And I feel that not changing the weights from the start would also move away from the optimal point over time. However, there seems to be some trial and error in this area as to how to estimate it then. Also, in Carroll et. al. (2017), it is stated that,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The estimation of covariance matrices for portfolios with a large number of assets still remains a fundamental challenge in portfolio optimization.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The estimates in this paper are based on the following models. All of them are unique in that the variance-covariance matrix is time-varying.&lt;/p&gt;
&lt;div id=&#34;a.-constant-conditional-correlation-ccc-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A. Constant conditional correlation (CCC) model&lt;/h3&gt;
&lt;p&gt;The original paper is &lt;a href=&#34;https://www.jstor.org/stable/2109358?read-now=1&amp;amp;seq=3#page_scan_tab_contents&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, from the relationship between the variance-covariance matrix and the correlation matrix, we have &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{t} = D_{t}R_{t}D_{t}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; is the variance-covariance matrix and &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(diag(\sigma_{1,t},...,\sigma_{N,t})\)&lt;/span&gt;, a matrix whose diagonal components are the standard deviation of each asset in &lt;span class=&#34;math inline&#34;&gt;\(tt\)&lt;/span&gt; period. From here, we estimate &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; separately. First, &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; is estimated using the following multivariate GARCH model (1,1).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_{t} = \mu + u_{t} \\
u_{t} = \sigma_{t}\epsilon \\
\sigma_{t}^{2} = \alpha_{0} + \alpha_{1}u_{t-1}^{2} + \alpha_{2}\sigma_{t-1}^{2} \\
\epsilon_{t} = NID(0,1) \\
E(u_{t}|u_{t-1}) = 0
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the sample mean of the returns. &lt;span class=&#34;math inline&#34;&gt;\(\alpha_{i}\)&lt;/span&gt; is the parameter to be estimated. Since we are estimating &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; with GARCH, we can say that we are modeling a relationship where the distribution of returns follows a thicker base distribution than the normal distribution and where the change in returns is not constant but depends on the variance of the previous day. We can say that we have estimated &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt; in this way. The next step in the estimation of &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; is to take a historical approach to the estimation of &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;, in which the returns are sampled. In other words, &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; is a constant. Thus, we are making the assumption that the magnitude of return variation varies with time, but the relative relationship of each asset is invariant.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-dynamic-conditional-correlation-dcc-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B. Dynamic Conditional Correlation (DCC) model&lt;/h3&gt;
&lt;p&gt;The original paper is &lt;a href=&#34;http://www.cass.city.ac.uk/__data/assets/pdf_file/0003/78960/Week7Engle_2002.pdf&#34;&gt;here&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;こちらのモデルでは、&lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;を求めるところまでは①と同じですが、[&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;の求め方が異なっており、ARMA(1,1)を用いて推計します。相関行列はやはり定数ではないということで、&lt;span class=&#34;math inline&#34;&gt;\(tex:t\)&lt;/span&gt;期までに利用可能なリターンを用いて推計をかけようということになっています。このモデルの相関行列&lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt;は、&lt;/p&gt;
&lt;p&gt;This model is the same as (1) up to the point of finding &lt;span class=&#34;math inline&#34;&gt;\(D_{t}\)&lt;/span&gt;, but the way of finding &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; is different, and we use &lt;code&gt;ARMA(1,1)&lt;/code&gt; to estimate it. Since the correlation matrix is still not a constant, we are going to use the available returns up to the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; period to make the estimate. The correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; in this model is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{t} = diag(Q_{t})^{-1/2}Q_{t}diag(Q_{t})^{-1/2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Q_{t}\)&lt;/span&gt; is a conditional variance-covariance matrix in the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; period, formulated as follows,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Q_{t} = \bar{Q}(1-a-b) + adiag(Q_{t-1})^{1/2}\epsilon_{i,t-1}\epsilon_{i,t-1}diag(Q_{t-1})^{1/2} + bQ_{t-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{Q}\)&lt;/span&gt; is the variance-covariance matrix calculated in a historical way and &lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt; is the parameter. This method differs from the previous one in that it assumes that not only does the magnitude of return variation vary with time, but also the relative relationship of each asset changes over time. Since we have observed some events in which the returns of all assets fall and the correlation of each asset becomes positive during a financial crisis, this formulation may be attractive.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-dynamic-equicorrelation-deco-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;C. Dynamic Equicorrelation (DECO) model&lt;/h3&gt;
&lt;p&gt;The original paper is &lt;a href=&#34;https://faculty.chicagobooth.edu/bryan.kelly/research/pdf/deco.pdf&#34;&gt;here&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;I haven’t read this paper exactly yet, but from the definition of the correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(R_{t}\)&lt;/span&gt; becomes&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
R_{t} = (1-\rho_{t})I_{N} + \rho_{t}1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\rho_{t}\)&lt;/span&gt; is a scalar and a coefficient for the degree of equicorrelation, and I understand that equicorrelation is an average pair-wise correlation. In other words, if there are no missing values, it’s no different than a normal correlation. However, it seems to be a good estimator in that respect, because as assets increase, such problems need to be addressed. We can calculate &lt;span class=&#34;math inline&#34;&gt;\(\rho_{t}\)&lt;/span&gt; as follows.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \rho_{t} = \frac{1}{N(N-1)}(\iota^{T}R_{t}^{DCC}\iota - N) = \frac{2}{N(N-1)}\sum_{i&amp;gt;j}\frac{q_{ij,t}}{\sqrt{q_{ii,t} q_{jj,t}}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\iota\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(N×1\)&lt;/span&gt; vector with all elements being 1. And &lt;span class=&#34;math inline&#34;&gt;\(q_{ij,t}\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i,j\)&lt;/span&gt; element of &lt;span class=&#34;math inline&#34;&gt;\(Q_{t}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now that we’ve modeled the variance-covariance matrix, we’ll implement it so far in &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;carroll &amp;lt;- function(r_dat,FLG){
  
  library(rmgarch)
  
  if(FLG == &amp;quot;benchmark&amp;quot;){
    H &amp;lt;- cov(r_dat)
  }else{
    #1. define variables
    N &amp;lt;- NCOL(r_dat) # the number of assets
    
    #2. estimate covariance matrix
    basic_garch = ugarchspec(mean.model = list(armaOrder = c(0, 0),include.mean=TRUE), variance.model = list(garchOrder = c(1,1), model = &amp;#39;sGARCH&amp;#39;), distribution.model = &amp;#39;norm&amp;#39;)
    multi_garch = multispec(replicate(N, basic_garch))
    dcc_set = dccspec(uspec = multi_garch, dccOrder = c(1, 1), distribution = &amp;quot;mvnorm&amp;quot;,model = &amp;quot;DCC&amp;quot;)
    fit_dcc_garch = dccfit(dcc_set, data = r_dat, fit.control = list(eval.se = TRUE))
    forecast_dcc_garch &amp;lt;- dccforecast(fit_dcc_garch)
    if (FLG == &amp;quot;CCC&amp;quot;){
      #Constant conditional correlation (CCC) model
      D &amp;lt;- sigma(forecast_dcc_garch)
      R_ccc &amp;lt;- cor(r_dat)
      H &amp;lt;- diag(D[,,1])%*%R_ccc%*%diag(D[,,1])
      colnames(H) &amp;lt;- colnames(r_dat)
      rownames(H) &amp;lt;- colnames(r_dat)
    }
    else{
      #Dynamic Conditional Correlation (DCC) model
      H &amp;lt;- as.matrix(rcov(forecast_dcc_garch)[[1]][,,1])
      if (FLG == &amp;quot;DECO&amp;quot;){
        #Dynamic Equicorrelation (DECO) model
        one &amp;lt;- matrix(1,N,N)
        iota &amp;lt;- rep(1,N)
        Q_dcc &amp;lt;- rcor(forecast_dcc_garch,type=&amp;quot;Q&amp;quot;)[[1]][,,1]
        rho &amp;lt;- as.vector((N*(N-1))^(-1)*(t(iota)%*%Q_dcc%*%iota-N))
        D &amp;lt;- sigma(forecast_dcc_garch)
        R_deco &amp;lt;- (1-rho)*diag(1,N,N) + rho*one
        H &amp;lt;- diag(D[,,1])%*%R_deco%*%diag(D[,,1])
        colnames(H) &amp;lt;- colnames(r_dat)
        rownames(H) &amp;lt;- colnames(r_dat)
      }
    }
  }
  return(H)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I shouldn’t normally use the package, but since it’s an exercise today I’m only going to pursue the results of the estimates, and I’ll be writing a post about GARCH in the next week or so.
Now we’re ready to go. We can now put the return data into this function to calculate the variance-covariance matrix and use it to calculate the minimum variance portfolio.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;collection-of-data-for-testing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Collection of data for testing&lt;/h2&gt;
&lt;p&gt;The data was based on the following article.&lt;/p&gt;
&lt;p&gt;(Introduction to Asset Allocation)[&lt;a href=&#34;https://www.r-bloggers.com/introduction-to-asset-allocation/&#34; class=&#34;uri&#34;&gt;https://www.r-bloggers.com/introduction-to-asset-allocation/&lt;/a&gt;]&lt;/p&gt;
&lt;p&gt;We used NAV data for an ETF (iShares) that is linked to the following index&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S&amp;amp;P500&lt;/li&gt;
&lt;li&gt;NASDAQ100&lt;/li&gt;
&lt;li&gt;MSCI Emerging Markets&lt;/li&gt;
&lt;li&gt;Russell 2000&lt;/li&gt;
&lt;li&gt;MSCI EAFE&lt;/li&gt;
&lt;li&gt;US 20 Year Treasury(the Barclays Capital 20+ Year Treasury Index)&lt;/li&gt;
&lt;li&gt;U.S. Real Estate(the Dow Jones US Real Estate Index)&lt;/li&gt;
&lt;li&gt;gold bullion market&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first step is to collect data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(quantmod)

#**************************
# ★8 ASSETS SIMULATION
# SPY - S&amp;amp;P 500 
# QQQ - Nasdaq 100
# EEM - Emerging Markets
# IWM - Russell 2000
# EFA - EAFE
# TLT - 20 Year Treasury
# IYR - U.S. Real Estate
# GLD - Gold
#**************************

# load historical prices from Yahoo Finance
symbol.names = c(&amp;quot;S&amp;amp;P 500&amp;quot;,&amp;quot;Nasdaq 100&amp;quot;,&amp;quot;Emerging Markets&amp;quot;,&amp;quot;Russell 2000&amp;quot;,&amp;quot;EAFE&amp;quot;,&amp;quot;20 Year Treasury&amp;quot;,&amp;quot;U.S. Real Estate&amp;quot;,&amp;quot;Gold&amp;quot;)
symbols = c(&amp;quot;SPY&amp;quot;,&amp;quot;QQQ&amp;quot;,&amp;quot;EEM&amp;quot;,&amp;quot;IWM&amp;quot;,&amp;quot;EFA&amp;quot;,&amp;quot;TLT&amp;quot;,&amp;quot;IYR&amp;quot;,&amp;quot;GLD&amp;quot;)
getSymbols(symbols, from = &amp;#39;1980-01-01&amp;#39;, auto.assign = TRUE)

#gn dates for all symbols &amp;amp; convert to monthly
hist.prices = merge(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)
month.ends = endpoints(hist.prices, &amp;#39;day&amp;#39;)
hist.prices = Cl(hist.prices)[month.ends, ]
colnames(hist.prices) = symbols

# remove any missing data
hist.prices = na.omit(hist.prices[&amp;#39;1995::&amp;#39;])

# compute simple returns
hist.returns = na.omit( ROC(hist.prices, type = &amp;#39;discrete&amp;#39;) )

# compute historical returns, risk, and correlation
ia = list()
ia$expected.return = apply(hist.returns, 2, mean, na.rm = T)
ia$risk = apply(hist.returns, 2, sd, na.rm = T)
ia$correlation = cor(hist.returns, use = &amp;#39;complete.obs&amp;#39;, method = &amp;#39;pearson&amp;#39;)

ia$symbols = symbols
ia$symbol.names = symbol.names
ia$n = length(symbols)
ia$hist.returns = hist.returns

# convert to annual, year = 12 months
annual.factor = 12
ia$expected.return = annual.factor * ia$expected.return
ia$risk = sqrt(annual.factor) * ia$risk

rm(SPY,QQQ,EEM,IWM,EFA,TLT,IYR,GLD)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s how the returns are plotted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(hist.returns, main = &amp;quot;Performance summary&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Next, we’ll code the backtest. We’ll publish the code all at once.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# BACK TEST
backtest &amp;lt;- function(r_dat,FLG,start_date,span,learning_term,port){
  #-----------------------------------------
  # BACKTEST
  # r_dat - return data(xts object) 
  # FLG - flag(CCC,DCC,DECO)
  # start_date - start date for backtest
  # span - rebalance frequency
  # learning_term - learning term (days)
  # port - method of portfolio optimization
  #-----------------------------------------
  
  library(stringi)

  initial_dat &amp;lt;- r_dat[stri_c(as.Date(start_date)-learning_term,&amp;quot;::&amp;quot;,as.Date(start_date))]
  for (i in NROW(initial_dat):NROW(r_dat)) {
    if (i == NROW(initial_dat)){
      H &amp;lt;- carroll(initial_dat[1:(NROW(initial_dat)-1),],FLG)
      if (port == &amp;quot;nlgmv&amp;quot;){
        result &amp;lt;- nlgmv(initial_dat,H)
      }else if (port == &amp;quot;risk parity&amp;quot;){
        result &amp;lt;- risk_parity(initial_dat,H)
      }
      weight &amp;lt;- t(result$weight)
      colnames(weight) &amp;lt;- colnames(initial_dat)
      p_return &amp;lt;- initial_dat[NROW(initial_dat),]*result$weight
    } else {
      if (i %in% endpoints(r_dat,span)){
        H &amp;lt;- carroll(test_dat[1:(NROW(test_dat)-1),],FLG)
        if (port == &amp;quot;nlgmv&amp;quot;){
          result &amp;lt;- nlgmv(test_dat,H)
        }else if (port == &amp;quot;risk parity&amp;quot;){
          result &amp;lt;- risk_parity(test_dat,H)
        }
        
      }
      weight &amp;lt;- rbind(weight,t(result$weight))
      p_return &amp;lt;- rbind(p_return,test_dat[NROW(test_dat),]*result$weight)
    }
    if (i != NROW(r_dat)){
      term &amp;lt;- stri_c(index(r_dat[i+1,])-learning_term,&amp;quot;::&amp;quot;,index(r_dat[i+1,])) 
      test_dat &amp;lt;- r_dat[term]
    }
  }
  p_return$portfolio &amp;lt;- xts(apply(p_return,1,sum),order.by = index(p_return))
  weight.xts &amp;lt;- xts(weight,order.by = index(p_return))

  result &amp;lt;- list(p_return,weight.xts)
  names(result) &amp;lt;- c(&amp;quot;return&amp;quot;,&amp;quot;weight&amp;quot;)
  return(result)
}

CCC &amp;lt;- backtest(hist.returns,&amp;quot;CCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DCC &amp;lt;- backtest(hist.returns,&amp;quot;DCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DECO &amp;lt;- backtest(hist.returns,&amp;quot;DECO&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
benchmark &amp;lt;- backtest(hist.returns,&amp;quot;benchmark&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)

result &amp;lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)
colnames(result) &amp;lt;- c(&amp;quot;CCC&amp;quot;,&amp;quot;DCC&amp;quot;,&amp;quot;DECO&amp;quot;,&amp;quot;benchmark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a graph of the calculation results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result,main = &amp;quot;BACKTEST&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I did not use the minimum variance portfolio defined above because I imposed a short sale constraint. Apparently it’s difficult to solve analytically with this alone, so I’m going to solve numerically. I used a weekly rebalancing period, so it took me a while to calculate the results on my own PC, but I was able to calculate the results.&lt;/p&gt;
&lt;p&gt;Since Lehman, it appears to have outperformed the benchmark equal weighted portfolio. In particular, DECO is looking good. I would like to rethink the meaning of Equicorrelation. The change in each incorporation ratio is as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot allocation weighting
d_allocation &amp;lt;- function(ggweight,title){
  #install.packages(&amp;quot;tidyverse&amp;quot;)
  library(tidyverse)
  ggweight &amp;lt;- gather(ggweight,key=ASSET,value=weight,-Date,-method)
  ggplot(ggweight, aes(x=Date, y=weight,fill=ASSET)) +
    geom_area(colour=&amp;quot;black&amp;quot;,size=.1) +
    scale_y_continuous(limits = c(0,1)) +
    labs(title=title) + facet_grid(method~.)
}

gmv_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)))

# plot allocation weighting
d_allocation(gmv_weight,&amp;quot;GMV Asset Allocation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;They seem to have increased their allocation to TLT, or U.S. Treasuries, during Lehman, while CCC and DCC have a high allocation to U.S. Treasuries in other areas as well, and the often-cited problem of a minimally diversified portfolio seems to be occurring here as well. On the other hand, DECO has a unique mix ratio, and I think we need to go back and read the paper again.&lt;/p&gt;
&lt;p&gt;PS（2019/3/3）
So far, I’ve been doing the analysis with a minimum variance portfolio, but I also wanted to see the results of risk parity, so I wrote the code for that as well.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;risk_parity &amp;lt;- function(r_dat,r_cov){
  fn &amp;lt;- function(weight, r_cov) {
    N &amp;lt;- NROW(r_cov)
    risks &amp;lt;-  weight * (r_cov %*% weight)
    g &amp;lt;- rep(risks, times = N) - rep(risks, each = N)
  return(sum(g^2))
  }
  dfn &amp;lt;- function(weight,r_cov){
    out &amp;lt;- weight
    for (i in 0:length(weight)) {
      up &amp;lt;- dn &amp;lt;- weight
      up[i] &amp;lt;- up[i]+.0001
      dn[i] &amp;lt;- dn[i]-.0001
      out[i] = (fn(up,r_cov) - fn(dn,r_cov))/.0002
    }
    return(out)
  }
  std &amp;lt;- sqrt(diag(r_cov)) 
  x0 &amp;lt;- 1/std/sum(1/std)
  res &amp;lt;- nloptr::nloptr(x0=x0,
                 eval_f=fn,
                 eval_grad_f=dfn,
                 eval_g_eq=function(weight,r_cov) { sum(weight) - 1 },
                 eval_jac_g_eq=function(weight,r_cov) { rep(1,length(std)) },
                 lb=rep(0,length(std)),ub=rep(1,length(std)),
                 opts = list(&amp;quot;algorithm&amp;quot;=&amp;quot;NLOPT_LD_SLSQP&amp;quot;,&amp;quot;print_level&amp;quot; = 0,&amp;quot;xtol_rel&amp;quot;=1.0e-8,&amp;quot;maxeval&amp;quot; = 1000),
                 r_cov = r_cov)
  r_weight &amp;lt;- res$solution
  names(r_weight) &amp;lt;- colnames(r_cov)
  wr_dat &amp;lt;- r_dat*r_weight
  portfolio &amp;lt;- apply(wr_dat,1,sum)
  pr_dat &amp;lt;- data.frame(wr_dat,portfolio)
  sd &amp;lt;- sd(portfolio)
  result &amp;lt;- list(r_weight,pr_dat,sd)
  names(result) &amp;lt;- c(&amp;quot;weight&amp;quot;,&amp;quot;return&amp;quot;,&amp;quot;portfolio risk&amp;quot;)
  return(result)
  }

CCC &amp;lt;- backtest(hist.returns,&amp;quot;CCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DCC &amp;lt;- backtest(hist.returns,&amp;quot;DCC&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
DECO &amp;lt;- backtest(hist.returns,&amp;quot;DECO&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)
benchmark &amp;lt;- backtest(hist.returns,&amp;quot;benchmark&amp;quot;,&amp;quot;2007-01-04&amp;quot;,&amp;quot;months&amp;quot;,365,&amp;quot;risk parity&amp;quot;)

result &amp;lt;- merge(CCC$return$portfolio,DCC$return$portfolio,DECO$return$portfolio,benchmark$return$portfolio)
colnames(result) &amp;lt;- c(&amp;quot;CCC&amp;quot;,&amp;quot;DCC&amp;quot;,&amp;quot;DECO&amp;quot;,&amp;quot;benchmark&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the result.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result, main = &amp;quot;BACKTEST COMPARISON&amp;quot;)

library(plotly)

# plot allocation weighting
riskparity_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=&amp;quot;benchmark&amp;quot;,Date=index(benchmark$weight)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PerformanceAnalytics::charts.PerformanceSummary(result, main = &amp;quot;BACKTEST COMPARISON&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;riskparity_weight &amp;lt;- rbind(data.frame(CCC$weight,method=&amp;quot;CCC&amp;quot;,Date=index(CCC$weight)),data.frame(DCC$weight,method=&amp;quot;DCC&amp;quot;,Date=index(DCC$weight)),data.frame(DECO$weight,method=&amp;quot;DECO&amp;quot;,Date=index(DECO$weight)),data.frame(benchmark$weight,method=&amp;quot;benchmark&amp;quot;,Date=index(benchmark$weight)))

# plot allocation weighting
d_allocation(riskparity_weight, &amp;quot;Risk Parity Asset Allocation&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are positive, with all methods outperforming benchmark.
As expected, the estimation of the variance-covariance matrix seems to be performing well. The good performance of DECO may also be due to the fact that it uses the average of the correlation coefficients of each asset pair in the correlation matrix, which resulted in a greater inclusion of risk assets than the other methods. The weights are as follows&lt;/p&gt;
&lt;p&gt;That’s it for today, for now.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Gaussian regression.</title>
      <link>/en/post/post1/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/en/post/post1/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;index_files/pagedtable/css/pagedtable.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/pagedtable/js/pagedtable.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-gpr&#34;&gt;1. What is &lt;code&gt;GPR&lt;/code&gt;?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-of-the-gpr&#34;&gt;2. Implementation of the `GPR’&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;　Hi. Yesterday, I wrote an article about &lt;code&gt;Bayesian Vector Autoregression&lt;/code&gt;.
　In the article, the topic of hyperparameter tuning came up, and looking for some efficient way to tune it, I found &lt;code&gt;Bayesian Optimization&lt;/code&gt;. Since I am planning to use machine learning methods in daily GDP, I thought that &lt;code&gt;Bayesian Optimization&lt;/code&gt; could be quite useful, and I spent all night yesterday to understand it.&lt;br /&gt;
　I will implement it here, but &lt;code&gt;Bayesian Optimization&lt;/code&gt; uses &lt;code&gt;Gaussian Pocess Regression&lt;/code&gt; (&lt;code&gt;GPR&lt;/code&gt;), and my motivation for writing this entry was to implement it first. I will write about the implementation of &lt;code&gt;Bayesian Optimization&lt;/code&gt; after this entry.&lt;/p&gt;
&lt;div id=&#34;what-is-gpr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. What is &lt;code&gt;GPR&lt;/code&gt;?&lt;/h2&gt;
&lt;p&gt;　The &lt;code&gt;GRP&lt;/code&gt; is, simply put, &lt;strong&gt;a type of nonlinear regression method using Bayesian estimation&lt;/strong&gt;. Although the model itself is linear, it is characterized by its ability to estimate &lt;strong&gt;infinite nonlinear transformations of input variables using a kernel trick&lt;/strong&gt; as explanatory variables (depending on what you choose for the kernel).
　The &lt;code&gt;GPR&lt;/code&gt; assumes that &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; input and teacher data are available for training, and the &lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt; of input data are also available. From this situation, we can predict the &lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt;th teacher data.&lt;br /&gt;
　The data contains noise and follows the following probability model.
　
&lt;span class=&#34;math display&#34;&gt;\[
t_{i} = y_{i} + \epsilon_{i}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th observable teacher data (scalar), &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; is the unobservable output data (scalar), and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{i}\)&lt;/span&gt; follows a normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(0, \beta^{-1})\)&lt;/span&gt; with measurement error. &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; follows the following probability model.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle y_{i}  = \textbf{w}^{T}\phi(x_{i})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt; is the ith input data vector, &lt;span class=&#34;math inline&#34;&gt;\(\phi(・)\)&lt;/span&gt; is the non-linear function and &lt;span class=&#34;math inline&#34;&gt;\(\bf{w}^{T}\)&lt;/span&gt; is the weight coefficient (regression coefficient) vector for each input data. As a nonlinear function, I assume &lt;span class=&#34;math inline&#34;&gt;\(\psi(x_{i}) = (x_{1,i}, x_{1,i}^{2},... ,x_{1,i}x_{2,i},...)\)&lt;/span&gt;. (&lt;span class=&#34;math inline&#34;&gt;\(x_{1,i}\)&lt;/span&gt; is the first variable in the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th input data &lt;span class=&#34;math inline&#34;&gt;\(x_{i}\)&lt;/span&gt;). The conditional probability of obtaining &lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt; from the probabilistic model of the teacher data, with the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th output data &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; obtained, is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 p(t_{i}|y_{i}) = N(t_{i}|y_{i},\beta^{-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{t} = (t_{1},... ,t_{n})^{T}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y} = (y_{1},... ,y_{n})^{T}\)&lt;/span&gt;, then by extending the above equation, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle p(\textbf{t}|\textbf{y}) = N(\textbf{t}|\textbf{y},\beta^{-1}\textbf{I}_{N})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We assume that the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{w}\)&lt;/span&gt; as a prior distribution is 0, and all variances are &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. We also assume that &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt; follows a Gaussian process. A Gaussian process is one where the simultaneous distribution of &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt; follows a multivariate Gaussian distribution. In code, it looks like this&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define Kernel function
Kernel_Mat &amp;lt;- function(X,sigma,beta){
  N &amp;lt;- NROW(X)
  K &amp;lt;- matrix(0,N,N)
  for (i in 1:N) {
    for (k in 1:N) {
      if(i==k) kdelta = 1 else kdelta = 0
      K[i,k] &amp;lt;- K[k,i] &amp;lt;- exp(-t(X[i,]-X[k,])%*%(X[i,]-X[k,])/(2*sigma^2)) + beta^{-1}*kdelta
    }
  }
  return(K)
}

N &amp;lt;- 10 # max value of X
M &amp;lt;- 1000 # sample size
X &amp;lt;- matrix(seq(1,N,length=M),M,1) # create X
testK &amp;lt;- Kernel_Mat(X,0.5,1e+18) # calc kernel matrix

library(MASS)

P &amp;lt;- 6 # num of sample path
Y &amp;lt;- matrix(0,M,P) # define Y

for(i in 1:P){
  Y[,i] &amp;lt;- mvrnorm(n=1,rep(0,M),testK) # sample Y
}

# Plot
matplot(x=X,y=Y,type = &amp;quot;l&amp;quot;,lwd = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;　The covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; between the elements of &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y_{i} = \textbf{w}^{T}\phi(x_{i})\)&lt;/span&gt; is calculated using the kernel method from the input &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. Then, from this &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; and average 0, we generate six series of multivariate normal random numbers and plot them.As these series are computed from a covariance matrix, we model that &lt;strong&gt;the more positive the covariance of each element, the more likely they are to be the same&lt;/strong&gt;. Also, as you can see in the graphs, the graphs are very smooth and very flexible in their representation. The code samples and plots 1000 input points, limiting the input to 0 to 10 due to computational cost, but in principle, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is defined in the real number space, so &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; follows an infinite dimensional multivariate normal distribution.&lt;/p&gt;
&lt;p&gt;As described above, since &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt; is assumed to follow a Gaussian process, &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; follows a multivariate normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(\textbf{y}|0,K)\)&lt;/span&gt; with simultaneous probability &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; averaging 0 and the variance covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. Each element &lt;span class=&#34;math inline&#34;&gt;\(K_{i,j}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
K_{i,j} &amp;amp;=&amp;amp; cov[y_{i},y_{j}] = cov[\textbf{w}\phi(x_{i}),\textbf{w}\phi(x_{j})] \\
&amp;amp;=&amp;amp;\phi(x_{i})\phi(x_{j})cov[\textbf{w},\textbf{w}]=\phi(x_{i})\phi(x_{j})\alpha
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\phi(x_{j})\alpha\)&lt;/span&gt; is more expensive &lt;strong&gt;as the dimensionality of the &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\)&lt;/span&gt; is increased&lt;/strong&gt; (i.e., the more non-linear transformation is applied, the less the calculation is completed). However, when the kernel function &lt;span class=&#34;math inline&#34;&gt;\(k(x,x&amp;#39;)\)&lt;/span&gt; is used, the computational complexity is higher in the dimensions of the sample size of the input data &lt;span class=&#34;math inline&#34;&gt;\(x_{i},x_{j}\)&lt;/span&gt;, so the computation becomes easier. There are several types of kernel functions, but the following Gaussian kernels are commonly used.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
k(x,x&amp;#39;) = a \exp(-b(x-x&amp;#39;)^{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now that we have defined the concurrent probability of &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{y}\)&lt;/span&gt;, we can find the joint probability of &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle \textbf{t}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\displaystyle p(\textbf{t}) &amp;amp;=&amp;amp; \int p(\textbf{t}|\textbf{y})p(\textbf{y}) d\textbf{y} \\
 \displaystyle &amp;amp;=&amp;amp; \int N(\textbf{t}|\textbf{y},\beta^{-1}\textbf{I}_{N})N(\textbf{y}|0,K)d\textbf{y} \\
 &amp;amp;=&amp;amp; N(\textbf{y}|0,\textbf{C}_{N})
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\textbf{C}_{N} = K + \beta^{-1}\beta^{I}_{N}\)&lt;/span&gt;. Note that the last expression expansion uses the regenerative nature of the normal distribution (the proof can be easily derived from the moment generating function of the normal distribution). The point is just to say that the covariance is the sum of the covariances of the two distributions, since they are independent. Personally, I imagine that &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; is the prior distribution of the Gaussian process I just described, &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t}|\textbf{y})\)&lt;/span&gt; is the likelihood function, and &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t})\)&lt;/span&gt; is the posterior distribution. The only constraint on the prior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{y})\)&lt;/span&gt; is that it is smooth with a loosely constrained distribution.
The joint probability of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; observable teacher data &lt;span class=&#34;math inline&#34;&gt;\(\textbf{t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_{N+1}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 p(\textbf{t},t_{N+1}) = N(\textbf{t},t_{N+1}|0,\textbf{C}_{N+1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\textbf{C}_{N+1}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
 \textbf{C}_{N+1} = \left(
    \begin{array}{cccc}
      \textbf{C}_{N} &amp;amp; \textbf{k} \\
      \textbf{k}^{T} &amp;amp; c \\
    \end{array}
  \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\textbf{k} = (k(x_{1},x_{N+1}),...,k(x_{N},x_{N+1}))\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = k(x_{N+1},x_{N+1})\)&lt;/span&gt;. The conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(t_{N+1}|\textbf{t})\)&lt;/span&gt; can be obtained from the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(\textbf{t}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_{N+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(t_{N+1}|\textbf{t}) = N(t_{N+1}|\textbf{k}^{T}\textbf{C}_{N+1}^{-1}\textbf{t},c-\textbf{k}^{T}\textbf{C}_{N+1}^{-1}\textbf{k})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In calculating the conditional distribution, we use &lt;a href=&#34;https://qiita.com/kilometer/items/34249479dc2ac3af5706&#34;&gt;Properties of the conditional multivariate normal distribution&lt;/a&gt;. As you can see from the above equation, the conditional distribution &lt;span class=&#34;math inline&#34;&gt;\(p(t_{N+1}|\textbf{t})\)&lt;/span&gt; can be calculated if &lt;span class=&#34;math inline&#34;&gt;\(N+1\)&lt;/span&gt; input data, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; teacher data, and parameters &lt;span class=&#34;math inline&#34;&gt;\(a,b\)&lt;/span&gt; of the kernel function are known, so if any point is given as input data, it is possible to approximate the Generating Process. The nice thing about the &lt;code&gt;GPR&lt;/code&gt; is that it gives predictions without the direct estimation of the above defined probabilistic model &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle y_{i} = \textbf{w}^{T}\phi(x_{i})\)&lt;/span&gt;. The stochastic model has &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\)&lt;/span&gt;, which converts the input data to a high-dimensional vector through a nonlinear transformation. Therefore, the higher the dimensionality, the larger the computational complexity of the &lt;span class=&#34;math inline&#34;&gt;\(\phi(x_{i})\phi(x_{j})\alpha\)&lt;/span&gt; will be, but the &lt;code&gt;GPR&lt;/code&gt; uses a kernel trick, so the computational complexity of the sample size dimension of the input data vector will be sufficient.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation-of-the-gpr&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Implementation of the `GPR’&lt;/h2&gt;
&lt;p&gt;　For now, let’s implement this in &lt;code&gt;R&lt;/code&gt;, which I’ve implemented in PRML test data, so I tweaked it.
　
&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(grid)

# 1.Gaussian Process Regression

# PRML&amp;#39;s synthetic data set
curve_fitting &amp;lt;- data.frame(
  x=c(0.000000,0.111111,0.222222,0.333333,0.444444,0.555556,0.666667,0.777778,0.888889,1.000000),
  t=c(0.349486,0.830839,1.007332,0.971507,0.133066,0.166823,-0.848307,-0.445686,-0.563567,0.261502))

f &amp;lt;- function(beta, sigma, xmin, xmax, input, train) {
  kernel &amp;lt;- function(x1, x2) exp(-(x1-x2)^2/(2*sigma^2)); # define Kernel function
  K &amp;lt;- outer(input, input, kernel); # calc gram matrix
  C_N &amp;lt;- K + diag(length(input))/beta
  m &amp;lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train) # coditiona mean 
  m_sig &amp;lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel)))) #conditional variance
  x &amp;lt;- seq(xmin,xmax,length=100)
  output &amp;lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),
                              tx=input,ty=train),
                   aes(x=x1,y=m)) + 
    geom_line() +
    geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +
    geom_point(aes(x=tx,y=ty))
  return(output)
}

grid.newpage() # make a palet
pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2
print(f(100,0.1,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))
print(f(4,0.10,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2))
print(f(25,0.30,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))
print(f(25,0.030,0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(beta^{-1}\)&lt;/span&gt; represents the measurement error. The higher the value of &lt;strong&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (i.e., the smaller the measurement error), the easier it is to overfit, since the error of the predictions is less than that of the data already available.&lt;/strong&gt; This is the case in the top left corner of the figure above. The top left corner is &lt;span class=&#34;math inline&#34;&gt;\(\beta=400\)&lt;/span&gt;, which means that it overfits the current data available. Conversely, a small value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; will produce predictions that ignore the errors with the teacher data, but may improve the generalization performance. The top right figure shows this. For &lt;span class=&#34;math inline&#34;&gt;\(beta=4\)&lt;/span&gt;, the average barely passes through the data points we have, and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is currently available. &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; represents the magnitude of the effect of the data we have at the moment on the surroundings. If &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is small, the adjacent points will interact strongly with each other, which may reduce the accuracy but increase the generalization performance. Conversely, if &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is large, the result will be unnatural, fitting only individual points. This is illustrated in the figure below right (&lt;span class=&#34;math inline&#34;&gt;\(b=\frac{1}{0.03}, \beta=25\)&lt;/span&gt;). As you can see, the graph is overfitting because of the large &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; and because &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is also large, so it fits only individual points, resulting in an absurdly large graph. The bottom left graph is the best. It has &lt;span class=&#34;math inline&#34;&gt;\(b=\frac{1}{0.3}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(b=2\)&lt;/span&gt;. Let’s try extending the x interval of this graph to [0,2]. Then we get the following graph.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid.newpage() # make a palet
pushViewport(viewport(layout=grid.layout(2, 2))) # divide the palet into 2 by 2
print(f(100,0.1,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1)) 
print(f(4,0.10,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=2)) 
print(f(25,0.30,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=1))
print(f(25,0.030,0,2,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=2, layout.pos.col=2)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see, all the graphs except the bottom left one have a band of 95% confidence intervals that immediately widen and are completely useless where there are no data points. On the other hand, the lower left graph has a decent band up to 1.3 to 1.4, and the average value seems to pass through a point that is consistent with our intuitive understanding of the function. You can also see that if you are too far away from the observable data points, you will get a normal distribution with a mean of 0 and a variance of 1 no matter what you give to the parameters.
Now that we have shown that the accuracy of the prediction of the out-sample varies depending on the value of the parameters, the question here is how to estimate these hyperparameters. This is done by using the gradient method to find the hyperparameters that maximize the log-likelihood function &lt;span class=&#34;math inline&#34;&gt;\(\ln p(\bf{t}|a,b)\)&lt;/span&gt; ((&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; seems to be of a slightly different type, and the developmental discussion appears to take other tuning methods. We haven’t gotten to that level yet, so we’ll calibrate it here). Since &lt;span class=&#34;math inline&#34;&gt;\(p(\textbf{t}) = N(\textbf{y}|0, \textbf{C}_{N})\)&lt;/span&gt;, the log-likelihood function is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \ln p(\textbf{t}|a,b,\beta) = -\frac{1}{2}\ln|\textbf{C}_{N}| - \frac{N}{2}\ln(2\pi) - \frac{1}{2}\textbf{t}^{T}\textbf{C}_{N}^{-1}\textbf{k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After that, we can differentiate this with the parameters and solve the obtained simultaneous equations to get the maximum likelihood estimator. Now let’s get the derivatives.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial}{\partial \theta_{i}} \ln p(\textbf{t}|\theta) = -\frac{1}{2}Tr(\textbf{C}_{N}^{-1}\frac{\partial \textbf{C}_{N}}{\partial \theta_{i}}) + \frac{1}{2}\textbf{t}^{T}\textbf{C}_{N}^{-1}
\frac{\partial\textbf{C}_{N}}{\partial\theta_{i}}\textbf{C}_{N}^{-1}\textbf{t}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(theta\)&lt;/span&gt; is the parameter set and &lt;span class=&#34;math inline&#34;&gt;\(theta_{i}\)&lt;/span&gt; represents the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;th parameter. If you don’t understand this derivative &lt;a href=&#34;http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20%20-%20Springer%20Springer%20%202006.pdf&#34;&gt;here&lt;/a&gt; in the supplement to (C.21) and (C.22) equations. Since we are using the Gaussian kernel in this case, we get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\displaystyle \frac{\partial k(x,x&amp;#39;)}{\partial a} = \exp(-b(x-x&amp;#39;)^{2}) \\
\displaystyle \frac{\partial k(x,x&amp;#39;)}{\partial b} = -a(x-x&amp;#39;)^{2}\exp(-b(x-x&amp;#39;)^{2})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;from the above formula. However, this time we will use the gradient method to find the best parameters. Here’s the code for the implementation (it’s pretty much a lost cause).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g &amp;lt;- function(xmin, xmax, input, train){
  # initial value
  beta = 100
  b = 1
  a = 1
  learning_rate = 0.1
  itermax &amp;lt;- 1000
  if (class(input) == &amp;quot;numeric&amp;quot;){
    N &amp;lt;- length(input)
  } else
  {
    N &amp;lt;- NROW(input)
  }
  kernel &amp;lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2); # define kernel
  derivative_a &amp;lt;- function(x1,x2) exp(-0.5*b*(x1-x2)^2)
  derivative_b &amp;lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)
  dloglik_a &amp;lt;- function(C_N,y,x1,x2) {
    -sum(diag(solve(C_N)%*%outer(input, input, derivative_a)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_a)%*%solve(C_N)%*%y 
  }
  dloglik_b &amp;lt;- function(C_N,y,x1,x2) {
    -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y 
  }
  # loglikelihood function
  likelihood &amp;lt;- function(b,a,x,y){
    kernel &amp;lt;- function(x1, x2) a*exp(-0.5*b*(x1-x2)^2)
    K &amp;lt;- outer(x, x, kernel)
    C_N &amp;lt;- K + diag(N)/beta
    itermax &amp;lt;- 1000
    l &amp;lt;- -1/2*log(det(C_N)) - N/2*(2*pi) - 1/2*t(y)%*%solve(C_N)%*%y
    return(l)
  }
  K &amp;lt;- outer(input, input, kernel) 
  C_N &amp;lt;- K + diag(N)/beta
  for (i in 1:itermax){
    kernel &amp;lt;- function(x1, x2) a*exp(-b*(x1-x2)^2)
    derivative_b &amp;lt;- function(x1,x2) -0.5*a*(x1-x2)^2*exp(-0.5*b*(x1-x2)^2)
    dloglik_b &amp;lt;- function(C_N,y,x1,x2) {
      -sum(diag(solve(C_N)%*%outer(input, input, derivative_b)))+t(y)%*%solve(C_N)%*%outer(input, input, derivative_b)%*%solve(C_N)%*%y 
    }
    K &amp;lt;- outer(input, input, kernel) # calc gram matrix
    C_N &amp;lt;- K + diag(N)/beta
    l &amp;lt;- 0
    if(abs(l-likelihood(b,a,input,train))&amp;lt;0.0001&amp;amp;i&amp;gt;2){
      break
    }else{
      a &amp;lt;- as.numeric(a + learning_rate*dloglik_a(C_N,train,input,input))
      b &amp;lt;- as.numeric(b + learning_rate*dloglik_b(C_N,train,input,input))
    }
    l &amp;lt;- likelihood(b,a,input,train)
  }
  K &amp;lt;- outer(input, input, kernel)
  C_N &amp;lt;- K + diag(length(input))/beta
  m &amp;lt;- function(x) (outer(x, input, kernel) %*% solve(C_N) %*% train)  
  m_sig &amp;lt;- function(x)(kernel(x,x) - diag(outer(x, input, kernel) %*% solve(C_N) %*% t(outer(x, input, kernel))))
  x &amp;lt;- seq(xmin,xmax,length=100)
  output &amp;lt;- ggplot(data.frame(x1=x,m=m(x),sig1=m(x)+1.96*sqrt(m_sig(x)),sig2=m(x)-1.96*sqrt(m_sig(x)),
                              tx=input,ty=train),
                   aes(x=x1,y=m)) + 
    geom_line() +
    geom_ribbon(aes(ymin=sig1,ymax=sig2),alpha=0.2) +
    geom_point(aes(x=tx,y=ty))
  return(output)
}

print(g(0,1,curve_fitting$x,curve_fitting$t), vp=viewport(layout.pos.row=1, layout.pos.col=1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes, it does sound like good (lol).
That’s it for today, for now.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
