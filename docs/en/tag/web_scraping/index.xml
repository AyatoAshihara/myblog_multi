<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Web_scraping | 東京の資産運用会社で働く社会人が研究に没頭するブログ</title>
    <link>/en/tag/web_scraping/</link>
      <atom:link href="/en/tag/web_scraping/index.xml" rel="self" type="application/rss+xml" />
    <description>Web_scraping</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 19 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Web_scraping</title>
      <link>/en/tag/web_scraping/</link>
    </image>
    
    <item>
      <title>Get macro panel data from OECD.org via API</title>
      <link>/en/post/post22/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post22/</guid>
      <description>
&lt;script src=&#34;../../../en/post/post22/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#oecd.stat-web-api&#34;&gt;1.OECD.Stat Web API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pandasdmx&#34;&gt;2.pandasdmx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;3.implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#another-matter&#34;&gt;4. Another matter…&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. There are several ways to collect macroeconomic data, but collecting data for each country can be a challenge. However, you can automate the tedious process of collecting data from the OECD via API. Today, I will introduce the method.&lt;/p&gt;
&lt;div id=&#34;oecd.stat-web-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.OECD.Stat Web API&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;OECD.org&lt;/code&gt; offers a service called &lt;a href=&#34;https://stats.oecd.org/&#34;&gt;OECD.Stat&lt;/a&gt;, which provides a variety of economic data for OECD and certain non-member countries. You can also download the csv data manually by going to the website. However, you only need to use &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;R&lt;/code&gt; to do this since OECD provides a web API.&lt;/p&gt;
&lt;p&gt;&lt;Specifics of the OECD implementation&gt;&lt;/p&gt;
&lt;p&gt;Below is a list of implementation details for specific OECD REST SDMX interfaces at this time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Only anonymous queries are supported and there is no authentication.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each response is limited to 1,000,000 observations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The maximum length of the request URL is 1000 characters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cross-origin requests are supported in the &lt;code&gt;CORS&lt;/code&gt; header (see &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/cors/&#34;&gt;here&lt;/a&gt; for more information about &lt;code&gt;CORS&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Errors are not returned in the results, but HTTP status codes and messages are set according to the Web Service Guidelines.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a nonexistent dataset is requested, &lt;code&gt;401 Unauthorized&lt;/code&gt; is returned.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The source (or Agency ID) parameter of the &lt;code&gt;REST&lt;/code&gt; query is required, but the &lt;code&gt;ALL&lt;/code&gt; keyword is supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Versioning is not supported: the latest implementation version is always used.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sorting of data is not supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;lastNObservations&lt;/code&gt; parameter is not supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even when &lt;code&gt;dimensionAtObservation=AllDimensions&lt;/code&gt; is used, the observations follow a chronological (or import-specific) order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Searching for reference metadata is not supported at this time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pandasdmx&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.pandasdmx&lt;/h2&gt;
&lt;p&gt;The Web API is provided in the form of &lt;code&gt;sdmx-json&lt;/code&gt;. There is a useful package for using it in &lt;code&gt;Python&lt;/code&gt;, which is called &lt;code&gt;pandasdmx&lt;/code&gt;. Here’s how to download the data.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import &lt;code&gt;pandasdmx&lt;/code&gt;, pass &lt;code&gt;OECD&lt;/code&gt; to &lt;code&gt;Request&lt;/code&gt; method as an argument and create &lt;code&gt;api.Request&lt;/code&gt; object.&lt;/li&gt;
&lt;li&gt;Pass the query condition to the data method of the &lt;code&gt;api.Request&lt;/code&gt; object, and download the data of &lt;code&gt;sdmx-json&lt;/code&gt; format from OECD.org.&lt;/li&gt;
&lt;li&gt;Format the downloaded data into a &lt;code&gt;pandas&lt;/code&gt; data frame with the method &lt;code&gt;to_pandas()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.implementation&lt;/h2&gt;
&lt;p&gt;Let’s do this in practice. What we’ll get is the &lt;code&gt;Revisions Analysis Dataset -- Infra-annual Economic Indicators&lt;/code&gt;. We have access to all data, including revisions to the Monthly Ecnomic Indicator (MEI), one of the OECD datasets, so we can check key economic variables &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; from preliminary data at initial release to revised, finalized data. The dataset provides a snapshot of major economic indicators, at monthly intervals beginning in February 1999, that were previously available. In other words, the dataset allows us to build predictive models based on the data available at each point in time. The most recent data is useful, but it is preliminary and therefore subject to uncertainty. The problem is that this situation cannot be replicated when backtesting, and the analysis is often done under a better environment than the actual operation. This is the so-called &lt;code&gt;Jagged Edge&lt;/code&gt; problem. I think this dataset is very useful because it can reproduce the actual operation situation. This time, you will get the following data items.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Indicators&lt;/th&gt;
&lt;th&gt;Statistical ID&lt;/th&gt;
&lt;th&gt;Frequency&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Gross Domestic Product&lt;/td&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Index of Industrial Production&lt;/td&gt;
&lt;td&gt;201&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Retail Trade Volume&lt;/td&gt;
&lt;td&gt;202&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Monetary Aggregates&lt;/td&gt;
&lt;td&gt;601&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;International Trade in Goods&lt;/td&gt;
&lt;td&gt;702+703&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Balance of Payments&lt;/td&gt;
&lt;td&gt;701&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Employment&lt;/td&gt;
&lt;td&gt;502&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Harmonised Unemployment Rates&lt;/td&gt;
&lt;td&gt;501&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Hourly Earnings in Manufacturing&lt;/td&gt;
&lt;td&gt;503&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Early Estimates of Unit Labor Cost&lt;/td&gt;
&lt;td&gt;504&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Production of Construction&lt;/td&gt;
&lt;td&gt;203&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;First, we define the functions. The arguments are database ID, other IDs (country IDs and statistical IDs), data acquisition start point and end point.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandasdmx as sdmx&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## C:\Users\aashi\ANACON~1\lib\site-packages\pandasdmx\remote.py:13: RuntimeWarning: optional dependency requests_cache is not installed; cache options to Session() have no effect
##   RuntimeWarning,&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;oecd = sdmx.Request(&amp;#39;OECD&amp;#39;)
def resp_OECD(dsname,dimensions,start,end):
    dim_args = [&amp;#39;+&amp;#39;.join(d) for d in dimensions]
    dim_str = &amp;#39;.&amp;#39;.join(dim_args)
    resp = oecd.data(resource_id=dsname, key=dim_str + &amp;quot;/all?startTime=&amp;quot; + start + &amp;quot;&amp;amp;endTime=&amp;quot; + end)
    df = resp.to_pandas().reset_index()
    return(df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specify the dimension from which the data will be obtained. Below, (1) country, (2) statistical items, (3) time of acquisition, and (4) frequency are specified with a tuple.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dimensions = ((&amp;#39;USA&amp;#39;,&amp;#39;JPN&amp;#39;,&amp;#39;GBR&amp;#39;,&amp;#39;FRA&amp;#39;,&amp;#39;DEU&amp;#39;,&amp;#39;ITA&amp;#39;,&amp;#39;CAN&amp;#39;,&amp;#39;NLD&amp;#39;,&amp;#39;BEL&amp;#39;,&amp;#39;SWE&amp;#39;,&amp;#39;CHE&amp;#39;),(&amp;#39;201&amp;#39;,&amp;#39;202&amp;#39;,&amp;#39;601&amp;#39;,&amp;#39;702&amp;#39;,&amp;#39;703&amp;#39;,&amp;#39;701&amp;#39;,&amp;#39;502&amp;#39;,&amp;#39;503&amp;#39;,&amp;#39;504&amp;#39;,&amp;#39;203&amp;#39;),(&amp;quot;202001&amp;quot;,&amp;quot;202002&amp;quot;,&amp;quot;202003&amp;quot;,&amp;quot;202004&amp;quot;,&amp;quot;202005&amp;quot;,&amp;quot;202006&amp;quot;,&amp;quot;202007&amp;quot;,&amp;quot;202008&amp;quot;),(&amp;quot;M&amp;quot;,&amp;quot;Q&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s execute the function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;result = resp_OECD(&amp;#39;MEI_ARCHIVE&amp;#39;,dimensions,&amp;#39;2019-Q1&amp;#39;,&amp;#39;2020-Q2&amp;#39;)
result.count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOCATION       8266
## VAR            8266
## EDI            8266
## FREQUENCY      8266
## TIME_PERIOD    8266
## value          8266
## dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at the first few cases of data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;result.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   LOCATION  VAR     EDI FREQUENCY TIME_PERIOD  value
## 0      BEL  201  202001         M     2019-01  112.5
## 1      BEL  201  202001         M     2019-02  111.8
## 2      BEL  201  202001         M     2019-03  109.9
## 3      BEL  201  202001         M     2019-04  113.5
## 4      BEL  201  202001         M     2019-05  112.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that the data is stored in tidy form (long type). The most right value is stored as a value, and the other indexes are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LOCATION - Country&lt;/li&gt;
&lt;li&gt;VAR - Items&lt;/li&gt;
&lt;li&gt;EDI - At the time of acquisition (in the case of MEI_ARCHIVE)&lt;/li&gt;
&lt;li&gt;FREQUENCY - Frequency (monthly, quarterly, etc.)&lt;/li&gt;
&lt;li&gt;TIME_PERIOD - Reference point for statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the same `TIME_PERIOD` exists in rows with different EDIs. For example, above you can see the data for 2019-01~2019-05 available as of 2020/01 for the Belgian (BEL) Industrial Production Index (201). This is very much appreciated as it is provided in Long format, which is also easy to visualize and regress. Here’s a visualization of the industrial production index as it is updated.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

result = result[result[&amp;#39;FREQUENCY&amp;#39;]==&amp;#39;M&amp;#39;]
result[&amp;#39;TIME_PERIOD&amp;#39;] = pd.to_datetime(result[&amp;#39;TIME_PERIOD&amp;#39;],format=&amp;#39;%Y-%m&amp;#39;)
sns.relplot(data=result[lambda df: (df.VAR==&amp;#39;201&amp;#39;) &amp;amp; (pd.to_numeric(df.EDI) &amp;gt; 202004)],x=&amp;#39;TIME_PERIOD&amp;#39;,y=&amp;#39;value&amp;#39;,hue=&amp;#39;LOCATION&amp;#39;,kind=&amp;#39;line&amp;#39;,col=&amp;#39;EDI&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../../en/post/post22/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1056&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../../en/post/post22/index_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;2035&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While we can see that the line graphs are depressed as the economic damage from the corona increases, there have been subtle but significant revisions to the historical values from preliminary to confirmed. We can also see that there is a lag in the release of statistical data by country. Belgium seems to be the slowest to release the data. When I have time, I would like to add a simple analysis of the forecasting model using this data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-matter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Another matter…&lt;/h2&gt;
&lt;p&gt;I passed the Python 3 Engineer Certification Data Analysis exam. It was pretty easy, with only a 70% pass rate, but it was a good opportunity to revisit the basics of &lt;code&gt;Python&lt;/code&gt;. I haven’t even had the opportunity to use &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;R&lt;/code&gt;, let alone data analysis, in the work I’m doing now, so I’m considering the possibility of a career change. In the meantime, I plan to get the following qualifications by the end of this year, and I’ll be looking for a post where I can use my skills without focusing on finance. Like a diet, I need to declare and push myself.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;G Test&lt;/li&gt;
&lt;li&gt;Oracle Database Master Silver SQL&lt;/li&gt;
&lt;li&gt;Linuc level 1&lt;/li&gt;
&lt;li&gt;Fundamental Information Technology Engineer Examination&lt;/li&gt;
&lt;li&gt;AWS Certified Solutions Architect - Associate&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will report on the status of my acceptance on my blog each time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;gross domestic product and its expenditure items, industrial production and construction output indices, balance of payments, composite leading indicators, consumer price index, retail turnover, unemployment rate, number of workers, hourly wages, money supply, trade statistics, etc.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>I predicted the standings based on horse photos using CNN.</title>
      <link>/en/post/post18/</link>
      <pubDate>Sun, 05 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post18/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#crawling-for-data-collection&#34;&gt;1. Crawling for data collection&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#where-to-get-the-data-from&#34;&gt;Where to get the data from&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#running-crawling-by-selenium&#34;&gt;Running crawling by selenium&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#training-cnn-using-keras&#34;&gt;2. Training CNN using &lt;code&gt;Keras&lt;/code&gt;&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-keras&#34;&gt;What is Keras?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-cnn&#34;&gt;What is CNN?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#coding&#34;&gt;Coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#undersampling-for-unbalanced-data-adjustment&#34;&gt;Undersampling for unbalanced data adjustment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-of-results-using-shap-values&#34;&gt;3. Interpretation of results using Shap values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#finally&#34;&gt;4. Finally&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. This time I would like to write an article about predicting horse racing. In the last post, I created a prediction model for horse racing rankings using table data obtained from yahoo horse racing using &lt;code&gt;LightGBM&lt;/code&gt;. I used structural data last time, but anyone can do this kind of analysis in these days. So this time, I developed a &lt;code&gt;Convolutional Neural Network&lt;/code&gt; (CNN) which extracts features from a horse’s body image and predicts its ranking. This is the second time I’ve used &lt;code&gt;Earth Engine&lt;/code&gt; to analyze satellite images, and the first time I’ve used deep learning in this blog. The code is written in Python.&lt;/p&gt;
&lt;div id=&#34;crawling-for-data-collection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Crawling for data collection&lt;/h2&gt;
&lt;p&gt;The first step is to collect images of the horse’s body from the internet; the best thing to do would be to use pictures of the paddock on race day. However, as far as I’ve been able to find, there are no sites that post photos of the paddock in a cohesive format. It may be interesting to use it as a clipped image or to apply it as a video to the &lt;code&gt;Encoder-Decoder&lt;/code&gt; model of CNN to RNN, because it may be that a horse racing fan may have a paddock video on Youtube. However, I don’t have the ability to do that much.&lt;/p&gt;
&lt;div id=&#34;where-to-get-the-data-from&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where to get the data from&lt;/h3&gt;
&lt;p&gt;So in this case, the data is taken from &lt;a href=&#34;https://www.daily.co.jp/horse/horsecheck/photo/&#34;&gt;Daley’s Web site&lt;/a&gt;. Here you can find pre-race photos of horses running in the last 1 year? You can find pre-race photos of horses running in G1 races in the past year? Horse bettors who can’t go to the actual racecourse can look at these pictures and analyze the condition of the horses.&lt;br /&gt;
Please note that this site does not include body photos of all the horses that are entered in the race. Also, since this is only a limited number of G1 races, it’s entirely possible that all the horses are finished to begin with, and it’s entirely possible that you won’t be able to tell the difference. However, I’ll make it a priority to try and do it quickly and use this data for this one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-crawling-by-selenium&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running crawling by selenium&lt;/h3&gt;
&lt;p&gt;For crawling, we’ll use selenium. I won’t go into web crawling as I’m mainly using CNN in this article. The code I used is as follows.&lt;br /&gt;
[Note] If you use the following codes, please do so at your own risk.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.select import Select
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.alert import Alert
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.action_chains import ActionChains
from time import sleep
from urllib import request
import random

# selenium option settings (spell)
options = Options()
options.add_argument(&amp;#39;--disable-gpu&amp;#39;);
options.add_argument(&amp;#39;--disable-extensions&amp;#39;);
options.add_argument(&amp;#39;--proxy-server=&amp;quot;direct://&amp;quot;&amp;#39;);
options.add_argument(&amp;#39;--proxy-bypass-list=*&amp;#39;);
options.add_argument(&amp;#39;--start-maximized&amp;#39;);

# driver specification
DRIVER_PATH = r&amp;#39;C:/Users/aashi/Desktop/chromedriver_win32/chromedriver.exe&amp;#39;
driver = webdriver.Chrome(executable_path=DRIVER_PATH, chrome_options=options)

# Pass the url and go to the site
url = &amp;#39;https://www.daily.co.jp/horse/horsecheck/photo/&amp;#39;
driver.get(url)
driver.implicitly_wait(15) # Maximum time to wait for an object to load, and if this is exceeded, an error
sleep(5) # 1 second sleep as the web page transition is performed

# Image data is saved for each race.
selector0 = &amp;quot;body &amp;gt; div &amp;gt; main &amp;gt; div &amp;gt; div.primaryContents &amp;gt; article &amp;gt; div &amp;gt; section &amp;gt; a&amp;quot;
elements = driver.find_elements_by_css_selector(selector0)
for i in range(0,len(elements)):
  elements = driver.find_elements_by_css_selector(selector0)
  element = elements[i]
  element.click()
  sleep(5) # 5 seconds sleep as the web page transition is performed

  target = driver.find_element_by_link_text(&amp;#39;Ｇ１馬体診断写真集のTOP&amp;#39;)
  actions = ActionChains(driver)
  actions.move_to_element(target)
  actions.perform()
  sleep(5) # 5 seconds sleep as the web page transition is performed
  selector = &amp;quot;body &amp;gt; div.wrapper.horse.is-fixedHeader.is-fixedAnimation &amp;gt; main &amp;gt; div &amp;gt; div.primaryContents &amp;gt; article &amp;gt; article &amp;gt; div.photoDetail-wrapper &amp;gt; section &amp;gt; div &amp;gt; figure&amp;quot;
  figures = driver.find_elements_by_css_selector(selector)
  download_dir = r&amp;#39;C:\Users\aashi\umanalytics\photo\image&amp;#39;
  selector = &amp;quot;body &amp;gt; div &amp;gt; main &amp;gt; div &amp;gt; div.primaryContents &amp;gt; article &amp;gt; article &amp;gt; div.photoDetail-wrapper &amp;gt; section &amp;gt; h1&amp;quot;
  race_name = driver.find_element_by_css_selector(selector).text
  for figure in figures:
    img_name = figure.find_element_by_tag_name(&amp;#39;figcaption&amp;#39;).text
    horse_src = figure.find_element_by_tag_name(&amp;#39;img&amp;#39;).get_attribute(&amp;quot;src&amp;quot;)    
    save_name = download_dir + &amp;#39;/&amp;#39; + race_name + &amp;#39;_&amp;#39; + img_name + &amp;#39;.jpg&amp;#39;
    request.urlretrieve(horse_src,save_name)
  driver.back()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The saved images were cross-checked with the actual race results and manually divided into the top three groups and the rest of the groups. The images are saved as follows.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;horse_photo.PNG&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Stored Horse Image&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;This completes the collection of the original data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;training-cnn-using-keras&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Training CNN using &lt;code&gt;Keras&lt;/code&gt;&lt;/h2&gt;
&lt;div id=&#34;what-is-keras&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is Keras?&lt;/h3&gt;
&lt;p&gt;Now, let’s train CNN using &lt;code&gt;Keras&lt;/code&gt;. &lt;code&gt;Keras&lt;/code&gt; is one of the &lt;code&gt;Neural Network&lt;/code&gt; libraries that runs on &lt;code&gt;Tensorflow&lt;/code&gt; and &lt;code&gt;Theano&lt;/code&gt;. &lt;code&gt;Keras&lt;/code&gt; is one of the &lt;code&gt;Neural Network&lt;/code&gt; libraries that runs on &lt;code&gt;Tensorflow&lt;/code&gt; and &lt;code&gt;Theano&lt;/code&gt;. &lt;code&gt;Keras&lt;/code&gt; is characterized by its ability to build models with relatively short code and its many learning algorithms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-cnn&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;What is CNN?&lt;/h3&gt;
&lt;p&gt;CNN is a type of &lt;code&gt;(Deep) Neural Network&lt;/code&gt; often used in image analysis, and as its name suggests, it is an additional &lt;code&gt;convolution&lt;/code&gt;. Convolution is a process like the following.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://cdn-ak.f.st-hatena.com/images/fotolife/t/tdualdir/20180501/20180501211957.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Convolutional Layer Processing&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The input here is the image data. Image analysis recognizes and analyzes images as numerical values. The image on the computer is represented by the &lt;code&gt;RGB&lt;/code&gt; value, which is a numerical value from 0 to 255 of three colors, red (Red), green (Green) and blue (Blue). There are three layers of vectors in the form of 255 red, 0 green, 0 blue, and so on, and in this case a perfect red is represented. In the case above, you can think of a, b, c, etc. as representing one of the &lt;code&gt;RGB&lt;/code&gt; values of each pixel. Convolution calculates the features of an image by taking the inner product of these &lt;code&gt;RGB&lt;/code&gt; values with a matrix called the kernel. The following video(Japanese) is a good example of what the convolution layer means.&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/vU-JfZNBdYU&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;By learning the kernel to successfully get the distinctive parts of that image, it is possible to identify the image. I think the convolutional layer is the most important part of the CNN.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://th.bing.com/th/id/OIP.F2Ik_XFzmu5jZF-byiAKQQHaCg?w=342&amp;amp;h=118&amp;amp;c=7&amp;amp;o=5&amp;amp;dpr=1.25&amp;amp;pid=1.7&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;The Big Picture of CNN&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As shown in the above figure, CNN has not only convolutional layers but also input and output layers as well as usual &lt;code&gt;Neural Network&lt;/code&gt; layers. If you want to know about the &lt;code&gt;MaxPooling&lt;/code&gt; layer, see the following movie(Japanese).&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/MLixg9K6oeU&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Although the gradient method is known as the most orthodox training method for deep learning, various extension algorithms such as &lt;code&gt;Adam&lt;/code&gt; have been proposed. Basically, &lt;code&gt;Adam&lt;/code&gt; or &lt;code&gt;momentum&lt;/code&gt; is often used.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coding&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Coding&lt;/h3&gt;
&lt;p&gt;Now, let’s get to the coding.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.convolutional import MaxPooling2D
from keras.layers import Activation, Conv2D, Flatten, Dense,Dropout
from sklearn.model_selection import train_test_split
from keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop, Nadam
from PIL import Image
import numpy as np
import glob
import matplotlib.pyplot as plt
import time
import os&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The first step is to convert the collected image data to numerical data to create the training data.
The directory structure is as follows, with the top image and other images being stored in separate directories. When we read in the images from each directory, we give a category variable of 1 for the top image and 0 for others.&lt;/p&gt;
&lt;p&gt;photograph of a horse&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;superior (in rank)&lt;/li&gt;
&lt;li&gt;Other&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#Specify a folder
folders = os.listdir(r&amp;quot;C:\Users\aashi\umanalytics\photo\image&amp;quot;)
#Specify the total number of strokes (50 x 50 x 3 in this case).
image_size = 300
dense_size = len(folders)

X = []
Y = []

#Reads an image from each folder and converts it to a numpy array of RGB values using the Image function
for i, folder in enumerate(folders):
  files = glob.glob(&amp;quot;C:/Users/aashi/umanalytics/photo/image/&amp;quot; + folder + &amp;quot;/*.jpg&amp;quot;)
  index = i
  for k, file in enumerate(files):
    image = Image.open(file)
    image = image.convert(&amp;quot;L&amp;quot;).convert(&amp;quot;RGB&amp;quot;)
    image = image.resize((image_size, image_size)) #I&amp;#39;m dropping the number of pixels.
 
    data = np.asarray(image)
    X.append(data)
    Y.append(index)

X = np.array(X)
Y = np.array(Y)
X = X.astype(&amp;#39;float32&amp;#39;)
X = X / 255.0 # Conversion to 0~1
X.shape
Y = np_utils.to_categorical(Y, dense_size)

#splitting training data and test data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ve been able to split the training data and the test data. What I’m considering now is a binary classification of “top” and “other”, but I defined “top” as the top 3, so the data is unbalanced (about 5 times as much other data as the top data). In this case, if we train on the data as it is, it is easier to predict the label with the larger sample size (in this case, “other”), and the model will have a bias. Therefore, it is necessary to adjust the training data so that the sample size is the same for each of the two classes.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;index_zero = np.random.choice(np.array(np.where(y_train[:,1]==0))[0,],np.count_nonzero(y_train[:,1]==1),replace=False)
index_one = np.array(np.where(y_train[:,1]==1))[0]
y_resampled = y_train[np.hstack((index_one,index_zero))]
X_resampled = X_train[np.hstack((index_one,index_zero))]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will use this &lt;code&gt;y_resampled&lt;/code&gt; and &lt;code&gt;X_resampled&lt;/code&gt; for the training data. Next, we will build the CNN. In &lt;code&gt;Keras&lt;/code&gt;, a model is defined by specifying a &lt;code&gt;sequential model&lt;/code&gt; and adding a layer by &lt;code&gt;add&lt;/code&gt; method.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;model = Sequential()
model.add(Conv2D(32, (3, 3), padding=&amp;#39;same&amp;#39;,input_shape=X_train.shape[1:]))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(Conv2D(32, (3, 3)))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), padding=&amp;#39;same&amp;#39;))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(Conv2D(64, (3, 3)))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(512))
model.add(Activation(&amp;#39;relu&amp;#39;))
model.add(Dropout(0.5))
model.add(Dense(dense_size))
model.add(Activation(&amp;#39;softmax&amp;#39;))

model.summary()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Model: &amp;quot;sequential&amp;quot;
## _________________________________________________________________
## Layer (type)                 Output Shape              Param #   
## =================================================================
## conv2d (Conv2D)              (None, 300, 300, 32)      896       
## _________________________________________________________________
## activation (Activation)      (None, 300, 300, 32)      0         
## _________________________________________________________________
## conv2d_1 (Conv2D)            (None, 298, 298, 32)      9248      
## _________________________________________________________________
## activation_1 (Activation)    (None, 298, 298, 32)      0         
## _________________________________________________________________
## max_pooling2d (MaxPooling2D) (None, 149, 149, 32)      0         
## _________________________________________________________________
## dropout (Dropout)            (None, 149, 149, 32)      0         
## _________________________________________________________________
## conv2d_2 (Conv2D)            (None, 149, 149, 64)      18496     
## _________________________________________________________________
## activation_2 (Activation)    (None, 149, 149, 64)      0         
## _________________________________________________________________
## conv2d_3 (Conv2D)            (None, 147, 147, 64)      36928     
## _________________________________________________________________
## activation_3 (Activation)    (None, 147, 147, 64)      0         
## _________________________________________________________________
## max_pooling2d_1 (MaxPooling2 (None, 73, 73, 64)        0         
## _________________________________________________________________
## dropout_1 (Dropout)          (None, 73, 73, 64)        0         
## _________________________________________________________________
## flatten (Flatten)            (None, 341056)            0         
## _________________________________________________________________
## dense (Dense)                (None, 512)               174621184 
## _________________________________________________________________
## activation_4 (Activation)    (None, 512)               0         
## _________________________________________________________________
## dropout_2 (Dropout)          (None, 512)               0         
## _________________________________________________________________
## dense_1 (Dense)              (None, 2)                 1026      
## _________________________________________________________________
## activation_5 (Activation)    (None, 2)                 0         
## =================================================================
## Total params: 174,687,778
## Trainable params: 174,687,778
## Non-trainable params: 0
## _________________________________________________________________&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s get to the learning part. We’ll use &lt;code&gt;Adadelta&lt;/code&gt; for the algorithm. I don’t really understand it.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;optimizers =&amp;quot;Adadelta&amp;quot;
results = {}
epochs = 50
model.compile(loss=&amp;#39;categorical_crossentropy&amp;#39;, optimizer=optimizers, metrics=[&amp;#39;accuracy&amp;#39;])
results = model.fit(X_resampled, y_resampled, validation_split=0.2, epochs=epochs)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;undersampling-for-unbalanced-data-adjustment&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Undersampling for unbalanced data adjustment&lt;/h3&gt;
&lt;p&gt;From here, we perform binary classification with Test data, but since we are undersampling the training data, we have an undersampled sample selection bias when calculating the prediction probability. The paper is available &lt;a href=&#34;https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Therefore, I would like to formulate this part of the problem here, although a correction is needed. I will describe the current binary classification problem as the problem of predicting the objective variable &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, which takes a binary value from the explanatory thousand &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt; be a dataset where the positive example is considerably less than the negative example and the sample size of the negative example is matched to the positive example as &lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt;. We define a categorical variable &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; that takes 1 if the &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt; sample set is also included in &lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt; and 0 if it is not.
Given an explanatory variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to the model constructed using the dataset &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;, the positive example and the conditional probability of predicting can be expressed as &lt;span class=&#34;math inline&#34;&gt;\(P(y=1|x)\)&lt;/span&gt;. On the other hand, the conditional probability of predicting a positive example in a model constructed using &lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt; can be expressed as &lt;span class=&#34;math inline&#34;&gt;\(P(y=1|x)\)&lt;/span&gt; using Bayes’ theorem and the categorical variable &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(y=1|x,s=1) = \frac{P(s=1|y=1)P(y=1|x)}{P(s=1|y=1)P(y=1|x) + P(s=1|y=0)P(y=0|x)}
\]&lt;/span&gt;
It can be written as. Since &lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt; matches the sample size of the negative example to the positive example, &lt;span class=&#34;math inline&#34;&gt;\(P(s=1,y=1)=1\)&lt;/span&gt;, the above formula is rewritten as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P(y=1|x,s=1) = \frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)P(y=0|x)}
= \frac{P(y=1|x)}{P(y=1|x) + P(s=1|y=0)(1-P(y=1|x))}
\]&lt;/span&gt;
It is self-evident from the definition of &lt;span class=&#34;math inline&#34;&gt;\((X_s,Y_s)\)&lt;/span&gt; that &lt;span class=&#34;math inline&#34;&gt;\(P(s=1|y=0)\neq0\)&lt;/span&gt; (0 would result in unbalanced data with only positive examples). Thus, as long as &lt;span class=&#34;math inline&#34;&gt;\(P(y=0,x) \neq0\)&lt;/span&gt;, the probability that the undersampling model will be rejected as a positive example is positively biased against the probability that the original data set will produce. What we want to find is &lt;span class=&#34;math inline&#34;&gt;\(P(y=1|x)\)&lt;/span&gt; with no bias, so &lt;span class=&#34;math inline&#34;&gt;\(P=P(y=1|x),P_s=P(y|x,s=1),\beta=P(s=1,y=0)\)&lt;/span&gt;, then we can get&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
P = \frac{\beta P_s}{\beta P_s-P_s+1}
\]&lt;/span&gt;
and can use this relationship formula to correct for bias.
Let’s define what we’ve just identified as a function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def calibration(y_proba, beta):
    return y_proba / (y_proba + (1 - y_proba) / beta)

sampling_rate = sum(y_train[:,1]) / sum(1-y_train[:,1])
y_proba_calib = calibration(model.predict(X_test), sampling_rate)
y_pred = np_utils.to_categorical(np.argmax(y_proba_calib,axis=1), dense_size)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
score = accuracy_score(y_test, y_pred)
print(&amp;#39;Test accuracy:&amp;#39;, score)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Test accuracy: 0.2711864406779661&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s not good at all. I ran the &lt;code&gt;ConfusionMatrix&lt;/code&gt; and found out that it doesn’t work.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ConfusionMatrixDisplay(confusion_matrix(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))).plot()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x0000000049341F88&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I did a bias correction for the imbalance data, but the model is still very predictive of negative values. This doesn’t work.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation-of-results-using-shap-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Interpretation of results using Shap values&lt;/h2&gt;
&lt;p&gt;I would like to consider the &lt;code&gt;shap&lt;/code&gt; value of the model we just learned and interpret the results. I’ll add an explanation of the &lt;code&gt;shap&lt;/code&gt; value when I have time. Simply put, the visualization captures which parts of the image the CNN captured features and predicted the horse to be at the top. We’ll be analyzing this horse.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.imshow(X_test[0])
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shap
background = X_train[np.random.choice(X_train.shape[0],100,replace=False)]

e = shap.GradientExplainer(model,background)

shap_values = e.shap_values(X_test[[0]])
shap.image_plot(shap_values[1],X_test[[0]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s very subtle, but it looks like you’re appreciating the legs and buttocks, etc. It needs to be cropped to take out the horse’s body only, since it seems to be responding to the background. I think I need to build a model for object detection. I’ll think about this another time.
I’d like to visualize which aspects of the image are captured at each layer.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from keras import models

layer_outputs = [layer.output for layer in model.layers[:8]]
layer_names = []
for layer in model.layers[:8]:
    layer_names.append(layer.name)
images_per_row = 16

activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
activations = activation_model.predict(X_train[[0]])

for layer_name, layer_activation in zip(layer_names, activations):
    n_features = layer_activation.shape[-1]

    size = layer_activation.shape[1]

    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))

    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0,
                                             :, :,
                                             col * images_per_row + row]
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype(&amp;#39;uint8&amp;#39;)
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, cmap=&amp;#39;viridis&amp;#39;)
    plt.show()
    plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-2.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-3.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-4.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-5.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-6.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-7.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-8.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-9.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s hard to interpret, partly because I’m inexperienced.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;finally&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Finally&lt;/h2&gt;
&lt;p&gt;To be honest, it hasn’t worked out at all. Is it still difficult to predict rankings from the horse’s body? Does multiplying it with other variables change the results? I don’t think I’m able to extract good features from the horses as it is.
Do I need to get to the point where I can get a paddock video from Youtube and analyze it with the &lt;code&gt;Encoder-Decoder&lt;/code&gt; model to make it work? I’d love to do it when I’m good enough to do it (I don’t know when that will be). Until then, I need to improve my PC specs. Maybe I’ll use the cash handout.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Scraping past race results on yahoo horse racing on rvest (for the second time)</title>
      <link>/en/post/post11/</link>
      <pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate>
      <guid>/en/post/post11/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#script&#34;&gt;1. Script&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;For the second time, I’d like to drop past race results in rvest again. If you haven’t seen the past articles, I suggest you look at that one first.&lt;/p&gt;
&lt;p&gt;The reason I decided to take back the data this time is because I wanted to add more items to the explanatory variables when analyzing horse racing. So this time I created the program as an addition to the previous R script. The 14 new data items I added are as follows&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;grass or dirt.&lt;/li&gt;
&lt;li&gt;right-handed or left-handed.&lt;/li&gt;
&lt;li&gt;race conditions (good or slight)&lt;/li&gt;
&lt;li&gt;weather&lt;/li&gt;
&lt;li&gt;the color of the horse’s coat (chestnut, deer hair, etc.)&lt;/li&gt;
&lt;li&gt;horse owner&lt;/li&gt;
&lt;li&gt;producers&lt;/li&gt;
&lt;li&gt;place of origin&lt;/li&gt;
&lt;li&gt;date of birth&lt;/li&gt;
&lt;li&gt;the father’s horse&lt;/li&gt;
&lt;li&gt;the mother’s horse&lt;/li&gt;
&lt;li&gt;winnings up to that race (available from 2003)&lt;/li&gt;
&lt;li&gt;jockey’s weight.&lt;/li&gt;
&lt;li&gt;increase or decrease in the weight of the jockey.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Actually, I haven’t finished collecting data yet, and the R program has been running for a long time (I’ve been running it for about 3 days). However, the program itself is running tightly and I’ll try to introduce the script. Maybe I’ll write the results in a postscript.&lt;/p&gt;
&lt;div id=&#34;script&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Script&lt;/h2&gt;
&lt;p&gt;The first step is to call the package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Web scraping of horse racing data by rvest

#install.packages(&amp;quot;rvest&amp;quot;)
#if (!require(&amp;quot;pacman&amp;quot;)) install.packages(&amp;quot;pacman&amp;quot;)
#install.packages(&amp;quot;beepr&amp;quot;)
#install.packages(&amp;quot;RSQLite&amp;quot;)
pacman::p_load(qdapRegex)
library(rvest)
library(stringr)
library(dplyr)
library(beepr)
library(RSQLite)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m pretty much warnning out, so I’m banning it and connecting to SQLite&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# warning prohibition
options(warn=-1)

# Connecting to SQLite
con = dbConnect(SQLite(), &amp;quot;horse_data.db&amp;quot;, synchronous=&amp;quot;off&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since the odds are only available since 1994, the data is taken from 1994 up to the most recent date. yahoo horse racing has races organized by month, so the data is taken using that as a variable. Basically, you go to &lt;a href=&#34;https://keiba.yahoo.co.jp/schedule/list/2018/?month=7&#34;&gt;List of race results for the relevant year and month&lt;/a&gt; and then go to &lt;a href=&#34;https://keiba.yahoo.co.jp/race/list/18020106/&#34;&gt;Timetable for each day of the race&lt;/a&gt; for each day on that page. Since there are roughly a dozen or so races at each individual track, get the link and go to the &lt;a href=&#34;https://keiba.yahoo.co.jp/race/result/1802010601/&#34;&gt;race results page&lt;/a&gt; for each race. Then you will get the race results. The first step is to get a link to the timetable for each individual racecourse for each day.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(year in 1994:2019){
  start.time &amp;lt;- Sys.time() # calculate time
  # Retrieve the race results page on Yahoo!
  for (k in 1:12){ # K is for the month.
    
    tryCatch(
      {
        keiba.yahoo &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp/schedule/list/&amp;quot;, year,&amp;quot;/?month=&amp;quot;,k)) # Access to the list of race results for a given year and month
        Sys.sleep(2)
        race_lists &amp;lt;- keiba.yahoo %&amp;gt;%
          html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% 
          html_attr(&amp;quot;href&amp;quot;) # Retrieve all URLs
        
        # Get the list of races for each day by track
        race_lists &amp;lt;- race_lists[str_detect(race_lists, pattern=&amp;quot;race/list/\\d+/&amp;quot;)==1] # Extracts URLs containing &amp;quot;results&amp;quot;.
      }
      , error = function(e){signal &amp;lt;- 1}
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, we only extract the links we get that contain the word result in the url. In essence, it is a link to the race tables of each racecourse. From here, we use the links to the race tables of the tracks to access that page and get the links to the pages that contain the results of all 12 races.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;    for (j in 1:length(race_lists)){ # where j is the link to the race table for the year in question.
      
      tryCatch(
        {
          race_list &amp;lt;- read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,race_lists[j]))
          race_url &amp;lt;- race_list %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) # 全urlを取得
          
          # Get the URL of the race results
          race_url &amp;lt;- race_url[str_detect(race_url, pattern=&amp;quot;result&amp;quot;)==1] # Extracts URLs containing &amp;quot;results&amp;quot;.
        }
        , error = function(e){signal &amp;lt;- 1}
      )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the links to the results of each race, it’s time to get the race results and the formatting part of the code. It’s quite a long and complicated code. The race results are stored in the following table attributes, so we’ll simply pull them first.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;      for (i in 1:length(race_url)){ # where i is the race that was held at the racecourse in question
        
        print(str_c(year, &amp;quot;/&amp;quot;, k, &amp;quot;/&amp;quot;,j, &amp;quot; group &amp;quot;, i,&amp;quot; order&amp;quot;))
        
        tryCatch(
          {
            race1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,race_url[i])) # Get the URL of the race results
            signal &amp;lt;- 0
            Sys.sleep(2)
          }
          , error = function(e){signal &amp;lt;- 1}
        )
        
        # If the race is aborted or there are no errors in the process so far, run the process
        if (identical(race1 %&amp;gt;%
                      html_nodes(xpath = &amp;quot;//div[@class = &amp;#39;resultAtt mgnBL fntSS&amp;#39;]&amp;quot;) %&amp;gt;%
                      html_text(),character(0)) == TRUE &amp;amp;&amp;amp; signal == 0){
          
          # Scraping the race results
          race_result &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//table[@id = &amp;#39;raceScore&amp;#39;]&amp;quot;) %&amp;gt;%
            html_table()
          race_result &amp;lt;- do.call(&amp;quot;data.frame&amp;quot;,race_result) # Change the list to a data frame.
          
          colnames(race_result) &amp;lt;- c(&amp;quot;order&amp;quot;,&amp;quot;frame_number&amp;quot;,&amp;quot;horse_number&amp;quot;,&amp;quot;horse_name/age&amp;quot;,&amp;quot;time/margin&amp;quot;,&amp;quot;passing_rank/last_3F&amp;quot;,&amp;quot;jockey/weight&amp;quot;,&amp;quot;popularity/odds&amp;quot;,&amp;quot;trainer&amp;quot;) #　column renaming&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you just get a table, it will not be useful for analysis because of the following or multiple information in one cell. So, we need to mold the data into a form.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          # Passing order and time for 3 furlongs uphill
          race_result &amp;lt;- dplyr::mutate(race_result,passing_rank=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;(\\d{2}-\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2}-\\d{2})|(\\d{2}-\\d{2})&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,last_3F=as.character(str_extract_all(race_result$`passing_rank/last_3F`,&amp;quot;\\d{2}\\.\\d&amp;quot;)))
          race_result &amp;lt;- race_result[-6]
          
          # Time and Difference
          race_result &amp;lt;- dplyr::mutate(race_result,time=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;\\d\\.\\d{2}\\.\\d|\\d{2}\\.\\d&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,margin=as.character(str_extract_all(race_result$`time/margin`,&amp;quot;./.馬身|.馬身|.[:space:]./.馬身|[ア-ン-]+&amp;quot;)))
          race_result$margin[race_result$order==1] &amp;lt;- &amp;quot;トップ&amp;quot;
          race_result$margin[race_result$margin==&amp;quot;character(0)&amp;quot;] &amp;lt;- &amp;quot;大差&amp;quot;
          race_result$margin[race_result$order==0] &amp;lt;- NA
          race_result &amp;lt;- race_result[-5]
          
          # Horse&amp;#39;s name, age and weight
          race_result &amp;lt;- dplyr::mutate(race_result,horse_name=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;[ァ-ヴー・]+&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,horse_age=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;牡\\d+|牝\\d+|せん\\d+&amp;quot;)))
          race_result$horse_sex &amp;lt;- str_extract(race_result$horse_age, pattern = &amp;quot;牡|牝|せん&amp;quot;)
          race_result$horse_age &amp;lt;- str_extract(race_result$horse_age, pattern = &amp;quot;\\d&amp;quot;)
          race_result &amp;lt;- dplyr::mutate(race_result,horse_weight=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\d{3}&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,horse_weight_change=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;\\([\\+|\\-]\\d+\\)|\\([\\d+]\\)&amp;quot;)))
          race_result$horse_weight_change &amp;lt;- sapply(rm_round(race_result$horse_weight_change, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
          race_result &amp;lt;- dplyr::mutate(race_result,brinker=as.character(str_extract_all(race_result$`horse_name/age`,&amp;quot;B&amp;quot;)))
          race_result$brinker[race_result$brinker!=&amp;quot;B&amp;quot;] &amp;lt;- &amp;quot;N&amp;quot;
          race_result &amp;lt;- race_result[-4]
          
          # jockey
          race_result &amp;lt;- dplyr::mutate(race_result,jockey=as.character(str_extract_all(race_result$`jockey/weight`,&amp;quot;[ぁ-ん一-龠]+\\s[ぁ-ん一-龠]+|[:upper:].[ァ-ヶー]+&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,jockey_weight=as.character(str_extract_all(race_result$`jockey/weight`,&amp;quot;\\d{2}&amp;quot;)))
          race_result$jockey_weight_change &amp;lt;- 0
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;☆&amp;quot;)==1] &amp;lt;- 1
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;△&amp;quot;)==1] &amp;lt;- 2
          race_result$jockey_weight_change[str_detect(race_result$`jockey/weight`,&amp;quot;△&amp;quot;)==1] &amp;lt;- 3
          race_result &amp;lt;- race_result[-4]
          
          # Odds and popularity
          race_result &amp;lt;- dplyr::mutate(race_result,odds=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\(.+\\)&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,popularity=as.character(str_extract_all(race_result$`popularity/odds`,&amp;quot;\\d+[^(\\d+.\\d)]&amp;quot;)))
          race_result$odds &amp;lt;- sapply(rm_round(race_result$odds, extract=TRUE), paste, collapse=&amp;quot;&amp;quot;)
          race_result &amp;lt;- race_result[-4]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we’ll import information other than the table we just retrieved. Specifically, race names, weather conditions, track conditions, dates, racecourses, etc. These information are listed at the top of the race results page.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          # Race Information
          race_date &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/p[@id = &amp;#39;raceTitDay&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
          race_name &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;raceTitName&amp;#39;]/h1[@class = &amp;#39;fntB&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
          race_distance &amp;lt;- race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//p[@id = &amp;#39;raceTitMeta&amp;#39;]&amp;quot;) %&amp;gt;%
            html_text()
        
          race_result &amp;lt;- dplyr::mutate(race_result,race_date=as.character(str_extract_all(race_date,&amp;quot;\\d+年\\d+月\\d+日&amp;quot;)))
          race_result$race_date &amp;lt;- str_replace_all(race_result$race_date,&amp;quot;年&amp;quot;,&amp;quot;/&amp;quot;)
          race_result$race_date &amp;lt;- str_replace_all(race_result$race_date,&amp;quot;月&amp;quot;,&amp;quot;/&amp;quot;)
          race_result$race_date &amp;lt;- as.Date(race_result$race_date)
          race_course &amp;lt;- as.character(str_extract_all(race_date,pattern = &amp;quot;札幌|函館|福島|新潟|東京|中山|中京|京都|阪神|小倉&amp;quot;))
          race_result$race_course &amp;lt;- race_course
          race_result &amp;lt;- dplyr::mutate(race_result,race_name=as.character(str_replace_all(race_name,&amp;quot;\\s&amp;quot;,&amp;quot;&amp;quot;)))
          race_result &amp;lt;- dplyr::mutate(race_result,race_distance=as.character(str_extract_all(race_distance,&amp;quot;\\d+m&amp;quot;)))
          race_type=as.character(str_extract_all(race_distance,pattern = &amp;quot;芝|ダート&amp;quot;))
          race_result$type &amp;lt;- race_type
          race_turn &amp;lt;- as.character(str_extract_all(race_distance,pattern = &amp;quot;右|左&amp;quot;))
          race_result$race_turn &amp;lt;- race_turn
          
          if(length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg ryou&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;良&amp;quot;
          } else if (length(race1 %&amp;gt;% 
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg yayaomo&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;稍重&amp;quot;
          } else if (length(race1 %&amp;gt;%
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg omo&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;重&amp;quot;
          } else if (length(race1 %&amp;gt;% 
                            html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg furyou&amp;#39;]&amp;quot;)) == 1){
            race_result$race_condition &amp;lt;- &amp;quot;不良&amp;quot;
          } else race_result$race_condition &amp;lt;- &amp;quot;NA&amp;quot;
          
          if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg hare&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;晴れ&amp;quot;
          } else if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg ame&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;曇り&amp;quot;
          } else if (length(race1 %&amp;gt;% html_nodes(xpath = &amp;quot;//img[@class = &amp;#39;spBg kumori&amp;#39;]&amp;quot;)) == 1){
            race_result$race_weather &amp;lt;- &amp;quot;雨&amp;quot;
          } else race_result$race_weather &amp;lt;- &amp;quot;その他&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to get information about each horse. In fact, the horse’s name in the table we got earlier is a link, and if you follow the link, you can get [information about each horse] (&lt;a href=&#34;https://keiba.yahoo.co.jp/directory/horse/2015105508/&#34; class=&#34;uri&#34;&gt;https://keiba.yahoo.co.jp/directory/horse/2015105508/&lt;/a&gt;) (such as coat color and date of birth).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          horse_url &amp;lt;- race1 %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) 
          horse_url &amp;lt;- horse_url[str_detect(horse_url, pattern=&amp;quot;directory/horse&amp;quot;)==1] # Extract only links to horse information.
          
          for (l in 1:length(horse_url)){
            tryCatch(
              {
                horse1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,horse_url[l]))
                Sys.sleep(0.5)
                horse_name &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;dirTitName&amp;#39;]/h1[@class = &amp;#39;fntB&amp;#39;]&amp;quot;) %&amp;gt;% 
                  html_text()
                horse &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//div[@id = &amp;#39;dirTitName&amp;#39;]/ul&amp;quot;) %&amp;gt;% 
                  html_text()
                race_result$colour[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;毛色：.+&amp;quot;)) 
                race_result$owner[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;馬主：.+&amp;quot;))
                race_result$farm[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;生産者：.+&amp;quot;))
                race_result$locality[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;産地：.+&amp;quot;))
                race_result$horse_birthday[race_result$horse_name==horse_name] &amp;lt;- as.character(str_extract_all(horse,&amp;quot;\\d+年\\d+月\\d+日&amp;quot;))
                race_result$father[race_result$horse_name==horse_name] &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;bloodM&amp;#39;][@rowspan = &amp;#39;4&amp;#39;]&amp;quot;) %&amp;gt;% html_text()
                race_result$mother[race_result$horse_name==horse_name] &amp;lt;- horse1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;bloodF&amp;#39;][@rowspan = &amp;#39;4&amp;#39;]&amp;quot;) %&amp;gt;% html_text()
              }
              , error = function(e){
                race_result$colour[race_result$horse_name==horse_name] &amp;lt;- NA 
                race_result$owner[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$farm[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$locality[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$horse_birthday[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$father[race_result$horse_name==horse_name] &amp;lt;- NA
                race_result$mother[race_result$horse_name==horse_name] &amp;lt;- NA
                }
            )
          }
          
          race_result$colour &amp;lt;- str_replace_all(race_result$colour,&amp;quot;毛色：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$owner &amp;lt;- str_replace_all(race_result$owner,&amp;quot;馬主：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$farm &amp;lt;- str_replace_all(race_result$farm,&amp;quot;生産者：&amp;quot;,&amp;quot;&amp;quot;)
          race_result$locality &amp;lt;- str_replace_all(race_result$locality,&amp;quot;産地：&amp;quot;,&amp;quot;&amp;quot;)
          #race_result$horse_birthday &amp;lt;- str_replace_all(race_result$horse_birthday,&amp;quot;年&amp;quot;,&amp;quot;/&amp;quot;)
          #race_result$horse_birthday &amp;lt;- str_replace_all(race_result$horse_birthday,&amp;quot;月&amp;quot;,&amp;quot;/&amp;quot;)
          #race_result$horse_birthday &amp;lt;- as.Date(race_result$horse_birthday)
          
          race_result &amp;lt;- dplyr::arrange(race_result,horse_number) # arrange in order of precedence&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The next step is to go and drop the amount of money you have won up to that race. You can access this by following the link marked [Runoffs] (&lt;a href=&#34;https://keiba.yahoo.co.jp/race/denma/1802010601/&#34; class=&#34;uri&#34;&gt;https://keiba.yahoo.co.jp/race/denma/1802010601/&lt;/a&gt;) on the race results page. This is where you will find the prize money and you will get it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          yosou_url &amp;lt;- race1 %&amp;gt;% html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% html_attr(&amp;quot;href&amp;quot;) 
          yosou_url &amp;lt;- yosou_url[str_detect(yosou_url, pattern=&amp;quot;denma&amp;quot;)==1]
          
          if (length(yosou_url)==1){
          yosou1 &amp;lt;-  read_html(str_c(&amp;quot;https://keiba.yahoo.co.jp&amp;quot;,yosou_url)) 
          Sys.sleep(2)
          yosou &amp;lt;- yosou1 %&amp;gt;% html_nodes(xpath = &amp;quot;//td[@class = &amp;#39;txC&amp;#39;]&amp;quot;) %&amp;gt;% as.character()
          prize &amp;lt;- yosou[grepl(&amp;quot;万&amp;quot;,yosou)==TRUE] %&amp;gt;% str_extract_all(&amp;quot;\\d+万&amp;quot;)
          prize &amp;lt;- t(do.call(&amp;quot;data.frame&amp;quot;,prize)) %&amp;gt;% as.character()
          race_result$prize &amp;lt;- prize
          race_result$prize &amp;lt;- str_replace_all(race_result$prize,&amp;quot;万&amp;quot;,&amp;quot;&amp;quot;) %&amp;gt;% as.numeric()
          } else race_result$prize &amp;lt;- NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We create a data frame called dataset to store the results of each race and store the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;          ## file storage
          if (k == 1 &amp;amp;&amp;amp; i == 1 &amp;amp;&amp;amp; j == 1){
            dataset &amp;lt;- race_result
          } else {
            dataset &amp;lt;- rbind(dataset,race_result)
          }
        }else
        {
          print(&amp;quot;We couldn&amp;#39;t save it.&amp;quot;) 
        }
      }
    }
  }
  beep(3)
  write.csv(dataset,&amp;quot;race_result2.csv&amp;quot;, row.names = FALSE)
  
  if (year == 1994){
    dbWriteTable(con, &amp;quot;race_result&amp;quot;, dataset)
  } else {
    dbWriteTable(con, &amp;quot;temp&amp;quot;, dataset)
    dbSendQuery(con, &amp;quot;INSERT INTO race_result select * from temp&amp;quot;)
    dbSendQuery(con, &amp;quot;DROP TABLE temp&amp;quot;)
  }
}
end.time &amp;lt;- Sys.time()
print(str_c(&amp;quot;処理時間は&amp;quot;,end.time-start.time,&amp;quot;です。&amp;quot;))
beep(5)

options(warn = 1)

dbDisconnect(con)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s all. The data taken is as follows&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(race_result)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   order frame_number horse_number   trainer passing_rank last_3F   time
## 1    10            1            1   田中 剛        09-09    39.0 1.14.3
## 2    16            1            2 天間 昭一        11-11    40.3 1.15.7
## 3    15            2            3 田中 清隆        14-14    39.4 1.15.1
## 4     9            2            4 中舘 英二        08-08    39.1 1.14.3
## 5    12            3            5 根本 康広        11-11    39.0 1.14.4
## 6     4            3            6 杉浦 宏昭        04-04    38.4 1.13.2
##      margin         horse_name horse_age horse_sex horse_weight
## 1    アタマ     サトノジョニー         3        牡          512
## 2 3 1/2馬身       ツギノイッテ         3        牡          464
## 3     3馬身           ギュウホ         3        牡          444
## 4 2 1/2馬身 セイウンメラビリア         3        牝          466
## 5      クビ サバイバルトリック         3        牝          450
## 6    アタマ       ステイホット         3        牝          474
##   horse_weight_change brinker      jockey jockey_weight jockey_weight_change
## 1                 +30       N   松岡 正海            56                    0
## 2                  +8       N 西田 雄一郎            56                    0
## 3                  +8       N   杉原 誠人            56                    0
## 4                 +10       N   村田 一誠            54                    0
## 5                  -2       N 野中 悠太郎            51                    0
## 6                  -2       N   大野 拓弥            54                    0
##    odds popularity  race_date race_course       race_name race_distance   type
## 1  40.3         9  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 2 340.9        16  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 3 283.1        14  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 4 299.7        15  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 5  26.7         8  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
## 6   2.4         1  2019-01-05        中山 サラ系3歳未勝利         1200m ダート
##   race_turn race_condition race_weather colour                           owner
## 1        右             良         晴れ   栗毛 株式会社 サトミホースカンパニー
## 2        右             良         晴れ 黒鹿毛                     西村 新一郎
## 3        右             良         晴れ   鹿毛           有限会社 ミルファーム
## 4        右             良         晴れ 青鹿毛                       西山 茂行
## 5        右             良         晴れ 黒鹿毛                       福田 光博
## 6        右             良         晴れ   栗毛                       小林 善一
##           farm   locality horse_birthday                 father
## 1   千代田牧場 新ひだか町  2016年1月29日         オルフェーヴル
## 2    織笠 時男     青森県  2016年4月17日 スクワートルスクワート
## 3    神垣 道弘 新ひだか町  2016年4月19日     ジャングルポケット
## 4  石郷岡 雅樹     新冠町  2016年4月21日     キンシャサノキセキ
## 5     原田牧場     日高町  2016年4月30日       リーチザクラウン
## 6 社台ファーム     千歳市  2016年3月13日     キャプテントゥーレ
##               mother prize
## 1 スパークルジュエル     0
## 2   エプソムアイリス     0
## 3     デライトシーン     0
## 4     ドリームシップ     0
## 5   フリーダムガール   180
## 6     ステイアライヴ   455&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
