<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>finance | 東京の資産運用会社で働く社会人が研究に没頭するブログ</title>
    <link>/en/tag/finance/</link>
      <atom:link href="/en/tag/finance/index.xml" rel="self" type="application/rss+xml" />
    <description>finance</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 10 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>finance</title>
      <link>/en/tag/finance/</link>
    </image>
    
    <item>
      <title>Rcpp to speed up data handling (using Tick data processing as an example)</title>
      <link>/en/post/post21/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post21/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what-i-want-to-do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;0. What I want to do&lt;/h2&gt;
&lt;p&gt;What I’m going to show you is pre-processing (and analysis) using the Tick data of the exchange rate, as I mentioned above. I won’t go into the details of this article because my main focus is to improve efficiency using &lt;code&gt;Rcpp&lt;/code&gt;, but I will briefly describe what I want to do first. I will not go into details, but I will show what we want to do in a nutshell first.&lt;/p&gt;
&lt;p&gt;What we want to do is to detect a &lt;em&gt;Jump&lt;/em&gt; from the five-minute incremental return of the JPY/USD rate. Here, a jump is the point at which the exchange rate has risen (fallen) sharply compared to the previous rate. During the day, the exchange rate moves in small increments, but when there is an event, it rises (or falls) significantly. It is very interesting to see what kind of event causes the jump. In order to test this, we first need to detect jumps. The following paper is a reference point.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/rfs/article-abstract/21/6/2535/1574138?redirectedFrom=fulltext&#34;&gt;Suzanne S. Lee &amp;amp; Per A. Mykland, 2008. “Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,” Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is a very well-respected paper with 204 Citation. We will scratch out the estimation method. First, let the continuous compound return be &lt;span class=&#34;math inline&#34;&gt;\(d\log S(t)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt;. where &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is the price of the asset at &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. If there are no jumps in the market, &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is assumed to follow the following stochastic process&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) \tag{1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt; is the standard Brownian motion, &lt;span class=&#34;math inline&#34;&gt;\(\mu(t)\)&lt;/span&gt; is the drift term, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma(t)\)&lt;/span&gt; is the spot volatility. Also, when there is a Jump, &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is assumed to follow the following stochastic process&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) + Y(t)dJ(t) \tag{2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(J(t)\)&lt;/span&gt; is a counting process independent of &lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt;. &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y(t)\)&lt;/span&gt; represents the size of the jump and is a predictable process.&lt;/p&gt;
&lt;p&gt;Next, consider the logarithmic return of &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;. That is, &lt;span class=&#34;math inline&#34;&gt;\(\log S(t_i)/S(t_{i-1})\)&lt;/span&gt;, which follows a normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(0, \sigma(t_i))\)&lt;/span&gt;. &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;Now, we define the statistic &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; when there is a jump from &lt;span class=&#34;math inline&#34;&gt;\(t_{i-1}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt; as follows.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} \equiv \frac{|\log S(t_i)/S(t_{i-1})|}{\hat{\sigma}_{t_i}} \tag{3}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is a simple standardization of the absolute value of the log return, but uses the “Realized Bipower Variation” defined below as an estimator of the standard deviation.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\sigma}_{t_i} = \frac{1}{K-2}\sum_{j=i-K+2}^{i-2}|\log S(t_j)/\log S(t_{j-1})||\log S(t_{j-1})/\log S(t_{j-2})| \tag{4}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the number of sample sizes contained in the Window. If we use a return in 5-minute increments and the jump occurs at 10:00 on 9/10/2020, and &lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt;, then we will calculate using samples from the previous day, 9/9/2020 11:30 to 9/11/2020 09:55. What we’re doing is adding up the absolute value of the return multiplied by the absolute value of the return, which seems to make it difficult for the estimate of the next instant after the jump occurs (i.e., &lt;span class=&#34;math inline&#34;&gt;\(t_{i+1}\)&lt;/span&gt; and so on) to be affected by the jump. Incidentally, &lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt; is introduced in another paper as a recommended value for returns in 5-minute increments.&lt;/p&gt;
&lt;p&gt;Let us move on to how the calculated jump statistic &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt; is used in the statistical test to detect jumps. This is done by considering the maximum value of the random variable &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt; (also a random variable), and if it deviates significantly from that distribution (like 95% of points), the return of the statistic is the jump.&lt;/p&gt;
&lt;p&gt;If there is no Jump in the period &lt;span class=&#34;math inline&#34;&gt;\([t_{i-1},t_{i}]\)&lt;/span&gt;, then with the length of this period &lt;span class=&#34;math inline&#34;&gt;\(\Delta=t_{i}-t_{i-1}\)&lt;/span&gt; approached to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\Delta\rightarrow0\)&lt;/span&gt;, the sample maximum of the absolute value of the standard normal variable converges to the Gumbel distribution. Thus, Jump can reject and detect the null hypothesis when the following conditions are met&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} &amp;gt; G^{-1}(1-\alpha)S_{n} + C_{n} \tag{5}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(G^{-1}(1-\alpha)\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\((1-\alpha)\)&lt;/span&gt; quantile function of the standard Gumbell distribution. If &lt;span class=&#34;math inline&#34;&gt;\(\alpha=10%\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(G^{-1}(1-\alpha)=2.25\)&lt;/span&gt;. Note that (We won’t derive it, but we can prove it using equations 1 and 2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_{n} = \frac{1}{c(2\log n)^{0.5}} \\
C_{n} = \frac{(2\log n)^{0.5}}{c}-\frac{\log \pi+\log(\log n)}{2c(2\log n)^{0.5}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(c=(2/\pi)^{0.5}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the total sample size used for estimation.
Finally, &lt;span class=&#34;math inline&#34;&gt;\(Jump_{t_i}\)&lt;/span&gt; is calculated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Jump_{t_i} = \log\frac{S(t_i)}{S(t_{i-1})}×I(\mathcal{L(i)} - G^{-1}(1-\alpha)S_{n} + C_{n})\tag{6}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(⋅)\)&lt;/span&gt; is an Indicator function that returns 1 if the content is greater than 0 and 0 otherwise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Loading data&lt;/h2&gt;
&lt;p&gt;So now that we know how to estimate, let’s load the Tick data first. The data is csv from &lt;code&gt;QuantDataManager&lt;/code&gt; and saved in a working directory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)

# Read Tick data
strPath &amp;lt;- r&amp;quot;(C:\Users\hogehoge\JPYUSD_Tick_2011.csv)&amp;quot;
JPYUSD &amp;lt;- readr::read_csv(strPath)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On an unrelated note, I recently upgraded R to 4.0.2, and I’m quite happy to say that with 4.0 and above, you can escape the strings made by &lt;code&gt;Python&lt;/code&gt;, which relieves some of the stress I’ve been experiencing.&lt;/p&gt;
&lt;p&gt;The data looks like the following: in addition to the date, the Bid value, Ask value and the volume of transactions are stored. Here, we use the 2011 tick. The reason for this is to cover the dollar/yen at the time of the Great East Japan Earthquake.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     DateTime                        Bid             Ask            Volume     
##  Min.   :2011-01-03 07:00:00   Min.   :75.57   Min.   :75.58   Min.   : 1.00  
##  1st Qu.:2011-03-30 15:09:23   1st Qu.:77.43   1st Qu.:77.44   1st Qu.: 2.00  
##  Median :2011-06-15 14:00:09   Median :80.40   Median :80.42   Median : 2.00  
##  Mean   :2011-06-22 05:43:11   Mean   :79.91   Mean   :79.92   Mean   : 2.55  
##  3rd Qu.:2011-09-09 13:54:51   3rd Qu.:81.93   3rd Qu.:81.94   3rd Qu.: 3.00  
##  Max.   :2011-12-30 06:59:59   Max.   :85.52   Max.   :85.54   Max.   :90.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the way, &lt;code&gt;DateTime&lt;/code&gt; includes the period from 07:00:00 on 2011/1/3 to 06:59:59 on 2011-12-30 (16:59:59 on 2011-12-30) in Japan by UTC. The sample size is approximately 12 million entries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NROW(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11946621&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Preprocessing&lt;/h2&gt;
&lt;p&gt;Then calculate the median value from Bid and Ask and take the logarithm to calculate the return later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate the median value of Ask and Bid and make it logarithmic (for calculating the logarithmic return).
JPYUSD &amp;lt;- JPYUSD %&amp;gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %&amp;gt;% 
                     dplyr::mutate(logMid = log(Mid))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I format the currently irregularly arranged trading data into 5-minute increments of returns. The way to do it is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a &lt;code&gt;POSIXct&lt;/code&gt; vector with 1 year chopped every 5 minutes.&lt;/li&gt;
&lt;li&gt;create a function that calculates the logarithmic return from the first and last sample in the window of 5min in turn, if you pass 1. as an argument.&lt;/li&gt;
&lt;li&gt;execute. This is the plan. First, create the vector of 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create POSIX vector to calculate returns in 5min increments (288 x days)
start &amp;lt;- as.POSIXct(&amp;quot;2011-01-02 22:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
end &amp;lt;- as.POSIXct(&amp;quot;2011-12-31 21:55:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
from &amp;lt;- seq(from=start,to=end,by=5*60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, let’s move on to 2. If you have 12 million data, even if you use &lt;code&gt;purrr::map&lt;/code&gt; or &lt;code&gt;apply&lt;/code&gt; with &lt;code&gt;R&lt;/code&gt;, it takes a long time to call a function and it’s quite inefficient. I tried to use &lt;code&gt;sapply&lt;/code&gt;, but it didn’t complete the process and it was forced to terminate. &lt;code&gt;RCCp&lt;/code&gt; is useful in such a case. Although &lt;code&gt;R&lt;/code&gt; has many very useful functions for graphs and statistics, it is not very good at large repetition, including calls to user-defined functions (because it is a scripting language, rather than a compiling language, I mean). So, I write the part of repetitive process in &lt;code&gt;C++&lt;/code&gt; and compile it as &lt;code&gt;R&lt;/code&gt; function using &lt;code&gt;Rcpp&lt;/code&gt; and execute it. It is very efficient to compile and visualize the results and write them in &lt;code&gt;R&lt;/code&gt;. Also, &lt;code&gt;Rccp&lt;/code&gt; helps you to write &lt;code&gt;C++&lt;/code&gt; in a similar way to &lt;code&gt;R&lt;/code&gt; with less sense of discomfort. I think the following will give you a good idea of the details. It’s pretty well organized and is God, to say the least.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://teuder.github.io/rcpp4everyone_en/&#34;&gt;Rcpp for everyone&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s write the code for the second step. In coding, I used articles on the net for reference. C++ has a longer history and more users than &lt;code&gt;R&lt;/code&gt;, so you can find information you want to know.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;algorithm&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
DataFrame Rolling_r_cpp(
    DataFrame input,               // Data frame of (measurement time, measured value data)
    newDatetimeVector from,        // The starting point vector for the timing of the calculation
    double time_window = 5*60)  // Calculated window width (in seconds)
{ 
  
  // Extract the measurement time and value as a vector
  newDatetimeVector time = input[&amp;quot;DateTime&amp;quot;]; // This time is assumed to be sorted in ascending order.
  NumericVector     data = input[&amp;quot;logMid&amp;quot;];
  
  // The endpoint vector of the timing to be calculated
  newDatetimeVector to = from + time_window;
  
  // Number to calculate
  R_xlen_t N = from.length();
  
  // vector for storage 
  NumericVector value(N);
  
  // An object representing the position of a vector element
  newDatetimeVector::iterator begin = time.begin();
  newDatetimeVector::iterator end   = time.end();
  newDatetimeVector::iterator p1    = begin;
  newDatetimeVector::iterator p2    = begin;
  
  // Loop for window i
  for(R_xlen_t i = 0; i &amp;lt; N; ++i){
    // Rcout &amp;lt;&amp;lt; &amp;quot;i=&amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    double f = from[i];         // Time of the window&amp;#39;s start point
    double t = f + time_window; // The time of the window&amp;#39;s endpoint
    
    // If the endpoint of the window is before the first measurement time, it is NA or
    // If the starting point of the window is after the last measurement time, NA
    if(t &amp;lt;= *begin || f &amp;gt; *(end-1)){ 
      value[i]  = NA_REAL;
      continue;// Go to the next loop
    }
    
    // Vector time from position p1 and subsequent elements x
    // Let p1 be the position of the first element whose time is at the start point f &amp;quot;after&amp;quot; the window
    p1 = std::find_if(p1, end, [&amp;amp;f](double x){return f&amp;lt;=x;});
    // p1 = std::lower_bound(p1, end, f); //Same as above
    
    // Vector time from position p1 and subsequent elements x
    // Let p2 be the position of the last element whose time is &amp;quot;before&amp;quot; the endpoint t of the window
    // (In the below, this is accomplished by making the time one position before the &amp;#39;first element&amp;#39;, where the time is the window&amp;#39;s endpoint t &amp;#39;after&amp;#39;)
    p2 = std::find_if(p1, end, [&amp;amp;t](double x){return t&amp;lt;=x;}) - 1 ;
    // p2 = std::lower_bound(p1, end, t) - 1 ;//Same as above
    
    // Convert the position p1,p2 of an element to the element numbers i1, i2
    R_xlen_t i1 = p1 - begin;
    R_xlen_t i2 = p2 - begin; 
    
    
    // Checking the element number
    // C++ starts with the element number 0, so I&amp;#39;m adding 1 to match the R
    // Rcout &amp;lt;&amp;lt; &amp;quot;i1 = &amp;quot; &amp;lt;&amp;lt; i1+1 &amp;lt;&amp;lt; &amp;quot; i2 = &amp;quot; &amp;lt;&amp;lt; i2+1 &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    
    // Calculate the data in the relevant range
    if(i1&amp;gt;i2) {
      value[i] = NA_REAL; // When there is no data in the window
    } else { 
      value[i] = data[i2] - data[i1];
    }
    // ↑You can create various window functions by changing above
    
  }
  
  // Output the calculated time and the value as a data frame.
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, from),
      Named(&amp;quot;r&amp;quot;, value*100));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you compile the program with &lt;code&gt;Rcpp::sourceCpp&lt;/code&gt;, the function of &lt;code&gt;R&lt;/code&gt; can be executed as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- Rolling_r_cpp(JPYUSD,from))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       0.02       0.00       0.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It takes less than a second to process 12 million records. So Convenient!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r         
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014  
##  Median :2011-07-03 09:57:30   Median : 0.000  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880  
##                                NA&amp;#39;s   :29977&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The return is precisely calculated. The recommended length of the window is 270 in 5 min increments, but we’ll make it flexible as well. And we carefully process the &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;cmath&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
float rbv_cpp(
    NumericVector x, // Return vector to calculate rbv
    bool na_rm = true) // If NA is included in x, remove it and calculate it or
{
  
  // Get the number of calculations
  R_xlen_t N = x.length();
  
  // Define variables to contain the results of a calculation
  float out = 0;

  // Check for missing x
  LogicalVector lg_NA = is_na(x);
  
  // If there is a NA in x, whether to exclude that NA and calculate
  if(any(lg_NA).is_true() and na_rm==FALSE){
    out = NA_REAL; // Output NA as a result of the calculation
  } else {
    
    // Excluding NA
    if (any(lg_NA).is_true() and na_rm==TRUE){
      x[is_na(x)==TRUE] = 0.00; // Fill in the NA with zeros and effectively exclude it from the calculation.
    }
    
    // Compute the numerator (sum of rbv)
    for(R_xlen_t i = 1; i &amp;lt; N; ++i){
      out = out + std::abs(x[i])*std::abs(x[i-1]);
    }
    
    // Calculate the average and take the route.
    long denomi; //denominator
    if(N-sum(lg_NA)-2&amp;gt;0){
      denomi = N-sum(lg_NA)-2;
    } else {
      denomi = 1;
    }
    out = out/denomi;
    out = std::sqrt(out);
  }
  
  return out;
}

// [[Rcpp::export]]
DataFrame Rolling_rbv_cpp(
    DataFrame input, //Data frame of (measurement time, measured value data)
    int K = 270, // Rolling Window width to calculate
    bool na_pad = false, // Returning NA when the window width is insufficient
    bool na_remove = false // If the NA exists in the window width, exclude it from the calculation
){
  // Extract the return vector and number of samples
  NumericVector data = input[&amp;quot;r&amp;quot;];
  R_xlen_t T = data.length();
  
  // Prepare a vector to store the results
  NumericVector value(T);
  
  // Calculate and store RBVs per Windows width
  if(na_pad==TRUE){
    value[0] = NA_REAL; // return NA.
    value[1] = NA_REAL; // return NA.
    value[2] = NA_REAL; // return NA.
  } else {
    value[0] = 0; // Return zero.
    value[1] = 0; // Return zero.
    value[2] = 0; // Return zero.
  }
  
  for(R_xlen_t t = 3; t &amp;lt; T; ++t){
    // Bifurcation of the process depending on whether or not there is enough Windows width
    if (t-K&amp;gt;=0){
      value[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // Run a normal calculation
    } else if(na_pad==FALSE) {
      value[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Run a calculation with an incomplete Widnows width of less than K
    } else {
      value[t] = NA_REAL; // return NA.
    }
  }
  
  // Output the calculated time and value as a data frame.
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, input[&amp;quot;from&amp;quot;]),
      Named(&amp;quot;r&amp;quot;, data),
      Named(&amp;quot;rbv&amp;quot;,value));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, compile it and run it with &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- results %&amp;gt;% Rolling_rbv_cpp(na_remove = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       0.24       0.09       0.33&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So fast!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-jump-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Calculating Jump Statistics&lt;/h2&gt;
&lt;p&gt;Now let’s calculate the statistic &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_{t_i}\)&lt;/span&gt; from the returns and standard deviation we just calculated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Standardize the absolute value of the log return = Jump statistic
results &amp;lt;- results %&amp;gt;% dplyr::mutate(J=ifelse(rbv&amp;gt;0,abs(r)/rbv,NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what it looks like now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s move on to the Jump test. First, we need to define the useful functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Preparing Constants &amp;amp; Functions for Calculating Jump Test
c &amp;lt;- (2/pi)^0.5
Cn &amp;lt;- function(n){
  return((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5))
}
Sn &amp;lt;- function(n){
  1/(c*(2*log(n))^0.5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we perform the test. Rejected samples return 1 and all others 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Perform a jump test (10%) (return value is logical)
N &amp;lt;- NROW(results$J)
results &amp;lt;- results %&amp;gt;% dplyr::mutate(Jump = J &amp;gt; 2.25*Sn(N) + Cn(N))
summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423  
##     Jump        
##  Mode :logical  
##  FALSE:59864    
##  TRUE :257      
##  NA&amp;#39;s :44423    
##                 
##                 
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-using-ggplot2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Visualization using ggplot2&lt;/h2&gt;
&lt;p&gt;Now that the numbers have been calculated, let’s visualize them by plotting the intraday logarithmic return of JPY/USD in 5-minute increments for 2011/03/11 and the jump. By the way, the horizontal axis has been adjusted to Japan time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plotting about Jump at the time of the 2011/03/11 Great East Japan Earthquake
results %&amp;gt;% 
  dplyr::filter(from &amp;gt;= as.POSIXct(&amp;quot;2011-03-11 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;),from &amp;lt; as.POSIXct(&amp;quot;2011-03-12 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)) %&amp;gt;% 
  ggplot2::ggplot(ggplot2::aes(x=from,y=r)) +
  ggplot2::geom_path(linetype=3) +
  ggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=&amp;quot;red&amp;quot;)) +
  ggplot2::scale_x_datetime(date_breaks = &amp;quot;2 hours&amp;quot;, labels = scales::date_format(format=&amp;quot;%H:%M&amp;quot;,tz=&amp;quot;Asia/Tokyo&amp;quot;)) +
  ggplot2::ggtitle(&amp;quot;JPY/USD Jumps within Tohoku earthquake on 2011-3-11&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 36 row(s) containing missing values (geom_path).

## Warning: Removed 36 row(s) containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ve spent quite a bit of time writing this, and it’s 23:37 right now, so I’ll refrain from discussing it in depth, but since the earthquake occurred at 14:46:18, you can see that the market reacted to the weakening of the yen immediately after the disaster. After that, for some reason, the yen moved higher and peaked at 19:00. It is said that the yen is a safe asset, but this is the only time it is not safe given the heightened uncertainty.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Summary&lt;/h2&gt;
&lt;p&gt;I introduced the use of &lt;code&gt;Rcpp&lt;/code&gt; to improve the efficiency of &lt;code&gt;R&lt;/code&gt; analysis. The &lt;code&gt;C++&lt;/code&gt; is much faster than &lt;code&gt;R&lt;/code&gt; even if you write the code honestly, so it is hard to make coding mistakes. The &lt;code&gt;C++&lt;/code&gt; is much faster than &lt;code&gt;R&lt;/code&gt; even if you write the code in a simple way. Also, even if a compile error occurs, RStudio gives you a clue as to where the compile error is occurring, so there is no stress in that respect either, which is why I recommend it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A stochastic process with non-negative, integer, non-decreasing values.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The mean is not necessarily zero, depending on the shape of the drift term, but we are assuming a small enough drift term now. In the paper, it is defined more precisely.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is that backtest really reproducible?</title>
      <link>/en/post/post19/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post19/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-backtesting&#34;&gt;1. What is “Backtesting”?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#backtesting-overfits.&#34;&gt;2. Backtesting overfits.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-the-distribution-that-the-sharpe-ratio-follows&#34;&gt;3. What is the distribution that the Sharpe ratio follows?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trying-to-derive-the-minimum-backtest-length&#34;&gt;4. Trying to derive &lt;code&gt;the minimum backtest length&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#at-the-end&#34;&gt;4. At the end&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;what-is-backtesting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. What is “Backtesting”?&lt;/h2&gt;
&lt;p&gt;Backtesting is an algorithmic historical simulation of an investment strategy. Backtesting uses an algorithm to calculate the gains and losses that would have been incurred if the investment strategy you have drafted had been implemented over a period of time. Common statistics that evaluate the performance of an investment strategy, such as the Sharpe ratio and the information ratio, are used. Investors typically examine these back-testing statistics to determine asset allocation to the best performing investment (management) strategies, so asset managers do trial-and-error back-testing of a bloody number of times for good performance and present customer by that materials.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stat.ameba.jp/user_images/20190212/22/nash210/51/5f/j/o0705061514355131242.jpg&#34; alt=&#34;Backtest&#34; /&gt;
From an investor’s perspective, it is important to distinguish between in-sample (IS) and out-of-sample (OOS) performance of a back-tested investment strategy; IS performance is defined as the sample used to design the investment strategy (referred to in the machine learning literature as the “training period” or “training set”) It is what is called “OOS”) simulated in a sample (aka “test set”). OOS performance, on the other hand, is simulated on a sample (aka “test set”) that was not used to design the investment strategy. Since backtesting predicts the effectiveness of an investment strategy with its performance, it can be guaranteed to be reproducible and realistic if the IS performance matches the OOS performance. However, it is difficult to judge whether the backtest is reliable when you receive it because the results of the out-sample are future results. It cannot be called a pure out-sample as long as the results of the OOS can be fed back to improve the strategy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.triton.biz/blog1/wp-content/uploads/2018/04/pic001.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, when you receive a backtest with good results from a fund manager, it is very important that you manage to assess how realistic the simulation is. It is also important for fund managers to understand the uncertainty of their own backtesting results. In this post, we will look at how to evaluate the realism of a backtesting simulation and what to look out for in order to ensure a repeatable backtest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;backtesting-overfits.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Backtesting overfits.&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2308659&#34;&gt;Bailey, Borwein, López de Prado and Zhu (2015)&lt;/a&gt;, which shows that any financial time series can be used to backtest We argue that it is (relatively) easy to overfit (overlearn) a simulation. Here, overfitting is a machine learning concept that describes a situation in which the model focuses on specific observational data (IS data) rather than on the general structure.&lt;/p&gt;
&lt;p&gt;Bailey et. al. (2015) cite a situation where the backtesting results of a stock strategy are not good as an example of this claim. As the name implies, backtesting uses historical data, so it is possible to identify specific stocks that are experiencing losses and design the trading system to improve performance by adding some parameters to remove recommendations for those stocks ( (a technique known as “data snooping”). With a few repetitions of the simulation, you can derive “optimal parameters” that benefit from characteristics that are present in a particular sample but may be rare in the population.&lt;/p&gt;
&lt;p&gt;There is a vast accumulation of research in the machine learning literature to address the problem of overfitting. But Bailey et. al. (2015) argue that the methods proposed in the context of machine learning are generally not applicable to multiple investment problems. The reasons for this seem to be the following four points.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Machine learning methods to prevent overfitting require explicit point estimates and confidence intervals in the domain in which the event is defined in order to assess the explanatory power and quality of the prediction, because few investment strategies make such explicit predictions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“The E-mini S&amp;amp;P 500 is predicted to close around 1,600 with one standard deviation of 5 points at Friday’s close,” for example, but rather qualitative recommendations such as “buy” or “strong buy” are typically provided. Moreover, these forecasts do not have an explicit expiration date and are subject to change in the event of an unexpected event. On the other hand, the quantitative forecast is stated as Friday’s close.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;even if a particular investment strategy relies on a prediction formula, other components of the investment strategy may be overfitted.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In other words, there are many ways to overfit an investment strategy other than simply adjusting the prediction equation.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;because the methods of regression overfitting are parametric and involve many assumptions about data that are unobservable in the case of finance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;because some methods do not control for the number of trials.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bailey et. al. (2015) show that a &lt;strong&gt;comparatively low number of trials&lt;/strong&gt; is needed to identify investment strategies with relatively poor backtesting performance. Think of the number of trials here as the number of trials and errors. It also calculates &lt;code&gt;the minimum backtest length&lt;/code&gt; (MinBTL), which is the length of the backtest required for the number of trials. In this paper, the Sharpe ratio is always used to evaluate performance, but it can be applied to other performance measures as well. Let’s take a look at what it does.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-distribution-that-the-sharpe-ratio-follows&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. What is the distribution that the Sharpe ratio follows?&lt;/h2&gt;
&lt;p&gt;To derive the MinBTL, we first derive the (asymptotic) distribution of the Sharpe ratio. To begin with, the design of an investment strategy usually starts with the prior knowledge or belief that a particular pattern may help to predict the future value of a financial variable. For example, if you are aware of the lead-lag effect between bonds of different maturities, you can design a strategy that bets on a return to the equilibrium value if the yield curve rises. This model could take the form of a cointegration equation, a vector error correction model, or a system of stochastic differential equations.&lt;/p&gt;
&lt;p&gt;The number of such model configurations (or trials) is vast, and fund managers naturally want to choose the one that maximizes the performance of their strategy, and to do so they conduct historical simulations (back testing) (see above). Backtesting evaluates the optimal sample size, frequency of signal updates, risk sizing, stop loss, maximum holding period, etc., in combination with other variables.&lt;/p&gt;
&lt;p&gt;The Sharpe ratio used as a measure of performance assessment in this paper is a statistic that assesses the performance of a strategy based on a sample of historical returns, defined as the average excess return/standard deviation (risk) to BM. It is usually interpreted as “return to risk 1 standard deviation” and, depending on the asset class, if it is greater than 1, it can be considered a very good strategy. In the following, we will assume that the excess return &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt; of a strategy is a random variable of i.i.d. and follows a normal distribution. That is, we assume that the distribution of &lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt; is independent of &lt;span class=&#34;math inline&#34;&gt;\(r_s(t\neq s)\)&lt;/span&gt;. It is not a very realistic assumption, though.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_t \sim \mathcal{N}(\mu,\sigma^2)
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt; is the normal distribution of the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;. Now, the excess return &lt;span class=&#34;math inline&#34;&gt;\(r_{t}(q)\)&lt;/span&gt; at time t~t-q+1 is defined (I’m ignoring the compounding part.)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_{t}(q) \equiv r_{t} + r_{t-1} + ... + r_{t-q+1}
\]&lt;/span&gt;
Then, the annualized Sharpe ratio is represented&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
SR(q) &amp;amp;=&amp;amp; \frac{E[r_{t}(q)]}{\sqrt{Var(r_{t}(q))}}\\
&amp;amp;=&amp;amp; \frac{q\mu}{\sqrt{q}\sigma}\\
&amp;amp;=&amp;amp; \frac{\mu}{\sigma}\sqrt{q}
\end{eqnarray}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can express this as where &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the number (frequency) of returns per year. For example, &lt;span class=&#34;math inline&#34;&gt;\(q=365\)&lt;/span&gt; for daily returns (excluding leap years).&lt;/p&gt;
&lt;p&gt;The true value of &lt;span class=&#34;math inline&#34;&gt;\(SR\)&lt;/span&gt; cannot be known because &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; are generally unknown. So, let &lt;span class=&#34;math inline&#34;&gt;\(R_t\)&lt;/span&gt; be the sample return and the risk-free rate &lt;span class=&#34;math inline&#34;&gt;\(R^f\)&lt;/span&gt;(constant), we calculate the estimated Sharpe ratio by the sample mean of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=1/T\sum_{t=1}^T R_{t}-R^f\)&lt;/span&gt; and the sample standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}=\sqrt{1/T\sum_{t=1}^{T}(R_{t}-\hat{\mu})}\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is the sample size to be back-tested).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q) = \frac{\hat{\mu}}{\hat{\sigma}}\sqrt{q}
\]&lt;/span&gt;
As an inevitable consequence, the calculation of &lt;span class=&#34;math inline&#34;&gt;\(SR\)&lt;/span&gt; is likely to be accompanied by considerable estimation errors. Now, let us derive the main topic of this section, the asymptotic distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}\)&lt;/span&gt;. First, since the asymptotic distributions of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt; are finite and i.i.d., applying the central limit theorem derives&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{T}\hat{\mu}\sim^{a}\mathcal{N}(\mu,\sigma^2), \\
\sqrt{T}\hat{\sigma}^2\sim^a\mathcal{N}(\sigma^2,2\sigma^4)
\]&lt;/span&gt;
ince the Sharpe ratio is a random variable which is calculated from this &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt;, let us denote this function as &lt;span class=&#34;math inline&#34;&gt;\(g(\hat{{\boldsymbol \theta}})\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}=(\hat{\mu},\hat{\sigma}^2)\)&lt;/span&gt;. Now, since it is i.i.d., &lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}\)&lt;/span&gt; is independent of each other, and, from the above discussion, the asymptotic joint distribution is written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{T}\hat{{\boldsymbol \theta}} \sim^a \mathcal{N}({\boldsymbol \theta},{\boldsymbol V_{\boldsymbol \theta}})
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol V_{\boldsymbol \theta}}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
{\boldsymbol V_{\boldsymbol \theta}} = \left( 
    \begin{array}{cccc}
      \sigma^2 &amp;amp; 0\\
      0 &amp;amp; 2\sigma^4\\
    \end{array}
  \right)
\]&lt;/span&gt;
Since the sharp ratio estimates are now only a function of &lt;span class=&#34;math inline&#34;&gt;\(g(\hat{{\boldsymbol \theta}})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}\)&lt;/span&gt;, from the delta method, the following&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR} = g(\hat{{\boldsymbol \theta}}) \sim^a \mathcal{N}(g({\boldsymbol \theta}),\boldsymbol V_g)
\]&lt;/span&gt;
asymptotically follows a normal distribution. where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol V_g\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol V_g=\frac{\partial g}{\partial{\boldsymbol \theta}}{\boldsymbol V_{\boldsymbol \theta}}\frac{\partial g}{\partial{\boldsymbol \theta}&amp;#39;}
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(g({\boldsymbol \theta})=\mu/\sigma\)&lt;/span&gt;なので、
Since &lt;span class=&#34;math inline&#34;&gt;\(g({\boldsymbol \theta})=\mu/\sigma\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial g}{\partial{\boldsymbol \theta}&amp;#39;} = \left[ 
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma^2}\\
    \end{array}
  \right]
  = \left[ 
    \begin{array}{cccc}
      \frac{1}{\sigma}\\
      -\frac{\mu}{2\sigma^3}\\
    \end{array}
  \right]
\]&lt;/span&gt;
Therefore, you can derive&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\boldsymbol V_g &amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}, \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right)
  \left( 
    \begin{array}{cccc}
      \sigma^2 &amp;amp; 0\\
      0 &amp;amp; 2\sigma^4\\
    \end{array}
  \right)
  \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right) \\
  &amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\sigma^2, \frac{\partial g}{\partial \sigma}2\sigma^4\\
    \end{array}
  \right)
    \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right) \\
  &amp;amp;=&amp;amp; (\frac{\partial g}{\partial \mu})^2\sigma^2 + (\frac{\partial g}{\partial \sigma})^2\sigma^4 \\
  &amp;amp;=&amp;amp; 1 + \frac{\mu^2}{2\sigma^2} \\
  &amp;amp;=&amp;amp; 1 + \frac{1}{2}SR^2
\end{eqnarray}
\]&lt;/span&gt;
You may need to be careful when you see good performance because the variance tends to be exponentially larger as the absolute value of the Sharpe ratio increases. Here’s the distribution that the annualized Sharpe ratio estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}(q)\)&lt;/span&gt; follows&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q)\sim^a \mathcal{N}(\sqrt{q}SR,\frac{V(q)}{T}) \\
V(q) = q{\boldsymbol V}_g = q(1 + \frac{1}{2}SR^2)
\]&lt;/span&gt;
Now, if &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is the number of years of backtesting, we can write &lt;span class=&#34;math inline&#34;&gt;\(T=yq\)&lt;/span&gt; and use this to rewrite the above equation as follows (for a 3-year measurement with daily returns, the sample size &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(T=3×365=1095\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q)\sim^a \mathcal{N}(\sqrt{q}SR,\frac{1+\frac{1}{2}SR^2}{y}) \tag{1}
\]&lt;/span&gt;
The frequency &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; affects the mean of the Sharpe ratio, but not the variance. We can now derive an asymptotic distribution of the Sharpe ratio estimates. Now, what do we wanted to do with this? We were thinking about the reliability of the backtest. In other words, what is the probability that a backtest of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; investment strategy ideas that FM twists and turns to develop a new product will produce a very high (good) value even though the true value of all of those Sharpe ratios is zero. In Bailey et. al. (2015), it was described as follows&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How high is the expected maximum Sharpe ratio IS among a set of strategy configurations where the true Sharpe ratio is zero?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We also want to know how long you should be backtesting for in order to reduce the value of the expected maximum Sharpe ratio.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;trying-to-derive-the-minimum-backtest-length&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Trying to derive &lt;code&gt;the minimum backtest length&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;The situation we are considering now is that let &lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; 1 year for simplicity, then from equation (1) &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}(q)\)&lt;/span&gt; follows the standard normal distribution &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(0,1)\)&lt;/span&gt;. Now, we will consider the expected value of the maximum value &lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]_N\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}_n(n=1,2,...N)\)&lt;/span&gt;, but as those with good instincts will have noticed, the discussion goes into the context of extreme value statistics. Since &lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}_n\sim\mathcal{N}(0,1)\)&lt;/span&gt; is i.i.d., the extreme value distribution of that maximum statistic becomes Gumbel distribution from the Fisher-Tippett-Gnedenko theorem (sorry, I haven’t been able to follow the proof).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\lim_{N\rightarrow\infty}prob[\frac{\max[\hat{SR}]_N-\alpha}{\beta}\leq x] = G(x) = e^{-e^{-x}}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\alpha=Z(x)^{-1}[1-1/N], \beta=Z(x)^{-1}[1-1/Ne^{-1}]-\alpha\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Z(x)\)&lt;/span&gt; represents the cumulative distribution function of the standard normal distribution. The moment generating function of the Gumbel distribution &lt;span class=&#34;math inline&#34;&gt;\(M_x(t)\)&lt;/span&gt; is written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
M_x(t) &amp;amp;=&amp;amp; E[e^{tx}] = \int_{-\infty}^\infty e^{tx}e^{-x}e^{-e^{-x}}dx \\
\end{eqnarray}
\]&lt;/span&gt;
Converting variables with &lt;span class=&#34;math inline&#34;&gt;\(x=-\log(y)\)&lt;/span&gt; gives &lt;span class=&#34;math inline&#34;&gt;\(dx/dy=-1/y=-(e^{-x})^{-1}\)&lt;/span&gt;, so&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
M_x(t) &amp;amp;=&amp;amp; \int_{\infty}^0-e^{-t\log(y)}e^{-y}dy \\
&amp;amp;=&amp;amp; \int_{0}^\infty y^{-t}e^{-y}dy \\
&amp;amp;=&amp;amp; \Gamma(1-t)
\end{eqnarray}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(Gamma(x)\)&lt;/span&gt; is a gamma function. From here, the expected value (mean) of the standardized maximum statistic is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\lim_{N\rightarrow\infty} E[\frac{\max[\hat{SR}]_N-\alpha}{\beta}] &amp;amp;=&amp;amp; M_x&amp;#39;(t)|_{t=0} \\
&amp;amp;=&amp;amp; (-1)\Gamma&amp;#39;(1) \\
&amp;amp;=&amp;amp; (-1)(-\gamma) = \gamma
\end{eqnarray}
\]&lt;/span&gt;
Here, $… $ is the Euler-Mascheroni constant. Thus, when &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is large, the expected value of the maximum statistic of the standard normal distribution of i.i.d. is approximated as (&lt;span class=&#34;math inline&#34;&gt;\(N&amp;gt;1\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[\max[\hat{SR}]] \approx \alpha + \gamma\beta = (1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}] \tag{2}
\]&lt;/span&gt;
This is Proposition 1 of Bailey et. al. (2015). We plot &lt;span class=&#34;math inline&#34;&gt;\(E[\max[\hat{SR}]]\)&lt;/span&gt; as a function of the number of strategies (number of trials and errors) &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, which is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ExMaxSR = function(N){
  gamma_ct = -digamma(1)
  Z = qnorm(0.99)
  return((1-gamma_ct)*Z*(1-1/N) + gamma_ct*Z*(1-1/N*exp(1)^{-1}))
}
N = list(0:100)
result = purrr::map(N,ExMaxSR)
ggplot2::ggplot(data.frame(ExpMaxSR = unlist(result),N = unlist(N)),aes(x=N,y=ExpMaxSR)) +
  geom_line(size=1) + ylim(0,3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;You can see that the expected value of &lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]\)&lt;/span&gt; is rapidly increasing for the small &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. At &lt;span class=&#34;math inline&#34;&gt;\(N=10\)&lt;/span&gt;, we expect to find at least one apparently quite good performing strategy, even though the true value of the Sharpe ratio of all strategies is zero. In finance, hold-out back-testing is often used, but this method does not take into account the number of trials, which is why it does not return reliable results when &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is large. Don’t you think it’s very risky to do a lot of simulations in order to improve the backtest results? In the end, only the best performing of the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; strategies will show up in the presentation material, so even if you consider 10 strategies, as in this example, any of them will have a Sharpe ratio distributed around 1.87. Of course, we don’t include the number of trials and errors in our materials, so it’s very misleading. When evaluating these materials, you might want to suspect false positives first.&lt;/p&gt;
&lt;p&gt;So, as for what to do, Bailey et. al. (2015) calculate the Minimum Backtest Length. In short, they caution that as the number of trials (errors) &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is increased, the number of years of backtesting &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; should also increase. Let’s show the relationship between &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and the Minimum Backtest Length. As before, we will assume that &lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt;, but consider the case with &lt;span class=&#34;math inline&#34;&gt;\(y\neq 1\)&lt;/span&gt;. The expected value of the maximum statistic of the annualized Sharpe ratio is from equation (2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[\max[\hat{SR}(q)]_N] \approx y^{-1/2}((1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}])
\]&lt;/span&gt;
By solving this for &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, you can get MinBTL.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MinBTL \approx (\frac{(1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}]}{\bar{E[\max[\hat{SR}(q)]_N]}})^2
\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\bar{E[\max[\hat{SR}(q)]_N]}\)&lt;/span&gt; is the upper limit of &lt;span class=&#34;math inline&#34;&gt;\(E[\max[\hat{SR}(q)]_N]\)&lt;/span&gt;, and the &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; strategy, where the true value of the Sharpe ratio is zero, suppresses the value that the maximum Sharpe ratio statistic can take. In doing so, the required backtesting years &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; are derived as MinBTL. Plotting the MinBTL as a function of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, with &lt;span class=&#34;math inline&#34;&gt;\(\bar{E[\max[\hat{SR}(q)]_N]}=1\)&lt;/span&gt; is shown below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MinBTL &amp;lt;- function(N,MaxSR){
  return((ExMaxSR(N)/MaxSR)^2)
}
N = list(1:100)
result = purrr::map2(N,1,MinBTL)
ggplot2::ggplot(data.frame(MinBTL = unlist(result),N = unlist(N)),aes(x=N,y=MinBTL)) +
  geom_line(size=1) + ylim(0,6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simSR &amp;lt;- function(T1){
    r = rnorm(T1)
    return(mean(r)/sd(r))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If your backtesting period is less than 3 years, the number of trials (or errors) &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; should be limited to one. It is important to note that even if you are backtesting within the MinBTL, it is still possible to overfit. In other words, the MinBTL is a necessary condition, not a sufficient condition.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;at-the-end&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. At the end&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257497&#34;&gt;López de Prado (2018)&lt;/a&gt; lists the following as generic means of preventing overfitting&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Develop models for entire asset classes or investment universes, rather than for specific securities. Investors diversify, hence they do not make mistake X only on security Y. If you find mistake X only on security Y, no matter how apparently profitable, it is likely a false discovery.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apply bagging as a means to both prevent overfitting and reduce the variance of the forecasting error. If bagging deteriorates the performance of a strategy, it was likely overfit to a small number of observations or outliers.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Do not backtest until all your research is complete.&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Record every backtest conducted on a dataset so that the probability of backtest overfitting may be estimated on the final selected result (see &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2326253&#34;&gt;Bailey, Borwein, López de Prado and Zhu(2017)&lt;/a&gt;), and the Sharpe ratio may be properly deflated by the number of trials carried out (&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2465675&#34;&gt;Bailey and López de Prado(2014.b)&lt;/a&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simulate scenarios rather than history. A standard backtest is a historical simulation, which can be easily overfit. History is just the random path that was realized, and it could have been entirely different. Your strategy should be profitable under a wide range of scenarios, not just the anecdotal historical path. It is harder to overfit the outcome of thousands of “what if” scenarios.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do not research under the influence of a backtest. If the backtest fails to identify a profitable strategy,
start from scratch. Resist the temptation of reusing those results.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think 3 and 6 are relevant contexts for today’s post. There is an accumulation of other research in this area, so if you’re going to do backtesting on the job, it’s good to study techniques, but I recommend learning about the proper way to operate backtesting as a primer in the first place.&lt;br /&gt;
In this post, I’ve decided to tackle a slightly different topic from the usual one. I often see the results of backtests in my own work, and I often do hold-out backtests on this blog. I will continue to follow my research on this topic so that I can understand and evaluate the uncertainty of the results obtained.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
