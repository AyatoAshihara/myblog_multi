<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>preprocessing | 東京の資産運用会社で働く社会人が研究に没頭するブログ</title>
    <link>/en/tag/preprocessing/</link>
      <atom:link href="/en/tag/preprocessing/index.xml" rel="self" type="application/rss+xml" />
    <description>preprocessing</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 19 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>preprocessing</title>
      <link>/en/tag/preprocessing/</link>
    </image>
    
    <item>
      <title>Get macro panel data from OECD.org via API</title>
      <link>/en/post/post22/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post22/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#oecd.stat-web-api&#34;&gt;1.OECD.Stat Web API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pandasdmx&#34;&gt;2.pandasdmx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;3.implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#another-matter&#34;&gt;4. Another matter…&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. There are several ways to collect macroeconomic data, but collecting data for each country can be a challenge. However, you can automate the tedious process of collecting data from the OECD via API. Today, I will introduce the method.&lt;/p&gt;
&lt;div id=&#34;oecd.stat-web-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.OECD.Stat Web API&lt;/h2&gt;
&lt;p&gt;OECD.org offers a service called &lt;a href=&#34;https://stats.oecd.org/&#34;&gt;OECD.Stat&lt;/a&gt;, which provides a variety of economic data for OECD and certain non-member countries. You can also download the csv data manually by going to the website. Since OECD provides a web API, you only need to use &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;R&lt;/code&gt; to do this.&lt;/p&gt;
&lt;p&gt;&lt;Specifics of the OECD implementation&gt;&lt;/p&gt;
&lt;p&gt;Below is a list of implementation details for specific OECD REST SDMX interfaces at this time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Only anonymous queries are supported and there is no authentication.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each response is limited to 1,000,000 observations.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The maximum length of the request URL is 1000 characters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cross-origin requests are supported in the &lt;code&gt;CORS&lt;/code&gt; header (see &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/cors/&#34;&gt;here&lt;/a&gt; for more information about &lt;code&gt;CORS&lt;/code&gt;).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Errors are not returned in the results, but HTTP status codes and messages are set according to the Web Service Guidelines.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If a nonexistent dataset is requested, &lt;code&gt;401 Unauthorized&lt;/code&gt; is returned.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The source (or Agency ID) parameter of the &lt;code&gt;REST&lt;/code&gt; query is required, but the &lt;code&gt;ALL&lt;/code&gt; keyword is supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Versioning is not supported: the latest implementation version is always used.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Sorting of data is not supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The &lt;code&gt;lastNObservations&lt;/code&gt; parameter is not supported.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Even when &lt;code&gt;dimensionAtObservation=AllDimensions&lt;/code&gt; is used, the observations follow a chronological (or import-specific) order.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Searching for reference metadata is not supported at this time.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pandasdmx&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.pandasdmx&lt;/h2&gt;
&lt;p&gt;The Web API is provided in the form of &lt;code&gt;sdmx-json&lt;/code&gt;. There is a useful package for using it in &lt;code&gt;Python&lt;/code&gt;, which is called &lt;code&gt;pandasdmx**&lt;/code&gt;. Here’s how to download the data.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Import &lt;code&gt;pandasdmx&lt;/code&gt;, pass &lt;code&gt;OECD&lt;/code&gt; to &lt;code&gt;Request&lt;/code&gt; method as an argument and create &lt;code&gt;api.Request&lt;/code&gt; object.&lt;/li&gt;
&lt;li&gt;Pass the query condition to the data method of the &lt;code&gt;api.Request&lt;/code&gt; object, and download the data of &lt;code&gt;sdmx-json&lt;/code&gt; format from OECD.org.&lt;/li&gt;
&lt;li&gt;Format the downloaded data into a &lt;code&gt;pandas&lt;/code&gt; data frame with the method &lt;code&gt;to_pandas()&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.implementation&lt;/h2&gt;
&lt;p&gt;Let’s do this in practice. What we’ll get is the &lt;code&gt;**Revisions Analysis Dataset -- Infra-annual Economic Indicators**&lt;/code&gt;, one of the OECD datasets, the &lt;code&gt;Monthly Ecnomic Indicator&lt;/code&gt; (MEI). We have access to all data, including revisions to the preliminary data on key economic variables (such as gross domestic product and its expenditure items, industrial production and construction output indices, balance of payments, composite key indicators, consumer price index, retail trade volume, unemployment rate, number of workers, hourly wages, money supply, and trade statistics), as first published You can see everything from data to confirmed data with corrections. The dataset provides a snapshot of data that were previously available for analysis in the Leading Economic Indicators database at monthly intervals beginning in February 1999. In other words, the dataset allows us to build predictive models based on the data available at each point in time. The most recent data is useful, but it is preliminary and therefore subject to uncertainty. The problem is that this situation cannot be replicated when backtesting, and the analysis is often done under a better environment than the actual operation. This is the so-called &lt;code&gt;Jagged Edge&lt;/code&gt; problem. In this dataset, we think it is very useful because we can reproduce the situation of actual operation. This time, you will get the following data items.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Indicators&lt;/th&gt;
&lt;th&gt;Statistical ID&lt;/th&gt;
&lt;th&gt;Frequency&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Gross Domestic Product&lt;/td&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Index of Industrial Production&lt;/td&gt;
&lt;td&gt;201&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Retail Trade Volume&lt;/td&gt;
&lt;td&gt;202&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Monetary Aggregates&lt;/td&gt;
&lt;td&gt;601&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;International Trade in Goods&lt;/td&gt;
&lt;td&gt;702+703&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Balance of Payments&lt;/td&gt;
&lt;td&gt;701&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Employment&lt;/td&gt;
&lt;td&gt;502&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Harmonised Unemployment Rates&lt;/td&gt;
&lt;td&gt;501&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Hourly Earnings in Manufacturing&lt;/td&gt;
&lt;td&gt;503&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Early Estimates of Unit Labor Cost&lt;/td&gt;
&lt;td&gt;504&lt;/td&gt;
&lt;td&gt;Quarterly&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Production of Construction&lt;/td&gt;
&lt;td&gt;203&lt;/td&gt;
&lt;td&gt;Monthly&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;First, we define the functions. The arguments are database ID, other IDs (country IDs and statistical IDs), start point and end point.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandasdmx as sdmx&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## C:\Users\aashi\Anaconda3\lib\site-packages\pandasdmx\remote.py:13: RuntimeWarning: optional dependency requests_cache is not installed; cache options to Session() have no effect
##   RuntimeWarning,&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;oecd = sdmx.Request(&amp;#39;OECD&amp;#39;)
def resp_OECD(dsname,dimensions,start,end):
    dim_args = [&amp;#39;+&amp;#39;.join(d) for d in dimensions]
    dim_str = &amp;#39;.&amp;#39;.join(dim_args)
    resp = oecd.data(resource_id=dsname, key=dim_str + &amp;quot;/all?startTime=&amp;quot; + start + &amp;quot;&amp;amp;endTime=&amp;quot; + end)
    df = resp.to_pandas().reset_index()
    return(df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Specify the dimension from which the data will be obtained. Below, (1) country, (2) statistical items, (3) time of acquisition, and (4) frequency are specified with a tuple.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dimensions = ((&amp;#39;USA&amp;#39;,&amp;#39;JPN&amp;#39;,&amp;#39;GBR&amp;#39;,&amp;#39;FRA&amp;#39;,&amp;#39;DEU&amp;#39;,&amp;#39;ITA&amp;#39;,&amp;#39;CAN&amp;#39;,&amp;#39;NLD&amp;#39;,&amp;#39;BEL&amp;#39;,&amp;#39;SWE&amp;#39;,&amp;#39;CHE&amp;#39;),(&amp;#39;201&amp;#39;,&amp;#39;202&amp;#39;,&amp;#39;601&amp;#39;,&amp;#39;702&amp;#39;,&amp;#39;703&amp;#39;,&amp;#39;701&amp;#39;,&amp;#39;502&amp;#39;,&amp;#39;503&amp;#39;,&amp;#39;504&amp;#39;,&amp;#39;203&amp;#39;),(&amp;quot;202001&amp;quot;,&amp;quot;202002&amp;quot;,&amp;quot;202003&amp;quot;,&amp;quot;202004&amp;quot;,&amp;quot;202005&amp;quot;,&amp;quot;202006&amp;quot;,&amp;quot;202007&amp;quot;,&amp;quot;202008&amp;quot;),(&amp;quot;M&amp;quot;,&amp;quot;Q&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s execute the function.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;result = resp_OECD(&amp;#39;MEI_ARCHIVE&amp;#39;,dimensions,&amp;#39;2019-Q1&amp;#39;,&amp;#39;2020-Q2&amp;#39;)
result.count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOCATION       8266
## VAR            8266
## EDI            8266
## FREQUENCY      8266
## TIME_PERIOD    8266
## value          8266
## dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s look at the first few cases of data.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;result.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   LOCATION  VAR     EDI FREQUENCY TIME_PERIOD  value
## 0      BEL  201  202001         M     2019-01  112.5
## 1      BEL  201  202001         M     2019-02  111.8
## 2      BEL  201  202001         M     2019-03  109.9
## 3      BEL  201  202001         M     2019-04  113.5
## 4      BEL  201  202001         M     2019-05  112.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see that the data is stored in tidy form (long type). The most right value is stored as a value, and the other indexes are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LOCATION - Country&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;VAR - Items&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;EDI - At the time of acquisition (in the case of MEI_ARCHIVE)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;FREQUENCY - Frequency (monthly, quarterly, etc.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TIME_PERIOD - Reference point&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, the same ` exists in rows with different EDIs. For example, above you can see the data for 2019-01~2019-05 available as of 2020/01 for the Belgian (BEL) Industrial Production Index (201). This is very much appreciated as it is provided in Long format, which is also easy to visualize and regress. Here’s a visualization of the industrial production index as it is updated.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

result = result[result[&amp;#39;FREQUENCY&amp;#39;]==&amp;#39;M&amp;#39;]
result[&amp;#39;TIME_PERIOD&amp;#39;] = pd.to_datetime(result[&amp;#39;TIME_PERIOD&amp;#39;],format=&amp;#39;%Y-%m&amp;#39;)
sns.relplot(data=result[lambda df: (df.VAR==&amp;#39;201&amp;#39;) &amp;amp; (pd.to_numeric(df.EDI) &amp;gt; 202004)],x=&amp;#39;TIME_PERIOD&amp;#39;,y=&amp;#39;value&amp;#39;,hue=&amp;#39;LOCATION&amp;#39;,kind=&amp;#39;line&amp;#39;,col=&amp;#39;EDI&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;seaborn.axisgrid.FacetGrid object at 0x00000000316BC3C8&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;2035&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While we can see that the line graphs are depressed as the economic damage from the corona increases, there have been subtle but significant revisions to the historical values from preliminary to confirmed. We can also see that there is a lag in the release of statistical data by country. Belgium seems to be the slowest to release the data. When I have time, I would like to add a simple analysis of the forecasting model using this data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-matter&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Another matter…&lt;/h2&gt;
&lt;p&gt;Python 3 エンジニア認定データ分析試験に合格しました。合格率70%だけあって、かなり簡単でしたが&lt;code&gt;Python&lt;/code&gt;を基礎から見返すいい機会になりました。今やっている業務ではデータ分析はおろか&lt;code&gt;Python&lt;/code&gt;や&lt;code&gt;R&lt;/code&gt;を使う機会すらないので、転職も含めた可能性を考えています。とりあえず、以下の資格を今年度中に取得する予定で、金融にこだわらずにスキルを活かせるポストを探していこうと思います。ダイエットと同じで宣言して自分を追い込まないと。。。&lt;/p&gt;
&lt;p&gt;I passed the Python 3 Engineer Certification Data Analysis exam. It was pretty easy, with only a 70% pass rate, but it was a good opportunity to revisit the basics of &lt;code&gt;Python&lt;/code&gt;. I haven’t even had the opportunity to use &lt;code&gt;Python&lt;/code&gt; or &lt;code&gt;R&lt;/code&gt;, let alone data analysis, in the work I’m doing now, so I’m considering the possibility of a career change. In the meantime, I plan to get the following qualifications by the end of this year, and I’ll be looking for a post where I can use my skills without focusing on finance. Like a diet, I need to declare and push myself.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;G Test&lt;/li&gt;
&lt;li&gt;Oracle Database Master Silver SQL&lt;/li&gt;
&lt;li&gt;Linuc level 1&lt;/li&gt;
&lt;li&gt;Fundamental Information Technology Engineer Examination&lt;/li&gt;
&lt;li&gt;AWS Certified Solutions Architect - Associate&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will report on the status of my acceptance on my blog each time.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rcpp to speed up data handling (using Tick data processing as an example)</title>
      <link>/en/post/post21/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post21/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;what-i-want-to-do&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;0. What I want to do&lt;/h2&gt;
&lt;p&gt;What I’m going to show you is pre-processing (and analysis) using the Tick data of the exchange rate, as I mentioned above. I won’t go into the details of this article because my main focus is to improve efficiency using &lt;code&gt;Rcpp&lt;/code&gt;, but I will briefly describe what I want to do first. I will not go into details, but I will show what we want to do in a nutshell first.&lt;/p&gt;
&lt;p&gt;What we want to do is to detect a &lt;em&gt;Jump&lt;/em&gt; from the five-minute incremental return of the JPY/USD rate. Here, a jump is the point at which the exchange rate has risen (fallen) sharply compared to the previous rate. During the day, the exchange rate moves in small increments, but when there is an event, it rises (or falls) significantly. It is very interesting to see what kind of event causes the jump. In order to test this, we first need to detect jumps. The following paper is a reference point.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/rfs/article-abstract/21/6/2535/1574138?redirectedFrom=fulltext&#34;&gt;Suzanne S. Lee &amp;amp; Per A. Mykland, 2008. “Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,” Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is a very well-respected paper with 204 Citation. We will scratch out the estimation method. First, let the continuous compound return be &lt;span class=&#34;math inline&#34;&gt;\(d\log S(t)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt;. where &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is the price of the asset at &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. If there are no jumps in the market, &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is assumed to follow the following stochastic process&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) \tag{1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt; is the standard Brownian motion, &lt;span class=&#34;math inline&#34;&gt;\(\mu(t)\)&lt;/span&gt; is the drift term, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma(t)\)&lt;/span&gt; is the spot volatility. Also, when there is a Jump, &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt; is assumed to follow the following stochastic process&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) + Y(t)dJ(t) \tag{2}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(J(t)\)&lt;/span&gt; is a counting process independent of &lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt;. &lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(Y(t)\)&lt;/span&gt; represents the size of the jump and is a predictable process.&lt;/p&gt;
&lt;p&gt;Next, consider the logarithmic return of &lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;. That is, &lt;span class=&#34;math inline&#34;&gt;\(\log S(t_i)/S(t_{i-1})\)&lt;/span&gt;, which follows a normal distribution &lt;span class=&#34;math inline&#34;&gt;\(N(0, \sigma(t_i))\)&lt;/span&gt;. &lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;Now, we define the statistic &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; when there is a jump from &lt;span class=&#34;math inline&#34;&gt;\(t_{i-1}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt; as follows.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} \equiv \frac{|\log S(t_i)/S(t_{i-1})|}{\hat{\sigma}_{t_i}} \tag{3}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is a simple standardization of the absolute value of the log return, but uses the “Realized Bipower Variation” defined below as an estimator of the standard deviation.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{\sigma}_{t_i} = \frac{1}{K-2}\sum_{j=i-K+2}^{i-2}|\log S(t_j)/\log S(t_{j-1})||\log S(t_{j-1})/\log S(t_{j-2})| \tag{4}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the number of sample sizes contained in the Window. If we use a return in 5-minute increments and the jump occurs at 10:00 on 9/10/2020, and &lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt;, then we will calculate using samples from the previous day, 9/9/2020 11:30 to 9/11/2020 09:55. What we’re doing is adding up the absolute value of the return multiplied by the absolute value of the return, which seems to make it difficult for the estimate of the next instant after the jump occurs (i.e., &lt;span class=&#34;math inline&#34;&gt;\(t_{i+1}\)&lt;/span&gt; and so on) to be affected by the jump. Incidentally, &lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt; is introduced in another paper as a recommended value for returns in 5-minute increments.&lt;/p&gt;
&lt;p&gt;Let us move on to how the calculated jump statistic &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt; is used in the statistical test to detect jumps. This is done by considering the maximum value of the random variable &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt; (also a random variable), and if it deviates significantly from that distribution (like 95% of points), the return of the statistic is the jump.&lt;/p&gt;
&lt;p&gt;If there is no Jump in the period &lt;span class=&#34;math inline&#34;&gt;\([t_{i-1},t_{i}]\)&lt;/span&gt;, then with the length of this period &lt;span class=&#34;math inline&#34;&gt;\(\Delta=t_{i}-t_{i-1}\)&lt;/span&gt; approached to &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;, i.e., &lt;span class=&#34;math inline&#34;&gt;\(\Delta\rightarrow0\)&lt;/span&gt;, the sample maximum of the absolute value of the standard normal variable converges to the Gumbel distribution. Thus, Jump can reject and detect the null hypothesis when the following conditions are met&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} &amp;gt; G^{-1}(1-\alpha)S_{n} + C_{n} \tag{5}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(G^{-1}(1-\alpha)\)&lt;/span&gt; is the &lt;span class=&#34;math inline&#34;&gt;\((1-\alpha)\)&lt;/span&gt; quantile function of the standard Gumbell distribution. If &lt;span class=&#34;math inline&#34;&gt;\(\alpha=10%\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(G^{-1}(1-\alpha)=2.25\)&lt;/span&gt;. Note that (We won’t derive it, but we can prove it using equations 1 and 2)&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_{n} = \frac{1}{c(2\log n)^{0.5}} \\
C_{n} = \frac{(2\log n)^{0.5}}{c}-\frac{\log \pi+\log(\log n)}{2c(2\log n)^{0.5}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(c=(2/\pi)^{0.5}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the total sample size used for estimation.
Finally, &lt;span class=&#34;math inline&#34;&gt;\(Jump_{t_i}\)&lt;/span&gt; is calculated by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Jump_{t_i} = \log\frac{S(t_i)}{S(t_{i-1})}×I(\mathcal{L(i)} - G^{-1}(1-\alpha)S_{n} + C_{n})\tag{6}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(⋅)\)&lt;/span&gt; is an Indicator function that returns 1 if the content is greater than 0 and 0 otherwise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;loading-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Loading data&lt;/h2&gt;
&lt;p&gt;So now that we know how to estimate, let’s load the Tick data first. The data is csv from &lt;code&gt;QuantDataManager&lt;/code&gt; and saved in a working directory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)

# Read Tick data
strPath &amp;lt;- r&amp;quot;(C:\Users\hogehoge\JPYUSD_Tick_2011.csv)&amp;quot;
JPYUSD &amp;lt;- readr::read_csv(strPath)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On an unrelated note, I recently upgraded R to 4.0.2, and I’m quite happy to say that with 4.0 and above, you can escape the strings made by &lt;code&gt;Python&lt;/code&gt;, which relieves some of the stress I’ve been experiencing.&lt;/p&gt;
&lt;p&gt;The data looks like the following: in addition to the date, the Bid value, Ask value and the volume of transactions are stored. Here, we use the 2011 tick. The reason for this is to cover the dollar/yen at the time of the Great East Japan Earthquake.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     DateTime                        Bid             Ask            Volume     
##  Min.   :2011-01-03 07:00:00   Min.   :75.57   Min.   :75.58   Min.   : 1.00  
##  1st Qu.:2011-03-30 15:09:23   1st Qu.:77.43   1st Qu.:77.44   1st Qu.: 2.00  
##  Median :2011-06-15 14:00:09   Median :80.40   Median :80.42   Median : 2.00  
##  Mean   :2011-06-22 05:43:11   Mean   :79.91   Mean   :79.92   Mean   : 2.55  
##  3rd Qu.:2011-09-09 13:54:51   3rd Qu.:81.93   3rd Qu.:81.94   3rd Qu.: 3.00  
##  Max.   :2011-12-30 06:59:59   Max.   :85.52   Max.   :85.54   Max.   :90.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By the way, &lt;code&gt;DateTime&lt;/code&gt; includes the period from 07:00:00 on 2011/1/3 to 06:59:59 on 2011-12-30 (16:59:59 on 2011-12-30) in Japan by UTC. The sample size is approximately 12 million entries.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NROW(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11946621&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Preprocessing&lt;/h2&gt;
&lt;p&gt;Then calculate the median value from Bid and Ask and take the logarithm to calculate the return later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Calculate the median value of Ask and Bid and make it logarithmic (for calculating the logarithmic return).
JPYUSD &amp;lt;- JPYUSD %&amp;gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %&amp;gt;% 
                     dplyr::mutate(logMid = log(Mid))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I format the currently irregularly arranged trading data into 5-minute increments of returns. The way to do it is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a &lt;code&gt;POSIXct&lt;/code&gt; vector with 1 year chopped every 5 minutes.&lt;/li&gt;
&lt;li&gt;create a function that calculates the logarithmic return from the first and last sample in the window of 5min in turn, if you pass 1. as an argument.&lt;/li&gt;
&lt;li&gt;execute. This is the plan. First, create the vector of 1.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create POSIX vector to calculate returns in 5min increments (288 x days)
start &amp;lt;- as.POSIXct(&amp;quot;2011-01-02 22:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
end &amp;lt;- as.POSIXct(&amp;quot;2011-12-31 21:55:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
from &amp;lt;- seq(from=start,to=end,by=5*60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, let’s move on to 2. If you have 12 million data, even if you use &lt;code&gt;purrr::map&lt;/code&gt; or &lt;code&gt;apply&lt;/code&gt; with &lt;code&gt;R&lt;/code&gt;, it takes a long time to call a function and it’s quite inefficient. I tried to use &lt;code&gt;sapply&lt;/code&gt;, but it didn’t complete the process and it was forced to terminate. &lt;code&gt;RCCp&lt;/code&gt; is useful in such a case. Although &lt;code&gt;R&lt;/code&gt; has many very useful functions for graphs and statistics, it is not very good at large repetition, including calls to user-defined functions (because it is a scripting language, rather than a compiling language, I mean). So, I write the part of repetitive process in &lt;code&gt;C++&lt;/code&gt; and compile it as &lt;code&gt;R&lt;/code&gt; function using &lt;code&gt;Rcpp&lt;/code&gt; and execute it. It is very efficient to compile and visualize the results and write them in &lt;code&gt;R&lt;/code&gt;. Also, &lt;code&gt;Rccp&lt;/code&gt; helps you to write &lt;code&gt;C++&lt;/code&gt; in a similar way to &lt;code&gt;R&lt;/code&gt; with less sense of discomfort. I think the following will give you a good idea of the details. It’s pretty well organized and is God, to say the least.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://teuder.github.io/rcpp4everyone_en/&#34;&gt;Rcpp for everyone&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s write the code for the second step. In coding, I used articles on the net for reference. C++ has a longer history and more users than &lt;code&gt;R&lt;/code&gt;, so you can find information you want to know.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;algorithm&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
DataFrame Rolling_r_cpp(
    DataFrame input,               // Data frame of (measurement time, measured value data)
    newDatetimeVector from,        // The starting point vector for the timing of the calculation
    double time_window = 5*60)  // Calculated window width (in seconds)
{ 
  
  // Extract the measurement time and value as a vector
  newDatetimeVector time = input[&amp;quot;DateTime&amp;quot;]; // This time is assumed to be sorted in ascending order.
  NumericVector     data = input[&amp;quot;logMid&amp;quot;];
  
  // The endpoint vector of the timing to be calculated
  newDatetimeVector to = from + time_window;
  
  // Number to calculate
  R_xlen_t N = from.length();
  
  // vector for storage 
  NumericVector value(N);
  
  // An object representing the position of a vector element
  newDatetimeVector::iterator begin = time.begin();
  newDatetimeVector::iterator end   = time.end();
  newDatetimeVector::iterator p1    = begin;
  newDatetimeVector::iterator p2    = begin;
  
  // Loop for window i
  for(R_xlen_t i = 0; i &amp;lt; N; ++i){
    // Rcout &amp;lt;&amp;lt; &amp;quot;i=&amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    double f = from[i];         // Time of the window&amp;#39;s start point
    double t = f + time_window; // The time of the window&amp;#39;s endpoint
    
    // If the endpoint of the window is before the first measurement time, it is NA or
    // If the starting point of the window is after the last measurement time, NA
    if(t &amp;lt;= *begin || f &amp;gt; *(end-1)){ 
      value[i]  = NA_REAL;
      continue;// Go to the next loop
    }
    
    // Vector time from position p1 and subsequent elements x
    // Let p1 be the position of the first element whose time is at the start point f &amp;quot;after&amp;quot; the window
    p1 = std::find_if(p1, end, [&amp;amp;f](double x){return f&amp;lt;=x;});
    // p1 = std::lower_bound(p1, end, f); //Same as above
    
    // Vector time from position p1 and subsequent elements x
    // Let p2 be the position of the last element whose time is &amp;quot;before&amp;quot; the endpoint t of the window
    // (In the below, this is accomplished by making the time one position before the &amp;#39;first element&amp;#39;, where the time is the window&amp;#39;s endpoint t &amp;#39;after&amp;#39;)
    p2 = std::find_if(p1, end, [&amp;amp;t](double x){return t&amp;lt;=x;}) - 1 ;
    // p2 = std::lower_bound(p1, end, t) - 1 ;//Same as above
    
    // Convert the position p1,p2 of an element to the element numbers i1, i2
    R_xlen_t i1 = p1 - begin;
    R_xlen_t i2 = p2 - begin; 
    
    
    // Checking the element number
    // C++ starts with the element number 0, so I&amp;#39;m adding 1 to match the R
    // Rcout &amp;lt;&amp;lt; &amp;quot;i1 = &amp;quot; &amp;lt;&amp;lt; i1+1 &amp;lt;&amp;lt; &amp;quot; i2 = &amp;quot; &amp;lt;&amp;lt; i2+1 &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    
    // Calculate the data in the relevant range
    if(i1&amp;gt;i2) {
      value[i] = NA_REAL; // When there is no data in the window
    } else { 
      value[i] = data[i2] - data[i1];
    }
    // ↑You can create various window functions by changing above
    
  }
  
  // Output the calculated time and the value as a data frame.
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, from),
      Named(&amp;quot;r&amp;quot;, value*100));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you compile the program with &lt;code&gt;Rcpp::sourceCpp&lt;/code&gt;, the function of &lt;code&gt;R&lt;/code&gt; can be executed as follows.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- Rolling_r_cpp(JPYUSD,from))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       0.02       0.00       0.02&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It takes less than a second to process 12 million records. So Convenient!!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r         
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014  
##  Median :2011-07-03 09:57:30   Median : 0.000  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880  
##                                NA&amp;#39;s   :29977&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The return is precisely calculated. The recommended length of the window is 270 in 5 min increments, but we’ll make it flexible as well. And we carefully process the &lt;code&gt;NA&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;cmath&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
float rbv_cpp(
    NumericVector x, // Return vector to calculate rbv
    bool na_rm = true) // If NA is included in x, remove it and calculate it or
{
  
  // Get the number of calculations
  R_xlen_t N = x.length();
  
  // Define variables to contain the results of a calculation
  float out = 0;

  // Check for missing x
  LogicalVector lg_NA = is_na(x);
  
  // If there is a NA in x, whether to exclude that NA and calculate
  if(any(lg_NA).is_true() and na_rm==FALSE){
    out = NA_REAL; // Output NA as a result of the calculation
  } else {
    
    // Excluding NA
    if (any(lg_NA).is_true() and na_rm==TRUE){
      x[is_na(x)==TRUE] = 0.00; // Fill in the NA with zeros and effectively exclude it from the calculation.
    }
    
    // Compute the numerator (sum of rbv)
    for(R_xlen_t i = 1; i &amp;lt; N; ++i){
      out = out + std::abs(x[i])*std::abs(x[i-1]);
    }
    
    // Calculate the average and take the route.
    long denomi; //denominator
    if(N-sum(lg_NA)-2&amp;gt;0){
      denomi = N-sum(lg_NA)-2;
    } else {
      denomi = 1;
    }
    out = out/denomi;
    out = std::sqrt(out);
  }
  
  return out;
}

// [[Rcpp::export]]
DataFrame Rolling_rbv_cpp(
    DataFrame input, //Data frame of (measurement time, measured value data)
    int K = 270, // Rolling Window width to calculate
    bool na_pad = false, // Returning NA when the window width is insufficient
    bool na_remove = false // If the NA exists in the window width, exclude it from the calculation
){
  // Extract the return vector and number of samples
  NumericVector data = input[&amp;quot;r&amp;quot;];
  R_xlen_t T = data.length();
  
  // Prepare a vector to store the results
  NumericVector value(T);
  
  // Calculate and store RBVs per Windows width
  if(na_pad==TRUE){
    value[0] = NA_REAL; // return NA.
    value[1] = NA_REAL; // return NA.
    value[2] = NA_REAL; // return NA.
  } else {
    value[0] = 0; // Return zero.
    value[1] = 0; // Return zero.
    value[2] = 0; // Return zero.
  }
  
  for(R_xlen_t t = 3; t &amp;lt; T; ++t){
    // Bifurcation of the process depending on whether or not there is enough Windows width
    if (t-K&amp;gt;=0){
      value[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // Run a normal calculation
    } else if(na_pad==FALSE) {
      value[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Run a calculation with an incomplete Widnows width of less than K
    } else {
      value[t] = NA_REAL; // return NA.
    }
  }
  
  // Output the calculated time and value as a data frame.
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, input[&amp;quot;from&amp;quot;]),
      Named(&amp;quot;r&amp;quot;, data),
      Named(&amp;quot;rbv&amp;quot;,value));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, compile it and run it with &lt;code&gt;R&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- results %&amp;gt;% Rolling_rbv_cpp(na_remove = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       0.24       0.09       0.33&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So fast!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-jump-statistics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Calculating Jump Statistics&lt;/h2&gt;
&lt;p&gt;Now let’s calculate the statistic &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_{t_i}\)&lt;/span&gt; from the returns and standard deviation we just calculated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Standardize the absolute value of the log return = Jump statistic
results &amp;lt;- results %&amp;gt;% dplyr::mutate(J=ifelse(rbv&amp;gt;0,abs(r)/rbv,NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what it looks like now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s move on to the Jump test. First, we need to define the useful functions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Preparing Constants &amp;amp; Functions for Calculating Jump Test
c &amp;lt;- (2/pi)^0.5
Cn &amp;lt;- function(n){
  return((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5))
}
Sn &amp;lt;- function(n){
  1/(c*(2*log(n))^0.5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we perform the test. Rejected samples return 1 and all others 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Perform a jump test (10%) (return value is logical)
N &amp;lt;- NROW(results$J)
results &amp;lt;- results %&amp;gt;% dplyr::mutate(Jump = J &amp;gt; 2.25*Sn(N) + Cn(N))
summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423  
##     Jump        
##  Mode :logical  
##  FALSE:59864    
##  TRUE :257      
##  NA&amp;#39;s :44423    
##                 
##                 
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualization-using-ggplot2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Visualization using ggplot2&lt;/h2&gt;
&lt;p&gt;Now that the numbers have been calculated, let’s visualize them by plotting the intraday logarithmic return of JPY/USD in 5-minute increments for 2011/03/11 and the jump. By the way, the horizontal axis has been adjusted to Japan time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plotting about Jump at the time of the 2011/03/11 Great East Japan Earthquake
results %&amp;gt;% 
  dplyr::filter(from &amp;gt;= as.POSIXct(&amp;quot;2011-03-11 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;),from &amp;lt; as.POSIXct(&amp;quot;2011-03-12 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)) %&amp;gt;% 
  ggplot2::ggplot(ggplot2::aes(x=from,y=r)) +
  ggplot2::geom_path(linetype=3) +
  ggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=&amp;quot;red&amp;quot;)) +
  ggplot2::scale_x_datetime(date_breaks = &amp;quot;2 hours&amp;quot;, labels = scales::date_format(format=&amp;quot;%H:%M&amp;quot;,tz=&amp;quot;Asia/Tokyo&amp;quot;)) +
  ggplot2::ggtitle(&amp;quot;JPY/USD Jumps within Tohoku earthquake on 2011-3-11&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 36 row(s) containing missing values (geom_path).

## Warning: Removed 36 row(s) containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I’ve spent quite a bit of time writing this, and it’s 23:37 right now, so I’ll refrain from discussing it in depth, but since the earthquake occurred at 14:46:18, you can see that the market reacted to the weakening of the yen immediately after the disaster. After that, for some reason, the yen moved higher and peaked at 19:00. It is said that the yen is a safe asset, but this is the only time it is not safe given the heightened uncertainty.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. Summary&lt;/h2&gt;
&lt;p&gt;I introduced the use of &lt;code&gt;Rcpp&lt;/code&gt; to improve the efficiency of &lt;code&gt;R&lt;/code&gt; analysis. The &lt;code&gt;C++&lt;/code&gt; is much faster than &lt;code&gt;R&lt;/code&gt; even if you write the code honestly, so it is hard to make coding mistakes. The &lt;code&gt;C++&lt;/code&gt; is much faster than &lt;code&gt;R&lt;/code&gt; even if you write the code in a simple way. Also, even if a compile error occurs, RStudio gives you a clue as to where the compile error is occurring, so there is no stress in that respect either, which is why I recommend it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;A stochastic process with non-negative, integer, non-decreasing values.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;The mean is not necessarily zero, depending on the shape of the drift term, but we are assuming a small enough drift term now. In the paper, it is defined more precisely.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Automatically cropping the background of a horse body photo with Pytorch&#39;s Pre-trained model</title>
      <link>/en/post/post20/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      <guid>/en/post/post20/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#downloading-the-pre-trained-model&#34;&gt;1. Downloading the pre-trained model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#executing-the-background-deletion-process&#34;&gt;2. Executing the Background Deletion Process&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#analysis-using-cnn&#34;&gt;3. Analysis using CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-of-results-using-shap-values&#34;&gt;4. Interpretation of results using Shap values&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;5.Summary&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;Hi. In the last post, I built a model to predict the order of a racehorse using CNN based on its body image. The results were not so good, and we needed to refine our analysis, especially when we analyzed the factors using &lt;code&gt;shap&lt;/code&gt; values, we found that the horses responded more to the stables in the background than to their bodies. This time, I would like to re-analyze the background from the horse’s body photo using &lt;code&gt;Pytorch&lt;/code&gt; pre-trained model, and analyze the photo with only the horse’s body.&lt;/p&gt;
&lt;div id=&#34;downloading-the-pre-trained-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Downloading the pre-trained model&lt;/h2&gt;
&lt;p&gt;The code is adapted from &lt;a href=&#34;https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/&#34;&gt;here&lt;/a&gt;. First, install the packages.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import numpy as np
import cv2
import matplotlib.pyplot as plt
import torch
import torchvision
from torchvision import transforms
import glob
from PIL import Image
import PIL
import os&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, install the pre-trained model.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#Install a pre-trained model
device = torch.device(&amp;quot;cuda:0&amp;quot; if torch.cuda.is_available() else &amp;quot;cpu&amp;quot;)
model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)
model = model.to(device)
model.eval()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Apparently, all pre-trained models assume a mini-batch of 3-channel RGB images of shape &lt;span class=&#34;math inline&#34;&gt;\((N, 3, H, W)\)&lt;/span&gt; normalized in the same way. Here, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is assumed to be the number of images and &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt; are assumed to be at least 224 pixels. The images need to be scaled to a range of [0, 1] and then normalized using the mean value = [0.485, 0.456, 0.406] and the standard value = [0.229, 0.224, 0.225]. So, we define a function that does the preprocessing.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#preprocessing
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;executing-the-background-deletion-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Executing the Background Deletion Process&lt;/h2&gt;
&lt;p&gt;Now, let’s load the images collected by the &lt;code&gt;selenium&lt;/code&gt; code in the previous post and remove the background one by one.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;#Specify a folder
folders = os.listdir(r&amp;quot;C:\Users\aashi\umanalytics\photo\image&amp;quot;)

#Reads an image from each folder and converts it to a numpy array of RGB values using the Image function
for i, folder in enumerate(folders):
  files = glob.glob(&amp;quot;C:/Users/aashi/umanalytics/photo/image/&amp;quot; + folder + &amp;quot;/*.jpg&amp;quot;)
  index = i
  for k, file in enumerate(files):
    img_array = np.fromfile(file, dtype=np.uint8)
    img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
    h,w,_ = img.shape
    input_tensor = preprocess(img)
    input_batch = input_tensor.unsqueeze(0).to(device)

    with torch.no_grad():
      output = model(input_batch)[&amp;#39;out&amp;#39;][0]
    output_predictions = output.argmax(0)
    mask_array = output_predictions.byte().cpu().numpy()
    Image.fromarray(mask_array*255).save(r&amp;#39;C:\Users\aashi\umanalytics\photo\image\mask.jpg&amp;#39;)
    mask = cv2.imread(r&amp;#39;C:\Users\aashi\umanalytics\photo\image\mask.jpg&amp;#39;)
    bg = np.full_like(img,255)
    img = cv2.multiply(img.astype(float), mask.astype(float)/255)
    bg = cv2.multiply(bg.astype(float), 1.0 - mask.astype(float)/255)
    outImage = cv2.add(img, bg)
    Image.fromarray(outImage.astype(np.uint8)).convert(&amp;#39;L&amp;#39;).save(file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following &lt;code&gt;mask&lt;/code&gt; image is output using the pre-trained model, and the &lt;code&gt;numpy&lt;/code&gt; array and the &lt;code&gt;mask&lt;/code&gt; image are merged to create a background delete image. The output looks like the following.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.gray()
plt.figure(figsize=(20,20))
plt.subplot(1,3,1)
plt.imshow(img)
plt.subplot(1,3,2)
plt.imshow(mask)
plt.subplot(1,3,3)
plt.imshow(outImage)
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;1920&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what the folders look like. Some of them are handled well and some of them show the trainer. I know there is a way to identify the objects and &lt;code&gt;mask&lt;/code&gt; only the horses, but this model doesn’t allow for object labeling, so we’ll continue with that.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;maskhorse.PNG&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;folder&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;analysis-using-cnn&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Analysis using CNN&lt;/h2&gt;
&lt;p&gt;Here is the same content as in the previous article. I will only post the results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Test accuracy: 0.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay object at 0x0000000030DEAD08&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results have been disastrous.
I have not been able to identify it at all. Aren’t there any characteristics in the horse photos that would predict the rankings? Or is it that there is no variation in the photos of the horses in G1 races and it is impossible to identify them? Either way, it seems a bit harsh.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretation-of-results-using-shap-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Interpretation of results using Shap values&lt;/h2&gt;
&lt;p&gt;As before, let’s see how it fails using the &lt;code&gt;shap&lt;/code&gt; value. We will use this image as an example.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.imshow(X_test[4])
plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import shap
background = X_resampled[np.random.choice(X_resampled.shape[0],100,replace=False)]

e = shap.GradientExplainer(model,background)

shap_values = e.shap_values(X_test[[4]])
shap.image_plot(shap_values[1],X_test[[4]])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;They seem to be evaluating from the paws to the face. Surprisingly, they do not seem to be evaluating the buttocks.
I would like to try to visualize which aspect of the image is captured in each layer.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from keras import models

layer_outputs = [layer.output for layer in model.layers[:8]]
layer_names = []
for layer in model.layers[:8]:
    layer_names.append(layer.name)
images_per_row = 16

activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
activations = activation_model.predict(X_train[[0]])

for layer_name, layer_activation in zip(layer_names, activations):
    n_features = layer_activation.shape[-1]

    size = layer_activation.shape[1]

    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))

    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0,
                                             :, :,
                                             col * images_per_row + row]
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype(&amp;#39;uint8&amp;#39;)
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, cmap=&amp;#39;viridis&amp;#39;)
    plt.show()
    plt.close()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-2.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-3.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-4.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-5.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-6.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-7.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-8.png&#34; width=&#34;1536&#34; /&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-13-9.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I still can’t understand this one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5.Summary&lt;/h2&gt;
&lt;p&gt;I removed the stable background and rerun it, but the results were the same - it was a good experience using PyTorch and removing the background, but not with any results, so I’ll stop with the horse photos for now.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
