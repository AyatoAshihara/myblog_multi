<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>単発 | 東京の資産運用会社で働く社会人が研究に没頭するブログ</title>
    <link>/category/%E5%8D%98%E7%99%BA/</link>
      <atom:link href="/category/%E5%8D%98%E7%99%BA/index.xml" rel="self" type="application/rss+xml" />
    <description>単発</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>ja</language><lastBuildDate>Tue, 03 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>単発</title>
      <link>/category/%E5%8D%98%E7%99%BA/</link>
    </image>
    
    <item>
      <title>Pythonのpandas_datareaderから色々なデータを取得してみる</title>
      <link>/post/post15/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/post15/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#econdbからのデータ取得&#34;&gt;1. ECONDBからのデータ取得&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#econdbとは&#34;&gt;ECONDBとは？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#データ取得方法&#34;&gt;データ取得方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#world-bankからのデータ取得方法&#34;&gt;2. World Bankからのデータ取得方法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#世界銀行から取得できるデータとは&#34;&gt;世界銀行から取得できるデータとは？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#データの検索方法&#34;&gt;データの検索方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#データの取得方法&#34;&gt;データの取得方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#famafrench-data-libraryからのデータ取得方法&#34;&gt;3. Fama/French Data Libraryからのデータ取得方法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#famafrench-data-libraryで取れるデータとは&#34;&gt;Fama/French Data Libraryで取れるデータとは&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#データ取得方法-1&#34;&gt;データ取得方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ferdからのデータ取得方法&#34;&gt;4. FERDからのデータ取得方法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#fredで取得できるデータとは&#34;&gt;FREDで取得できるデータとは&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#データ取得方法-2&#34;&gt;データ取得方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#oecdからのデータ取得方法&#34;&gt;5. OECDからのデータ取得方法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eurostatからのデータ取得方法&#34;&gt;6. Eurostatからのデータ取得方法&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#eurostatから取得できるデータとは&#34;&gt;Eurostatから取得できるデータとは&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#データ取得方法-3&#34;&gt;データ取得方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#最後に&#34;&gt;最後に&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。最近会社のPCに&lt;code&gt;Anaconda&lt;/code&gt;を入れてもらいました。業務で使用することはないのですが、ワークショップで使用するので色々勉強しています。以前、Googleが提供している&lt;code&gt;Earth Engine&lt;/code&gt;から衛星画像を取得して解析した際に&lt;code&gt;Python&lt;/code&gt;を使用しましたが、今回は&lt;code&gt;Python&lt;/code&gt;から様々なデータが取得できる&lt;code&gt;pandas_datareader&lt;/code&gt;を使用したいと思います。&lt;code&gt;pandas_datareader&lt;/code&gt;では以下のようなデータソースからデータが取得できます。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Tiingo&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;IEX&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Alpha Vantage&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Enigma&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Quandl&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;St.Louis FED&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Kenneth French’s data library&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;World Bank&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;OECD&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Eurostat&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Thrift Saving Plan&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Nasdaq Trader symbol definitions&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Stooq&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MOEX&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Naver Finance&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;なお、このブログでは&lt;code&gt;Rstuio&lt;/code&gt;と&lt;code&gt;blogdown&lt;/code&gt;パッケージ、&lt;code&gt;git&lt;/code&gt;を組み合わせて&lt;code&gt;github&lt;/code&gt;上に記事を投稿しています。ですが、&lt;code&gt;Rstudio&lt;/code&gt;と&lt;code&gt;reticulate&lt;/code&gt;パッケージのおかげで、&lt;code&gt;python&lt;/code&gt;を使用した記事も&lt;code&gt;rmd&lt;/code&gt;で作成し、&lt;code&gt;html&lt;/code&gt;として出力できています。ここでまず、&lt;code&gt;reticulate&lt;/code&gt;パッケージを用いて&lt;code&gt;conda&lt;/code&gt;仮想環境へ接続する方法を紹介しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reticulate)
conda_path &amp;lt;- &amp;quot;C:\\Users\\hoge\\Anaconda3\\envs\\環境名&amp;quot;
use_condaenv(conda_path)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これで接続できます。&lt;code&gt;conda_path&lt;/code&gt;には仮想環境へのパスを入力してください。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import sys
sys.version&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;econdbからのデータ取得&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. ECONDBからのデータ取得&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;pandas_datareader&lt;/code&gt;では、&lt;a href=&#34;https://www.econdb.com/&#34;&gt;ECOMDB&lt;/a&gt;からマクロ経済関連のデータを取得することができます。&lt;/p&gt;
&lt;div id=&#34;econdbとは&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;ECONDBとは？&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;econdb.PNG&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ECONDBは各国の主要マクロ経済データをdashboard形式で提供してくれるWebサイトで、またAPIをサポートしており、PythonやExcelにシームレスにデータを連係してくれます。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;econdb2.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;データ取得方法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;データ取得方法&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;pandas_datareader&lt;/code&gt;を用いた使用方法は以下の通りです。&lt;/p&gt;
&lt;div id=&#34;基本的な使用方法&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;基本的な使用方法&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;pandas_datareader&lt;/code&gt;からデータモジュールをインポートすることから始めます。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas_datareader.data as web&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;EconDBからデータを取得するには、&lt;code&gt;DataReader&lt;/code&gt;メソッドを呼び出し、以下のように&lt;code&gt;data_source&lt;/code&gt;引数に&lt;code&gt;&#39;econdb&#39;&lt;/code&gt;と適当な&lt;code&gt;query&lt;/code&gt;を渡せばよいです。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = web.DataReader(query, data_source=&amp;#39;econdb&amp;#39;, **kwargs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;クエリパラメータの形式は、取得するデータの種類によって異なります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;クエリ指定方法&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;クエリ指定方法&lt;/h4&gt;
&lt;p&gt;データはいくつかのデータセットに分割されます。データセットには、トピック、頻度、調査方法などの共通の特徴を抽出できるティッカーが付与されています。ユーザーは検索機能を使用してデータセットを探すことができます。UST_MSPDデータセットを例にしてみます。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;econdb3.PNG&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ページに入ると、いくつかのフィルターがあり、特定のシリーズと特定のタイムフレームに選択を絞り込むことができます。適切なフィルタが設定された状態で、&lt;code&gt;Export&lt;/code&gt;ドロップダウンボタンをクリックすると、選択したデータをエクスポートするための多くのオプションとフォーマットが表示されます。その中でも、&lt;code&gt;Export to Python&lt;/code&gt;は、事前にフォーマットされたパラメータを持つコードの重要な部分を表示します。これをそのまま貼り付けてしまえばデータを取得できます。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;econdb4.PNG&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;query = &amp;quot;&amp;amp;&amp;quot;.join([
    &amp;quot;dataset=UST_MSPD&amp;quot;,
    &amp;quot;v=Category&amp;quot;,
    &amp;quot;h=TIME&amp;quot;,
    &amp;quot;from=2018-01-01&amp;quot;,
    &amp;quot;to=2019-12-31&amp;quot;
])
df = web.DataReader(query, &amp;#39;econdb&amp;#39;)
df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Category                         Bills  ... United States Savings Securities
## Holder      Intragovernmental Holdings  ...                           Totals
## TIME_PERIOD                             ...                                 
## 2015-12-01                      2928.0  ...                           171630
## 2016-01-01                      2642.0  ...                           171160
## 2016-02-01                      3584.0  ...                           170824
## 2016-03-01                      3582.0  ...                           170370
## 2016-04-01                      4176.0  ...                           169956
## 
## [5 rows x 51 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;実践的な取得コード&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;実践的な取得コード&lt;/h4&gt;
&lt;p&gt;こんなこともできます。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandas as pd
from matplotlib import pyplot as plt
import pandas_datareader.data as web
from datetime import datetime
import seaborn as sns

start = datetime(1980,1,1)
end = datetime(2019,12,31)

# parameters for data from econdb
country = [&amp;#39;US&amp;#39;,&amp;#39;UK&amp;#39;,&amp;#39;JP&amp;#39;,&amp;#39;EU&amp;#39;]
indicator = [&amp;#39;RGDP&amp;#39;,&amp;#39;CPI&amp;#39;,&amp;#39;URATE&amp;#39;,&amp;#39;CA&amp;#39;,&amp;#39;HOU&amp;#39;,&amp;#39;POP&amp;#39;,&amp;#39;RETA&amp;#39;,&amp;#39;IP&amp;#39;]

# Parse API from econdb
econ = pd.DataFrame()
for cnty in country:
    temp2 = pd.DataFrame()
    for idctr in indicator:
        temp = web.DataReader(&amp;#39;ticker=&amp;#39; + idctr + cnty,&amp;#39;econdb&amp;#39;,start,end)
        temp.columns = [idctr]
        temp2 = pd.concat([temp2,temp],join=&amp;#39;outer&amp;#39;,axis=1)
    temp2 = temp2.assign(kuni=cnty,kijyundate=temp2.index)
    econ = pd.concat([econ,temp2],join=&amp;#39;outer&amp;#39;)
    econ = econ.reset_index(drop=True)
econ.head()

# Plot CPI for example&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         RGDP   CPI  URATE       CA  HOU       POP  RETA     IP kuni kijyundate
## 0  6837641.0  78.0    6.3 -10666.0  NaN  226554.0   NaN  53.50   US 1980-01-01
## 1        NaN  79.0    6.3      NaN  NaN  226753.0   NaN  53.51   US 1980-02-01
## 2        NaN  80.1    6.3      NaN  NaN  226955.0   NaN  53.33   US 1980-03-01
## 3  6696753.0  80.9    6.9   9844.0  NaN  227156.0   NaN  52.23   US 1980-04-01
## 4        NaN  81.7    7.5      NaN  NaN  227387.0   NaN  50.96   US 1980-05-01&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sns.set&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;function set at 0x000000002EA68558&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;sns.relplot(data=econ,x=&amp;#39;kijyundate&amp;#39;,y=&amp;#39;CPI&amp;#39;,hue=&amp;#39;kuni&amp;#39;,kind=&amp;#39;line&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;seaborn.axisgrid.FacetGrid object at 0x00000000307EFE48&amp;gt;
## 
## C:\Users\aashi\ANACON~1\envs\FINANC~1\lib\site-packages\pandas\plotting\_matplotlib\converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.
## 
## To register the converters:
##  &amp;gt;&amp;gt;&amp;gt; from pandas.plotting import register_matplotlib_converters
##  &amp;gt;&amp;gt;&amp;gt; register_matplotlib_converters()
##   warnings.warn(msg, FutureWarning)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;556&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;world-bankからのデータ取得方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. World Bankからのデータ取得方法&lt;/h2&gt;
&lt;div id=&#34;世界銀行から取得できるデータとは&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;世界銀行から取得できるデータとは？&lt;/h3&gt;
&lt;p&gt;世界銀行は前身が国際復興開発銀行(IBRD)、国際開発協会(IDA)であることからもわかるように開発系のデータが取得できます。最近ではCOVID-19関連のデータも取得することができます。 &lt;code&gt;pandas_datareader&lt;/code&gt;では、&lt;code&gt;wb&lt;/code&gt;関数を使用することで、&lt;a href=&#34;https://data.worldbank.org/&#34;&gt;World Bank’s World Development Indicators&lt;/a&gt;と呼ばれる世界銀行の数千ものパネルデータに簡単にアクセスできます。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;データの検索方法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;データの検索方法&lt;/h3&gt;
&lt;p&gt;例えば、北米地域の国々の一人当たりの国内総生産をドルベースで比較したい場合は、&lt;code&gt;search&lt;/code&gt;関数を使用します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from pandas_datareader import wb
matches = wb.search(&amp;#39;gdp.*capita.*const&amp;#39;)
print(matches.loc[:,[&amp;#39;id&amp;#39;,&amp;#39;name&amp;#39;]])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                         id                                               name
## 680     6.0.GDPpc_constant  GDP per capita, PPP (constant 2011 internation...
## 9266        NY.GDP.PCAP.KD                 GDP per capita (constant 2010 US$)
## 9268        NY.GDP.PCAP.KN                      GDP per capita (constant LCU)
## 9270     NY.GDP.PCAP.PP.KD  GDP per capita, PPP (constant 2017 internation...
## 9271  NY.GDP.PCAP.PP.KD.87  GDP per capita, PPP (constant 1987 internation...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;NY.GDP.PCAP.KD&lt;/code&gt;がそれに当たることがわかります。2010年のUSドルベースで実質化されているようです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;データの取得方法&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;データの取得方法&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;download&lt;/code&gt;関数でデータを取得します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dat = wb.download(indicator=&amp;#39;NY.GDP.PCAP.KD&amp;#39;, country=[&amp;#39;US&amp;#39;, &amp;#39;CA&amp;#39;, &amp;#39;MX&amp;#39;], start=2010, end=2018)
print(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     NY.GDP.PCAP.KD
## country       year                
## Canada        2018    51476.200779
##               2017    51170.475834
##               2016    50193.750417
##               2015    50262.027666
##               2014    50306.944612
##               2013    49397.523320
##               2012    48785.936079
##               2011    48464.496279
##               2010    47448.013220
## Mexico        2018    10403.540397
##               2017    10301.357885
##               2016    10205.795753
##               2015    10037.201490
##               2014     9839.050191
##               2013     9693.722969
##               2012     9690.869065
##               2011     9477.887185
##               2010     9271.398233
## United States 2018    54659.198268
##               2017    53382.764823
##               2016    52555.518032
##               2015    52116.738813
##               2014    51028.824895
##               2013    50171.237133
##               2012    49603.253474
##               2011    48866.053277
##               2010    48467.515777&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;pandas&lt;/code&gt;の&lt;code&gt;dataframe&lt;/code&gt;形式でデータを取得できていることが分かります。年と国がindexになっていますね。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;famafrench-data-libraryからのデータ取得方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Fama/French Data Libraryからのデータ取得方法&lt;/h2&gt;
&lt;div id=&#34;famafrench-data-libraryで取れるデータとは&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fama/French Data Libraryで取れるデータとは&lt;/h3&gt;
&lt;p&gt;金融関連データになりますが、有名なFama/Frechの3 Factor modelのデータセットが&lt;a href=&#34;http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html&#34;&gt;Fama/French Data Library&lt;/a&gt;から取得できます。&lt;code&gt;get_available_datasets&lt;/code&gt;関数は、利用可能なすべてのデータセットのリストを返します。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;データ取得方法-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;データ取得方法&lt;/h3&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;from pandas_datareader.famafrench import get_available_datasets
len(get_available_datasets())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 297&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;利用可能なデータセットは297です。 データセットにどんなものがあるか、20個ほどサンプリングしてみます。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import random
print(random.sample(get_available_datasets(),20))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [&amp;#39;6_Portfolios_2x3&amp;#39;, &amp;#39;6_Portfolios_ME_INV_2x3_daily&amp;#39;, &amp;#39;Portfolios_Formed_on_ME&amp;#39;, &amp;#39;F-F_ST_Reversal_Factor&amp;#39;, &amp;#39;Portfolios_Formed_on_VAR&amp;#39;, &amp;#39;Developed_ex_US_6_Portfolios_ME_INV_Daily&amp;#39;, &amp;#39;Developed_ex_US_3_Factors_Daily&amp;#39;, &amp;#39;Developed_25_Portfolios_ME_INV&amp;#39;, &amp;#39;48_Industry_Portfolios_daily&amp;#39;, &amp;#39;Europe_5_Factors&amp;#39;, &amp;#39;Asia_Pacific_ex_Japan_32_Portfolios_ME_BE-ME_INV(TA)_2x4x4&amp;#39;, &amp;#39;Developed_3_Factors&amp;#39;, &amp;#39;Europe_3_Factors&amp;#39;, &amp;#39;Europe_25_Portfolios_ME_INV_Daily&amp;#39;, &amp;#39;6_Portfolios_ME_INV_2x3_Wout_Div&amp;#39;, &amp;#39;6_Portfolios_ME_CFP_2x3_Wout_Div&amp;#39;, &amp;#39;North_America_25_Portfolios_ME_INV&amp;#39;, &amp;#39;Developed_ex_US_Mom_Factor&amp;#39;, &amp;#39;6_Portfolios_ME_OP_2x3_Wout_Div&amp;#39;, &amp;#39;17_Industry_Portfolios_Wout_Div&amp;#39;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;日本株のポートフォリオも存在します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ds = web.DataReader(&amp;#39;5_Industry_Portfolios&amp;#39;, &amp;#39;famafrench&amp;#39;)
print(ds[&amp;#39;DESCR&amp;#39;])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 5 Industry Portfolios
## ---------------------
## 
## This file was created by CMPT_IND_RETS using the 202009 CRSP database. It contains value- and equal-weighted returns for 5 industry portfolios. The portfolios are constructed at the end of June. The annual returns are from January to December. Missing data are indicated by -99.99 or -999. Copyright 2020 Kenneth R. French
## 
##   0 : Average Value Weighted Returns -- Monthly (59 rows x 5 cols)
##   1 : Average Equal Weighted Returns -- Monthly (59 rows x 5 cols)
##   2 : Average Value Weighted Returns -- Annual (5 rows x 5 cols)
##   3 : Average Equal Weighted Returns -- Annual (5 rows x 5 cols)
##   4 : Number of Firms in Portfolios (59 rows x 5 cols)
##   5 : Average Firm Size (59 rows x 5 cols)
##   6 : Sum of BE / Sum of ME (6 rows x 5 cols)
##   7 : Value-Weighted Average of BE/ME (6 rows x 5 cols)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;5つ目がポートフォリオに含まれる銘柄数、1つ目がvalue weightedポートフォリオの月次リターンです。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ds[4].head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Cnsmr  Manuf  HiTec  Hlth   Other
## Date                                      
## 2015-11    544    653    736    586   1109
## 2015-12    542    649    730    583   1099
## 2016-01    539    638    725    581   1091
## 2016-02    537    635    718    576   1083
## 2016-03    536    630    715    576   1074&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;ds[0].head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          Cnsmr  Manuf  HiTec  Hlth   Other
## Date                                      
## 2015-11   0.29  -0.08   0.57   0.72   1.17
## 2015-12   0.13  -4.66  -2.59   0.38  -2.69
## 2016-01  -3.30  -3.46  -5.05  -9.40  -8.24
## 2016-02   0.51   1.39  -0.51  -1.06  -0.08
## 2016-03   5.81   8.10   7.91   2.92   7.06&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ferdからのデータ取得方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. FERDからのデータ取得方法&lt;/h2&gt;
&lt;div id=&#34;fredで取得できるデータとは&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;FREDで取得できるデータとは&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://fred.stlouisfed.org/&#34;&gt;FRED&lt;/a&gt;では多種多様な経済統計データを取得することができます。サイトへ行くと、以下のように統計毎にページが存在します。この統計名の横についている&lt;code&gt;CPIAUCSL&lt;/code&gt;がTickerになっており、これを渡すことで、データを取得することができます。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;FRED.PNG&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;データ取得方法-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;データ取得方法&lt;/h3&gt;
&lt;p&gt;先ほど見たTickerを&lt;code&gt;DataReader&lt;/code&gt;関数に渡し、データソースを&lt;code&gt;fred&lt;/code&gt;とすることで、データを取得することができます。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import datetime
start = datetime.datetime(2010, 1, 1)
end = datetime.datetime(2013, 1, 27)

gdp = web.DataReader(&amp;#39;GDP&amp;#39;, &amp;#39;fred&amp;#39;, start, end)
inflation = web.DataReader([&amp;#39;CPIAUCSL&amp;#39;, &amp;#39;CPILFESL&amp;#39;], &amp;#39;fred&amp;#39;, start, end)

gdp.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                   GDP
## DATE                 
## 2010-01-01  14721.350
## 2010-04-01  14926.098
## 2010-07-01  15079.917
## 2010-10-01  15240.843
## 2011-01-01  15285.828&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;inflation.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             CPIAUCSL  CPILFESL
## DATE                          
## 2010-01-01   217.488   220.633
## 2010-02-01   217.281   220.731
## 2010-03-01   217.353   220.783
## 2010-04-01   217.403   220.822
## 2010-05-01   217.290   220.962&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;oecdからのデータ取得方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. OECDからのデータ取得方法&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://stats.oecd.org/&#34;&gt;OECD&lt;/a&gt;は以前以下の記事で紹介しましたが、&lt;code&gt;pandas_datareader&lt;/code&gt;でも取得することができます。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;ttps://ayatoashihara.github.io/myblog_multi/post/post22/&#34;&gt;OECD.orgからマクロパネルデータをAPIで取得する&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ただ、&lt;code&gt;OECD dataset code&lt;/code&gt;を指定するだけ&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;なので、&lt;code&gt;pandasdmx&lt;/code&gt;よりは自由度が低いです。 あと、前回取得した&lt;code&gt;MEI_ARCHIVE&lt;/code&gt;とか指定するとデータが多すぎて、エラーが出ます。OECDデータを取得するときには、国や期間など細かい指定のできる&lt;code&gt;pandasdmx&lt;/code&gt;のほうが良いと個人的に思います。&lt;/p&gt;
&lt;p&gt;なお、使用方法はFREDと同様で、データソースに&lt;code&gt;oecd&lt;/code&gt;を指定します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = web.DataReader(&amp;#39;TUD&amp;#39;, &amp;#39;oecd&amp;#39;)
df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Country                Hungary                ...       Germany                     
## Source     Administrative data                ...   Survey data                     
## Series               Employees Union members  ... Union members Trade union  density
## Year                                          ...                                   
## 2016-01-01                 NaN           NaN  ...           NaN                  NaN
## 2017-01-01                 NaN           NaN  ...           NaN                  NaN
## 2018-01-01                 NaN           NaN  ...           NaN                  NaN
## 
## [3 rows x 216 columns]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;eurostatからのデータ取得方法&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;6. Eurostatからのデータ取得方法&lt;/h2&gt;
&lt;div id=&#34;eurostatから取得できるデータとは&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Eurostatから取得できるデータとは&lt;/h3&gt;
&lt;p&gt;Eurostatは欧州連合の統計局で、主にEU地域のデータを取得することができます。データは以下のように多岐にわたっており、経済金融だけでなく農業や人口動態、輸送、環境等々多種多様なデータを取得することができます。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;eurostat.PNG&#34; /&gt;&lt;/p&gt;
&lt;p&gt;IDをどのように取得すればよいのかですが、以下の&lt;a href=&#34;https://ec.europa.eu/eurostat/data/database&#34;&gt;ページ&lt;/a&gt;にて、取得したいデータを順々に掘り進めていくと黄色で色を付けたようなIDコードが出てきます。これで取得データのIDを特定します。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;eurostat2.PNG&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ただ、eurostatもOECDと同じくsdmxに対応しているため、&lt;code&gt;pandasdmx&lt;/code&gt;のほうが使いやすいかもしれません。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;データ取得方法-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;データ取得方法&lt;/h3&gt;
&lt;p&gt;一例として、 先ほど見た&lt;code&gt;Employment and activity by sex and age - annual data&lt;/code&gt;を取得してみます。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;df = web.DataReader(&amp;#39;lfsi_emp_a&amp;#39;,&amp;#39;eurostat&amp;#39;).unstack()
df.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## UNIT                            AGE                  SEX      INDIC_EM           GEO      FREQ    TIME_PERIOD
## Percentage of total population  From 15 to 24 years  Females  Active population  Austria  Annual  2016-01-01     54.6
##                                                                                                   2017-01-01     53.7
##                                                                                                   2018-01-01     53.8
##                                                                                                   2019-01-01     52.5
##                                                                                  Belgium  Annual  2016-01-01     26.2
## dtype: float64&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;最後に&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;最後に&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;pandas_datareader&lt;/code&gt;を使用して、様々なソースから多種多様なデータを取得しました。資産運用会社などで働いている方はbloombergやEIKONからデータを取得できるため、あまり魅力的に感じないかもしれませんが、個人で分析をしている方や定期的にデータを取得したい方は非常によいパッケージだと思います。自分自身、この新しいWebサイトにリニューアルしてから、週次や月次単位で経済分析を上げようかなと思っており、これらを使用して経済の定点観測をしたいなと思っているところです。皆さんも興味あるデータを&lt;code&gt;pandas_datareader&lt;/code&gt;で自動収集してみてください！&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;サイトで統計を選び、&lt;code&gt;export &amp;gt;- SDMX Query&lt;/code&gt;とするとその統計のコードが見れます。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>OECD.orgからマクロパネルデータをAPIで取得する</title>
      <link>/post/post22/</link>
      <pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/post22/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#oecd.stat-web-api&#34;&gt;1.OECD.Stat Web API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#pandasdmx&#34;&gt;2.pandasdmx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#実装&#34;&gt;3.実装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#別件ですが&#34;&gt;4.別件ですが。。。&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;おはこんばんにちは。マクロ経済データを集める方法はいくつかありますが、各国のデータを集めるとなると一苦労です。ですが、OECDからAPI経由でデータ取得すれば面倒な処理を自動化できます。今日はその方法をご紹介します。&lt;/p&gt;
&lt;div id=&#34;oecd.stat-web-api&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1.OECD.Stat Web API&lt;/h2&gt;
&lt;p&gt;OECD.orgでは&lt;a href=&#34;https://stats.oecd.org/&#34;&gt;OECD.Stat&lt;/a&gt;というサービスを提供しており、OECD加盟国と特定の非加盟国の様々な経済データが提供されています。WEBサイトに行けば手動でcsvデータをダウンロードすることもできますが、定期的にデータを取得し、分析する必要があるならばデータ取得処理を自動化したい衝動に駆られます。OECDはWeb APIを提供しているので、&lt;code&gt;Python&lt;/code&gt;や&lt;code&gt;R&lt;/code&gt;さえ使えればこれを実現できます。&lt;/p&gt;
&lt;p&gt;&lt;OECD実施の具体的な内容&gt;&lt;/p&gt;
&lt;p&gt;以下は、現時点での特定のOECD REST SDMXインターフェースの実装詳細のリストです。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;匿名クエリのみがサポートされ、認証はありません。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;各レスポンスは1,000,000件のオブザベーションに制限されています。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;リクエストURLの最大長は1000文字です。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;クロスオリジンリクエストは、&lt;code&gt;CORS&lt;/code&gt; ヘッダでサポートされています (&lt;code&gt;CORS&lt;/code&gt;についての詳細は &lt;a href=&#34;http://www.html5rocks.com/en/tutorials/cors/&#34;&gt;こちら&lt;/a&gt;を参照)。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;エラーは結果には返されませんが、HTTP ステータスコードとメッセージは Web サービスガイドラインに従って設定されます。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;存在しないデータセットが要求された場合は、401 Unauthorizedが返されます。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;REST&lt;/code&gt; クエリの source (または Agency ID) パラメータは必須ですが、「ALL」キーワードはサポートされています。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;バージョニングはサポートされていません: 常に最新の実装バージョンが使用されます。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;データの並べ替えはサポートされていません。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;lastNObservations&lt;/code&gt;パラメータはサポートされていません。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;dimensionAtObservation=AllDimensions&lt;/code&gt; が使用されている場合でも、観測は時系列 (またはインポート固有) の順序に従います。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;現時点では、参照メタデータの検索はサポートされていません。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;pandasdmx&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.pandasdmx&lt;/h2&gt;
&lt;p&gt;Web APIは&lt;code&gt;sdmx-json&lt;/code&gt;という形式で提供されます。&lt;code&gt;Python&lt;/code&gt;ではこれを使用するための便利なパッケージが存在します。それが&lt;code&gt;**pandasdmx**&lt;/code&gt;です。データをダウンロードする方法は以下の通りです。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;code&gt;pandasdmx&lt;/code&gt;を&lt;code&gt;import&lt;/code&gt;し、&lt;code&gt;Request&lt;/code&gt;メソッドに引数として’OECD’を渡し、&lt;code&gt;api.Request&lt;/code&gt;オブジェクトを作成する。&lt;/li&gt;
&lt;li&gt;作成した&lt;code&gt;api.Request&lt;/code&gt;オブジェクトのdataメソッドにクエリ条件を渡し、OECD.orgから&lt;code&gt;sdmx-json&lt;/code&gt;形式のデータをダウンロードする。&lt;/li&gt;
&lt;li&gt;ダウンロードしたデータを&lt;code&gt;to_pandas()&lt;/code&gt;メソッドで&lt;code&gt;pandas&lt;/code&gt;データフレームへ整形する。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;実装&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.実装&lt;/h2&gt;
&lt;p&gt;では、実際にやってみましょう。取得するのは、「&lt;code&gt;**Revisions Analysis Dataset -- Infra-annual Economic Indicators**&lt;/code&gt;」というデータセットです。OECDのデータセットの一つである&lt;code&gt;Monthly Ecnomic Indicator&lt;/code&gt;(MEI)の修正を含む全てのデータにアクセスしているので、主要な経済変数(国内総生産とその支出項目、鉱工業生産と建設生産指数、国際収支、複合主要指標、消費者物価指数、小売取引高、失業率、就業者数、時間当たり賃金、貨マネーサプライ、貿易統計など)について、初出時の速報データから修正が加えられた確報データまで確認することができます。このデータセットでは、1999年2月から毎月の間隔で、過去に主要経済指標データベースで分析可能だったデータのスナップショットが提供されています。つまり、各時点で入手可能なデータに基づく、予測モデルの構築ができるデータセットになっています。最新のデータは有用ですが速報値なので不確実性がつきまといます。バックテストを行う際にはこの状況が再現できず実際の運用よりも良い環境で分析してしまうことが問題になったりします。いわゆる&lt;code&gt;Jagged edge&lt;/code&gt;問題です。このデータセットでは実運用の状況が再現できるため非常に有用であると思います。今回は以下のデータ項目を取得します。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;統計概要&lt;/th&gt;
&lt;th&gt;統計ID&lt;/th&gt;
&lt;th&gt;頻度&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;GDP&lt;/td&gt;
&lt;td&gt;101&lt;/td&gt;
&lt;td&gt;四半期&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;鉱工業生産指数&lt;/td&gt;
&lt;td&gt;201&lt;/td&gt;
&lt;td&gt;月次&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;小売業取引高&lt;/td&gt;
&lt;td&gt;202&lt;/td&gt;
&lt;td&gt;月次&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;マネーサプライ - 広義流動性&lt;/td&gt;
&lt;td&gt;601&lt;/td&gt;
&lt;td&gt;月次&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;貿易統計&lt;/td&gt;
&lt;td&gt;702+703&lt;/td&gt;
&lt;td&gt;月次&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;経常収支&lt;/td&gt;
&lt;td&gt;701&lt;/td&gt;
&lt;td&gt;四半期&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;就業者数&lt;/td&gt;
&lt;td&gt;502&lt;/td&gt;
&lt;td&gt;月次&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;失業率&lt;/td&gt;
&lt;td&gt;501&lt;/td&gt;
&lt;td&gt;月次&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;時間当たり賃金（製造業）&lt;/td&gt;
&lt;td&gt;503&lt;/td&gt;
&lt;td&gt;月次&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;単位あたり労働コスト&lt;/td&gt;
&lt;td&gt;504&lt;/td&gt;
&lt;td&gt;四半期&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;建築生産指数&lt;/td&gt;
&lt;td&gt;203&lt;/td&gt;
&lt;td&gt;月次&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;まず、関数を定義します。引数はデータベースID、その他ID(国IDや統計ID)、開始地点、終了地点です。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pandasdmx as sdmx&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## C:\Users\aashi\Anaconda3\lib\site-packages\pandasdmx\remote.py:13: RuntimeWarning: optional dependency requests_cache is not installed; cache options to Session() have no effect
##   RuntimeWarning,&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;oecd = sdmx.Request(&amp;#39;OECD&amp;#39;)
def resp_OECD(dsname,dimensions,start,end):
    dim_args = [&amp;#39;+&amp;#39;.join(d) for d in dimensions]
    dim_str = &amp;#39;.&amp;#39;.join(dim_args)
    resp = oecd.data(resource_id=dsname, key=dim_str + &amp;quot;/all?startTime=&amp;quot; + start + &amp;quot;&amp;amp;endTime=&amp;quot; + end)
    df = resp.to_pandas().reset_index()
    return(df)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;データを取得する次元を指定します。以下では、①国、②統計項目、③入手時点、④頻度をタプルで指定しています。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;dimensions = ((&amp;#39;USA&amp;#39;,&amp;#39;JPN&amp;#39;,&amp;#39;GBR&amp;#39;,&amp;#39;FRA&amp;#39;,&amp;#39;DEU&amp;#39;,&amp;#39;ITA&amp;#39;,&amp;#39;CAN&amp;#39;,&amp;#39;NLD&amp;#39;,&amp;#39;BEL&amp;#39;,&amp;#39;SWE&amp;#39;,&amp;#39;CHE&amp;#39;),(&amp;#39;201&amp;#39;,&amp;#39;202&amp;#39;,&amp;#39;601&amp;#39;,&amp;#39;702&amp;#39;,&amp;#39;703&amp;#39;,&amp;#39;701&amp;#39;,&amp;#39;502&amp;#39;,&amp;#39;503&amp;#39;,&amp;#39;504&amp;#39;,&amp;#39;203&amp;#39;),(&amp;quot;202001&amp;quot;,&amp;quot;202002&amp;quot;,&amp;quot;202003&amp;quot;,&amp;quot;202004&amp;quot;,&amp;quot;202005&amp;quot;,&amp;quot;202006&amp;quot;,&amp;quot;202007&amp;quot;,&amp;quot;202008&amp;quot;),(&amp;quot;M&amp;quot;,&amp;quot;Q&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;関数を実行します。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;result = resp_OECD(&amp;#39;MEI_ARCHIVE&amp;#39;,dimensions,&amp;#39;2019-Q1&amp;#39;,&amp;#39;2020-Q2&amp;#39;)
result.count()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## LOCATION       8266
## VAR            8266
## EDI            8266
## FREQUENCY      8266
## TIME_PERIOD    8266
## value          8266
## dtype: int64&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;データの最初数件を見てみます。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;result.head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   LOCATION  VAR     EDI FREQUENCY TIME_PERIOD  value
## 0      BEL  201  202001         M     2019-01  112.5
## 1      BEL  201  202001         M     2019-02  111.8
## 2      BEL  201  202001         M     2019-03  109.9
## 3      BEL  201  202001         M     2019-04  113.5
## 4      BEL  201  202001         M     2019-05  112.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;データがTidyな形(Long型)で入っているのがわかります。一番右側の&lt;code&gt;value&lt;/code&gt;が値として格納されており、その他インデックスは&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;LOCATION - 国&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;VAR - 統計項目&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;EDI - 入手時点(MEI_ARCHIVEの場合)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;FREQUENCY - 頻度(月次、四半期等)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;TIME_PERIOD - 統計の基準時点&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;となっています。よって、&lt;code&gt;EDI&lt;/code&gt;が異なる行で同じ&lt;code&gt;TIME_PERIOD&lt;/code&gt;が存在します。例えば、上ではベルギー(&lt;code&gt;BEL&lt;/code&gt;)の鉱工業生産指数(201)の2020/01時点で利用可能な2019-01~2019-05のデータが表示されています。可視化や回帰も行いやすいLongフォーマットでの提供なので非常にありがたいですね。鉱工業生産指数がアップデートされていく様子を可視化してみました。&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

result = result[result[&amp;#39;FREQUENCY&amp;#39;]==&amp;#39;M&amp;#39;]
result[&amp;#39;TIME_PERIOD&amp;#39;] = pd.to_datetime(result[&amp;#39;TIME_PERIOD&amp;#39;],format=&amp;#39;%Y-%m&amp;#39;)
sns.relplot(data=result[lambda df: (df.VAR==&amp;#39;201&amp;#39;) &amp;amp; (pd.to_numeric(df.EDI) &amp;gt; 202004)],x=&amp;#39;TIME_PERIOD&amp;#39;,y=&amp;#39;value&amp;#39;,hue=&amp;#39;LOCATION&amp;#39;,kind=&amp;#39;line&amp;#39;,col=&amp;#39;EDI&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;seaborn.axisgrid.FacetGrid object at 0x00000000316C0188&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;plt.show()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;2035&#34; /&gt;&lt;/p&gt;
&lt;p&gt;コロナの経済的な被害が大きくなるにつれて折れ線グラフが落ち込んでいく様子が見て取れる一方、微妙にですが過去値についても速報値→確報値へと修正が行われています。また、国によって統計データの公表にラグがあることも分かります。ベルギーは最も公表が遅いようです。時間があるときに、このデータを使った簡単な予測モデルの分析を追記したいと思います。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;別件ですが&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4.別件ですが。。。&lt;/h2&gt;
&lt;p&gt;Python 3 エンジニア認定データ分析試験に合格しました。合格率70%だけあって、かなり簡単でしたが&lt;code&gt;Python&lt;/code&gt;を基礎から見返すいい機会になりました。今やっている業務ではデータ分析はおろか&lt;code&gt;Python&lt;/code&gt;や&lt;code&gt;R&lt;/code&gt;を使う機会すらないので、転職も含めた可能性を考えています。とりあえず、以下の資格を今年度中に取得する予定で、金融にこだわらずにスキルを活かせるポストを探していこうと思います。ダイエットと同じで宣言して自分を追い込まないと。。。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;G検定&lt;/li&gt;
&lt;li&gt;Oracle Database Master Silver SQL&lt;/li&gt;
&lt;li&gt;Linuc レベル 1&lt;/li&gt;
&lt;li&gt;基本情報技術者&lt;/li&gt;
&lt;li&gt;AWS 認定ソリューションアーキテクト - アソシエイト&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;合格状況は都度ブログで報告していきたいと思います。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Rcppでデータハンドリングを高速に行う(Tickデータの処理を事例に)</title>
      <link>/post/post21/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/post21/</guid>
      <description>
&lt;script src=&#34;../../post/post21/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;やりたいこと&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;0. やりたいこと&lt;/h2&gt;
&lt;p&gt;今回お見せするのは前述の通り、為替のTickデータを使った前処理(と解析)になります。主眼を&lt;code&gt;Rcpp&lt;/code&gt;を用いた効率化に置いていますので詳しくは踏み入りませんが、やりたいことをざっくりと先に示しておきます。&lt;br /&gt;
やりたいのは、JPY/USDレートの5分刻みリターンから&lt;em&gt;Jump&lt;/em&gt;を検知することです。ここでのJumpとはそれまでと比べて為替レートがガクッと上昇(下落)した点です。日中為替レートは小刻みに動きますが、なにかイベントがあると大きく上昇(下落)します。どんなイベントがJumpを引き起こすのかは非常に興味深い点です。これを検証するにはまずJumpを検知する必要があるのです。 参考とするのは以下の論文です。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://academic.oup.com/rfs/article-abstract/21/6/2535/1574138?redirectedFrom=fulltext&#34;&gt;Suzanne S. Lee &amp;amp; Per A. Mykland, 2008. “Jumps in Financial Markets: A New Nonparametric Test and Jump Dynamics,” Review of Financial Studies, Society for Financial Studies, vol. 21(6), pages 2535-2563, November.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Citationが204もある非常に評価されている論文です。推定方法を掻い摘んで説明します。まず、連続複利リターンを&lt;span class=&#34;math inline&#34;&gt;\(d\log S(t)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t&amp;gt;0\)&lt;/span&gt;とします。ここで、&lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;時点での資産価格です。市場にJumpがない場合、&lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;は以下の確率過程に従うと仮定します。
&lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) \tag{1}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt;は標準ブラウン運動、&lt;span class=&#34;math inline&#34;&gt;\(\mu(t)\)&lt;/span&gt;はドリフト項、&lt;span class=&#34;math inline&#34;&gt;\(\sigma(t)\)&lt;/span&gt;はスポットボラティリティです。また、Jumpがあるとき、&lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;は
&lt;span class=&#34;math display&#34;&gt;\[
d\log S(t) = \mu(t)dt + \sigma(t)dW(t) + Y(t)dJ(t) \tag{2}
\]&lt;/span&gt;
に従うと仮定します。ここで、&lt;span class=&#34;math inline&#34;&gt;\(J(t)\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(W(t)\)&lt;/span&gt;とは独立したカウント過程です。&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y(t)\)&lt;/span&gt;はジャンプのサイズを表現しており、予測可能な過程であるとします。&lt;/p&gt;
&lt;p&gt;次に、&lt;span class=&#34;math inline&#34;&gt;\(S(t)\)&lt;/span&gt;の対数リターンを考えます。それはつまり&lt;span class=&#34;math inline&#34;&gt;\(\log S(t_i)/S(t_{i-1})\)&lt;/span&gt;ですが、これは正規分布&lt;span class=&#34;math inline&#34;&gt;\(N(0,\sigma(t_i))\)&lt;/span&gt;に従います。&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;ここで、&lt;span class=&#34;math inline&#34;&gt;\(t_{i-1}\)&lt;/span&gt;から&lt;span class=&#34;math inline&#34;&gt;\(t_{i}\)&lt;/span&gt;にJumpがあった際の&lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt;時点の統計量&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt;を以下で定義します。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
    \mathcal{L(i)} \equiv \frac{|\log S(t_i)/S(t_{i-1})|}{\hat{\sigma}_{t_i}} \tag{3}
\]&lt;/span&gt;
上記は対数リターンの絶対値を単純に標準化したものですが、標準偏差の推定量には以下で定義される”Realized Bipower Variation”を使用しています。
&lt;span class=&#34;math display&#34;&gt;\[
    \hat{\sigma}_{t_i} = \frac{1}{K-2}\sum_{j=i-K+2}^{i-2}|\log S(t_j)/\log S(t_{j-1})||\log S(t_{j-1})/\log S(t_{j-2})| \tag{4}
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;はWindowに含まれるサンプルサイズの数です。仮に5min刻みリターンを用い、2020/9/10 10:00にJumpが発生した場合、&lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt;としている場合は前日2020/9/9 11:30から2020/9/11 09:55までのサンプルを用いて計算することになります。やっていることは、リターンの絶対値をかけたものを足し合わせるということですが、これでJumpが生じた次の瞬間(つまり&lt;span class=&#34;math inline&#34;&gt;\(t_{i+1}\)&lt;/span&gt;とか）の推定値がJumpに影響されにくいようです。ちなみに&lt;span class=&#34;math inline&#34;&gt;\(K=270\)&lt;/span&gt;は5min刻みリターンの場合の推奨値と別の文献で紹介されています。&lt;/p&gt;
&lt;p&gt;こうして計算されたJump統計量&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt;をどのように統計的検定に用いてJumpを検出するかに話を移しましょう。これは確率変数である&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L(i)}\)&lt;/span&gt;の最大値(こちらも確率変数)を考え、その分布から大きく逸脱した値を取った場合(95%点とか)、そのリターンをJumpとします。
期間&lt;span class=&#34;math inline&#34;&gt;\([t_{i-1},t_{i}]\)&lt;/span&gt;にJumpがないとした場合、この期間の長さ&lt;span class=&#34;math inline&#34;&gt;\(\Delta=t_{i}-t_{i-1}\)&lt;/span&gt;を&lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;に近づけると、つまり&lt;span class=&#34;math inline&#34;&gt;\(\Delta\rightarrow0\)&lt;/span&gt;とすると、標準正規変数の絶対値の最大値は、ガンベル分布に収束します。皆さん大好き極値統計ですね。よって、Jumpは以下の条件が満たされた際に帰無仮説が棄却され、検出することができます。
&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L(i)} &amp;gt; G^{-1}(1-\alpha)S_{n} + C_{n} \tag{5}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(G^{-1}(1-\alpha)\)&lt;/span&gt;は標準ガンベル分布の&lt;span class=&#34;math inline&#34;&gt;\((1-\alpha)\)&lt;/span&gt;分位関数です。&lt;span class=&#34;math inline&#34;&gt;\(\alpha=10%\)&lt;/span&gt;だと2.25になります。また、
&lt;span class=&#34;math display&#34;&gt;\[
S_{n} = \frac{1}{c(2\log n)^{0.5}} \\
C_{n} = \frac{(2\log n)^{0.5}}{c}-\frac{\log \pi+\log(\log n)}{2c(2\log n)^{0.5}}
\]&lt;/span&gt;
です(導出はしませんが、1式と2式を使って証明できます)。ここで、&lt;span class=&#34;math inline&#34;&gt;\(c=(2/\pi)^{0.5}\)&lt;/span&gt;で、&lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;は推定に使用する総サンプルサイズです。 最終的に、&lt;span class=&#34;math inline&#34;&gt;\(Jump_{t_i}\)&lt;/span&gt;は
&lt;span class=&#34;math display&#34;&gt;\[
Jump_{t_i} = \log\frac{S(t_i)}{S(t_{i-1})}×I(\mathcal{L(i)} - G^{-1}(1-\alpha)S_{n} + C_{n})\tag{6}
\]&lt;/span&gt;
で求められることになります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(I(・)\)&lt;/span&gt;は中身が0より大きいと1、それ以外は0を返すIndicator関数です。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;データの読み込み&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. データの読み込み&lt;/h2&gt;
&lt;p&gt;では、推定方法がわかったのでまずTickデータの読み込みをしましょう。データは&lt;code&gt;QuantDataManager&lt;/code&gt;からcsvを取得し、それを作業ディレクトリに保存しています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)

# Tick dataの読み込み
strPath &amp;lt;- r&amp;quot;(C:\Users\hogehoge\JPYUSD_Tick_2011.csv)&amp;quot;
JPYUSD &amp;lt;- readr::read_csv(strPath)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;関係ないんですが、最近Rを4.0.2へ上げました。4.0以上では&lt;code&gt;Python&lt;/code&gt;でできた文字列のEscapeができるとうことで今までのストレスが解消されてかなりうれしいです。
データは以下のような感じで、日付の他にBid値、Ask値と取引量が格納されています。なお、ここでは2011年のTickを使用しています。東日本大震災の時のドル円を対象とするためです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     DateTime                        Bid             Ask            Volume     
##  Min.   :2011-01-03 07:00:00   Min.   :75.57   Min.   :75.58   Min.   : 1.00  
##  1st Qu.:2011-03-30 15:09:23   1st Qu.:77.43   1st Qu.:77.44   1st Qu.: 2.00  
##  Median :2011-06-15 14:00:09   Median :80.40   Median :80.42   Median : 2.00  
##  Mean   :2011-06-22 05:43:11   Mean   :79.91   Mean   :79.92   Mean   : 2.55  
##  3rd Qu.:2011-09-09 13:54:51   3rd Qu.:81.93   3rd Qu.:81.94   3rd Qu.: 3.00  
##  Max.   :2011-12-30 06:59:59   Max.   :85.52   Max.   :85.54   Max.   :90.00&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ちなみに、&lt;code&gt;DateTime&lt;/code&gt;はUTC基準で日本時間だと2011/1/3 07:00:00から2011-12-30 06:59::59(米国時間2011-12-30 16:59:59)までを含んでいます。サンプルサイズは約1200万件です。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NROW(JPYUSD)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 11946621&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;前処理&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. 前処理&lt;/h2&gt;
&lt;p&gt;では次にBidとAskから仲値を計算し、後でリターンを算出するために対数を取っておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# AskとBidの仲値を計算し、対数化(対数リターン算出用)
JPYUSD &amp;lt;- JPYUSD %&amp;gt;% dplyr::mutate(Mid = (Ask+Bid)/2) %&amp;gt;% 
                     dplyr::mutate(logMid = log(Mid))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;現状不規則に並んでいる取引データを5min刻みのリターンに整形します。やり方は、
1. 1年間を5min毎に刻んだ&lt;code&gt;POSIXct&lt;/code&gt;ベクトルを作る。
2. 1.を引数として渡すと、その5minのWindowのうち、最初と最後のサンプルから対数リターンを順々に計算する関数を作成する。
3. 実行。
という計画です。まず、1.のベクトルを作成します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 5min刻みでのリターンを算出するためのPOSIXベクトルを作成(288×日数)
start &amp;lt;- as.POSIXct(&amp;quot;2011-01-02 22:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
end &amp;lt;- as.POSIXct(&amp;quot;2011-12-31 21:55:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)
from &amp;lt;- seq(from=start,to=end,by=5*60)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、2.に移ろうということなんですが、データが1200万件もあると&lt;code&gt;R&lt;/code&gt;で&lt;code&gt;purrr::map&lt;/code&gt;とか&lt;code&gt;apply&lt;/code&gt;属を使用したとしても、関数呼び出しに時間がかかって結構非効率だったりします。。。&lt;code&gt;sapply&lt;/code&gt;でやってみましたがなかなか処理が完了せず、強制終了しました。こういうときには、&lt;code&gt;Rccp&lt;/code&gt;が便利です。&lt;code&gt;R&lt;/code&gt;はグラフや統計処理のための非常に便利な関数が多数ありますが、ユーザーで定義した関数の呼び出しを含む、大量の繰り返し処理を苦手とします(スクリプト言語なのでコンパイル言語よりはという意味です)。なので、繰り返し処理の部分だけ、&lt;code&gt;C++&lt;/code&gt;で書いてしまって、それを&lt;code&gt;Rcpp&lt;/code&gt;をつかって&lt;code&gt;R&lt;/code&gt;の関数としてコンパイルし、実行。結果の集計や可視化、執筆は&lt;code&gt;R&lt;/code&gt;で行うというフローが非常に効率的です。
また、&lt;code&gt;Rccp&lt;/code&gt;は&lt;code&gt;R&lt;/code&gt;に似た違和感の少ない記述方法で&lt;code&gt;C++&lt;/code&gt;を記述するのを助けてくれます。詳しいことは以下を見れば問題ないと思います。かなりまとまっていて控えめに言って神です。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://teuder.github.io/rcpp4everyone_ja/&#34;&gt;みんなのRcpp&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;では、2.にあたるコードを書いていきます。コーディングに当たってはネット上の記事を参考にしました。&lt;code&gt;C++&lt;/code&gt;は&lt;code&gt;R&lt;/code&gt;よりも歴史があるし、使用者も多いので知りたい情報はすぐ見つけられます。&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;algorithm&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
DataFrame Rolling_r_cpp(
    DataFrame input,               //（計測時刻time, 計測値data）のデータフレーム
    newDatetimeVector from,        //計算するタイミングの始点ベクトル
    double time_window = 5*60)  //計算するwindow幅（秒）
{ 
  
  // 計測時刻と計測値をベクトルとして取り出す
  newDatetimeVector time = input[&amp;quot;DateTime&amp;quot;]; // 今回は time は昇順にソートされているのが前提です。
  NumericVector     data = input[&amp;quot;logMid&amp;quot;];
  
  // 計算するタイミングの終点ベクトル
  newDatetimeVector to = from + time_window;
  
  // 計算する数
  R_xlen_t N = from.length();
  
  // 格納するベクトル
  NumericVector value(N);
  
  // ベクトル要素の位置をあらわすオブジェクト
  newDatetimeVector::iterator begin = time.begin();
  newDatetimeVector::iterator end   = time.end();
  newDatetimeVector::iterator p1    = begin;
  newDatetimeVector::iterator p2    = begin;
  
  // window i についてループ
  for(R_xlen_t i = 0; i &amp;lt; N; ++i){
    // Rcout &amp;lt;&amp;lt; &amp;quot;i=&amp;quot; &amp;lt;&amp;lt; i &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    double f = from[i];         //windowの始点の時刻
    double t = f + time_window; //windowの終点の時刻
    
    // windowの終点が最初の計測時刻以前の時はNA、または
    // windowの始点が最後の計測時刻のより後の時はNA
    if(t &amp;lt;= *begin || f &amp;gt; *(end-1)){ 
      value[i]  = NA_REAL;
      continue;//次のループへ
    }
    
    // ベクトル time の位置 p1 以降の要素xから
    // 時刻がwindowの始点f「以降」である「最初の要素」の位置を p1 とする
    p1 = std::find_if(p1, end, [&amp;amp;f](double x){return f&amp;lt;=x;});
    // p1 = std::lower_bound(p1, end, f); //上と同義
    
    // ベクトル time の位置 p1 以降の要素xから
    // 時刻がwindowの終点t「より前」である「最後の要素」の位置を p2 とする
    // （下では、時刻がwindowの終点t「以降」である「最初の要素」の１つ前の位置、にすることで実現している’）
    p2 = std::find_if(p1, end, [&amp;amp;t](double x){return t&amp;lt;=x;}) - 1 ;
    // p2 = std::lower_bound(p1, end, t) - 1 ;//上と同義
    
    // 要素の位置p1,p2を、要素番号i1, i2に変換する
    R_xlen_t i1 = p1 - begin;
    R_xlen_t i2 = p2 - begin; 
    
    
    // 要素番号の確認
    // C++は要素番号が0から始まるのでRに合わせるために1を足している
    // Rcout &amp;lt;&amp;lt; &amp;quot;i1 = &amp;quot; &amp;lt;&amp;lt; i1+1 &amp;lt;&amp;lt; &amp;quot; i2 = &amp;quot; &amp;lt;&amp;lt; i2+1 &amp;lt;&amp;lt; &amp;quot;\n&amp;quot;;
    
    
    // 該当する範囲のデータについて計算する
    if(i1&amp;gt;i2) {
      value[i] = NA_REAL; // window内にデータがない場合
    } else { 
      value[i] = data[i2] - data[i1];
    }
    // ↑を変更することで様々なwindow関数を作成できる
    
  }
  
  // 計算した時間と、値をデータフレームとして出力する
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, from),
      Named(&amp;quot;r&amp;quot;, value*100));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;Rcpp::sourceCpp&lt;/code&gt;でコンパイルしたら、以下のように&lt;code&gt;R&lt;/code&gt;の関数として実行します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- Rolling_r_cpp(JPYUSD,from))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       0.05       0.00       0.04&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;はい。1200万件のデータの処理に1秒かかりません。便利ー。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r         
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014  
##  Median :2011-07-03 09:57:30   Median : 0.000  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880  
##                                NA&amp;#39;s   :29977&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;問題なく、リターンが計算されています。では、&lt;code&gt;Realized Bipower Variation&lt;/code&gt;の計算に移りましょう。5min刻みの場合はWindowの長さは270が推奨でしたが、そこも引数として柔軟を持たせた作りにします。また、&lt;code&gt;NA&lt;/code&gt;の処理についても丁寧に行います。&lt;/p&gt;
&lt;pre class=&#34;cpp&#34;&gt;&lt;code&gt;#include &amp;lt;Rcpp.h&amp;gt;
#include &amp;lt;cmath&amp;gt;

using namespace Rcpp;
//[[Rcpp::plugins(cpp11)]]

// [[Rcpp::export]]
float rbv_cpp(
    NumericVector x, // rbvを計算するリターンベクトル
    bool na_rm = true) // xにNAが含まれている場合、取り除いて計算するか
{
  
  // 計算回数を取得
  R_xlen_t N = x.length();
  
  // 計算結果を入れる変数を定義
  float out = 0;

  // xの欠損有無を確認
  LogicalVector lg_NA = is_na(x);
  
  // xにNAが存在した場合、そのNAを除いて計算するかどうか
  if(any(lg_NA).is_true() and na_rm==FALSE){
    out = NA_REAL; // NAを計算結果として出力
  } else {
    
    // NAを除く場合
    if (any(lg_NA).is_true() and na_rm==TRUE){
      x[is_na(x)==TRUE] = 0.00; // NAに0を埋め、実質的に計算から除外する
    }
    
    // rbvの分子(総和)を計算
    for(R_xlen_t i = 1; i &amp;lt; N; ++i){
      out = out + std::abs(x[i])*std::abs(x[i-1]);
    }
    
    // 平均値を計算し、ルートをとる
    long denomi; //分母
    if(N-sum(lg_NA)-2&amp;gt;0){
      denomi = N-sum(lg_NA)-2;
    } else {
      denomi = 1;
    }
    out = out/denomi;
    out = std::sqrt(out);
  }
  
  return out;
}

// [[Rcpp::export]]
DataFrame Rolling_rbv_cpp(
    DataFrame input, //（計測時刻time, 計測値data）のデータフレーム
    int K = 270, // 計算するRolling Window幅
    bool na_pad = false, // Window幅が足りないときにNAを返すか
    bool na_remove = false // Window幅の中にNAが存在した場合、除いて計算を行うか
){
  // リターンベクトルとサンプル数を取り出す
  NumericVector data = input[&amp;quot;r&amp;quot;];
  R_xlen_t T = data.length();
  
  // 計算結果を格納するベクトルを準備
  NumericVector value(T);
  
  // Windows幅毎にRBVを計算し、格納する
  if(na_pad==TRUE){
    value[0] = NA_REAL; // NAを返す
    value[1] = NA_REAL; // NAを返す
    value[2] = NA_REAL; // NAを返す
  } else {
    value[0] = 0; // 0を返す
    value[1] = 0; // 0を返す
    value[2] = 0; // NAを返す
  }
  
  for(R_xlen_t t = 3; t &amp;lt; T; ++t){
    // Windows幅が足りるかどうかで処理を分岐
    if (t-K&amp;gt;=0){
      value[t] = rbv_cpp(data[seq(t-K,t-1)],na_remove); // 通常計算を実行
    } else if(na_pad==FALSE) {
      value[t] = rbv_cpp(data[seq(0,t-1)],na_remove); // Kに満たない不完全なWidnows幅で計算を実行
    } else {
      value[t] = NA_REAL; // NAを返す
    }
  }
  
  // 計算した時間と値をデータフレームとして出力する
  DataFrame out =
    DataFrame::create(
      Named(&amp;quot;from&amp;quot;, input[&amp;quot;from&amp;quot;]),
      Named(&amp;quot;r&amp;quot;, data),
      Named(&amp;quot;rbv&amp;quot;,value));
  
  return out;
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、これもコンパイルし、&lt;code&gt;R&lt;/code&gt;で実行します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(results &amp;lt;- results %&amp;gt;% Rolling_rbv_cpp(na_remove = FALSE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    ユーザ   システム       経過  
##       1.40       0.45       1.89&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;こちらも一瞬ですね。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;jump統計量の計算&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Jump統計量の計算&lt;/h2&gt;
&lt;p&gt;では、次に今計算したリターンと標準偏差から統計量&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{L}_{t_i}\)&lt;/span&gt;を計算しましょう。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 対数リターンの絶対値を標準化=Jump統計量
results &amp;lt;- results %&amp;gt;% dplyr::mutate(J=ifelse(rbv&amp;gt;0,abs(r)/rbv,NA))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;今こんな感じです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では、Jump検定に移りましょう。まず、必要な関数を定義しておきます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Jump検定を計算するための定数&amp;amp;関数を準備
c &amp;lt;- (2/pi)^0.5
Cn &amp;lt;- function(n){
  return((2*log(n))^0.5/c - (log(pi)+log(log(n)))/(2*c*(2*log(n))^0.5))
}
Sn &amp;lt;- function(n){
  1/(c*(2*log(n))^0.5)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;では検定を行います。棄却されたサンプルは1、それ以外は0を返します。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Jump検定(10%)を実行(返り値はlogical)
N &amp;lt;- NROW(results$J)
results &amp;lt;- results %&amp;gt;% dplyr::mutate(Jump = J &amp;gt; 2.25*Sn(N) + Cn(N))
summary(results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       from                           r               rbv              J        
##  Min.   :2011-01-02 22:00:00   Min.   :-1.823   Min.   :0.00    Min.   : 0.00  
##  1st Qu.:2011-04-03 15:58:45   1st Qu.:-0.014   1st Qu.:0.02    1st Qu.: 0.28  
##  Median :2011-07-03 09:57:30   Median : 0.000   Median :0.02    Median : 0.64  
##  Mean   :2011-07-03 09:57:30   Mean   : 0.000   Mean   :0.03    Mean   : 0.93  
##  3rd Qu.:2011-10-02 03:56:15   3rd Qu.: 0.015   3rd Qu.:0.03    3rd Qu.: 1.23  
##  Max.   :2011-12-31 21:55:00   Max.   : 2.880   Max.   :0.16    Max.   :58.60  
##                                NA&amp;#39;s   :29977    NA&amp;#39;s   :44367   NA&amp;#39;s   :44423  
##     Jump        
##  Mode :logical  
##  FALSE:59864    
##  TRUE :257      
##  NA&amp;#39;s :44423    
##                 
##                 
## &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;ggplot2を用いた可視化&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. ggplot2を用いた可視化&lt;/h2&gt;
&lt;p&gt;数値が計算できましたので可視化しましょう。2011/03/11の日中のJPY/USDの5min刻み対数リターンの推移とJumpを重ねてPlotします。ちなみに横軸は日本時間に修正しています。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 2011/03/11の東日本大震災発生時のJumpについてPlot
results %&amp;gt;% 
  dplyr::filter(from &amp;gt;= as.POSIXct(&amp;quot;2011-03-11 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;),from &amp;lt; as.POSIXct(&amp;quot;2011-03-12 00:00:00&amp;quot;,tz=&amp;quot;UTC&amp;quot;)) %&amp;gt;% 
  ggplot2::ggplot(ggplot2::aes(x=from,y=r)) +
  ggplot2::geom_path(linetype=3) +
  ggplot2::geom_path(ggplot2::aes(x=from,y=r*Jump,colour=&amp;quot;red&amp;quot;)) +
  ggplot2::scale_x_datetime(date_breaks = &amp;quot;2 hours&amp;quot;, labels = scales::date_format(format=&amp;quot;%H:%M&amp;quot;,tz=&amp;quot;Asia/Tokyo&amp;quot;)) +
  ggplot2::ggtitle(&amp;quot;JPY/USD Jumps within Tohoku earthquake on 2011-3-11&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 36 row(s) containing missing values (geom_path).

## Warning: Removed 36 row(s) containing missing values (geom_path).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../post/post21/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ここまで執筆するのに結構時間使っていて、今23:37なんで深い考察は控えますが、震災が発生したのが14:46:18ですから市場は震災直後即座に円安に反応したことが分かります。その後なぜか円高方向へ進み19:00にはピークになっています。安全資産の円とか言われますが、この時ばかりは不確実性の高まりからして安全じゃないだろと思いますが。。。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;まとめ&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;5. まとめ&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Rcpp&lt;/code&gt;を使った&lt;code&gt;R&lt;/code&gt;分析の効率化について紹介しました。&lt;code&gt;C++&lt;/code&gt;は愚直にコードを書いてもRより格段に処理が早いのでコーディングミスしにくい印象です。学術的な実装をやるときは内容が複雑になるのでこれはありがたいです。また、コンパイルエラーが起こってもRStudioを使っていればどこでコンパイルエラーが起こっているか手がかりをくれますのでその点でもストレスはないのでお勧めです。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;非負、整数、非減少の値を持つ確率過程のこと。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;平均はドリフト項の形状により必ずしも0にはなりませんが、今ドリフト項は十分小さい値を想定しているのでこの書き方にさせてください。論文ではより厳密に定義しています。&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>そのバックテスト本当に再現性ありますか？</title>
      <link>/post/post19/</link>
      <pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/post19/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#今回のテーマバックテストとは&#34;&gt;1. 今回のテーマ「バックテスト」とは？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#バックテストはオーバーフィットする&#34;&gt;2. バックテストはオーバーフィットする&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#シャープレシオが従う分布とは&#34;&gt;3. シャープレシオが従う分布とは&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#the-minimum-backtest-lengthを導出してみる&#34;&gt;4. &lt;code&gt;the minimum backtest length&lt;/code&gt;を導出してみる&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#終わりに&#34;&gt;4. 終わりに&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;今回のテーマバックテストとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. 今回のテーマ「バックテスト」とは？&lt;/h2&gt;
&lt;p&gt;バックテストは、アルゴリズムによる投資戦略のヒストリカルシミュレーションです。バックテストは、立案した投資戦略がある期間にわたって実行されていた場合に発生したであろう利益と損失をアルゴリズムを用いて計算します。その際、シャープレシオやインフォメーションレシオなどの投資戦略のパフォーマンスを評価する一般的な統計量が使用されています。投資家は通常、これらのバックテストの統計量を調査し、最高のパフォーマンスを発揮する投資(運用)戦略に資産配分を決定するため、資産運用会社は良好なパフォーマンスを血のにじむような回数のバックテストを試行錯誤し、資料を作ってプレゼンしたりするわけです。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://stat.ameba.jp/user_images/20190212/22/nash210/51/5f/j/o0705061514355131242.jpg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;3倍3分法のバックテスト&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;投資家の立場に立つなら、バックテストされた投資戦略のパフォーマンスについては、インサンプル(IS)とアウトオブサンプル(OOS)を区別することが重要です。ISのパフォーマンスは、投資戦略の設計に使用したサンプル（機械学習の文献では「学習期間」や「訓練セット」と呼ばれる物です）でシミュレートしたものです。一方、OOSパフォーマンスは、投資戦略の設計に使用されなかったサンプル（別名「テストセット」）でシミュレーションされたものです。バックテストは、そのパフォーマンスを持ってその投資戦略の有効性を占う物ですので、ISのパフォーマンスがOOSのパフォーマンスと一致している場合に再現性が担保され、現実的であるということができます。ただ、アウトサンプルの結果はこれからの結果であるので、バックテストを受け取った時点でそのバックテストが信頼に足るものか判断することは難しいです。hold-out法などで、以下のように学習データとテストデータを分け、OOSでのテストを行っているものもありますが、OOSの結果をフィードバックして戦略の改善ができる以上、純粋なアウトサンプルとは呼べません。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.triton.biz/blog1/wp-content/uploads/2018/04/pic001.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;ですので、ファンドマネージャーから良い結果のバックテストを受け取った場合、そのシミュレーションがどれだけ現実的であるかをなんとかして評価することが非常に重要となります。また、ファンドマネージャーも自身のバックテスト結果が持つ不確実性を理解しておくことが重要です。今回はバックテストのシミュレーションの現実性をどのようにして評価するのか、再現性のあるバックテストを行うためには何に注意すれば良いのかを調べてみたいと思います。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;バックテストはオーバーフィットする&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. バックテストはオーバーフィットする&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2308659&#34;&gt;Bailey, Borwein, López de Prado and Zhu(2015)&lt;/a&gt;は、どのような金融時系列でも、バックテストのシミュレーションをオーバーフィット(過学習)させることが(比較的)簡単にできると主張しています。ここで、オーバーフィットとは、機械学習の概念であり，モデルが一般的な構造よりも特定の観察データ(ISデータ)にフォーカスしてしまう状況を表します。&lt;/p&gt;
&lt;p&gt;Bailey et. al.(2015)では、この主張の一例として株式戦略のバックテスト結果が芳しくない状況が挙げられています。バックテストではその名の通り過去データを使用しているので、具体的に損失が発生している銘柄を特定することが可能で、その銘柄の推奨を削除するためにいくつかのパラメータを追加し、取引システムを設計することで、パフォーマンスを向上させることができるというわけです（「データ・スヌーピング」として知られているテクニック）。数回シミュレーションを繰り返えせば、特定のサンプルに存在するが、母集団の中では稀であるかもしれない特徴から利益を得る「最適なパラメータ」を導くことができます。&lt;/p&gt;
&lt;p&gt;機械学習の文献では、オーバーフィッティングの問題を対処するための膨大な研究の蓄積があります。ですが、Bailey et. al.(2015)は、機械学習の文脈で提案されている手法は一般的に複数の投資問題には適用できないと主張します。その理由は以下4点のようです。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;機械学習でオーバーフィッティングを防ぐ手法は、予測の説明力や質を評価するために、その事象が定義される領域において明示的な点推定と信頼区間を必要としますが、このような明確な予測を行う投資戦略はほとんどないため。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;例えば、「E-mini S&amp;amp;P500は、金曜日の終値で1標準偏差5ポイントで1,600前後になると予測されています」とはあまり言われず、むしろ「買い」または「強い買い」といった定性的な推奨が提供されることが一般的です。しかも、この予想は予測の有効期限も明示されず、なにか予期せぬ事象が発生した際に変更がなされます。一方、定量予測では金曜日の終値と明記されています。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;仮に特定の投資戦略が予測式に依存していたとしても、投資戦略の他の構成要素がオーバーフィットされている可能性がある。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;言い換えれば、単に予測式を調整する以外にも、投資戦略をオーバーフィットさせる方法はたくさんあるということです。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;回帰のオーバーフィットの方法はパラメトリックであり、金融の場合観察不可能なデータに関する多くの仮定を含むため。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;いくつかの手法は試行回数をコントロールしていないため。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Bailey et. al.(2015)では、バックテストのパフォーマンスが比較的低い投資戦略を特定するためには、&lt;strong&gt;比較的少ない試行回数&lt;/strong&gt;が必要であることを示しています。ここでの試行回数とは試行錯誤の回数だと思ってください。また、試行回数に応じて必要とされるバックテストの期間である&lt;code&gt;the minimum backtest length&lt;/code&gt;（MinBTL）を計算しています。この論文では、パフォーマンスを評価するために常にシャープレシオが使用されていますが、他のパフォーマンス指標にも応用できるそうです。その内容を見てみましょう。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;シャープレシオが従う分布とは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. シャープレシオが従う分布とは&lt;/h2&gt;
&lt;p&gt;MinBTLを導出するために、まずシャープレシオの(漸近)分布を導出します。そもそも、投資戦略の設計は、通常、特定のパターンが金融変数の将来値を予測するのに役立つかもしれないという事前知識または信念から始まります。例えば、さまざまな満期の債券の間にリードラグ効果を認識している場合は、イールドカーブが上昇した場合に均衡値への回帰に賭ける戦略を設計することができます。このモデルは、cointegration equation、ベクトル誤差補正モデル、確率微分方程式のシステムなどの形をとることが考えられます。&lt;/p&gt;
&lt;p&gt;このようなモデル構成（または試行）の数は膨大であり、ファンドマネージャーは当然、戦略のパフォーマンスを最大化するものを選択したいと考え、そのためにヒストリカルシミュレーション（バックテスト）を行います(前述)。バックテストでは、最適なサンプルサイズ、シグナルの更新頻度、リスクサイジング、ストップロス、最大保有期間などなどを他の変数との兼ね合いの中で評価します。&lt;/p&gt;
&lt;p&gt;この論文中でパフォーマンス評価の尺度として使用されるシャープレシオは、過去のリターンのサンプルに基づいて、戦略のパフォーマンスを評価する統計量で、BMに対する平均超過リターン/標準偏差(リスク)として定義されます。通常には、「リスク1標準偏差に対するリターン」と解釈され、資産クラスにもよりますが1を上回っていると非常に良い戦略であると見なせます。以下では、ある戦略の超過リターン&lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt;がi.i.d.の確率変数であり、正規分布に従うと仮定します。つまり、&lt;span class=&#34;math inline&#34;&gt;\(r_t\)&lt;/span&gt;の分布は&lt;span class=&#34;math inline&#34;&gt;\(r_s(t\neq s)\)&lt;/span&gt;と独立であることを仮定しています。あまり現実的な仮定ではありませんが。。。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_t \sim \mathcal{N}(\mu,\sigma^2)
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}\)&lt;/span&gt;は平均&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;、分散&lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt;の正規分布を表しています。今、時点t~t-q+1の超過リターン&lt;span class=&#34;math inline&#34;&gt;\(r_{t}(q)\)&lt;/span&gt;を&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r_{t}(q) \equiv r_{t} + r_{t-1} + ... + r_{t-q+1}
\]&lt;/span&gt;
と定義すると(複利部分を無視してます)、年率化されたシャープレシオは&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
SR(q) &amp;amp;=&amp;amp; \frac{E[r_{t}(q)]}{\sqrt{Var(r_{t}(q))}}\\
&amp;amp;=&amp;amp; \frac{q\mu}{\sqrt{q}\sigma}\\
&amp;amp;=&amp;amp; \frac{\mu}{\sigma}\sqrt{q}
\end{eqnarray}
\]&lt;/span&gt;
と表すことができます。ここで、&lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;は年毎のリターンの数(頻度)です。例えば、日次リターンの場合&lt;span class=&#34;math inline&#34;&gt;\(q=365\)&lt;/span&gt;となります(閏年を除く)。
&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;は一般に未知ですので、&lt;span class=&#34;math inline&#34;&gt;\(SR\)&lt;/span&gt;の真値を知ることはできません。なので、&lt;span class=&#34;math inline&#34;&gt;\(R_t\)&lt;/span&gt;を標本リターン、リスクフリーレート&lt;span class=&#34;math inline&#34;&gt;\(R^f\)&lt;/span&gt;(定数)とすると、標本平均&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}=1/T\sum_{t=1}^T R_{t}-R^f\)&lt;/span&gt;と標本標準偏差&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}=\sqrt{1/T\sum_{t=1}^{T}(R_{t}-\hat{\mu})}\)&lt;/span&gt;を用いてシャープレシオの推定値を計算することになります(&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;はバックテストを行うサンプルサイズ)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q) = \frac{\hat{\mu}}{\hat{\sigma}}\sqrt{q}
\]&lt;/span&gt;
必然的な結果として、&lt;span class=&#34;math inline&#34;&gt;\(SR\)&lt;/span&gt;の計算はかなりの推定誤差が伴う可能性が高くなります。では、本節の本題、&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}\)&lt;/span&gt;の漸近分布を導出してみましょう。まず、&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt;の漸近分布はi.i.d.と&lt;span class=&#34;math inline&#34;&gt;\(\mu, \sigma\)&lt;/span&gt;が有限な値をとることから中心極限定理を適用することにより、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{T}\hat{\mu}\sim^{a}\mathcal{N}(\mu,\sigma^2), \\
\sqrt{T}\hat{\sigma}^2\sim^a\mathcal{N}(\sigma^2,2\sigma^4)
\]&lt;/span&gt;
となります。シャープレシオはこの&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mu}\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}^2\)&lt;/span&gt;から計算される確率変数であるので、この関数を&lt;span class=&#34;math inline&#34;&gt;\(g(\hat{{\boldsymbol \theta}})\)&lt;/span&gt;と表しましょう。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}=(\hat{\mu},\hat{\sigma}^2)&amp;#39;\)&lt;/span&gt;です。今、i.i.d.であるので&lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}\)&lt;/span&gt;は互いに独立となり、上記の議論から漸近同時分布は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{T}\hat{{\boldsymbol \theta}} \sim^a \mathcal{N}({\boldsymbol \theta},{\boldsymbol V_{\boldsymbol \theta}})
\]&lt;/span&gt;
と書けます。ここで、&lt;span class=&#34;math inline&#34;&gt;\({\boldsymbol V_{\boldsymbol \theta}}\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
{\boldsymbol V_{\boldsymbol \theta}} = \left( 
    \begin{array}{cccc}
      \sigma^2 &amp;amp; 0\\
      0 &amp;amp; 2\sigma^4\\
    \end{array}
  \right)
\]&lt;/span&gt;
です。シャープレシオの推定値は今&lt;span class=&#34;math inline&#34;&gt;\(g(\hat{{\boldsymbol \theta}})\)&lt;/span&gt;と&lt;span class=&#34;math inline&#34;&gt;\(\hat{{\boldsymbol \theta}}\)&lt;/span&gt;だけの関数になっていますのでデルタ法より、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR} = g(\hat{{\boldsymbol \theta}}) \sim^a \mathcal{N}(g({\boldsymbol \theta}),\boldsymbol V_g)
\]&lt;/span&gt;
と漸近的に正規分布に従います。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol V_g\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol V_g=\frac{\partial g}{\partial{\boldsymbol \theta}}{\boldsymbol V_{\boldsymbol \theta}}\frac{\partial g}{\partial{\boldsymbol \theta}&amp;#39;}
\]&lt;/span&gt;
です。&lt;span class=&#34;math inline&#34;&gt;\(g({\boldsymbol \theta})=\mu/\sigma\)&lt;/span&gt;なので、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial g}{\partial{\boldsymbol \theta}&amp;#39;} = \left[ 
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma^2}\\
    \end{array}
  \right]
  = \left[ 
    \begin{array}{cccc}
      \frac{1}{\sigma}\\
      -\frac{\mu}{2\sigma^3}\\
    \end{array}
  \right]
\]&lt;/span&gt;
よって、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\boldsymbol V_g &amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}, \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right)
  \left( 
    \begin{array}{cccc}
      \sigma^2 &amp;amp; 0\\
      0 &amp;amp; 2\sigma^4\\
    \end{array}
  \right)
  \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right) \\
  &amp;amp;=&amp;amp; \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\sigma^2, \frac{\partial g}{\partial \sigma}2\sigma^4\\
    \end{array}
  \right)
    \left(
    \begin{array}{cccc}
      \frac{\partial g}{\partial \mu}\\
      \frac{\partial g}{\partial \sigma}\\
    \end{array}
  \right) \\
  &amp;amp;=&amp;amp; (\frac{\partial g}{\partial \mu})^2\sigma^2 + (\frac{\partial g}{\partial \sigma})^2\sigma^4 \\
  &amp;amp;=&amp;amp; 1 + \frac{\mu^2}{2\sigma^2} \\
  &amp;amp;=&amp;amp; 1 + \frac{1}{2}SR^2
\end{eqnarray}
\]&lt;/span&gt;
と導出することができます。シャープレシオの絶対値が大きくなるほど指数的に分散が大きくなる傾向があるので良いパフォーマンスを見た時には注意が必要かもしれません。年率化されたシャープレシオの推定値&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}(q)\)&lt;/span&gt;が従う分布はここから&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q)\sim^a \mathcal{N}(\sqrt{q}SR,\frac{V(q)}{T}) \\
V(q) = q{\boldsymbol V}_g = q(1 + \frac{1}{2}SR^2)
\]&lt;/span&gt;
となります。今、&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;をバックテストを行う年数とすると&lt;span class=&#34;math inline&#34;&gt;\(T=yq\)&lt;/span&gt;と書け、これを用いて上式を以下のように書き換えることができます(日次リターンで3年計測の場合、サンプルサイズ&lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(T=3×365=1095\)&lt;/span&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat{SR}(q)\sim^a \mathcal{N}(\sqrt{q}SR,\frac{1+\frac{1}{2}SR^2}{y}) \tag{1}
\]&lt;/span&gt;
頻度&lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt;はシャープレシオの平均には影響しますが分散には影響を及ぼしません。これでシャープレシオの推定値の漸近分布を導出することができました。さて、これを使ってなにをしたかったのかということですが、私たちは今バックテストの信頼性について考えていたのでした。つまり、FMが新商品を開発するために頭をひねって考え出した&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;個の投資戦略案のバックテストをした際に、それらのシャープレシオの真値がどれも0であるにも関わらず、非常に高い(良い)値が出る確率はいかほどなのかということです。Bailey et. al.(2015)では以下のように記述されていました。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;How high is the expected maximum Sharpe ratio IS among a set of strategy configurations where the true Sharpe ratio is zero?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;また、期待最大シャープレシオの値を小さくするためには、いったいどれほどの期間バックテストをすべきなのかも知りたいわけです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-minimum-backtest-lengthを導出してみる&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. &lt;code&gt;the minimum backtest length&lt;/code&gt;を導出してみる&lt;/h2&gt;
&lt;p&gt;今考えている状況は、&lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt;で&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;を簡単化のために1年とすると(1)式より&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}(q)\)&lt;/span&gt;は標準正規分布&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{N}(0,1)\)&lt;/span&gt;に従います。さて、今から私たちは&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}_n(n=1,2,...N)\)&lt;/span&gt;の最大値&lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]_N\)&lt;/span&gt;の期待値について考えていくのですが、勘の良い人ならお気づきの通り、議論は極値統計の文脈に入っていくことになります。&lt;span class=&#34;math inline&#34;&gt;\(\hat{SR}_n\sim\mathcal{N}(0,1)\)&lt;/span&gt;はi.i.d.なので、その最大統計量の極値分布はFisher-Tippett-Gnedenko定理よりガンベル分布になります(証明追えてないです、ごめんなさい)。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\lim_{N\rightarrow\infty}prob[\frac{\max[\hat{SR}]_N-\alpha}{\beta}\leq x] = G(x) = e^{-e^{-x}}
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(\alpha=Z(x)^{-1}[1-1/N], \beta=Z(x)^{-1}[1-1/Ne^{-1}]-\alpha\)&lt;/span&gt;で、&lt;span class=&#34;math inline&#34;&gt;\(Z(x)\)&lt;/span&gt;は標準正規分布の累積分布関数を表しています。ガンベル分布のモーメント母関数&lt;span class=&#34;math inline&#34;&gt;\(M_x(t)\)&lt;/span&gt;は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
M_x(t) &amp;amp;=&amp;amp; E[e^{tx}] = \int_{-\infty}^\infty e^{tx}e^{-x}e^{-e^{-x}}dx \\
\end{eqnarray}
\]&lt;/span&gt;
と書け、&lt;span class=&#34;math inline&#34;&gt;\(x=-\log(y)\)&lt;/span&gt;と変数変換すると&lt;span class=&#34;math inline&#34;&gt;\(dx/dy=-1/y=-(e^{-x})^{-1}\)&lt;/span&gt;なので、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
M_x(t) &amp;amp;=&amp;amp; \int_{\infty}^0-e^{-t\log(y)}e^{-y}dy \\
&amp;amp;=&amp;amp; \int_{0}^\infty y^{-t}e^{-y}dy \\
&amp;amp;=&amp;amp; \Gamma(1-t)
\end{eqnarray}
\]&lt;/span&gt;
となります。&lt;span class=&#34;math inline&#34;&gt;\(\Gamma(x)\)&lt;/span&gt;はガンマ関数です。ここから、標準化された最大統計量の期待値(平均)は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{eqnarray}
\lim_{N\rightarrow\infty} E[\frac{\max[\hat{SR}]_N-\alpha}{\beta}] &amp;amp;=&amp;amp; M_x&amp;#39;(t)|_{t=0} \\
&amp;amp;=&amp;amp; (-1)\Gamma&amp;#39;(1) \\
&amp;amp;=&amp;amp; (-1)(-\gamma) = \gamma
\end{eqnarray}
\]&lt;/span&gt;
となります。ここで、&lt;span class=&#34;math inline&#34;&gt;\(\gamma\approx0.5772156649...\)&lt;/span&gt;はEuler-Mascheroni定数です。よって、&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;が大きいとき、i.i.d.の標準正規分布の最大統計量の期待値は&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[\max[\hat{SR}]] \approx \alpha + \gamma\beta = (1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}] \tag{2}
\]&lt;/span&gt;
と近似できます(&lt;span class=&#34;math inline&#34;&gt;\(N&amp;gt;1\)&lt;/span&gt;)。これがBailey et. al.(2015)のProposition 1.になります。&lt;span class=&#34;math inline&#34;&gt;\(E[\max[\hat{SR}]]\)&lt;/span&gt;を戦略数(試行錯誤数)&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;の関数としてプロットしたのが以下になります。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ExMaxSR = function(N){
  gamma_ct = -digamma(1)
  Z = qnorm(0.99)
  return((1-gamma_ct)*Z*(1-1/N) + gamma_ct*Z*(1-1/N*exp(1)^{-1}))
}
N = list(0:100)
result = purrr::map(N,ExMaxSR)
ggplot2::ggplot(data.frame(ExpMaxSR = unlist(result),N = unlist(N)),aes(x=N,y=ExpMaxSR)) +
  geom_line(size=1) + ylim(0,3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;小さい&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;に対して急激に&lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]\)&lt;/span&gt;の期待値が上昇していることがわかると思います。&lt;span class=&#34;math inline&#34;&gt;\(N=10\)&lt;/span&gt;の時、&lt;span class=&#34;math inline&#34;&gt;\(\max[\hat{SR}]=1.54\)&lt;/span&gt;となっており、全ての戦略のシャープレシオの真値が0にも拘わらず、少なくとも1つは見かけ上かなり良いパフォーマンスの戦略が見つかることが期待されます。金融ではhold-out法でのバックテストはしばしば使用されるかと思いますが、この方法は試行(錯誤)回数を考慮に入れていないため、&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;が大きいときには信頼に足る結果を返してくれないわけです。バックテストの結果を向上させるため、闇雲にあれやこれやとシミュレーションを行うことは非常に危険だと思いませんか？最終的にプレゼン資料に上がってくるのは&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;個の戦略のうち、最もパフォーマンスが良いもののみですから、今回の例のように10個戦略を考えただけでもどれかはシャープレシオが1.87付近に分布しているわけです。試行錯誤数なんてもちろん資料には記載しませんから、非常にミスリーディングなわけです。こういった資料を評価する際にはまず偽陽性を疑ってかかった方がいいかもしれません。&lt;/p&gt;
&lt;p&gt;では、どうすれば良いのかという話ですが、Bailey et. al.(2015)では、Minimum Backtest Lengthを計算しています。要は試行(錯誤)数&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;を増やすにつれて、バックテストの年数&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;も伸ばしていけよと戒めているわけです。&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;とMinimum Backtest Lengthの関係性を示していきましょう。先ほどと同じく&lt;span class=&#34;math inline&#34;&gt;\(\mu=0\)&lt;/span&gt;を仮定しますが、&lt;span class=&#34;math inline&#34;&gt;\(y\neq 1\)&lt;/span&gt;であるケースを考えます。年率化シャープレシオの最大統計量の期待値は(2)式より、&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
E[\max[\hat{SR}(q)]_N] \approx y^{-1/2}((1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}])
\]&lt;/span&gt;
となります。これを&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;に対して解いてやることでMinBTLが求まります。&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
MinBTL \approx (\frac{(1-\gamma)Z^{-1}[1-\frac{1}{N}]+\gamma Z^{-1}[1-\frac{1}{N}e^{-1}]}{\bar{E[\max[\hat{SR}(q)]_N]}})^2
\]&lt;/span&gt;
ここで、&lt;span class=&#34;math inline&#34;&gt;\(\bar{E[\max[\hat{SR}(q)]_N]}\)&lt;/span&gt;は&lt;span class=&#34;math inline&#34;&gt;\(E[\max[\hat{SR}(q)]_N]\)&lt;/span&gt;の上限値で、シャープレシオの真値が0である&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;戦略でシャープレシオの最大統計量が取りうる値を抑えます。その際に、必要なバックテスト年数&lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;がMinBTLとして導出されるのです。&lt;span class=&#34;math inline&#34;&gt;\(\bar{E[\max[\hat{SR}(q)]_N]}=1\)&lt;/span&gt;として、MinBTLを&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;の関数としてプロットしたものが以下です。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MinBTL &amp;lt;- function(N,MaxSR){
  return((ExMaxSR(N)/MaxSR)^2)
}
N = list(1:100)
result = purrr::map2(N,1,MinBTL)
ggplot2::ggplot(data.frame(MinBTL = unlist(result),N = unlist(N)),aes(x=N,y=MinBTL)) +
  geom_line(size=1) + ylim(0,6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;simSR &amp;lt;- function(T1){
    r = rnorm(T1)
    return(mean(r)/sd(r))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;仮にバックテスト年数が3年以内しかできない場合は試行(錯誤)回数&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;はほぼ1回に抑えないといけないことになります。3年以内の場合は一発で当ててねという厳しめの制約です。注意しないといけないのは、MinBTLの範囲内でバックテストを行っていたとしてもオーバーフィットすることは考えられるということです。つまり、MinBTLは必要条件であって十分条件でないというわけです。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;終わりに&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. 終わりに&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257497&#34;&gt;López de Prado(2018)&lt;/a&gt;では、オーバーフィッティングを防ぐ汎用的な手段として以下が挙げられています。&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Develop models for entire asset classes or investment universes, rather than for specific securities. Investors diversify, hence they do not make mistake X only on security Y. If you find mistake X only on security Y, no matter how apparently profitable, it is likely a false discovery.
(拙訳：特定の有価証券ではなく、アセットクラス全体またはユニバース全体のモデルを開発すること。投資家はリスクを分散させているので、彼らはある証券Yだけに対してミスXをすることはありません。あなたが証券YだけにミスXを見つけた場合は、それがどんなに明らかに有益であっても、誤発見である可能性が高い。)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apply bagging as a means to both prevent overfitting and reduce the variance of the forecasting error. If bagging deteriorates the performance of a strategy, it was likely overfit to a small number of observations or outliers.
(拙訳：オーバーフィットを防ぎ、予測誤差の分散を減らすための手段として、バギングを適用すること。バギングが戦略のパフォーマンスを悪化させる場合、それは少数の観測値または外れ値にオーバーフィットした可能性が高い。)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Do not backtest until all your research is complete.&lt;/strong&gt;
(拙訳：&lt;strong&gt;すべてのリサーチが完了するまでバックテストをしないこと。&lt;/strong&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Record every backtest conducted on a dataset so that the probability of backtest overfitting may be estimated on the final selected result (see &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2326253&#34;&gt;Bailey, Borwein, López de Prado and Zhu(2017)&lt;/a&gt;), and the Sharpe ratio may be properly deflated by the number of trials carried out (&lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2465675&#34;&gt;Bailey and López de Prado(2014.b)&lt;/a&gt;).
(拙訳：研究者が最終的に選択したバックテスト結果がオーバーフィットしている確率を推定できるように、単一の(同じ)データセットで実施されたバックテストをすべて記録すること（Bailey, Borwein, López de Prado and Zhu [2017]）、また、実施された試行数によってシャープレシオを適切にデフレーションできるようにすること（Bailey and López de Prado [2014]）。)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Simulate scenarios rather than history. A standard backtest is a historical simulation, which can be easily overfit. History is just the random path that was realized, and it could have been entirely different. Your strategy should be profitable under a wide range of scenarios, not just the anecdotal historical path. It is harder to overfit the outcome of thousands of “what if” scenarios.
(拙訳：ヒストリカルではなくシナリオをシミュレーションすること。標準的なバックテストはヒストリカルシミュレーションであり、オーバーフィットしやすい。歴史(これまでの実績)はランダムなパスの実現値に過ぎず、全く違ったものになっていた可能性があります。あなたの戦略は、逸話的なヒストリカルパスではなく、様々なシナリオの下で利益を得ることができるものであるべきです。何千もの「もしも」のシナリオ結果をオーバーフィットさせるのは(ヒストリカルシミュレーションで過学習するよりも)より難しいことです。)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Do not research under the influence of a backtest. If the backtest fails to identify a profitable strategy,
start from scratch. Resist the temptation of reusing those results.
(拙訳：バックテストのフィードバックを受けてリサーチしないこと。バックテストが有益な戦略を見つけ出すことに失敗した場合は、ゼロからリサーチを再始動してください。それらの結果を再利用する誘惑に抗ってください。)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;3と6は本日の論文と関係のある文脈だと思います。この分野は他にも研究の蓄積があるので、業務でバックテストを行うという人は運用手法の勉強もいいですが、そもそものお作法としてバックテストの正しい運用方法について学ぶことをお勧めします。&lt;br /&gt;
さて、いつもとは違う観点で、少しメタ的なトピックに取り組んでみました。自分自身仕事柄バックテスト結果などを見ることも多いですし、このブログでもしばしばhold-out法でのバックテストをしています。得られた結果の不確実性を理解して、評価できるよう今後もこのトピックの研究を追っていきたいと思います。&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>WindowsにNEologd辞書をインストールして、RMeCabを実行する方法</title>
      <link>/post/post17/</link>
      <pubDate>Sun, 19 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/post17/</guid>
      <description>
&lt;script src=&#34;index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#mecabrmecabとは&#34;&gt;1. Mecab(RMeCab)とは？&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#固有名詞に弱いデフォルトのmecab&#34;&gt;固有名詞に弱いデフォルトのMeCab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#新語に強いneologd辞書&#34;&gt;新語に強いNEologd辞書&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#インストール手順&#34;&gt;2. インストール手順&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#a.-windows-subsystem-for-linuxwslのインストール&#34;&gt;A. &lt;code&gt;Windows Subsystem for Linux&lt;/code&gt;(WSL)のインストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#b.-ubuntu-linuxのインストール&#34;&gt;B. Ubuntu Linuxのインストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#c.-ubuntu-linuxにmecabをインストール&#34;&gt;C. Ubuntu LinuxにMeCabをインストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#d.-ubuntu-for-linuxにneologd辞書をインストール&#34;&gt;D. Ubuntu for Linuxに&lt;code&gt;NEologd&lt;/code&gt;辞書をインストール&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#e.-ubuntu-for-linuxからwindowsへ辞書ファイルをコピー&#34;&gt;E. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#f.-windows内で辞書ファイルのコンパイルshift-jisを行う&#34;&gt;F. Windows内で辞書ファイルのコンパイル(&lt;code&gt;SHIFT-JIS&lt;/code&gt;)を行う。&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#まとめ&#34;&gt;3. まとめ&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;p&gt;皆さんおはこんばんにちは。
最近在宅勤務で運動不足ですが、平日休日問わず研究活動をしています。
学術誌投稿を目指したBlog記事には書けない内容なので、更新はストップしてしまっていますがちゃんと活動はしています。&lt;/p&gt;
&lt;p&gt;今回はその研究の中で利用しているMeCabにまつわるTipsのご紹介です。MeCabというと形態素解析ソフトということはこのブログを読まれている方はお分かりかと思いますが、その辞書にNEologd辞書を使用できるようにしてみたというのが内容です。MeCabってなに？という方もいらっしゃるかもしれませんので、かなり簡単にご紹介します。&lt;/p&gt;
&lt;div id=&#34;mecabrmecabとは&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. Mecab(RMeCab)とは？&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;MeCab&lt;/code&gt;は日本語テキストマイニングで使用される形態素解析ソフトです。英語などの言語とは異なり、日本語は単語毎にスペースを置かないため、テキストマイニングを行う際、そのままでは文章を単語単位に区切って集計するといったことができません。例えば、「これはペンです。」という文章はそのままでは「これはペンです。」という&lt;em&gt;1つ&lt;/em&gt;の単語として認識されてしまいます。ですが、単語の頻度分析を行う際などは、「これ/は/ペン/です/。」という風に文章を単語レベルにまで分割し、「ペン」という特徴量を取得したいわけです。それができるのが&lt;code&gt;RMeCab&lt;/code&gt;で、この処理は形態素解析と呼ばれます。&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;そして、&lt;code&gt;RMeCab&lt;/code&gt;とはこの&lt;code&gt;MeCab&lt;/code&gt;のラッパーになります。&lt;code&gt;R&lt;/code&gt;から&lt;code&gt;MeCab&lt;/code&gt;を使用するには&lt;code&gt;RMeCab&lt;/code&gt;を使用する必要があります。使用方法は簡単で、&lt;code&gt;RMeCabC&lt;/code&gt;関数に文章を渡すだけです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
library(RMeCab)

RMeCabC(&amp;quot;これはペンです。&amp;quot;) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   名詞   助詞   名詞 助動詞   記号 
## &amp;quot;これ&amp;quot;   &amp;quot;は&amp;quot; &amp;quot;ペン&amp;quot; &amp;quot;です&amp;quot;   &amp;quot;。&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;RMeCabC&lt;/code&gt;関数は結果をリストで返してくるので、&lt;code&gt;unlist&lt;/code&gt;でcharacterに変換しています。&lt;/p&gt;
&lt;div id=&#34;固有名詞に弱いデフォルトのmecab&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;固有名詞に弱いデフォルトのMeCab&lt;/h3&gt;
&lt;p&gt;便利なように思えるのですが、デフォルトの&lt;code&gt;MeCab&lt;/code&gt;は固有名詞に弱いという弱点があります。例えば、「欅坂46が赤いきつねを食べている。」という文章を形態素解析してみましょう。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;欅坂46が赤いきつねを食べている。&amp;quot;) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     名詞     名詞     名詞     助詞   形容詞     名詞     助詞     動詞 
##     &amp;quot;欅&amp;quot;     &amp;quot;坂&amp;quot;     &amp;quot;46&amp;quot;     &amp;quot;が&amp;quot;   &amp;quot;赤い&amp;quot; &amp;quot;きつね&amp;quot;     &amp;quot;を&amp;quot;   &amp;quot;食べ&amp;quot; 
##     助詞     動詞     記号 
##     &amp;quot;て&amp;quot;   &amp;quot;いる&amp;quot;     &amp;quot;。&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;予想していた結果は「欅坂46/が/赤いきつね/を/食べ/て/いる/。」でしょう。ただ、固有名詞を上手く形態素解析することができていないため、不必要なところで分割がなされており、「赤い/きつね/を/食べ」という部分については「赤い（動物の）きつね」を食べているかのような解析結果になっています。赤いきつねの「きつね」と動物の「きつね」を同等に扱ってしまうので、問題があります。また、経済においても以下のように日経平均株価が分割され、新聞社の「日経」と株価の「日経」が、大統領の「トランプ」とカードの「トランプ」が区別できないといったことが想定されます。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;今週の日経平均株価は、上値抵抗線を突破して上昇！&amp;quot;) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   名詞   助詞   名詞   名詞   名詞   助詞   記号   名詞   名詞   名詞   助詞 
## &amp;quot;今週&amp;quot;   &amp;quot;の&amp;quot; &amp;quot;日経&amp;quot; &amp;quot;平均&amp;quot; &amp;quot;株価&amp;quot;   &amp;quot;は&amp;quot;   &amp;quot;、&amp;quot; &amp;quot;上値&amp;quot; &amp;quot;抵抗&amp;quot;   &amp;quot;線&amp;quot;   &amp;quot;を&amp;quot; 
##   名詞   動詞   助詞   名詞   記号 
## &amp;quot;突破&amp;quot;   &amp;quot;し&amp;quot;   &amp;quot;て&amp;quot; &amp;quot;上昇&amp;quot;   &amp;quot;！&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;トランプ政権による経済活動再開の指針発表が評価される&amp;quot;) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       名詞       名詞       助詞       名詞       名詞       名詞       助詞 
## &amp;quot;トランプ&amp;quot;     &amp;quot;政権&amp;quot;   &amp;quot;による&amp;quot;     &amp;quot;経済&amp;quot;     &amp;quot;活動&amp;quot;     &amp;quot;再開&amp;quot;       &amp;quot;の&amp;quot; 
##       名詞       名詞       助詞       名詞       動詞       動詞 
##     &amp;quot;指針&amp;quot;     &amp;quot;発表&amp;quot;       &amp;quot;が&amp;quot;     &amp;quot;評価&amp;quot;       &amp;quot;さ&amp;quot;     &amp;quot;れる&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;これはデフォルトで&lt;code&gt;MeCab&lt;/code&gt;が使用している&lt;em&gt;辞書&lt;/em&gt;に原因があります。IPA辞書(&lt;code&gt;ipadic&lt;/code&gt;)と呼ばれる物で、奈良先端科学技術大学院大学が公開している茶筌と呼ばれる形態素解析ソフト用に作られました。&lt;a href=&#34;https://ja.osdn.net/projects/ipadic/releases/&#34;&gt;こちら&lt;/a&gt;を見ると辞書の更新は2007年でストップしており、新語や流行語がアップデートされていないために上記の固有名詞が上手く形態素解析できないことがわかります。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;新語に強いneologd辞書&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;新語に強いNEologd辞書&lt;/h3&gt;
&lt;p&gt;最近&lt;code&gt;MeCab&lt;/code&gt;でよく使用されている辞書に&lt;code&gt;NEologd&lt;/code&gt;辞書というものがあります。&lt;code&gt;NEologd&lt;/code&gt;辞書とは、Web上から得た新語に対応しており、頻繁に更新される&lt;code&gt;MeCab&lt;/code&gt;用のシステム辞書です。&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;Twitterのアカウントには、&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;特色は語彙の多さと更新頻度、新語の採録の速さ、読み仮名の正確さ、表記揺れへの対応。おもな解析対象はWeb上のニュース記事や流行した出来事。
おもな用途は文書分類、文書ベクトル作成、単語埋め込みベクトル作成、読み仮名付与。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;と記載されており、上述したIPA辞書の弱点を補完する辞書となっています。今回の記事はその辞書のインストール方法についてですが、インストールした辞書の威力を先にお見せしておきます。実行するには、&lt;code&gt;RMeCab&lt;/code&gt;関数に辞書ファイル(.dicファイル)のパスを渡してやれば良いです。&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dic_directory &amp;lt;- &amp;quot;C:\\hogehoge\\mecab-user-dict-seed.yyyymmdd.dic&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;欅坂46が赤いきつねを食べている。&amp;quot;,dic=dic_directory) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         名詞         助詞         名詞         助詞         動詞         助詞 
##     &amp;quot;欅坂46&amp;quot;         &amp;quot;が&amp;quot; &amp;quot;赤いきつね&amp;quot;         &amp;quot;を&amp;quot;       &amp;quot;食べ&amp;quot;         &amp;quot;て&amp;quot; 
##         動詞         記号 
##       &amp;quot;いる&amp;quot;         &amp;quot;。&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;今週の日経平均株価は、上値抵抗線を突破して上昇！&amp;quot;,dic=dic_directory) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           名詞           助詞           名詞           助詞           記号 
##         &amp;quot;今週&amp;quot;           &amp;quot;の&amp;quot; &amp;quot;日経平均株価&amp;quot;           &amp;quot;は&amp;quot;           &amp;quot;、&amp;quot; 
##           名詞           名詞           名詞           助詞           名詞 
##         &amp;quot;上値&amp;quot;         &amp;quot;抵抗&amp;quot;           &amp;quot;線&amp;quot;           &amp;quot;を&amp;quot;         &amp;quot;突破&amp;quot; 
##           動詞           助詞           名詞           記号 
##           &amp;quot;し&amp;quot;           &amp;quot;て&amp;quot;         &amp;quot;上昇&amp;quot;           &amp;quot;！&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RMeCabC(&amp;quot;トランプ政権による経済活動再開の指針発表が評価される&amp;quot;,dic=dic_directory) %&amp;gt;% unlist()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           名詞           助詞           名詞           名詞           助詞 
## &amp;quot;トランプ政権&amp;quot;       &amp;quot;による&amp;quot;     &amp;quot;経済活動&amp;quot;         &amp;quot;再開&amp;quot;           &amp;quot;の&amp;quot; 
##           名詞           名詞           助詞           名詞           動詞 
##         &amp;quot;指針&amp;quot;         &amp;quot;発表&amp;quot;           &amp;quot;が&amp;quot;         &amp;quot;評価&amp;quot;           &amp;quot;さ&amp;quot; 
##           動詞 
##         &amp;quot;れる&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上値抵抗線は1語カウントしてほしいところではありますが、それ以外は上手く形態素解析できていそうです。
この&lt;code&gt;NEologd&lt;/code&gt;辞書ですが、&lt;em&gt;Windowsのインストールが想定されていません&lt;/em&gt;。つまり、Windowsユーザーは直接インストールすることができないのです。Windowsユーザーは少々ややこしい手順を踏まなければなりません。今回はそのややこしい手順を解説する記事です。Linuxでインストールを行い、それをWindows環境にコピー、その後辞書ファイルを作成します。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;インストール手順&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. インストール手順&lt;/h2&gt;
&lt;div id=&#34;a.-windows-subsystem-for-linuxwslのインストール&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A. &lt;code&gt;Windows Subsystem for Linux&lt;/code&gt;(WSL)のインストール&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Windows Power Shell&lt;/code&gt;を&lt;em&gt;管理者権限&lt;/em&gt;で開き、&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を実行する。WSLをインストールすることができます。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../my_blog/post/post21_files/powershell.PNG&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;powerShellでの実行の様子&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;b.-ubuntu-linuxのインストール&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;B. Ubuntu Linuxのインストール&lt;/h3&gt;
&lt;p&gt;Microsoft Storeよりubuntuをダウンロードする。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;ubuntu.PNG&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;ubuntuの画面&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;インストールが完了したらubuntuを起動し、初期設定を完了させる(ID、パスワード)。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;c.-ubuntu-linuxにmecabをインストール&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;C. Ubuntu LinuxにMeCabをインストール&lt;/h3&gt;
&lt;p&gt;ubuntuのコマンドプロンプトで、&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt-get update
sudo apt install mecab
sudo apt install libmecab-dev
sudo apt install mecab-ipadic-utf8&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を入力し、&lt;code&gt;MeCab&lt;/code&gt;をインストール。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;ubuntumecab.PNG&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;ubuntuでmecabをインストール&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;d.-ubuntu-for-linuxにneologd辞書をインストール&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;D. Ubuntu for Linuxに&lt;code&gt;NEologd&lt;/code&gt;辞書をインストール&lt;/h3&gt;
&lt;p&gt;ubuntsuのコマンドプロンプトで&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;sudo apt install make
git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git
cd mecab-ipadic-neologd
sudo bin/install-mecab-ipadic-neologd -n -a&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を実行。&lt;code&gt;NEologd&lt;/code&gt;辞書ファイルが(&lt;code&gt;/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/&lt;/code&gt;)にインストールできます。&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;e.-ubuntu-for-linuxからwindowsへ辞書ファイルをコピー&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;E. Ubuntu for LinuxからWindowsへ辞書ファイルをコピー&lt;/h3&gt;
&lt;p&gt;ubuntuのコマンドプロンプトで&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;explorer.exe .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を入力。エクスプローラーが立ち上がるので、&lt;code&gt;/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd/(ディレクトリ)&lt;/code&gt;をWindowsの任意のディレクトリにコピーする。完了したらUbuntuは閉じる。&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;explorer.PNG&#34; style=&#34;width:100.0%&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;エクスプローラーでubuntu内部ディレクトリを確認する図&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;f.-windows内で辞書ファイルのコンパイルshift-jisを行う&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;F. Windows内で辞書ファイルのコンパイル(&lt;code&gt;SHIFT-JIS&lt;/code&gt;)を行う。&lt;/h3&gt;
&lt;p&gt;コピーしたディレクトリ内の以下のcsvを辞書ファイル(.dic)にコンパイルします。(yyyymmddは辞書の最終更新日なので変更してください)&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;-mecab-ipadic-neologd-buildmecab-ipadic-2.7.0-20070801-neologd-yyyymmdd-mecab-user-dict-seed.yyyymmdd.csv&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;その際、元ファイルは&lt;strong&gt;エンコーディングが&lt;code&gt;UTF-8&lt;/code&gt;となっているので、&lt;code&gt;SHIFT-JIS&lt;/code&gt;へ変換することに注意&lt;/strong&gt;です。これをしないと、&lt;code&gt;RMeCab&lt;/code&gt;を実行したときに結果が文字化けします。コンパイルにはMeCabのmecab-dict-indexというバイナリファイルを使用します。自分は以下のディレクトリに存在しました。&lt;/p&gt;
&lt;p&gt;C:
-Program Files (x86)
-MeCab
-bin
-mecab-dict-index.exe&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mecab_dict_index.PNG&#34; style=&#34;width:100.0%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;コマンドプロンプトを立ち上げ、&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;C:\Program Files (x86)\MeCab\bin\mecab-dict-index.exe -d .../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmdd(ファイルの保存場所) -u NEologd.yyyymmdd.dic -f utf-8 -t shift-jis mecab-ipadic-neologd\buildmecab\ipadic-2.7.0-20070801-neologd-yyyymmdd\mecab-user-dict-seed.yyyymmdd.csv &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;を適宜変更の上、入力。&lt;code&gt;.../mecab-ipadic-neologd/buildmecab/ipadic-2.7.0-20070801-neologd-yyyymmdd&lt;/code&gt;に辞書がコンパイルされ、&lt;code&gt;NEologd.yyyymmdd.dic&lt;/code&gt;ができます。&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;まとめ&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. まとめ&lt;/h2&gt;
&lt;p&gt;以上で、&lt;code&gt;NEologd&lt;/code&gt;辞書が使用できるようになります。非常に強力なツールなので使用してみてください。なお、辞書がアップデートされた際は同じ手続きを行う必要があります。&lt;/p&gt;
&lt;p&gt;&lt;後日談&gt;
Ubuntu for Linuxを使用しなくてもダウンロードできそうな方法ありました。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://qiita.com/zincjp/items/c61c441426b9482b5a48&#34; class=&#34;uri&#34;&gt;https://qiita.com/zincjp/items/c61c441426b9482b5a48&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;ただ、自分は実行していないので実際にできるかはわかりません。辞書が更新されたら、やってみたいと思います。&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;MeCab内部の仕組みについては&lt;a href=&#34;https://techlife.cookpad.com/entry/2016/05/11/170000&#34;&gt;こちら&lt;/a&gt;の記事が参考になります。&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;2020/4/19時点の最近版は2020/3/15更新の辞書です。&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
